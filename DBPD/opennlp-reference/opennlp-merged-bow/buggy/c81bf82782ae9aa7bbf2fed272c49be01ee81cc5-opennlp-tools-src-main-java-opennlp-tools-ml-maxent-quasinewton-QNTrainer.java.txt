OPENNLP-671 Add L1-regularization into L-BFGS. Thanks to Vinh Khuc  for providing a patch.

git-svn-id: https://svn.apache.org/repos/asf/opennlp/trunk@1590233 13f79535-47bb-0310-9956-ffa450edef68

+import opennlp.tools.ml.maxent.quasinewton.QNMinimizer.Evaluator;
-
-  // function change rate tolerance
-  private static final double CONVERGE_TOLERANCE = 1e-4;
-  // relative gradient norm tolerance. Currently not being used.
-  private static final boolean USE_REL_GRAD_NORM = false;
-  private static final double REL_GRAD_NORM_TOL = 1e-8; 
-
-  // minimum step size
-  public static final double MIN_STEP_SIZE = 1e-10;
+  public static final String L1COST_PARAM = "L1Cost";
+  public static final double L1COST_DEFAULT = 0.5; 
-  public static final double L2COST_DEFAULT = 1.0; 
+  public static final double L2COST_DEFAULT = 0.5; 
-  // number of Hessian updates to store
-  private static final String M_PARAM = "numOfUpdates";
-  private static final int M_DEFAULT = 15;
+  // Number of Hessian updates to store
+  public static final String M_PARAM = "numOfUpdates";
+  public static final int M_DEFAULT = 15;
-  private static final String MAX_FCT_EVAL_PARAM = "maxFctEval";
-  private static final int MAX_FCT_EVAL_DEFAULT = 30000;
+  // Maximum number of function evaluations
+  public static final String MAX_FCT_EVAL_PARAM = "maxFctEval";
+  public static final int MAX_FCT_EVAL_DEFAULT = 30000;
+  // L1-regularization cost
+  private double l1Cost;
+  
-  // settings for objective function and optimizer.
-  private int dimension;
+  // Settings for QNMinimizer
-  private double initialGradNorm;
-  private QNInfo updateInfo;
-  // constructor -- to log. For testing purpose
+  // Constructor -- to log. For testing purpose
-  // constructor -- m : number of hessian updates to store. For testing purpose
+  // Constructor -- m : number of hessian updates to store. For testing purpose
-  // constructor -- to log, number of hessian updates to store. For testing purpose
+  // Constructor -- to log, number of hessian updates to store. For testing purpose
-  // for testing purpose
+  // For testing purpose
+    this.l1Cost     = L1COST_DEFAULT;
-  // >> members related to AbstractEventTrainer
+  // >> Members related to AbstractEventTrainer
-    // L2-regularization cost must be >= 0
+    // Regularization costs must be >= 0
+    double l1Cost = getDoubleParam(L1COST_PARAM, L1COST_DEFAULT);
+    if (l1Cost < 0) {
+      return false;
+    }
+    this.l1Cost = l1Cost;
+    
-  // << members related to AbstractEventTrainer
+  // << Members related to AbstractEventTrainer
-    NegLogLikelihoodFunction objectiveFunction = new NegLogLikelihoodFunction(indexer, l2Cost);
-    this.dimension  = objectiveFunction.getDomainDimension();
-    this.updateInfo = new QNInfo(this.m, this.dimension);
+    
+    // Train model's parameters
+    Function objectiveFunction = new NegLogLikelihood(indexer);
+    
+    QNMinimizer minimizer = new QNMinimizer(
+        l1Cost, l2Cost, iterations, m, maxFctEval, verbose);
+    minimizer.setEvaluator(new ModelEvaluator(indexer));
-    // current point is at the origin
-    double[] currPoint = new double[dimension];
-    
-    double currValue = objectiveFunction.valueAt(currPoint);
-    
-    // gradient at the current point
-    double[] currGrad = new double[dimension]; 
-    System.arraycopy(objectiveFunction.gradientAt(currPoint), 0, 
-        currGrad, 0, dimension);
-    
-    // initial L2-norm of the gradient
-    this.initialGradNorm = ArrayMath.norm(currGrad); 
-    
-    LineSearchResult lsr = LineSearchResult.getInitialObject(
-        currValue, currGrad, currPoint, 0);
+    double[] parameters = minimizer.minimize(objectiveFunction);
-    if (verbose) 
-      display("\nPerforming " + iterations + " iterations with " +
-      		"L2-cost = " + l2Cost + "\n");
-    
-    double[] direction = new double[this.dimension];
-    long startTime = System.currentTimeMillis();
-    
-    for (int iter = 1; iter <= iterations; iter++) {
-      computeDirection(lsr, direction);
-      LineSearch.doLineSearch(objectiveFunction, direction, lsr);
-      updateInfo.updateInfo(lsr);
-      
-      if (verbose) {
-        double accurarcy = evaluateModel(indexer, lsr.getNextPoint());
-        if (iter < 10)
-          display("  " + iter + ":  ");
-        else if (iter < 100)
-          display(" " + iter + ":  ");
-        else
-          display(iter + ":  ");
-        
-        display("\t " + lsr.getValueAtCurr());
-        display("\t" + lsr.getFuncChangeRate());
-        display("\t" + accurarcy);
-        display("\n");
-      }
-      if (isConverged(lsr))
-        break;
-    }
-    
-    long endTime = System.currentTimeMillis();
-    long duration = endTime - startTime;
-    display("Training time: " + (duration / 1000.) + "s\n");
-    
-    double[] parameters = lsr.getNextPoint();
-    
+    // Construct model with trained parameters
-        // Only save data corresponding to non-zero values
-        if (val != 0) {
-          outcomePattern.add(oi);
-          alpha.add(val);
-        }
+        outcomePattern.add(oi);
+        alpha.add(val);
-   * L-BFGS two-loop recursion (see Nocedal & Wright 2006, Numerical Optimization, p. 178) 
+   * For measuring model's training accuracy
-  private void computeDirection(LineSearchResult lsr, double[] direction) {
-    
-    // implemented two-loop Hessian update method.
-    System.arraycopy(lsr.getGradAtNext(), 0, direction, 0, direction.length);
+  private class ModelEvaluator implements Evaluator {
-    int k = updateInfo.kCounter;
-    double[] rho    = updateInfo.rho;
-    double[] alpha  = updateInfo.alpha; // just to avoid recreating alpha
-    double[][] S    = updateInfo.S;
-    double[][] Y    = updateInfo.Y;
-    
-    // first loop
-    for (int i = k - 1; i >= 0; i--) {
-      alpha[i] = rho[i] * ArrayMath.innerProduct(S[i], direction);
-      for (int j = 0; j < dimension; j++) {
-        direction[j] = direction[j] - alpha[i] * Y[i][j];
-      }
+    private DataIndexer indexer;
+
+    public ModelEvaluator(DataIndexer indexer) {
+      this.indexer = indexer;
-    // second loop
-    for (int i = 0; i < k; i++) {
-      double beta = rho[i] * ArrayMath.innerProduct(Y[i], direction);
-      for (int j = 0; j < dimension; j++) {
-        direction[j] = direction[j] + S[i][j] * (alpha[i] - beta);
-      }
-    }
-
-    for (int i = 0; i < dimension; i++) {
-      direction[i] = -direction[i];
-    }
-  }
-  
-  // TODO: Need an improvement in convergence condition
-  private boolean isConverged(LineSearchResult lsr) {
-    
-      if (lsr.getFuncChangeRate() < CONVERGE_TOLERANCE) {
-        if (verbose)
-          display("Function change rate is smaller than the threshold " 
-                    + CONVERGE_TOLERANCE + ".\nTraining will stop.\n\n");
-        return true;
-      }
+    /**
+     * Evaluate the current model on training data set 
+     * @return model's training accuracy
+     */
+    @Override
+    public double evaluate(double[] parameters) {
+      int[][] contexts  = indexer.getContexts();
+      float[][] values  = indexer.getValues();
+      int[] nEventsSeen = indexer.getNumTimesEventsSeen();
+      int[] outcomeList = indexer.getOutcomeList(); 
+      int nOutcomes     = indexer.getOutcomeLabels().length;
+      int nPredLabels   = indexer.getPredLabels().length;
-      if (USE_REL_GRAD_NORM) {
-        double gradNorm = ArrayMath.norm(lsr.getGradAtNext());
-        if (gradNorm / initialGradNorm < REL_GRAD_NORM_TOL) {
-          if (verbose)
-            display("Relative L2-norm of the gradient is smaller than the threshold " 
-                + REL_GRAD_NORM_TOL + ".\nTraining will stop.\n\n");
-          return true;
-        }
-      }
+      int nCorrect     = 0;
+      int nTotalEvents = 0;
-      if (lsr.getStepSize() < MIN_STEP_SIZE) {
-        if (verbose) 
-          display("Step size is smaller than the minimum step size " 
-              + MIN_STEP_SIZE + ".\nTraining will stop.\n\n");
-        return true;
-      }
+      for (int ei = 0; ei < contexts.length; ei++) {
+        int[] context  = contexts[ei];
+        float[] value  = values == null? null: values[ei];
-      if (lsr.getFctEvalCount() > this.maxFctEval) {
-        if (verbose)
-          display("Maximum number of function evaluations has exceeded the threshold " 
-              + this.maxFctEval + ".\nTraining will stop.\n\n");
-        return true;
-      }
-    
-    return false;  
-  }
-  
-  /**
-   * Evaluate the current model on training data set 
-   * @return model's training accuracy
-   */
-  private double evaluateModel(DataIndexer indexer, double[] parameters) {
-    int[][] contexts  = indexer.getContexts();
-    float[][] values  = indexer.getValues();
-    int[] nEventsSeen = indexer.getNumTimesEventsSeen();
-    int[] outcomeList = indexer.getOutcomeList(); 
-    int nOutcomes     = indexer.getOutcomeLabels().length;
-    int nPredLabels   = indexer.getPredLabels().length;
-    
-    int nCorrect     = 0;
-    int nTotalEvents = 0;
-    
-    for (int ei = 0; ei < contexts.length; ei++) {
-      int[] context  = contexts[ei];
-      float[] value  = values == null? null: values[ei];
-      
-      double[] probs = new double[nOutcomes];
-      QNModel.eval(context, value, probs, nOutcomes, nPredLabels, parameters);
-      int outcome = ArrayMath.maxIdx(probs);
-      if (outcome == outcomeList[ei]) {
-        nCorrect += nEventsSeen[ei];
-      }
-      nTotalEvents += nEventsSeen[ei];
-    }
-    
-    return (double) nCorrect / nTotalEvents;
-  }
-  
-  /**
-   * Shorthand for System.out.print
-   */
-  private void display(String s) {
-    System.out.print(s);
-  }
-  
-  /**
-   * Class to store vectors for Hessian approximation update.
-   */
-  private class QNInfo {
-    private double[][] S;
-    private double[][] Y;
-    private double[] rho;
-    private double[] alpha;
-    private int m;
-
-    private int kCounter;
-
-    // constructor
-    QNInfo(int numCorrection, int dimension) {
-      this.m = numCorrection;
-      this.kCounter = 0;
-      S     = new double[this.m][dimension];
-      Y     = new double[this.m][dimension];
-      rho   = new double[this.m];
-      alpha = new double[this.m];
-    }
-    
-    public void updateInfo(LineSearchResult lsr) {
-      double[] currPoint  = lsr.getCurrPoint();
-      double[] gradAtCurr = lsr.getGradAtCurr(); 
-      double[] nextPoint  = lsr.getNextPoint();
-      double[] gradAtNext = lsr.getGradAtNext(); 
-      
-      // inner product of S_k and Y_k
-      double SYk = 0.0; 
-      
-      // add new ones.
-      if (kCounter < m) {
-        for (int j = 0; j < dimension; j++) {
-          S[kCounter][j] = nextPoint[j] - currPoint[j];
-          Y[kCounter][j] = gradAtNext[j] - gradAtCurr[j];
-          SYk += S[kCounter][j] * Y[kCounter][j];
+        double[] probs = new double[nOutcomes];
+        QNModel.eval(context, value, probs, nOutcomes, nPredLabels, parameters);
+        int outcome = ArrayMath.maxIdx(probs);
+        if (outcome == outcomeList[ei]) {
+          nCorrect += nEventsSeen[ei];
-        rho[kCounter] = 1.0 / SYk;
-      } 
-      else if (m > 0) {
-        // discard oldest vectors and add new ones.
-        for (int i = 0; i < m - 1; i++) {
-          S[i] = S[i + 1];
-          Y[i] = Y[i + 1];
-          rho[i] = rho[i + 1];
-        }
-        for (int j = 0; j < dimension; j++) {
-          S[m - 1][j] = nextPoint[j] - currPoint[j];
-          Y[m - 1][j] = gradAtNext[j] - gradAtCurr[j];
-          SYk += S[m - 1][j] * Y[m - 1][j];  
-        }
-        rho[m - 1] = 1.0 / SYk;
+        nTotalEvents += nEventsSeen[ei];
-      if (kCounter < m) 
-        kCounter++;
+      return (double) nCorrect / nTotalEvents;

INS26 MOV23 INS40 INS55 UPD83 MOV43 UPD83 UPD83 UPD83 UPD83 INS29 INS83 INS42 INS43 INS23 MOV31 MOV31 UPD42 INS45 UPD42 UPD34 UPD34 UPD42 INS21 INS60 INS25 INS21 INS60 MOV21 MOV65 UPD42 MOV42 INS83 INS43 MOV59 INS42 MOV44 INS8 INS78 UPD83 UPD42 INS7 INS39 INS59 INS27 INS8 INS7 UPD43 INS43 INS59 MOV5 UPD66 UPD42 MOV42 UPD42 MOV21 INS42 INS22 INS42 INS42 INS32 INS42 INS34 MOV41 INS22 INS42 UPD42 UPD42 MOV42 INS42 INS14 UPD42 UPD42 MOV42 INS14 INS52 INS42 INS42 INS42 INS42 INS52 INS42 UPD43 INS43 INS42 INS42 INS42 INS42 INS42 INS42 INS43 INS42 UPD42 UPD42 INS42 MOV8 INS22 INS42 UPD42 UPD42 MOV42 INS42 MOV60 INS52 INS42 DEL83 DEL83 DEL83 DEL39 DEL42 DEL34 DEL59 DEL23 DEL39 DEL9 DEL83 DEL83 DEL83 DEL39 DEL42 DEL34 DEL59 DEL23 DEL83 DEL39 DEL42 DEL59 DEL23 DEL83 DEL42 DEL43 DEL42 DEL59 DEL23 DEL42 DEL45 DEL42 DEL32 DEL27 DEL52 DEL42 DEL22 DEL42 DEL42 DEL32 DEL7 DEL21 DEL52 DEL42 DEL22 DEL43 DEL52 DEL42 DEL22 DEL52 DEL42 DEL22 DEL14 DEL7 DEL21 DEL42 DEL39 DEL85 DEL5 DEL42 DEL3 DEL59 DEL60 DEL39 DEL42 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL39 DEL85 DEL5 DEL42 DEL39 DEL85 DEL5 DEL42 DEL3 DEL59 DEL60 DEL42 DEL42 DEL42 DEL42 DEL42 DEL32 DEL34 DEL42 DEL34 DEL42 DEL32 DEL21 DEL52 DEL42 DEL22 DEL42 DEL42 DEL42 DEL32 DEL7 DEL21 DEL43 DEL42 DEL42 DEL42 DEL42 DEL42 DEL42 DEL34 DEL32 DEL59 DEL60 DEL42 DEL42 DEL45 DEL42 DEL45 DEL45 DEL42 DEL45 DEL27 DEL32 DEL21 DEL25 DEL39 DEL85 DEL5 DEL42 DEL39 DEL85 DEL5 DEL52 DEL42 DEL22 DEL3 DEL59 DEL60 DEL39 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL39 DEL42 DEL34 DEL59 DEL58 DEL42 DEL42 DEL27 DEL42 DEL37 DEL42 DEL42 DEL42 DEL32 DEL21 DEL42 DEL42 DEL42 DEL42 DEL42 DEL32 DEL21 DEL42 DEL42 DEL42 DEL32 DEL21 DEL42 DEL39 DEL42 DEL42 DEL42 DEL42 DEL42 DEL32 DEL32 DEL59 DEL60 DEL42 DEL34 DEL27 DEL42 DEL45 DEL42 DEL45 DEL27 DEL32 DEL21 DEL42 DEL34 DEL27 DEL42 DEL45 DEL42 DEL45 DEL27 DEL32 DEL21 DEL42 DEL42 DEL45 DEL27 DEL32 DEL21 DEL25 DEL25 DEL42 DEL45 DEL42 DEL42 DEL32 DEL27 DEL32 DEL21 DEL42 DEL45 DEL42 DEL27 DEL32 DEL21 DEL42 DEL45 DEL32 DEL21 DEL8 DEL25 DEL42 DEL42 DEL32 DEL10 DEL25 DEL8 DEL24 DEL39 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL39 DEL42 DEL42 DEL42 DEL27 DEL59 DEL60 DEL42 DEL45 DEL42 DEL34 DEL27 DEL36 DEL45 DEL27 DEL32 DEL21 DEL39 DEL85 DEL5 DEL42 DEL34 DEL27 DEL25 DEL8 DEL42 DEL42 DEL2 DEL42 DEL42 DEL34 DEL27 DEL2 DEL39 DEL42 DEL42 DEL43 DEL42 DEL44 DEL39 DEL85 DEL5 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL39 DEL85 DEL5 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL39 DEL85 DEL5 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL39 DEL85 DEL5 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL39 DEL42 DEL34 DEL59 DEL60 DEL42 DEL42 DEL27 DEL39 DEL42 DEL34 DEL59 DEL58 DEL42 DEL42 DEL27 DEL42 DEL37 DEL42 DEL42 DEL2 DEL42 DEL2 DEL42 DEL42 DEL2 DEL42 DEL42 DEL2 DEL27 DEL7 DEL21 DEL42 DEL42 DEL2 DEL42 DEL2 DEL42 DEL42 DEL2 DEL42 DEL42 DEL2 DEL27 DEL7 DEL21 DEL42 DEL42 DEL42 DEL2 DEL42 DEL2 DEL42 DEL42 DEL2 DEL42 DEL2 DEL27 DEL7 DEL21 DEL8 DEL24 DEL42 DEL42 DEL2 DEL34 DEL42 DEL27 DEL7 DEL21 DEL8 DEL42 DEL34 DEL27 DEL39 DEL42 DEL34 DEL59 DEL58 DEL42 DEL42 DEL34 DEL27 DEL27 DEL42 DEL37 DEL42 DEL42 DEL2 DEL42 DEL42 DEL34 DEL27 DEL2 DEL7 DEL21 DEL42 DEL42 DEL2 DEL42 DEL42 DEL34 DEL27 DEL2 DEL7 DEL21 DEL8 DEL24 DEL39 DEL42 DEL34 DEL59 DEL58 DEL42 DEL42 DEL27 DEL42 DEL37 DEL42 DEL42 DEL34 DEL27 DEL2 DEL42 DEL2 DEL42 DEL42 DEL2 DEL42 DEL42 DEL2 DEL27 DEL7 DEL21 DEL42 DEL42 DEL34 DEL27 DEL2 DEL42 DEL2 DEL42 DEL42 DEL2 DEL42 DEL42 DEL2 DEL27 DEL7 DEL21 DEL42 DEL42 DEL42 DEL34 DEL27 DEL2 DEL42 DEL2 DEL42 DEL42 DEL34 DEL27 DEL2 DEL42 DEL2 DEL27 DEL7 DEL21 DEL8 DEL24 DEL42 DEL42 DEL34 DEL27 DEL2 DEL34 DEL42 DEL27 DEL7 DEL21 DEL8 DEL25 DEL25 DEL42 DEL42 DEL27 DEL42 DEL37 DEL21 DEL25 DEL8 DEL29 DEL83 DEL39 DEL42 DEL43 DEL42 DEL44 DEL39 DEL85 DEL5 DEL42 DEL44 DEL42 DEL42 DEL42 DEL42 DEL32 DEL34 DEL42 DEL34 DEL40 DEL32 DEL21 DEL39 DEL42 DEL40 DEL59 DEL60 DEL39 DEL85 DEL5 DEL42 DEL40 DEL59 DEL60 DEL39 DEL85 DEL5 DEL42 DEL40 DEL59 DEL60 DEL39 DEL85 DEL85 DEL5 DEL42 DEL40 DEL59 DEL60 DEL39 DEL85 DEL85 DEL5 DEL42 DEL40 DEL59 DEL60 DEL39 DEL42 DEL42 DEL34 DEL27 DEL59 DEL58 DEL42 DEL34 DEL27 DEL42 DEL37 DEL42 DEL42 DEL2 DEL42 DEL42 DEL2 DEL42 DEL42 DEL42 DEL42 DEL2 DEL42 DEL32 DEL27 DEL7 DEL21 DEL39 DEL42 DEL34 DEL59 DEL58 DEL42 DEL42 DEL27 DEL42 DEL37 DEL42 DEL42 DEL2 DEL42 DEL42 DEL2 DEL42 DEL42 DEL2 DEL42 DEL42 DEL2 DEL42 DEL2 DEL27 DEL27 DEL7 DEL21 DEL8 DEL24 DEL8 DEL24 DEL39 DEL42 DEL34 DEL59 DEL58 DEL42 DEL42 DEL27 DEL42 DEL37 DEL39 DEL42 DEL42 DEL42 DEL2 DEL42 DEL42 DEL42 DEL42 DEL2 DEL42 DEL32 DEL27 DEL59 DEL60 DEL39 DEL42 DEL34 DEL59 DEL58 DEL42 DEL42 DEL27 DEL42 DEL37 DEL42 DEL42 DEL2 DEL42 DEL42 DEL2 DEL42 DEL42 DEL2 DEL42 DEL2 DEL42 DEL42 DEL2 DEL42 DEL27 DEL36 DEL27 DEL27 DEL7 DEL21 DEL8 DEL24 DEL8 DEL24 DEL39 DEL42 DEL34 DEL59 DEL58 DEL42 DEL42 DEL27 DEL42 DEL37 DEL42 DEL42 DEL2 DEL42 DEL42 DEL2 DEL38 DEL7 DEL21 DEL8 DEL24 DEL8 DEL31 DEL83 DEL39 DEL42 DEL43 DEL42 DEL44 DEL42 DEL42 DEL32 DEL42 DEL27 DEL42 DEL42 DEL45 DEL42 DEL45 DEL27 DEL32 DEL21 DEL25 DEL9 DEL41 DEL8 DEL25 DEL42 DEL39 DEL42 DEL42 DEL42 DEL42 DEL42 DEL32 DEL32 DEL59 DEL60 DEL42 DEL42 DEL27 DEL42 DEL27 DEL42 DEL42 DEL45 DEL42 DEL45 DEL27 DEL32 DEL21 DEL25 DEL9 DEL41 DEL8 DEL25 DEL8 DEL25 DEL42 DEL42 DEL32 DEL42 DEL27 DEL42 DEL42 DEL45 DEL42 DEL45 DEL27 DEL32 DEL21 DEL25 DEL9 DEL41 DEL8 DEL25 DEL42 DEL42 DEL32 DEL52 DEL42 DEL22 DEL27 DEL42 DEL42 DEL45 DEL52 DEL42 DEL22 DEL45 DEL27 DEL32 DEL21 DEL25 DEL9 DEL41 DEL8 DEL25 DEL8 DEL31 DEL66 DEL65 DEL29 DEL83 DEL39 DEL42 DEL42 DEL44 DEL40 DEL42 DEL42 DEL32 DEL21 DEL8 DEL31 DEL66 DEL65 DEL29 DEL83 DEL42 DEL83 DEL39 DEL85 DEL85 DEL5 DEL42 DEL59 DEL23 DEL83 DEL39 DEL85 DEL85 DEL5 DEL42 DEL59 DEL23 DEL83 DEL39 DEL85 DEL5 DEL42 DEL59 DEL23 DEL83 DEL39 DEL85 DEL5 DEL42 DEL59 DEL23 DEL83 DEL39 DEL42 DEL59 DEL23 DEL83 DEL39 DEL23 DEL42 DEL39 DEL42 DEL44 DEL39 DEL42 DEL44 DEL52 DEL42 DEL22 DEL42 DEL7 DEL21 DEL52 DEL42 DEL22 DEL34 DEL7 DEL21 DEL42 DEL39 DEL85 DEL85 DEL5 DEL52 DEL42 DEL22 DEL42 DEL3 DEL7 DEL21 DEL42 DEL39 DEL85 DEL85 DEL5 DEL52 DEL42 DEL22 DEL42 DEL3 DEL7 DEL21 DEL42 DEL39 DEL85 DEL5 DEL52 DEL42 DEL22 DEL3 DEL7 DEL21 DEL42 DEL39 DEL85 DEL5 DEL52 DEL42 DEL22 DEL3 DEL7 DEL21 DEL8 DEL31 DEL55