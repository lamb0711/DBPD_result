Correct indentation and white spaces

This closes #30

See issue OPENNLP-914

-        for (int vi=1;vi<values[ci].length;vi++) {
-          cl+=values[ci][vi];
+        for (int vi = 1; vi < values[ci].length; vi++) {
+          cl += values[ci][vi];
-          predCount[contexts[ti][j]][outcomeList[ti]] += numTimesEventsSeen[ti]*values[ti][j];
+          predCount[contexts[ti][j]][outcomeList[ti]] += numTimesEventsSeen[ti] * values[ti][j];
-    for (int i = 0; i< modelExpects.length; i++)
+    for (int i = 0; i < modelExpects.length; i++)
-    int[] allOutcomesPattern= new int[numOutcomes];
+    int[] allOutcomesPattern = new int[numOutcomes];
-      for (int i = 0; i< modelExpects.length; i++)
+      for (int i = 0; i < modelExpects.length; i++)
-      for (int aoi=0;aoi<numActiveOutcomes;aoi++) {
+      for (int aoi = 0;aoi < numActiveOutcomes; aoi++) {
-            observedExpects[pi].setParameter(aoi, predCount[pi][oi]);
+          observedExpects[pi].setParameter(aoi, predCount[pi][oi]);
-      display("Computing model parameters in " + threads +" threads...\n");
+      display("Computing model parameters in " + threads + " threads...\n");
-	int threads=modelExpects.length;
-	ExecutorService executor = Executors.newFixedThreadPool(threads);
-	CompletionService<ModelExpactationComputeTask> completionService = new ExecutorCompletionService<>(executor);
+    int threads = modelExpects.length;
+    ExecutorService executor = Executors.newFixedThreadPool(threads);
+    CompletionService<ModelExpactationComputeTask> completionService = new ExecutorCompletionService<>(executor);
-            for (int aoi=0;aoi<activeOutcomes.length;aoi++) {
+            for (int aoi = 0; aoi < activeOutcomes.length; aoi++) {
-    // is not divisible by the number of threads, the first "leftOver" 
+    // is not divisible by the number of threads, the first "leftOver"
-        completionService.submit(new ModelExpactationComputeTask(i, i*taskSize+i, taskSize+1));
+        completionService.submit(new ModelExpactationComputeTask(i, i * taskSize + i, taskSize + 1));
-        completionService.submit(new ModelExpactationComputeTask(i, i*taskSize+leftOver, taskSize));
+        completionService.submit(new ModelExpactationComputeTask(i, i * taskSize + leftOver, taskSize));
-    for (int i=0; i<numberOfThreads; i++) {
+    for (int i = 0; i < numberOfThreads; i++) {
-      for (int aoi=0;aoi<activeOutcomes.length;aoi++) {
+      for (int aoi = 0; aoi < activeOutcomes.length; aoi++) {
-      for (int aoi=0;aoi<activeOutcomes.length;aoi++) {
+      for (int aoi = 0; aoi < activeOutcomes.length; aoi++) {
-            System.err.println("Model expects == 0 for "+predLabels[pi]+" "+outcomeLabels[aoi]);
+            System.err.println("Model expects == 0 for " + predLabels[pi] + " " + outcomeLabels[aoi]);
-          params[pi].updateParameter(aoi,((Math.log(observed[aoi]) - Math.log(model[aoi]))/correctionConstant));
+          params[pi].updateParameter(aoi,((Math.log(observed[aoi]) - Math.log(model[aoi])) / correctionConstant));

