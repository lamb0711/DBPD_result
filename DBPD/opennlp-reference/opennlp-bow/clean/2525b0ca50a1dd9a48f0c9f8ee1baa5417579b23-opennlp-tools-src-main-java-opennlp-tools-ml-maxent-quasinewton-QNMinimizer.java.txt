OPENNLP-767 Removed trailing white spaces on all lines

git-svn-id: https://svn.apache.org/repos/asf/opennlp/trunk@1674262 13f79535-47bb-0310-9956-ffa450edef68

- * 
+ *
- * 
+ *
- * Implementation of L-BFGS which supports L1-, L2-regularization 
+ * Implementation of L-BFGS which supports L1-, L2-regularization
- *  
+ *
- *    public int getDimension() { 
- *      return 1; 
+ *    public int getDimension() {
+ *      return 1;
- *    
+ *
- *    public double valueAt(double[] x) { 
- *      return Math.pow(x[0]-1, 2) + 10; 
+ *    public double valueAt(double[] x) {
+ *      return Math.pow(x[0]-1, 2) + 10;
- *    
+ *
- *    
+ *
- *  
- *  QNMinimizer minimizer = new QNMinimizer(); 
+ *
+ *  QNMinimizer minimizer = new QNMinimizer();
-  
+
-  
+
-  public static final double REL_GRAD_NORM_TOL = 1e-4; 
+  public static final double REL_GRAD_NORM_TOL = 1e-4;
-  
+
-  
+
-  
+
-  
+
-  
+
-  
+
-  
+
-  
+
-  
+
-  
+
-  
+
-  
+
-  
+
-  
+
-  
+
-    this(l1Cost, l2Cost, iterations, M_DEFAULT, MAX_FCT_EVAL_DEFAULT); 
+    this(l1Cost, l2Cost, iterations, M_DEFAULT, MAX_FCT_EVAL_DEFAULT);
-  
-  public QNMinimizer(double l1Cost, double l2Cost, 
+
+  public QNMinimizer(double l1Cost, double l2Cost,
-  
+
-      int m, int maxFctEval, boolean verbose) 
+      int m, int maxFctEval, boolean verbose)
-    if (l1Cost < 0 || l2Cost < 0) 
+    if (l1Cost < 0 || l2Cost < 0)
-      
+
-    
+
-    
+
-    
+
-   * @param function objective function 
+   * @param function objective function
-    
+
-    
+
-    
+
-    
+
-    double[] currGrad = new double[dimension]; 
-    System.arraycopy(l2RegFunction.gradientAt(currPoint), 0, 
+    double[] currGrad = new double[dimension];
+    System.arraycopy(l2RegFunction.gradientAt(currPoint), 0,
-    
+
-    
+
-    
+
-    
+
-    
+
-      
+
-      
+
-      
+
-          display("\t " + lsr.getValueAtNext() + 
+          display("\t " + lsr.getValueAtNext() +
-      
+
-    
-    // Undo L2-shrinkage if Elastic Net is used (since 
+
+    // Undo L2-shrinkage if Elastic Net is used (since
-    
+
-    
+
-    
-    // Avoid returning the reference to LineSearchResult's member so that GC can 
+
+    // Avoid returning the reference to LineSearchResult's member so that GC can
-    
+
-  
+
-   * Pseudo-gradient for L1-regularization (see equation 4 in the paper 
+   * Pseudo-gradient for L1-regularization (see equation 4 in the paper
-   * 
+   *
-      if (x[i] < 0) { 
+      if (x[i] < 0) {
-      else if (x[i] > 0) { 
+      else if (x[i] > 0) {
-  
+
-   * L-BFGS two-loop recursion (see Nocedal & Wright 2006, Numerical Optimization, p. 178) 
+   * L-BFGS two-loop recursion (see Nocedal & Wright 2006, Numerical Optimization, p. 178)
-    
+
-    
+
-  
+
-    
+
-        display("Function change rate is smaller than the threshold " 
+        display("Function change rate is smaller than the threshold "
-    
+
-    double gradNorm = l1Cost > 0? 
+    double gradNorm = l1Cost > 0?
-        display("Relative L2-norm of the gradient is smaller than the threshold " 
+        display("Relative L2-norm of the gradient is smaller than the threshold "
-    
+
-      if (verbose) 
-        display("Step size is smaller than the minimum step size " 
+      if (verbose)
+        display("Step size is smaller than the minimum step size "
-    
+
-        display("Maximum number of function evaluations has exceeded the threshold " 
+        display("Maximum number of function evaluations has exceeded the threshold "
-    
-    return false;  
+
+    return false;
-  
+
-  
+
-    
+
-      double[] gradAtCurr = lsr.getGradAtCurr(); 
+      double[] gradAtCurr = lsr.getGradAtCurr();
-      double[] gradAtNext = lsr.getGradAtNext(); 
-      
+      double[] gradAtNext = lsr.getGradAtNext();
+
-      double SYk = 0.0; 
-      
+      double SYk = 0.0;
+
-      } 
+      }
-          SYk += S[m - 1][j] * Y[m - 1][j];  
+          SYk += S[m - 1][j] * Y[m - 1][j];
-      
-      if (kCounter < m) 
+
+      if (kCounter < m)
-    
+
-    
+
-  
+
-   * we train a Maximum Entropy classifier. 
+   * we train a Maximum Entropy classifier.
