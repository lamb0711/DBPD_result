OPENNLP-923: Wrap all lines longer than 110 chars

And also add checkstyle enforcement

- * <b>Note:</b> <br> The nodes within
- * the returned parses are shared with other parses and therefore their parent node references will not be consistent
- * with their child node reference.  {@link #setParents setParents} can be used to make the parents consistent
- * with a particular parse, but subsequent calls to <code>setParents</code> can invalidate the results of earlier
- * calls.<br>
+ * <b>Note:</b> <br> The nodes within the returned parses are shared with other parses
+ * and therefore their parent node references will not be consistent with their child
+ * node reference.  {@link #setParents setParents} can be used to make the parents consistent
+ * with a particular parse, but subsequent calls to <code>setParents</code> can invalidate
+ * the results of earlier calls.<br>
-  public AbstractBottomUpParser(POSTagger tagger, Chunker chunker, HeadRules headRules, int beamSize, double advancePercentage) {
+  public AbstractBottomUpParser(POSTagger tagger, Chunker chunker, HeadRules headRules,
+      int beamSize, double advancePercentage) {
-   * adjacent to the punctuation is specified, and returns a new array of parses with the punctuation
-   * removed.
+   * adjacent to the punctuation is specified, and returns a new array of parses with
+   * the punctuation removed.
+   *
-   * Advances the specified parse and returns the an array advanced parses whose probability accounts for
-   * more than the specified amount of probability mass.
+   * Advances the specified parse and returns the an array advanced parses whose
+   * probability accounts for more than the specified amount of probability mass.
+   *
-   * @param probMass The amount of probability mass that should be accounted for by the advanced parses.
+   * @param probMass The amount of probability mass that should be accounted for
+   *                 by the advanced parses.
-    while (odh.size() > 0 && (completeParses.size() < M || (odh.first()).getProb() < minComplete) && derivationStage < maxDerivationLength) {
+    while (odh.size() > 0 && (completeParses.size() < M || (odh.first()).getProb() < minComplete)
+        && derivationStage < maxDerivationLength) {
-      for (Iterator<Parse> pi = odh.iterator(); pi.hasNext() && derivationRank < K; derivationRank++) { // forearch derivation
+      for (Iterator<Parse> pi = odh.iterator(); pi.hasNext()
+          && derivationRank < K; derivationRank++) { // forearch derivation
-         if (tp.getProb() < bestComplete) { //this parse and the ones which follow will never win, stop advancing.
+         //this parse and the ones which follow will never win, stop advancing.
+         if (tp.getProb() < bestComplete) {
-          //  System.err.println("Couldn't advance parse "+derivationStage+" stage "+derivationRank+"!\n");
+          //  System.err.println("Couldn't advance parse " + derivationStage
+          //      + " stage " + derivationRank + "!\n");
-        //if (j != tags.length) {System.err.println(words[j]+" "+ptags[j]+" "+tags[j]+" "+probs.get(j));}
+        // if (j != tags.length) {System.err.println(words[j]+" "
+        // +ptags[j]+" "+tags[j]+" "+probs.get(j));}
-        if (j != tags.length && tags[j].startsWith(CONT)) { // if continue just update end chunking tag don't use contTypeMap
+        // if continue just update end chunking tag don't use contTypeMap
+        if (j != tags.length && tags[j].startsWith(CONT)) {
-            //System.err.println("Putting "+type+" at "+start+","+end+" for "+j+" "+newParses[si].getProb());
+            // System.err.println("Putting "+type+" at "+start+","+end+" for "
+            // +j+" "+newParses[si].getProb());
-            Parse chunk = new Parse(p1.getText(), new Span(p1.getSpan().getStart(), p2.getSpan().getEnd()), type, 1, headRules.getHead(cons, type));
+            Parse chunk = new Parse(p1.getText(), new Span(p1.getSpan().getStart(),
+                p2.getSpan().getEnd()), type, 1, headRules.getHead(cons, type));
-   * Creates a n-gram dictionary from the specified data stream using the specified head rule and specified cut-off.
+   * Creates a n-gram dictionary from the specified data stream using the specified
+   * head rule and specified cut-off.
-  public static Dictionary buildDictionary(ObjectStream<Parse> data, HeadRules rules, TrainingParameters params)
-      throws IOException {
+  public static Dictionary buildDictionary(ObjectStream<Parse> data, HeadRules rules,
+      TrainingParameters params) throws IOException {
-      Parse[] chunks = collapsePunctuation(ParserEventStream.getInitialChunks(p),rules.getPunctuationTags());
+      Parse[] chunks = collapsePunctuation(ParserEventStream.getInitialChunks(p),
+          rules.getPunctuationTags());
-        //System.err.println("chunks["+ci+"]="+chunks[ci].getHead().getCoveredText()+" chunks.length="+chunks.length + "  " + chunks[ci].getParent());
+        // System.err.println("chunks["+ci+"]="+chunks[ci].getHead().getCoveredText()
+        // +" chunks.length="+chunks.length + "  " + chunks[ci].getParent());
-   * Creates a n-gram dictionary from the specified data stream using the specified head rule and specified cut-off.
+   * Creates a n-gram dictionary from the specified data stream using the specified
+   * head rule and specified cut-off.
-   * @param cutoff The minimum number of entries required for the n-gram to be saved as part of the dictionary.
+   * @param cutoff The minimum number of entries required for the n-gram to be
+   *               saved as part of the dictionary.
