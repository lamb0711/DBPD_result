HADOOP-10930. Refactor: Wrap Datanode IO related operations. Contributed by Xiaoyu Yao.

+import org.apache.hadoop.hdfs.server.datanode.LocalReplica;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaInputStreams;
+
-import org.apache.hadoop.io.nativeio.NativeIO;
-      FileUtil.fullyDelete(tmpDir);
+      DataStorage.fullyDelete(tmpDir);
-            NativeIO.renameTo(metaFile, targetMetaFile);
+            LocalReplica.rename(metaFile, targetMetaFile);
-            NativeIO.renameTo(blockFile, targetBlockFile);
+            LocalReplica.rename(blockFile, targetBlockFile);
+    ReplicaInputStreams ris = null;
-      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);
+      ris = new ReplicaInputStreams(blockIn, checksumIn,
+          volume.obtainReference());
+      ris.skipChecksumFully((numChunks-1)*checksumSize);
-      IOUtils.skipFully(blockIn, lastChunkStartPos);
+      ris.skipDataFully(lastChunkStartPos);
-      checksumIn.readFully(buf, lastChunkSize, checksumSize);
-      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);
-
+      ris.readChecksumFully(buf, lastChunkSize, checksumSize);
+      ris.readDataFully(buf, 0, lastChunkSize);
-      } else { // last chunck is corrupt
+      } else { // last chunk is corrupt
-      IOUtils.closeStream(checksumIn);
-      IOUtils.closeStream(blockIn);
+      if (ris != null) {
+        ris.close();
+      } else {
+        IOUtils.closeStream(checksumIn);
+        IOUtils.closeStream(blockIn);
+      }
