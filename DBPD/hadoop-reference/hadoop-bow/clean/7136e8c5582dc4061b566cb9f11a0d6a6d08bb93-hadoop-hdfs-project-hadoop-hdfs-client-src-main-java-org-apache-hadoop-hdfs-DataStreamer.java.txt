HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.

-import org.apache.hadoop.fs.FileSystem;
-import org.apache.htrace.core.Sampler;
+import javax.annotation.Nonnull;
+
-    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());
+    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(),
+        conf.getSocketTimeout());
-  
+
-   * @throws IOException if error occurs
-               ByteArrayManager byteArrayManage) throws IOException {
+               ByteArrayManager byteArrayManage) {
-        final int halfSocketTimeout = dfsClient.getConf().getSocketTimeout()/2; 
+        final int halfSocketTimeout = dfsClient.getConf().getSocketTimeout()/2;
-        TraceScope writeScope = dfsClient.getTracer().
-            newScope("DataStreamer#writeTo", spanId);
-        try {
+        try (TraceScope ignored = dfsClient.getTracer().
+            newScope("DataStreamer#writeTo", spanId)) {
-        } finally {
-          writeScope.close();
-    TraceScope scope = dfsClient.getTracer().
-        newScope("waitForAckedSeqno");
-    try {
+    try (TraceScope ignored = dfsClient.getTracer().
+        newScope("waitForAckedSeqno")) {
-      } catch (ClosedChannelException e) {
+      } catch (ClosedChannelException cce) {
-    } finally {
-      scope.close();
-      } catch (ClosedChannelException e) {
+      } catch (ClosedChannelException ignored) {
-    if (addr != null && NetUtils.isLocalAddress(addr)) {
-      return true;
-    }
-    return false;
+    return addr != null && NetUtils.isLocalAddress(addr);
-          new StringBuilder()
-              .append("Failed to replace a bad datanode on the existing pipeline ")
-              .append("due to no more good datanodes being available to try. ")
-              .append("(Nodes: current=").append(Arrays.asList(nodes))
-              .append(", original=").append(Arrays.asList(original)).append("). ")
-              .append("The current failed datanode replacement policy is ")
-              .append(dfsClient.dtpReplaceDatanodeOnFailure).append(", and ")
-              .append("a client may configure this via '")
-              .append(BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY)
-              .append("' in its configuration.")
-              .toString());
+          "Failed to replace a bad datanode on the existing pipeline "
+              + "due to no more good datanodes being available to try. "
+              + "(Nodes: current=" + Arrays.asList(nodes)
+              + ", original=" + Arrays.asList(original) + "). "
+              + "The current failed datanode replacement policy is "
+              + dfsClient.dtpReplaceDatanodeOnFailure
+              + ", and a client may configure this via '"
+              + BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY
+              + "' in its configuration.");
-    ArrayList<DatanodeInfo> exclude = new ArrayList<DatanodeInfo>(failed);
+    ArrayList<DatanodeInfo> exclude = new ArrayList<>(failed);
-                        final Token<BlockTokenIdentifier> blockToken) throws IOException {
+                        final Token<BlockTokenIdentifier> blockToken)
+      throws IOException {
-      int multi = 2 + (int)(bytesSent/dfsClient.getConf().getWritePacketSize())/200;
+      int multi = 2 + (int)(bytesSent /dfsClient.getConf().getWritePacketSize())
+          / 200;
-      } catch (InterruptedException ie) {}
+      } catch (InterruptedException ignored) {
+      }
-    return dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);
+    return dfsClient.namenode.updateBlockForPipeline(block,
+        dfsClient.clientName);
-    LocatedBlock lb = null;
-    DatanodeInfo[] nodes = null;
-    StorageType[] storageTypes = null;
-    int count = getNumBlockWriteRetry();
-    boolean success = false;
+    LocatedBlock lb;
+    DatanodeInfo[] nodes;
+    StorageType[] storageTypes;
+    int count = dfsClient.getConf().getNumBlockWriteRetry();
+    boolean success;
-    Status pipelineStatus = SUCCESS;
-        BlockConstructionStage bcs = recoveryFlag? stage.getRecoveryStage(): stage;
+        BlockConstructionStage bcs = recoveryFlag ?
+            stage.getRecoveryStage() : stage;
-        boolean[] targetPinnings = getPinnings(nodes, true);
+        boolean[] targetPinnings = getPinnings(nodes);
-            (targetPinnings == null ? false : targetPinnings[0]), targetPinnings);
+            (targetPinnings != null && targetPinnings[0]), targetPinnings);
-        pipelineStatus = resp.getStatus();
+        Status pipelineStatus = resp.getStatus();
-		
+
-        if (ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {
+        if (ie instanceof InvalidEncryptionKeyException &&
+            refetchEncryptionKey > 0) {
-          assert checkRestart == false;
+          assert !checkRestart;
-          errorState.initRestartingNode(i, "Datanode " + i + " is restarting: " + nodes[i]);
+          errorState.initRestartingNode(i, "Datanode " + i + " is restarting: "
+              + nodes[i]);
-          out = null;
-  private boolean[] getPinnings(DatanodeInfo[] nodes, boolean shouldLog) {
+  private boolean[] getPinnings(DatanodeInfo[] nodes) {
-      HashSet<String> favoredSet =
-          new HashSet<String>(Arrays.asList(favoredNodes));
+      HashSet<String> favoredSet = new HashSet<>(Arrays.asList(favoredNodes));
-      if (shouldLog && !favoredSet.isEmpty()) {
+      if (!favoredSet.isEmpty()) {
-  private DFSPacket createHeartbeatPacket() throws InterruptedIOException {
+  private DFSPacket createHeartbeatPacket() {
-              RemovalNotification<DatanodeInfo, DatanodeInfo> notification) {
+              @Nonnull RemovalNotification<DatanodeInfo, DatanodeInfo>
+                  notification) {
