HDFS-3318. Use BoundedInputStream in ByteRangeInputStream, otherwise, it hangs on transfers >2 GB.  Contributed by Daryn Sharp 


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1330500 13f79535-47bb-0310-9956-ffa450edef68

+import org.apache.commons.io.input.BoundedInputStream;
-      filelength = (cl == null) ? -1 : Long.parseLong(cl);
-      in = connection.getInputStream();
+      if (cl == null) {
+        throw new IOException(StreamFile.CONTENT_LENGTH+" header is missing");
+      }
+      final long streamlength = Long.parseLong(cl);
+      filelength = startPos + streamlength;
+      // Java has a bug with >2GB request streams.  It won't bounds check
+      // the reads so the transfer blocks until the server times out
+      in = new BoundedInputStream(connection.getInputStream(), streamlength);
-  private void update(final boolean isEOF, final int n)
-      throws IOException {
-    if (!isEOF) {
+  private int update(final int n) throws IOException {
+    if (n != -1) {
+    return n;
+  @Override
-    update(b == -1, 1);
+    update((b == -1) ? -1 : 1);
+
+  @Override
+  public int read(byte b[], int off, int len) throws IOException {
+    return update(getInputStream().read(b, off, len));
+  }
