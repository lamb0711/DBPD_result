HDFS-3934. duplicative dfs_hosts entries handled wrong. (cmccabe)



git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489065 13f79535-47bb-0310-9956-ffa450edef68

-import java.util.HashMap;
-import java.util.Set;
+import org.apache.hadoop.hdfs.server.namenode.HostFileManager;
+import org.apache.hadoop.hdfs.server.namenode.HostFileManager.Entry;
+import org.apache.hadoop.hdfs.server.namenode.HostFileManager.EntrySet;
+import org.apache.hadoop.hdfs.server.namenode.HostFileManager.MutableEntrySet;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.net.NetUtils;
-import org.apache.hadoop.util.HostsFileReader;
+  private final int defaultXferPort;
+  
+  private final int defaultInfoPort;
+
+  private final int defaultIpcPort;
+
-  private final HostsFileReader hostsReader;
+  private final HostFileManager hostFileManager;
+    int dnsResolutionSeconds = conf.getInt(
+        DFSConfigKeys.DFS_NAMENODE_HOSTS_DNS_RESOLUTION_INTERVAL_SECONDS,
+        DFSConfigKeys.DFS_NAMENODE_HOSTS_DNS_RESOLUTION_INTERVAL_SECONDS_DEFAULT);
+    this.hostFileManager = new HostFileManager(dnsResolutionSeconds);
+     
-    this.hostsReader = new HostsFileReader(
-        conf.get(DFSConfigKeys.DFS_HOSTS, ""),
+    this.defaultXferPort = NetUtils.createSocketAddr(
+          conf.get(DFSConfigKeys.DFS_DATANODE_ADDRESS_KEY,
+              DFSConfigKeys.DFS_DATANODE_ADDRESS_DEFAULT)).getPort();
+    this.defaultInfoPort = NetUtils.createSocketAddr(
+          conf.get(DFSConfigKeys.DFS_DATANODE_HTTP_ADDRESS_KEY,
+              DFSConfigKeys.DFS_DATANODE_HTTPS_ADDRESS_DEFAULT)).getPort();
+    this.defaultIpcPort = NetUtils.createSocketAddr(
+          conf.get(DFSConfigKeys.DFS_DATANODE_IPC_ADDRESS_KEY,
+              DFSConfigKeys.DFS_DATANODE_IPC_ADDRESS_DEFAULT)).getPort();
+    try {
+      this.hostFileManager.refresh(conf.get(DFSConfigKeys.DFS_HOSTS, ""),
+    } catch (IOException e) {
+      LOG.error("error reading hosts files: ", e);
+    }
-      dnsToSwitchMapping.resolve(new ArrayList<String>(hostsReader.getHosts()));
-    }
-    
+      final ArrayList<String> locations = new ArrayList<String>();
+      for (Entry entry : hostFileManager.getIncludes()) {
+        if (!entry.getIpAddress().isEmpty()) {
+          locations.add(entry.getIpAddress());
+        }
+      }
+      dnsToSwitchMapping.resolve(locations);
+    };
+
+    IOUtils.cleanup(LOG, hostFileManager);
-  private boolean inHostsList(DatanodeID node) {
-     return checkInList(node, hostsReader.getHosts(), false);
-  }
-  
-  private boolean inExcludedHostsList(DatanodeID node) {
-    return checkInList(node, hostsReader.getExcludedHosts(), true);
-  }
-
-    if (hostsReader.getHosts().isEmpty()) {
+    if (!hostFileManager.hasIncludes()) {
-    
+
-      if ((!inHostsList(node)) && (!inExcludedHostsList(node))
+      if ((!hostFileManager.isIncluded(node)) && (!hostFileManager.isExcluded(node))
-   * Check if the given DatanodeID is in the given (include or exclude) list.
-   * 
-   * @param node the DatanodeID to check
-   * @param hostsList the list of hosts in the include/exclude file
-   * @param isExcludeList true if this is the exclude list
-   * @return true if the node is in the list, false otherwise
-   */
-  private static boolean checkInList(final DatanodeID node,
-      final Set<String> hostsList,
-      final boolean isExcludeList) {
-    // if include list is empty, host is in include list
-    if ( (!isExcludeList) && (hostsList.isEmpty()) ){
-      return true;
-    }
-    for (String name : getNodeNamesForHostFiltering(node)) {
-      if (hostsList.contains(name)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-    if (inExcludedHostsList(nodeReg)) {
+    if (hostFileManager.isExcluded(nodeReg)) {
-      if (!inHostsList(nodeReg)) {
+      if (!hostFileManager.isIncluded(nodeReg)) {
-    hostsReader.updateFileNames(conf.get(DFSConfigKeys.DFS_HOSTS, ""), 
-                                conf.get(DFSConfigKeys.DFS_HOSTS_EXCLUDE, ""));
-    hostsReader.refresh();
+    this.hostFileManager.refresh(conf.get(DFSConfigKeys.DFS_HOSTS, ""),
+      conf.get(DFSConfigKeys.DFS_HOSTS_EXCLUDE, ""));
-      if (!inHostsList(node)) {
+      if (!hostFileManager.isIncluded(node)) {
-        if (inExcludedHostsList(node)) {
+        if (hostFileManager.isExcluded(node)) {
-    HashMap<String, String> mustList = new HashMap<String, String>();
-
-    if (listDeadNodes) {
-      // Put all nodes referenced in the hosts files in the map
-      Iterator<String> it = hostsReader.getHosts().iterator();
-      while (it.hasNext()) {
-        mustList.put(it.next(), "");
-      }
-      it = hostsReader.getExcludedHosts().iterator(); 
-      while (it.hasNext()) {
-        mustList.put(it.next(), "");
-      }
-    }
-
-    
+    final MutableEntrySet foundNodes = new MutableEntrySet();
-      nodes = new ArrayList<DatanodeDescriptor>(datanodeMap.size() + 
-                                                mustList.size());
+      nodes = new ArrayList<DatanodeDescriptor>(datanodeMap.size());
-        for (String name : getNodeNamesForHostFiltering(dn)) {
-          mustList.remove(name);
+        foundNodes.add(dn);
+      }
+    }
+
+    if (listDeadNodes) {
+      final EntrySet includedNodes = hostFileManager.getIncludes();
+      final EntrySet excludedNodes = hostFileManager.getExcludes();
+      for (Entry entry : includedNodes) {
+        if ((foundNodes.find(entry) == null) &&
+            (excludedNodes.find(entry) == null)) {
+          // The remaining nodes are ones that are referenced by the hosts
+          // files but that we do not know about, ie that we have never
+          // head from. Eg. an entry that is no longer part of the cluster
+          // or a bogus entry was given in the hosts files
+          //
+          // If the host file entry specified the xferPort, we use that.
+          // Otherwise, we guess that it is the default xfer port.
+          // We can't ask the DataNode what it had configured, because it's
+          // dead.
+          DatanodeDescriptor dn =
+              new DatanodeDescriptor(new DatanodeID(entry.getIpAddress(),
+                  entry.getPrefix(), "",
+                  entry.getPort() == 0 ? defaultXferPort : entry.getPort(),
+                  defaultInfoPort, defaultIpcPort));
+          dn.setLastUpdate(0); // Consider this node dead for reporting
+          nodes.add(dn);
-    
-    if (listDeadNodes) {
-      Iterator<String> it = mustList.keySet().iterator();
-      while (it.hasNext()) {
-        // The remaining nodes are ones that are referenced by the hosts
-        // files but that we do not know about, ie that we have never
-        // head from. Eg. a host that is no longer part of the cluster
-        // or a bogus entry was given in the hosts files
-        DatanodeID dnId = parseDNFromHostsEntry(it.next());
-        DatanodeDescriptor dn = new DatanodeDescriptor(dnId); 
-        dn.setLastUpdate(0); // Consider this node dead for reporting
-        nodes.add(dn);
-      }
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("getDatanodeListForReport with " +
+          "includedNodes = " + hostFileManager.getIncludes() +
+          ", excludedNodes = " + hostFileManager.getExcludes() +
+          ", foundNodes = " + foundNodes +
+          ", nodes = " + nodes);
-  private static List<String> getNodeNamesForHostFiltering(DatanodeID node) {
-    String ip = node.getIpAddr();
-    String regHostName = node.getHostName();
-    int xferPort = node.getXferPort();
-    
-    List<String> names = new ArrayList<String>(); 
-    names.add(ip);
-    names.add(ip + ":" + xferPort);
-    names.add(regHostName);
-    names.add(regHostName + ":" + xferPort);
-
-    String peerHostName = node.getPeerHostName();
-    if (peerHostName != null) {
-      names.add(peerHostName);
-      names.add(peerHostName + ":" + xferPort);
-    }
-    return names;
-  }
-
