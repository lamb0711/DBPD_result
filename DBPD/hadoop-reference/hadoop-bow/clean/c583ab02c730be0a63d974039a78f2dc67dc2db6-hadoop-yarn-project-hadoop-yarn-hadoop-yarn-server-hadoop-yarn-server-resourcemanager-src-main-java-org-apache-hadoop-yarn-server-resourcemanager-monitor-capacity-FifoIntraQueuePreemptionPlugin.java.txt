YARN-2113. Add cross-user preemption within CapacityScheduler's leaf-queue. (Sunil G via wangda)

Change-Id: I9b19f69788068be05b3295247cdd7b972f8a573c

+import java.util.ArrayList;
+import java.util.List;
+import org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.IntraQueuePreemptionOrderPolicy;
+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage;
+  public Collection<FiCaSchedulerApp> getPreemptableApps(String queueName,
+      String partition) {
+    TempQueuePerPartition tq = context.getQueueByPartition(queueName,
+        partition);
+
+    List<FiCaSchedulerApp> apps = new ArrayList<FiCaSchedulerApp>();
+    for (TempAppPerPartition tmpApp : tq.getApps()) {
+      // If a lower priority app was not selected to get preempted, mark such
+      // apps out from preemption candidate selection.
+      if (Resources.equals(tmpApp.getActuallyToBePreempted(),
+          Resources.none())) {
+        continue;
+      }
+
+      apps.add(tmpApp.app);
+    }
+    return apps;
+  }
+
+  @Override
-      Resource partitionBasedResource, TempQueuePerPartition tq,
+      TempQueuePerPartition tq,
-    TAPriorityComparator taComparator = new TAPriorityComparator();
-    PriorityQueue<TempAppPerPartition> orderedByPriority =
-        createTempAppForResCalculation(tq.partition, apps, taComparator);
+    PriorityQueue<TempAppPerPartition> orderedByPriority = createTempAppForResCalculation(
+        tq, apps, clusterResource, perUserAMUsed);
-    TreeSet<TempAppPerPartition> orderedApps =
-        calculateIdealAssignedResourcePerApp(clusterResource,
-            partitionBasedResource, tq, selectedCandidates,
-            queueReassignableResource, orderedByPriority, perUserAMUsed);
+    TreeSet<TempAppPerPartition> orderedApps = calculateIdealAssignedResourcePerApp(
+        clusterResource, tq, selectedCandidates, queueReassignableResource,
+        orderedByPriority);
-        preemptionLimit);
+        Resources.clone(preemptionLimit));
-        (TreeSet<TempAppPerPartition>) tq.getApps());
+        (TreeSet<TempAppPerPartition>) orderedApps, tq.getUsersPerPartition(),
+        context.getIntraQueuePreemptionOrderPolicy());
-      Resources.subtractFrom(preemtableFromApp, tmpApp.selected);
-      Resources.subtractFrom(preemtableFromApp, tmpApp.getAMUsed());
+      Resources.subtractFromNonNegative(preemtableFromApp, tmpApp.selected);
+      Resources.subtractFromNonNegative(preemtableFromApp, tmpApp.getAMUsed());
-          preemptionLimit);
+          Resources.clone(preemptionLimit));
-      preemptionLimit = Resources.subtract(preemptionLimit,
+      preemptionLimit = Resources.subtractFromNonNegative(preemptionLimit,
-   * @param partitionBasedResource resource per partition
-   * @param perUserAMUsed AM used resource
-      Resource clusterResource, Resource partitionBasedResource,
-      TempQueuePerPartition tq,
+      Resource clusterResource, TempQueuePerPartition tq,
-      PriorityQueue<TempAppPerPartition> orderedByPriority,
-      Map<String, Resource> perUserAMUsed) {
+      PriorityQueue<TempAppPerPartition> orderedByPriority) {
-    Map<String, Resource> userIdealAssignedMapping = new HashMap<>();
-
-    Map<String, Resource> preCalculatedUserLimit =
-        new HashMap<String, Resource>();
+    Map<String, TempUserPerPartition> usersPerPartition = tq.getUsersPerPartition();
-          queueReassignableResource, Resources.none())) {
+          queueReassignableResource, Resources.none())
+          || Resources.isAnyMajorResourceZero(rc, queueReassignableResource)) {
-      Resource userLimitResource = preCalculatedUserLimit.get(userName);
-
-      // Verify whether we already calculated headroom for this user.
-      if (userLimitResource == null) {
-        userLimitResource = Resources.clone(
-            tq.leafQueue.getResourceLimitForAllUsers(userName, clusterResource,
-                partition, SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY));
-
-        Resource amUsed = perUserAMUsed.get(userName);
-        if (null == amUsed) {
-          amUsed = Resources.createResource(0, 0);
-        }
-
-        // Real AM used need not have to be considered for user-limit as well.
-        userLimitResource = Resources.subtract(userLimitResource, amUsed);
-        if (LOG.isDebugEnabled()) {
-          LOG.debug("Userlimit for user '" + userName + "' is :"
-              + userLimitResource + ", and amUsed is:" + amUsed);
-        }
-
-        preCalculatedUserLimit.put(userName, userLimitResource);
-      }
-
-      Resource idealAssignedForUser = userIdealAssignedMapping.get(userName);
-
-      if (idealAssignedForUser == null) {
-        idealAssignedForUser = Resources.createResource(0, 0);
-        userIdealAssignedMapping.put(userName, idealAssignedForUser);
-      }
+      TempUserPerPartition tmpUser = usersPerPartition.get(userName);
+      Resource userLimitResource = tmpUser.getUserLimit();
+      Resource idealAssignedForUser = tmpUser.idealAssigned;
-      getAlreadySelectedPreemptionCandidatesResource(selectedCandidates,
-          tmpApp, partition);
+      getAlreadySelectedPreemptionCandidatesResource(selectedCandidates, tmpApp,
+          tmpUser, partition);
-        appIdealAssigned = Resources.min(rc, clusterResource, appIdealAssigned,
+        Resource idealAssigned = Resources.min(rc, clusterResource,
+            appIdealAssigned,
-            clusterResource, queueReassignableResource, appIdealAssigned));
+            clusterResource, queueReassignableResource, idealAssigned));
-      Resources.subtractFrom(queueReassignableResource, tmpApp.idealAssigned);
+      Resources.subtractFromNonNegative(queueReassignableResource,
+          tmpApp.idealAssigned);
-      TempAppPerPartition tmpApp, String partition) {
+      TempAppPerPartition tmpApp, TempUserPerPartition tmpUser,
+      String partition) {
+        Resources.addTo(tmpUser.selected, cont.getAllocatedResource());
-      String partition, Collection<FiCaSchedulerApp> apps,
-      TAPriorityComparator taComparator) {
+      TempQueuePerPartition tq, Collection<FiCaSchedulerApp> apps,
+      Resource clusterResource,
+      Map<String, Resource> perUserAMUsed) {
+    TAPriorityComparator taComparator = new TAPriorityComparator();
+    String partition = tq.partition;
+    Map<String, TempUserPerPartition> usersPerPartition = tq
+        .getUsersPerPartition();
+
+
+      // Create a TempUserPerPartition structure to hold more information
+      // regarding each user's entities such as UserLimit etc. This could
+      // be kept in a user to TempUserPerPartition map for further reference.
+      String userName = app.getUser();
+      if (!usersPerPartition.containsKey(userName)) {
+        ResourceUsage userResourceUsage = tq.leafQueue.getUser(userName)
+            .getResourceUsage();
+
+        TempUserPerPartition tmpUser = new TempUserPerPartition(
+            tq.leafQueue.getUser(userName), tq.queueName,
+            Resources.clone(userResourceUsage.getUsed(partition)),
+            Resources.clone(perUserAMUsed.get(userName)),
+            Resources.clone(userResourceUsage.getReserved(partition)),
+            Resources.none());
+
+        Resource userLimitResource = Resources.clone(
+            tq.leafQueue.getResourceLimitForAllUsers(userName, clusterResource,
+                partition, SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY));
+
+        // Real AM used need not have to be considered for user-limit as well.
+        userLimitResource = Resources.subtract(userLimitResource,
+            tmpUser.amUsed);
+        tmpUser.setUserLimit(userLimitResource);
+
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("TempUser:" + tmpUser);
+        }
+
+        tmpUser.idealAssigned = Resources.createResource(0, 0);
+        tq.addUserPerPartition(userName, tmpUser);
+      }
-   * same priority level. Such cases will be validated out.
+   * same priority level. Such cases will be validated out. But if the demand is
+   * from an app of different user, force to preempt resources even if apps are
+   * at same priority.
-      TreeSet<TempAppPerPartition> appsOrderedfromLowerPriority) {
+      TreeSet<TempAppPerPartition> orderedApps,
+      Map<String, TempUserPerPartition> usersPerPartition,
+      IntraQueuePreemptionOrderPolicy intraQueuePreemptionOrder) {
-    TempAppPerPartition[] apps = appsOrderedfromLowerPriority
-        .toArray(new TempAppPerPartition[appsOrderedfromLowerPriority.size()]);
+    TempAppPerPartition[] apps = orderedApps
+        .toArray(new TempAppPerPartition[orderedApps.size()]);
-    int lPriority = 0;
-    int hPriority = apps.length - 1;
+    for (int hPriority = apps.length - 1; hPriority >= 0; hPriority--) {
-    while (lPriority < hPriority
-        && !apps[lPriority].equals(apps[hPriority])
-        && apps[lPriority].getPriority() < apps[hPriority].getPriority()) {
-      Resource toPreemptFromOther = apps[hPriority]
-          .getToBePreemptFromOther();
-      Resource actuallyToPreempt = apps[lPriority].getActuallyToBePreempted();
-      Resource delta = Resources.subtract(apps[lPriority].toBePreempted,
-          actuallyToPreempt);
+      // Check whether high priority app with demand needs resource from other
+      // user.
+      if (Resources.greaterThan(rc, cluster,
+          apps[hPriority].getToBePreemptFromOther(), Resources.none())) {
-      if (Resources.greaterThan(rc, cluster, delta, Resources.none())) {
-        Resource toPreempt = Resources.min(rc, cluster,
-            toPreemptFromOther, delta);
+        // Given we have a demand from a high priority app, we can do a reverse
+        // scan from lower priority apps to select resources.
+        // Since idealAssigned of each app has considered user-limit, this logic
+        // will provide eventual consistency w.r.t user-limit as well.
+        for (int lPriority = 0; lPriority < apps.length; lPriority++) {
-        apps[hPriority].setToBePreemptFromOther(
-            Resources.subtract(toPreemptFromOther, toPreempt));
-        apps[lPriority].setActuallyToBePreempted(
-            Resources.add(actuallyToPreempt, toPreempt));
-      }
+          // Check whether app with demand needs resource from other user.
+          if (Resources.greaterThan(rc, cluster, apps[lPriority].toBePreempted,
+              Resources.none())) {
-      if (Resources.lessThanOrEqual(rc, cluster,
-          apps[lPriority].toBePreempted,
-          apps[lPriority].getActuallyToBePreempted())) {
-        lPriority++;
-        continue;
-      }
+            // If apps are of same user, and priority is same, then skip.
+            if ((apps[hPriority].getUser().equals(apps[lPriority].getUser()))
+                && (apps[lPriority].getPriority() >= apps[hPriority]
+                    .getPriority())) {
+              continue;
+            }
-      if (Resources.equals(apps[hPriority].getToBePreemptFromOther(),
-          Resources.none())) {
-        hPriority--;
-        continue;
+            if (Resources.lessThanOrEqual(rc, cluster,
+                apps[lPriority].toBePreempted,
+                apps[lPriority].getActuallyToBePreempted())
+                || Resources.equals(apps[hPriority].getToBePreemptFromOther(),
+                    Resources.none())) {
+              continue;
+            }
+
+            // Ideally if any application has a higher priority, then it can
+            // force to preempt any lower priority app from any user. However
+            // if admin enforces user-limit over priority, preemption module
+            // will not choose lower priority apps from usre's who are not yet
+            // met its user-limit.
+            TempUserPerPartition tmpUser = usersPerPartition
+                .get(apps[lPriority].getUser());
+            if ((!apps[hPriority].getUser().equals(apps[lPriority].getUser()))
+                && (!tmpUser.isUserLimitReached(rc, cluster))
+                && (intraQueuePreemptionOrder
+                    .equals(IntraQueuePreemptionOrderPolicy.USERLIMIT_FIRST))) {
+              continue;
+            }
+
+            Resource toPreemptFromOther = apps[hPriority]
+                .getToBePreemptFromOther();
+            Resource actuallyToPreempt = apps[lPriority]
+                .getActuallyToBePreempted();
+
+            // A lower priority app could offer more resource to preempt, if
+            // multiple higher priority/under served users needs resources.
+            // After one iteration, we need to ensure that actuallyToPreempt is
+            // subtracted from the resource to preempt.
+            Resource preemptableFromLowerPriorityApp = Resources
+                .subtract(apps[lPriority].toBePreempted, actuallyToPreempt);
+
+            // In case of user-limit preemption, when app's are from different
+            // user and of same priority, we will do user-limit preemption if
+            // there is a demand from under UL quota app.
+            // However this under UL quota app's demand may be more.
+            // Still we should ensure that we are not doing over preemption such
+            // that only a maximum of (user's used - UL quota) could be
+            // preempted.
+            if ((!apps[hPriority].getUser().equals(apps[lPriority].getUser()))
+                && (apps[lPriority].getPriority() == apps[hPriority]
+                    .getPriority())
+                && tmpUser.isUserLimitReached(rc, cluster)) {
+
+              Resource deltaULQuota = Resources
+                  .subtract(tmpUser.getUsedDeductAM(), tmpUser.selected);
+              Resources.subtractFrom(deltaULQuota, tmpUser.getUserLimit());
+
+              if (tmpUser.isPreemptionQuotaForULDeltaDone()) {
+                deltaULQuota = Resources.createResource(0, 0);
+              }
+
+              if (Resources.lessThan(rc, cluster, deltaULQuota,
+                  preemptableFromLowerPriorityApp)) {
+                tmpUser.updatePreemptionQuotaForULDeltaAsDone(true);
+                preemptableFromLowerPriorityApp = deltaULQuota;
+              }
+            }
+
+            if (Resources.greaterThan(rc, cluster,
+                preemptableFromLowerPriorityApp, Resources.none())) {
+              Resource toPreempt = Resources.min(rc, cluster,
+                  toPreemptFromOther, preemptableFromLowerPriorityApp);
+
+              apps[hPriority].setToBePreemptFromOther(
+                  Resources.subtract(toPreemptFromOther, toPreempt));
+              apps[lPriority].setActuallyToBePreempted(
+                  Resources.add(actuallyToPreempt, toPreempt));
+            }
+          }
+        }
+
+
+  @Override
+  public boolean skipContainerBasedOnIntraQueuePolicy(FiCaSchedulerApp app,
+      Resource clusterResource, Resource usedResource, RMContainer c) {
+    // Ensure below checks
+    // 1. This check must be done only when preemption order is USERLIMIT_FIRST
+    // 2. By selecting container "c", check whether this user's resource usage
+    // is going below its user-limit.
+    // 3. Used resource of user must be always greater than user-limit to
+    // skip some containers as per this check. If used resource is under user
+    // limit, then these containers of this user has to be preempted as demand
+    // might be due to high priority apps running in same user.
+    String partition = context.getScheduler()
+        .getSchedulerNode(c.getAllocatedNode()).getPartition();
+    TempQueuePerPartition tq = context.getQueueByPartition(app.getQueueName(),
+        partition);
+    TempUserPerPartition tmpUser = tq.getUsersPerPartition().get(app.getUser());
+
+    // Given user is not present, skip the check.
+    if (tmpUser == null) {
+      return false;
+    }
+
+    // For ideal resource computations, user-limit got saved by subtracting am
+    // used resource in TempUser. Hence it has to be added back here for
+    // complete check.
+    Resource userLimit = Resources.add(tmpUser.getUserLimit(), tmpUser.amUsed);
+
+    return Resources.lessThanOrEqual(rc, clusterResource,
+        Resources.subtract(usedResource, c.getAllocatedResource()), userLimit)
+        && context.getIntraQueuePreemptionOrderPolicy()
+            .equals(IntraQueuePreemptionOrderPolicy.USERLIMIT_FIRST);
+  }
