HDFS-8499. Refactor BlockInfo class hierarchy with static helper class. Contributed by Zhe Zhang.

+import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;
-  public static final BlockInfoContiguous[] EMPTY_ARRAY = {};
-  protected BlockInfoContiguous(BlockInfoContiguous from) {
+  protected BlockInfoContiguous(BlockInfo from) {
-  /**
-   * Ensure that there is enough  space to include num more triplets.
-   * @return first free triplet index.
-   */
-  private int ensureCapacity(int num) {
-    assert this.triplets != null : "BlockInfo is not initialized";
-    int last = numNodes();
-    if (triplets.length >= (last+num)*3) {
-      return last;
-    }
-    /* Not enough space left. Create a new array. Should normally
-     * happen only when replication is manually increased by the user. */
-    Object[] old = triplets;
-    triplets = new Object[(last+num)*3];
-    System.arraycopy(old, 0, triplets, 0, last * 3);
-    return last;
-  }
-
-    // find the last null node
-    int lastNode = ensureCapacity(1);
-    setStorageInfo(lastNode, storage);
-    setNext(lastNode, null);
-    setPrevious(lastNode, null);
-    return true;
+    return ContiguousBlockStorageOp.addStorage(this, storage);
-    int dnIndex = findStorageInfo(storage);
-    if (dnIndex < 0) { // the node is not found
-      return false;
-    }
-    assert getPrevious(dnIndex) == null && getNext(dnIndex) == null :
-        "Block is still in the list and must be removed first.";
-    // find the last not null node
-    int lastNode = numNodes()-1;
-    // replace current node triplet by the lastNode one
-    setStorageInfo(dnIndex, getStorageInfo(lastNode));
-    setNext(dnIndex, getNext(lastNode));
-    setPrevious(dnIndex, getPrevious(lastNode));
-    // set the last triplet to null
-    setStorageInfo(lastNode, null);
-    setNext(lastNode, null);
-    setPrevious(lastNode, null);
-    return true;
+    return ContiguousBlockStorageOp.removeStorage(this, storage);
-    assert this.triplets != null : "BlockInfo is not initialized";
-    assert triplets.length % 3 == 0 : "Malformed BlockInfo";
-
-    for (int idx = getCapacity()-1; idx >= 0; idx--) {
-      if (getDatanode(idx) != null) {
-        return idx + 1;
-      }
-    }
-    return 0;
+    return ContiguousBlockStorageOp.numNodes(this);
-    assert newBlock instanceof BlockInfoContiguous;
-    for (int i = this.numNodes() - 1; i >= 0; i--) {
-      final DatanodeStorageInfo storage = this.getStorageInfo(i);
-      final boolean removed = storage.removeBlock(this);
-      assert removed : "currentBlock not found.";
+    ContiguousBlockStorageOp.replaceBlock(this, newBlock);
+  }
-      final DatanodeStorageInfo.AddBlockResult result = storage.addBlock(
-          newBlock);
-      assert result == DatanodeStorageInfo.AddBlockResult.ADDED :
-          "newBlock already exists.";
-    }
+  @Override
+  BlockInfoUnderConstruction convertCompleteBlockToUC(
+      HdfsServerConstants.BlockUCState s, DatanodeStorageInfo[] targets) {
+    BlockInfoUnderConstructionContiguous ucBlock =
+        new BlockInfoUnderConstructionContiguous(this,
+            getBlockCollection().getPreferredBlockReplication(), s, targets);
+    ucBlock.setBlockCollection(getBlockCollection());
+    return ucBlock;
