Minor update to HDFS-2110.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1141416 13f79535-47bb-0310-9956-ffa450edef68

-  /** getting a client for connecting to dfs */
+  /* Return a DFS client to use to make the given HTTP request */
+  @SuppressWarnings("unchecked")
-    Enumeration<?> reqRanges = request.getHeaders("Range");
-    if (reqRanges != null && !reqRanges.hasMoreElements())
+    Enumeration<String> reqRanges = request.getHeaders("Range");
+    if (reqRanges != null && !reqRanges.hasMoreElements()) {
+    }
-        List<?> ranges = InclusiveByteRange.satisfiableRanges(reqRanges,
-                                                           fileLen);
-        StreamFile.sendPartialData(in, os, response, fileLen, ranges);
+        List<InclusiveByteRange> ranges = 
+          InclusiveByteRange.satisfiableRanges(reqRanges, fileLen);
+        StreamFile.sendPartialData(in, os, response, fileLen, ranges, true);
-        StreamFile.copyFromOffset(in, os, 0L, fileLen);
+        StreamFile.copyFromOffset(in, os, 0L, fileLen, true);
-    } catch(IOException e) {
+    } catch (IOException e) {
-      try {
-        in.close();
-        os.close();
-      } finally {
-        dfs.close();
-      }
+      dfs.close();
+   * @param close whether to close the streams
-                              List<?> ranges)
+                              List<InclusiveByteRange> ranges,
+                              boolean close)
-      InclusiveByteRange singleSatisfiableRange =
-        (InclusiveByteRange)ranges.get(0);
+      InclusiveByteRange singleSatisfiableRange = ranges.get(0);
-                     singleLength);
+                     singleLength, close);
-      long count) throws IOException {
+      long count, boolean close) throws IOException {
-    IOUtils.copyBytes(in, out, count);
+    IOUtils.copyBytes(in, out, count, close);
