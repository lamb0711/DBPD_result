MAPREDUCE-6337. Added a mode to replay MR job history files and put them into the timeline service v2. Contributed by Sangjin Lee.

(cherry picked from commit 463e070a8e7c882706a96eaa20ea49bfe9982875)

-import java.util.Random;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities;
-import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity;
-import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent;
-import org.apache.hadoop.yarn.api.records.timelineservice.TimelineMetric;
-import org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollector;
-import org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext;
+import org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager;
-  private static final Log LOG =
-      LogFactory.getLog(TimelineServicePerformanceV2.class);
-
-  // constants for mtype = 1
-  static final String KBS_SENT = "kbs sent";
-  static final int KBS_SENT_DEFAULT = 1;
-  static final String TEST_TIMES = "testtimes";
-  static final int TEST_TIMES_DEFAULT = 100;
-  static final String TIMELINE_SERVICE_PERFORMANCE_RUN_ID =
-      "timeline.server.performance.run.id";
-
+  static final int JOB_HISTORY_FILE_REPLAY_MAPPER = 2;
-    // TODO is there a way to handle mapper-specific options more gracefully?
-        "     [-mtype <mapper type in integer>] \n" +
+        "     [-mtype <mapper type in integer>]\n" +
-        "     [-s <(KBs)test>] number of KB per put (default: " +
-            KBS_SENT_DEFAULT + " KB)\n" +
-        "     [-t] package sending iterations per mapper (default: " +
-            TEST_TIMES_DEFAULT + ")\n");
+        "          2. job history file replay mapper\n" +
+        "     [-s <(KBs)test>] number of KB per put (mtype=1, default: " +
+             SimpleEntityWriter.KBS_SENT_DEFAULT + " KB)\n" +
+        "     [-t] package sending iterations per mapper (mtype=1, default: " +
+             SimpleEntityWriter.TEST_TIMES_DEFAULT + ")\n" +
+        "     [-d <path>] root path of job history files (mtype=2)\n" +
+        "     [-r <replay mode>] (mtype=2)\n" +
+        "          1. write all entities for a job in one put (default)\n" +
+        "          2. write one entity at a time\n");
-    // set the defaults
+    // set the common defaults
-    conf.setInt(KBS_SENT, KBS_SENT_DEFAULT);
-    conf.setInt(TEST_TIMES, TEST_TIMES_DEFAULT);
-                .setInt(MRJobConfig.NUM_MAPS, (Integer.parseInt(args[i])));
+                .setInt(MRJobConfig.NUM_MAPS, Integer.parseInt(args[i]));
-          switch (mapperType) {
-          case SIMPLE_ENTITY_WRITER:
-            job.setMapperClass(SimpleEntityWriter.class);
-            break;
-          default:
-            job.setMapperClass(SimpleEntityWriter.class);
-          }
-            conf.setInt(KBS_SENT, (Integer.parseInt(args[i])));
+            conf.setInt(SimpleEntityWriter.KBS_SENT, Integer.parseInt(args[i]));
-            conf.setInt(TEST_TIMES, (Integer.parseInt(args[i])));
+            conf.setInt(SimpleEntityWriter.TEST_TIMES,
+                Integer.parseInt(args[i]));
+        } else if ("-d".equals(args[i])) {
+          conf.set(JobHistoryFileReplayMapper.PROCESSING_PATH, args[++i]);
+        } else if ("-r".equals(args[i])) {
+          conf.setInt(JobHistoryFileReplayMapper.REPLAY_MODE,
+              Integer.parseInt(args[++i]));
+    // handle mapper-specific settings
+    switch (mapperType) {
+    case JOB_HISTORY_FILE_REPLAY_MAPPER:
+      job.setMapperClass(JobHistoryFileReplayMapper.class);
+      String processingPath =
+          conf.get(JobHistoryFileReplayMapper.PROCESSING_PATH);
+      if (processingPath == null || processingPath.isEmpty()) {
+        System.out.println("processing path is missing while mtype = 2");
+        return printUsage() == 0;
+      }
+      break;
+    case SIMPLE_ENTITY_WRITER:
+    default:
+      job.setMapperClass(SimpleEntityWriter.class);
+      // use the current timestamp as the "run id" of the test: this will
+      // be used as simulating the cluster timestamp for apps
+      conf.setLong(SimpleEntityWriter.TIMELINE_SERVICE_PERFORMANCE_RUN_ID,
+          System.currentTimeMillis());
+      break;
+    }
+
-    // for mtype = 1
-    // use the current timestamp as the "run id" of the test: this will be used
-    // as simulating the cluster timestamp for apps
-    Configuration conf = job.getConfiguration();
-    conf.setLong(TIMELINE_SERVICE_PERFORMANCE_RUN_ID,
-        System.currentTimeMillis());
-
-    int numMaps = Integer.parseInt(conf.get(MRJobConfig.NUM_MAPS));
+    int numMaps =
+        Integer.parseInt(job.getConfiguration().get(MRJobConfig.NUM_MAPS));
-   * Adds simple entities with random string payload, events, metrics, and
-   * configuration.
+   * Base mapper for writing entities to the timeline service. Subclasses
+   * override {@link #writeEntities(Configuration, TimelineCollectorManager,
+   * org.apache.hadoop.mapreduce.Mapper.Context)} to create and write entities
+   * to the timeline service.
-  public static class SimpleEntityWriter
+  public static abstract class EntityWriter
+    @Override
-      Configuration conf = context.getConfiguration();
-      // simulate the app id with the task id
-      int taskId = context.getTaskAttemptID().getTaskID().getId();
-      long timestamp = conf.getLong(TIMELINE_SERVICE_PERFORMANCE_RUN_ID, 0);
-      ApplicationId appId = ApplicationId.newInstance(timestamp, taskId);
-
-      // create the app level timeline collector
+      // create the timeline collector manager wired with the writer
-      AppLevelTimelineCollector collector =
-          new AppLevelTimelineCollector(appId);
-      collector.init(tlConf);
-      collector.start();
-
+      TimelineCollectorManager manager = new TimelineCollectorManager("test");
+      manager.init(tlConf);
+      manager.start();
-        // set the context
-        // flow id: job name, flow run id: timestamp, user id
-        TimelineCollectorContext tlContext =
-            collector.getTimelineEntityContext();
-        tlContext.setFlowName(context.getJobName());
-        tlContext.setFlowRunId(timestamp);
-        tlContext.setUserId(context.getUser());
-
-        final int kbs = Integer.parseInt(conf.get(KBS_SENT));
-
-        long totalTime = 0;
-        final int testtimes = Integer.parseInt(conf.get(TEST_TIMES));
-        final Random rand = new Random();
-        final TaskAttemptID taskAttemptId = context.getTaskAttemptID();
-        final char[] payLoad = new char[kbs * 1024];
-
-        for (int i = 0; i < testtimes; i++) {
-          // Generate a fixed length random payload
-          for (int xx = 0; xx < kbs * 1024; xx++) {
-            int alphaNumIdx = rand.nextInt(alphaNums.length);
-            payLoad[xx] = alphaNums[alphaNumIdx];
-          }
-          String entId = taskAttemptId + "_" + Integer.toString(i);
-          final TimelineEntity entity = new TimelineEntity();
-          entity.setId(entId);
-          entity.setType("FOO_ATTEMPT");
-          entity.addInfo("PERF_TEST", payLoad);
-          // add an event
-          TimelineEvent event = new TimelineEvent();
-          event.setTimestamp(System.currentTimeMillis());
-          event.addInfo("foo_event", "test");
-          entity.addEvent(event);
-          // add a metric
-          TimelineMetric metric = new TimelineMetric();
-          metric.setId("foo_metric");
-          metric.addValue(System.currentTimeMillis(), 123456789L);
-          entity.addMetric(metric);
-          // add a config
-          entity.addConfig("foo", "bar");
-
-          TimelineEntities entities = new TimelineEntities();
-          entities.addEntity(entity);
-          // use the current user for this purpose
-          UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
-          long startWrite = System.nanoTime();
-          try {
-            collector.putEntities(entities, ugi);
-          } catch (Exception e) {
-            context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES).
-                increment(1);
-            e.printStackTrace();
-          }
-          long endWrite = System.nanoTime();
-          totalTime += (endWrite-startWrite)/1000000L;
-        }
-        LOG.info("wrote " + testtimes + " entities (" + kbs*testtimes +
-            " kB) in " + totalTime + " ms");
-        context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_TIME).
-            increment(totalTime);
-        context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_COUNTER).
-            increment(testtimes);
-        context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_KBS).
-            increment(kbs*testtimes);
+        // invoke the method to have the subclass write entities
+        writeEntities(tlConf, manager, context);
-        // clean up
-        collector.close();
+        manager.close();
+
+    protected abstract void writeEntities(Configuration tlConf,
+        TimelineCollectorManager manager, Context context) throws IOException;
