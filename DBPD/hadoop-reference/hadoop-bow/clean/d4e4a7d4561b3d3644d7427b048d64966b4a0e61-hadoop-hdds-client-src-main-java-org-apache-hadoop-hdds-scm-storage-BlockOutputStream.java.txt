HDDS-1317. KeyOutputStream#write throws ArrayIndexOutOfBoundsException when running RandomWrite MR examples. Contributed by Shashikant Banerjee.

+import com.google.common.annotations.VisibleForTesting;
-import java.util.concurrent.*;
+import java.util.Map;
+import java.util.concurrent.ConcurrentSkipListMap;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.TimeoutException;
+import java.util.concurrent.CompletionException;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Executors;
+  // List containing buffers for which the putBlock call will
+  // update the length in the datanodes. This list will just maintain
+  // references to the buffers in the BufferPool which will be cleared
+  // when the watchForCommit acknowledges a putBlock logIndex has been
+  // committed on all datanodes. This list will be a  place holder for buffers
+  // which got written between successive putBlock calls.
+  private List<ByteBuffer> bufferList;
+
-  // map containing mapping for putBlock logIndex to to flushedDataLength Map.
-  private ConcurrentSkipListMap<Long, Long> commitIndex2flushedDataMap;
+  // Also, corresponding to the logIndex, the corresponding list of buffers will
+  // be released from the buffer pool.
+  private ConcurrentSkipListMap<Long, List<ByteBuffer>>
+      commitIndex2flushedDataMap;
+    bufferList = null;
+
-  public long getTotalSuccessfulFlushedData() {
+  public long getTotalAckDataLength() {
+  @VisibleForTesting
+  public XceiverClientSpi getXceiverClient() {
+    return xceiverClient;
+  }
+
+  @VisibleForTesting
+  public long getTotalDataFlushedLength() {
+    return totalDataFlushedLength;
+  }
+
+  @VisibleForTesting
+  public BufferPool getBufferPool() {
+    return bufferPool;
+  }
+
+  @VisibleForTesting
+  public IOException getIoException() {
+    return ioException;
+  }
+
+  @VisibleForTesting
+  public Map<Long, List<ByteBuffer>> getCommitIndex2flushedDataMap() {
+    return commitIndex2flushedDataMap;
+  }
+
+
-
-        totalDataFlushedLength += streamBufferFlushSize;
-        handlePartialFlush();
+        updateFlushLength();
+        executePutBlock();
-    return writtenDataLength % streamBufferFlushSize == 0;
+    return bufferPool.computeBufferData() % streamBufferFlushSize == 0;
+  }
+
+  private void updateFlushLength() {
+    totalDataFlushedLength += writtenDataLength - totalDataFlushedLength;
-      if (shouldFlush()) {
+      // we should not call isBufferFull/shouldFlush here.
+      // The buffer might already be full as whole data is already cached in
+      // the buffer. We should just validate
+      // if we wrote data of size streamBufferMaxSize/streamBufferFlushSize to
+      // call for handling full buffer/flush buffer condition.
+      if (writtenDataLength % streamBufferFlushSize == 0) {
-        totalDataFlushedLength += streamBufferFlushSize;
-        handlePartialFlush();
+        updateFlushLength();
+        executePutBlock();
-
-      // we should not call isBufferFull here. The buffer might already be full
-      // as whole data is already cached in the buffer. We should just validate
-      // if we wrote data of size streamBufferMaxSize to call for handling
-      // full buffer condition.
-      long length = commitIndex2flushedDataMap.remove(index);
-
-      // totalAckDataLength replicated yet should always be less than equal to
-      // the current length being returned from commitIndex2flushedDataMap.
-      // The below precondition would ensure commitIndex2flushedDataMap entries
-      // are removed in order of the insertion to the map.
-      Preconditions.checkArgument(totalAckDataLength < length);
-      totalAckDataLength = length;
+      List<ByteBuffer> buffers = commitIndex2flushedDataMap.remove(index);
+      long length = buffers.stream().mapToLong(value -> {
+        int pos = value.position();
+        Preconditions.checkArgument(pos <= chunkSize);
+        return pos;
+      }).sum();
+      // totalAckDataLength replicated yet should always be incremented
+      // with the current length being returned from commitIndex2flushedDataMap.
+      totalAckDataLength += length;
-      // just release the current buffer from the buffer pool.
-
-      // every entry removed from the putBlock future Map signifies
-      // streamBufferFlushSize/chunkSize no of chunks successfully committed.
-      // Release the buffers from the buffer pool to be reused again.
-      int chunkCount = (int) (streamBufferFlushSize / chunkSize);
-      for (int i = 0; i < chunkCount; i++) {
-        bufferPool.releaseBuffer();
+      // just release the current buffer from the buffer pool corresponding
+      // to the buffers that have been committed with the putBlock call.
+      for (ByteBuffer byteBuffer : buffers) {
+        bufferPool.releaseBuffer(byteBuffer);
-      adjustBuffersOnException();
-      throw new IOException(
+      ioException = new IOException(
+      adjustBuffersOnException();
+      throw ioException;
-      ContainerCommandResponseProto> handlePartialFlush()
+      ContainerCommandResponseProto> executePutBlock()
+    Preconditions.checkNotNull(bufferList);
+    List<ByteBuffer> byteBufferList = bufferList;
+    bufferList = null;
+    Preconditions.checkNotNull(byteBufferList);
-          LOG.debug(
-              "Adding index " + asyncReply.getLogIndex() + " commitMap size "
-                  + commitIndex2flushedDataMap.size());
+          LOG.debug(
+              "Adding index " + asyncReply.getLogIndex() + " commitMap size "
+                  + commitIndex2flushedDataMap.size() + " flushLength "
+                  + flushPos + " numBuffers " + byteBufferList.size()
+                  + " blockID " + blockID + " bufferPool size" + bufferPool
+                  .getSize() + " currentBufferIndex " + bufferPool
+                  .getCurrentBufferIndex());
-          commitIndex2flushedDataMap.put(asyncReply.getLogIndex(), flushPos);
+          commitIndex2flushedDataMap
+              .put(asyncReply.getLogIndex(), byteBufferList);
-        adjustBuffersOnException();
-        throw new IOException(
+        // just set the exception here as well in order to maintain sanctity of
+        // ioException field
+        ioException = new IOException(
+        adjustBuffersOnException();
+        throw ioException;
+    // This data in the buffer will be pushed to datanode and a reference will
+    // be added to the bufferList. Once putBlock gets executed, this list will
+    // be marked null. Hence, during first writeChunk call after every putBlock
+    // call or during the first call to writeChunk here, the list will be null.
+
+    if (bufferList == null) {
+      bufferList = new ArrayList<>();
+    }
+    bufferList.add(buffer);
-      ByteBuffer currentBuffer = bufferPool.getBuffer();
-      int pos = currentBuffer.position();
-      writeChunk(currentBuffer);
-      totalDataFlushedLength += pos;
-      handlePartialFlush();
+      ByteBuffer currentBuffer = bufferPool.getCurrentBuffer();
+      Preconditions.checkArgument(currentBuffer.position() > 0);
+      if (currentBuffer.position() != chunkSize) {
+        writeChunk(currentBuffer);
+      }
+      // This can be a partially filled chunk. Since we are flushing the buffer
+      // here, we just limit this buffer to the current position. So that next
+      // write will happen in new buffer
+      updateFlushLength();
+      executePutBlock();
+    if (!commitIndex2flushedDataMap.isEmpty()) {
+      // wait for the last commit index in the commitIndex2flushedDataMap
+      // to get committed to all or majority of nodes in case timeout
+      // happens.
+      long lastIndex =
+          commitIndex2flushedDataMap.keySet().stream().mapToLong(v -> v)
+              .max().getAsLong();
+      LOG.debug(
+          "waiting for last flush Index " + lastIndex + " to catch up");
+      watchForCommit(lastIndex);
+    }
+
-
-        if (!commitIndex2flushedDataMap.isEmpty()) {
-          // wait for the last commit index in the commitIndex2flushedDataMap
-          // to get committed to all or majority of nodes in case timeout
-          // happens.
-          long lastIndex =
-              commitIndex2flushedDataMap.keySet().stream().mapToLong(v -> v)
-                  .max().getAsLong();
-          LOG.debug(
-              "waiting for last flush Index " + lastIndex + " to catch up");
-          watchForCommit(lastIndex);
-        }
-        adjustBuffersOnException();
-        throw new IOException(
+        ioException = new IOException(
+        adjustBuffersOnException();
+        throw ioException;
+    if (bufferList !=  null) {
+      bufferList.clear();
+    }
+    bufferList = null;
