svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68

+import java.net.Socket;
-import org.apache.hadoop.hdfs.net.Peer;
-import org.apache.hadoop.hdfs.protocol.DatanodeID;
+import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
+import org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair;
+import org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier;
+import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.security.token.Token;
-  private final Peer peer;
-  private final DatanodeID datanodeID;
+  Socket dnSock; //for now just sending the status code (e.g. checksumOk) after the read.
-        sendReadResult(peer, Status.CHECKSUM_OK);
+        sendReadResult(dnSock, Status.CHECKSUM_OK);
-        sendReadResult(peer, Status.SUCCESS);
+        sendReadResult(dnSock, Status.SUCCESS);
-      long startOffset, long firstChunkOffset, long bytesToRead, 
-      Peer peer, DatanodeID datanodeID) {
+      long startOffset, long firstChunkOffset, long bytesToRead, Socket dnSock) {
-    this.peer = peer;
-    this.datanodeID = datanodeID;
+    this.dnSock = dnSock;
+    Socket sock = params.getSocket();
-        params.getPeer().getOutputStream()));
+          NetUtils.getOutputStream(sock, HdfsServerConstants.WRITE_TIMEOUT)));
-        new BufferedInputStream(params.getPeer().getInputStream(),
+        new BufferedInputStream(NetUtils.getInputStream(sock), 
-    RemoteBlockReader2.checkSuccess(status, params.getPeer(),
-        params.getBlock(), params.getFile());
+    RemoteBlockReader2.checkSuccess(status, sock, params.getBlock(),
+        params.getFile());
-        params.getStartOffset(), firstChunkOffset, params.getLen(),
-        params.getPeer(), params.getDatanodeID());
+        params.getStartOffset(), firstChunkOffset, params.getLen(), sock);
-  public synchronized void close(PeerCache peerCache) throws IOException {
+  public synchronized void close() throws IOException {
-    if (peerCache != null && sentStatusCode) {
-      peerCache.put(datanodeID, peer);
-    } else {
-      peer.close();
+    if (dnSock != null) {
+      dnSock.close();
-    // in will be closed when its Peer is closed.
+
+    // in will be closed when its Socket is closed.
+  @Override
+  public Socket takeSocket() {
+    assert hasSentStatusCode() :
+      "BlockReader shouldn't give back sockets mid-read";
+    Socket res = dnSock;
+    dnSock = null;
+    return res;
+  }
+
+  @Override
+  public boolean hasSentStatusCode() {
+    return sentStatusCode;
+  }
+
-  void sendReadResult(Peer peer, Status statusCode) {
-    assert !sentStatusCode : "already sent status code to " + peer;
+  void sendReadResult(Socket sock, Status statusCode) {
+    assert !sentStatusCode : "already sent status code to " + sock;
-      RemoteBlockReader2.writeReadResult(peer.getOutputStream(), statusCode);
+      RemoteBlockReader2.writeReadResult(
+          NetUtils.getOutputStream(sock, HdfsServerConstants.WRITE_TIMEOUT),
+          statusCode);
-               datanodeID + ": " + e.getMessage());
+               sock.getInetAddress() + ": " + e.getMessage());
+
+  @Override
+  public IOStreamPair getStreams() {
+    // This class doesn't support encryption, which is the only thing this
+    // method is used for. See HDFS-3637.
+    return null;
+  }
+
