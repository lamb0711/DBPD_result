HDFS-11402. HDFS Snapshots should capture point-in-time copies of OPEN files. (Manoj Govindassamy via Yongjun Zhang)

+import java.util.Set;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import com.google.common.collect.Lists;
+import org.apache.hadoop.util.Time;
-
-
+  static final int INODE_FILTER_WORKER_COUNT_MAX = 4;
+  static final int INODE_FILTER_WORKER_TASK_MIN = 512;
-  //
-  //
+  /**
+   * Get {@link INodesInPath} for all {@link INode} in the system
+   * which has a valid lease.
+   *
+   * @return Set<INodesInPath>
+   */
+  public Set<INodesInPath> getINodeWithLeases() {
+    return getINodeWithLeases(null);
+  }
+
+  private synchronized INode[] getINodesWithLease() {
+    int inodeCount = 0;
+    INode[] inodes = new INode[leasesById.size()];
+    for (long inodeId : leasesById.keySet()) {
+      inodes[inodeCount] = fsnamesystem.getFSDirectory().getInode(inodeId);
+      inodeCount++;
+    }
+    return inodes;
+  }
+
+  /**
+   * Get {@link INodesInPath} for all files under the ancestor directory which
+   * has valid lease. If the ancestor directory is null, then return all files
+   * in the system with valid lease. Callers must hold {@link FSNamesystem}
+   * read or write lock.
+   *
+   * @param ancestorDir the ancestor {@link INodeDirectory}
+   * @return Set<INodesInPath>
+   */
+  public Set<INodesInPath> getINodeWithLeases(final INodeDirectory
+      ancestorDir) {
+    assert fsnamesystem.hasReadLock();
+    final long startTimeMs = Time.monotonicNow();
+    Set<INodesInPath> iipSet = new HashSet<>();
+    final INode[] inodes = getINodesWithLease();
+    int inodeCount = inodes.length;
+    if (inodeCount == 0) {
+      return iipSet;
+    }
+
+    List<Future<List<INodesInPath>>> futureList = Lists.newArrayList();
+    final int workerCount = Math.min(INODE_FILTER_WORKER_COUNT_MAX,
+        (((inodeCount - 1) / INODE_FILTER_WORKER_TASK_MIN) + 1));
+    ExecutorService inodeFilterService =
+        Executors.newFixedThreadPool(workerCount);
+    for (int workerIdx = 0; workerIdx < workerCount; workerIdx++) {
+      final int startIdx = workerIdx;
+      Callable<List<INodesInPath>> c = new Callable<List<INodesInPath>>() {
+        @Override
+        public List<INodesInPath> call() {
+          List<INodesInPath> iNodesInPaths = Lists.newArrayList();
+          for (int idx = startIdx; idx < inodeCount; idx += workerCount) {
+            INode inode = inodes[idx];
+            if (!inode.isFile()) {
+              continue;
+            }
+            INodesInPath inodesInPath = INodesInPath.fromINode(
+                fsnamesystem.getFSDirectory().getRoot(), inode.asFile());
+            if (ancestorDir != null &&
+                !inodesInPath.isDescendant(ancestorDir)) {
+              continue;
+            }
+            iNodesInPaths.add(inodesInPath);
+          }
+          return iNodesInPaths;
+        }
+      };
+
+      // Submit the inode filter task to the Executor Service
+      futureList.add(inodeFilterService.submit(c));
+    }
+    inodeFilterService.shutdown();
+
+    for (Future<List<INodesInPath>> f : futureList) {
+      try {
+        iipSet.addAll(f.get());
+      } catch (Exception e) {
+        LOG.warn("INode filter task encountered exception: ", e);
+      }
+    }
+    final long endTimeMs = Time.monotonicNow();
+    if ((endTimeMs - startTimeMs) > 1000) {
+      LOG.info("Took " + (endTimeMs - startTimeMs) + " ms to collect "
+          + iipSet.size() + " open files with leases" +
+          ((ancestorDir != null) ?
+              " under " + ancestorDir.getFullPathName() : "."));
+    }
+    return iipSet;
+  }
+
