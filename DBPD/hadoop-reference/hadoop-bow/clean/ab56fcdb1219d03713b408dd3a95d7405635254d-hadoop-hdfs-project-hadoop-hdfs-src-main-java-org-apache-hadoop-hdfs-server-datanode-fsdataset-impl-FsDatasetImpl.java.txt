Merge remote-tracking branch 'apache/trunk' into HDFS-7285

-import java.nio.ByteBuffer;
-import org.apache.hadoop.hdfs.HdfsConfiguration;
-import org.apache.hadoop.hdfs.protocol.HdfsBlocksMetadata;
-    asyncLazyPersistService = new RamDiskAsyncLazyPersistService(datanode);
+    asyncLazyPersistService = new RamDiskAsyncLazyPersistService(datanode, conf);
-      int smallBufferSize) throws IOException {
+      int smallBufferSize, final Configuration conf) throws IOException {
-        smallBufferSize);
+        smallBufferSize, conf);
-                               int smallBufferSize)
+                               int smallBufferSize, final Configuration conf)
-      computeChecksum(srcMeta, dstMeta, srcFile, smallBufferSize);
+      computeChecksum(srcMeta, dstMeta, srcFile, smallBufferSize, conf);
-          replicaInfo.isOnTransientStorage(), smallBufferSize);
+          replicaInfo.isOnTransientStorage(), smallBufferSize, conf);
-      File blockFile, int smallBufferSize)
+      File blockFile, int smallBufferSize, final Configuration conf)
-    final DataChecksum checksum = BlockMetadataHeader.readDataChecksum(srcMeta);
+    final DataChecksum checksum = BlockMetadataHeader.readDataChecksum(srcMeta,
+        DFSUtil.getIoFileBufferSize(conf));
-          dstMetaFile, dstBlockFile, true, smallBufferSize);
+          dstMetaFile, dstBlockFile, true, smallBufferSize, conf);
-  @Override // FsDatasetSpi
-  public HdfsBlocksMetadata getHdfsBlocksMetadata(String poolId,
-      long[] blockIds) throws IOException {
-    List<FsVolumeImpl> curVolumes = volumes.getVolumes();
-    // List of VolumeIds, one per volume on the datanode
-    List<byte[]> blocksVolumeIds = new ArrayList<>(curVolumes.size());
-    // List of indexes into the list of VolumeIds, pointing at the VolumeId of
-    // the volume that the block is on
-    List<Integer> blocksVolumeIndexes = new ArrayList<Integer>(blockIds.length);
-    // Initialize the list of VolumeIds simply by enumerating the volumes
-    for (int i = 0; i < curVolumes.size(); i++) {
-      blocksVolumeIds.add(ByteBuffer.allocate(4).putInt(i).array());
-    }
-    // Determine the index of the VolumeId of each block's volume, by comparing 
-    // the block's volume against the enumerated volumes
-    for (int i = 0; i < blockIds.length; i++) {
-      long blockId = blockIds[i];
-      boolean isValid = false;
-
-      ReplicaInfo info = volumeMap.get(poolId, blockId);
-      int volumeIndex = 0;
-      if (info != null) {
-        FsVolumeSpi blockVolume = info.getVolume();
-        for (FsVolumeImpl volume : curVolumes) {
-          // This comparison of references should be safe
-          if (blockVolume == volume) {
-            isValid = true;
-            break;
-          }
-          volumeIndex++;
-        }
-      }
-      // Indicates that the block is not present, or not found in a data dir
-      if (!isValid) {
-        volumeIndex = Integer.MAX_VALUE;
-      }
-      blocksVolumeIndexes.add(volumeIndex);
-    }
-    return new HdfsBlocksMetadata(poolId, blockIds,
-        blocksVolumeIds, blocksVolumeIndexes);
-  }
-
