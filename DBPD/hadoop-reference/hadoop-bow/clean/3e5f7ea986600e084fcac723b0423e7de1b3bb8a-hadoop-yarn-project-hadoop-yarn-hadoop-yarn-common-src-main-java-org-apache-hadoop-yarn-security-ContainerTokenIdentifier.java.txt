YARN-8310. Handle old NMTokenIdentifier, AMRMTokenIdentifier, and ContainerTokenIdentifier formats. Contributed by Robert Kanter.

-* Licensed to the Apache Software Foundation (ASF) under one
-* or more contributor license agreements.  See the NOTICE file
-* distributed with this work for additional information
-* regarding copyright ownership.  The ASF licenses this file
-* to you under the Apache License, Version 2.0 (the
-* "License"); you may not use this file except in compliance
-* with the License.  You may obtain a copy of the License at
-*
-*     http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing, software
-* distributed under the License is distributed on an "AS IS" BASIS,
-* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-* See the License for the specific language governing permissions and
-* limitations under the License.
-*/
-
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.ByteArrayInputStream;
+import java.io.EOFException;
+import com.google.protobuf.InvalidProtocolBufferException;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
+import org.apache.hadoop.yarn.api.records.ApplicationId;
+import org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationContextProto;
-    proto = ContainerTokenIdentifierProto.parseFrom((DataInputStream)in);
+    byte[] data = IOUtils.readFullyToByteArray(in);
+    try {
+      proto = ContainerTokenIdentifierProto.parseFrom(data);
+    } catch (InvalidProtocolBufferException e) {
+      LOG.warn("Recovering old formatted token");
+      readFieldsInOldFormat(
+          new DataInputStream(new ByteArrayInputStream(data)));
+    }
+  }
+
+  private void readFieldsInOldFormat(DataInputStream in) throws IOException {
+    ContainerTokenIdentifierProto.Builder builder =
+        ContainerTokenIdentifierProto.newBuilder();
+    builder.setNodeLabelExpression(CommonNodeLabelsManager.NO_LABEL);
+    builder.setContainerType(ProtoUtils.convertToProtoFormat(
+        ContainerType.TASK));
+    builder.setExecutionType(ProtoUtils.convertToProtoFormat(
+        ExecutionType.GUARANTEED));
+    builder.setAllocationRequestId(-1);
+    builder.setVersion(0);
+
+    ApplicationId applicationId =
+        ApplicationId.newInstance(in.readLong(), in.readInt());
+    ApplicationAttemptId applicationAttemptId =
+        ApplicationAttemptId.newInstance(applicationId, in.readInt());
+    ContainerId containerId =
+        ContainerId.newContainerId(applicationAttemptId, in.readLong());
+    builder.setContainerId(ProtoUtils.convertToProtoFormat(containerId));
+    builder.setNmHostAddr(in.readUTF());
+    builder.setAppSubmitter(in.readUTF());
+    int memory = in.readInt();
+    int vCores = in.readInt();
+    Resource resource = Resource.newInstance(memory, vCores);
+    builder.setResource(ProtoUtils.convertToProtoFormat(resource));
+    builder.setExpiryTimeStamp(in.readLong());
+    builder.setMasterKeyId(in.readInt());
+    builder.setRmIdentifier(in.readLong());
+    Priority priority = Priority.newInstance(in.readInt());
+    builder.setPriority(((PriorityPBImpl)priority).getProto());
+    builder.setCreationTime(in.readLong());
+
+    int logAggregationSize = -1;
+    try {
+      logAggregationSize = in.readInt();
+    } catch (EOFException eof) {
+      // In the old format, there was no versioning or proper handling of new
+      // fields.  Depending on how old, the log aggregation size and data, may
+      // or may not exist.  To handle that, we try to read it and ignore the
+      // EOFException that's thrown if it's not there.
+    }
+    if (logAggregationSize != -1) {
+      byte[] bytes = new byte[logAggregationSize];
+      in.readFully(bytes);
+      builder.setLogAggregationContext(
+          LogAggregationContextProto.parseFrom(bytes));
+    }
+    proto = builder.build();
