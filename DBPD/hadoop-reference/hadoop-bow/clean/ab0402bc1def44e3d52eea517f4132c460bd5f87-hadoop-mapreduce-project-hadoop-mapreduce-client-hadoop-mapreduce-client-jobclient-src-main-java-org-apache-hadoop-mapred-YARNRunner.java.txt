Merging trunk to HDFS-1623 branch


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1177130 13f79535-47bb-0310-9956-ffa450edef68

-import org.apache.hadoop.mapreduce.v2.MRConstants;
+import org.apache.hadoop.yarn.api.ApplicationConstants.Environment;
+   this(conf, resMgrDelegate, new ClientCache(conf, resMgrDelegate));
+  }
+  
+  /**
+   * Similar to {@link YARNRunner#YARNRunner(Configuration, ResourceMgrDelegate)} 
+   * but allowing injecting {@link ClientCache}. Enable mocking and testing.
+   * @param conf the configuration object
+   * @param resMgrDelegate the resource manager delegate 
+   * @param clientCache the client cache object.
+   */
+  public YARNRunner(Configuration conf, ResourceMgrDelegate resMgrDelegate,
+      ClientCache clientCache) {
-      this.clientCache = new ClientCache(this.conf, resMgrDelegate);
+      this.clientCache = clientCache;
-        new Path(jobSubmitDir, MRConstants.APPLICATION_TOKENS_FILE);
+        new Path(jobSubmitDir, MRJobConfig.APPLICATION_TOKENS_FILE);
-    String diagnostics = (appMaster == null ? "application report is null" : appMaster.getDiagnostics());
+    String diagnostics = 
+        (appMaster == null ? 
+            "application report is null" : appMaster.getDiagnostics());
-    Path jobConfPath = new Path(jobSubmitDir, MRConstants.JOB_CONF_FILE);
+    Path jobConfPath = new Path(jobSubmitDir, MRJobConfig.JOB_CONF_FILE);
-    localResources.put(MRConstants.JOB_CONF_FILE,
+    localResources.put(MRJobConfig.JOB_CONF_FILE,
-      localResources.put(MRConstants.JOB_JAR,
+      localResources.put(MRJobConfig.JOB_JAR,
-              new Path(jobSubmitDir, MRConstants.JOB_JAR)));
+              new Path(jobSubmitDir, MRJobConfig.JOB_JAR)));
-    for (String s : new String[] { "job.split", "job.splitmetainfo",
-        MRConstants.APPLICATION_TOKENS_FILE }) {
+    for (String s : new String[] { 
+        MRJobConfig.JOB_SPLIT, 
+        MRJobConfig.JOB_SPLIT_METAINFO,
+        MRJobConfig.APPLICATION_TOKENS_FILE }) {
-          MRConstants.JOB_SUBMIT_DIR + "/" + s,
+          MRJobConfig.JOB_SUBMIT_DIR + "/" + s,
-    String javaHome = "$JAVA_HOME";
-    vargs.add(javaHome + "/bin/java");
-    vargs.add("-Dhadoop.root.logger="
-        + conf.get(MRJobConfig.MR_AM_LOG_OPTS,
-            MRJobConfig.DEFAULT_MR_AM_LOG_OPTS) + ",console");
+    vargs.add(Environment.JAVA_HOME.$() + "/bin/java");
+    
+    long logSize = TaskLog.getTaskLogLength(new JobConf(conf));
+    vargs.add("-Dlog4j.configuration=container-log4j.properties");
+    vargs.add("-D" + MRJobConfig.TASK_LOG_DIR + "="
+        + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
+    vargs.add("-D" + MRJobConfig.TASK_LOG_SIZE + "=" + logSize);
-    vargs.add("org.apache.hadoop.mapreduce.v2.app.MRAppMaster");
-    vargs.add(String.valueOf(applicationId.getClusterTimestamp()));
-    vargs.add(String.valueOf(applicationId.getId()));
-    vargs.add(ApplicationConstants.AM_FAIL_COUNT_STRING);
-    vargs.add("1>" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stdout");
-    vargs.add("2>" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stderr");
+    vargs.add(MRJobConfig.APPLICATION_MASTER_CLASS);
+    vargs.add("1>" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + 
+        Path.SEPARATOR + ApplicationConstants.STDOUT);
+    vargs.add("2>" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + 
+        Path.SEPARATOR + ApplicationConstants.STDERR);
+
-    // Setup the environment - Add { job jar, MR app jar } to classpath.
+    // Setup the CLASSPATH in environment 
+    // i.e. add { job jar, CWD, Hadoop jars} to classpath.
-    MRApps.setInitialClasspath(environment);
-    MRApps.addToClassPath(environment, MRConstants.JOB_JAR);
-    MRApps.addToClassPath(environment,
-        MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);
-
+    MRApps.setClasspath(environment);
+    
-    MRApps.setupDistributedCache(jobConf, localResources, environment);
+    MRApps.setupDistributedCache(jobConf, localResources);
-    if (!clientCache.getClient(arg0).killJob(arg0)) {
-    resMgrDelegate.killApplication(TypeConverter.toYarn(arg0).getAppId());
-  }
+    /* check if the status is not running, if not send kill to RM */
+    JobStatus status = clientCache.getClient(arg0).getJobStatus(arg0);
+    if (status.getState() != JobStatus.State.RUNNING) {
+      resMgrDelegate.killApplication(TypeConverter.toYarn(arg0).getAppId());
+      return;
+    } 
+    
+    try {
+      /* send a kill to the AM */
+      clientCache.getClient(arg0).killJob(arg0);
+      long currentTimeMillis = System.currentTimeMillis();
+      long timeKillIssued = currentTimeMillis;
+      while ((currentTimeMillis < timeKillIssued + 10000L) && (status.getState()
+          != JobStatus.State.KILLED)) {
+          try {
+            Thread.sleep(1000L);
+          } catch(InterruptedException ie) {
+            /** interrupted, just break */
+            break;
+          }
+          currentTimeMillis = System.currentTimeMillis();
+          status = clientCache.getClient(arg0).getJobStatus(arg0);
+      }
+    } catch(IOException io) {
+      LOG.debug("Error when checking for application status", io);
+    }
+    if (status.getState() != JobStatus.State.KILLED) {
+      resMgrDelegate.killApplication(TypeConverter.toYarn(arg0).getAppId());
+    }
