Merge trunk into HA branch


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1196458 13f79535-47bb-0310-9956-ffa450edef68

+import org.apache.hadoop.mapreduce.v2.LogParams;
+import org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogDumper;
+  private Cluster cluster;
+    boolean logs = false;
+    } else if ("-logs".equals(cmd)) {
+      if (argv.length == 2 || argv.length ==3) {
+        logs = true;
+        jobid = argv[1];
+        if (argv.length == 3) {
+          taskid = argv[2];
+        }  else {
+          taskid = null;
+        }
+      } else {
+        displayUsage(cmd);
+        return exitCode;
+      }
-    Cluster cluster = new Cluster(getConf());
+    cluster = new Cluster(getConf());
+      } else if (logs) {
+        try {
+        JobID jobID = JobID.forName(jobid);
+        TaskAttemptID taskAttemptID = TaskAttemptID.forName(taskid);
+        LogParams logParams = cluster.getLogParams(jobID, taskAttemptID);
+        LogDumper logDumper = new LogDumper();
+        logDumper.setConf(getConf());
+        logDumper.dumpAContainersLogs(logParams.getApplicationId(),
+            logParams.getContainerId(), logParams.getNodeId(),
+            logParams.getOwner());
+        } catch (IOException e) {
+          if (e instanceof RemoteException) {
+            throw e;
+          } 
+          System.out.println(e.getMessage());
+        }
+    } else if ("-logs".equals(cmd)) {
+      System.err.println(prefix + "[" + cmd +
+          " <job-id> <task-attempt-id>]. " +
+          " <task-attempt-id> is optional to get task attempt logs.");      
-      System.err.printf("\t[-fail-task <task-attempt-id>]\n\n");
+      System.err.printf("\t[-fail-task <task-attempt-id>]\n");
+      System.err.printf("\t[-logs <job-id> <task-attempt-id>]\n\n");
-        "UserName\tQueue\tPriority\tSchedulingInfo");
+        "UserName\tQueue\tPriority\tMaps\tReduces\tUsedContainers\t" +
+        "RsvdContainers\tUsedMem\tRsvdMem\tNeededMem\tAM info");
-      System.out.printf("%s\t%s\t%d\t%s\t%s\t%s\t%s\n", job.getJobID().toString(),
-          job.getState(), job.getStartTime(),
+      TaskReport[] mapReports =
+                 cluster.getJob(job.getJobID()).getTaskReports(TaskType.MAP);
+      TaskReport[] reduceReports =
+                 cluster.getJob(job.getJobID()).getTaskReports(TaskType.REDUCE);
+
+      System.out.printf("%s\t%s\t%d\t%s\t%s\t%s\t%d\t%d\t%d\t%d\t%dM\t%dM\t%dM\t%s\n",
+          job.getJobID().toString(), job.getState(), job.getStartTime(),
-          job.getPriority().name(), job.getSchedulingInfo());
+          job.getPriority().name(),
+          mapReports.length,
+          reduceReports.length,
+          job.getNumUsedSlots(),
+          job.getNumReservedSlots(),
+          job.getUsedMem(),
+          job.getReservedMem(),
+          job.getNeededMem(),
+          job.getSchedulingInfo());
