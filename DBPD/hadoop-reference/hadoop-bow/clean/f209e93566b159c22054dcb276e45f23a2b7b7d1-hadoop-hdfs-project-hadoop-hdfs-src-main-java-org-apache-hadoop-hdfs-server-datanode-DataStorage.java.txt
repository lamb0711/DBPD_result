HDFS-10638. Modifications to remove the assumption that StorageLocation is associated with java.io.File in Datanode. (Virajith Jalaparti via lei)

-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.hdfs.HdfsConfiguration;
-import org.apache.hadoop.util.DiskChecker;
-
-      NamespaceInfo nsInfo, File dataDir, StorageLocation location,
-      StartupOption startOpt, List<Callable<StorageDirectory>> callables)
-          throws IOException {
-    StorageDirectory sd = new StorageDirectory(dataDir, null, false, location);
+      NamespaceInfo nsInfo, StorageLocation location, StartupOption startOpt,
+      List<Callable<StorageDirectory>> callables) throws IOException {
+    StorageDirectory sd = new StorageDirectory(null, false, location);
-        LOG.info("Storage directory " + dataDir + " does not exist");
-        throw new IOException("Storage directory " + dataDir
+        LOG.info("Storage directory with location " + location
+            + " does not exist");
+        throw new IOException("Storage directory with location " + location
-        LOG.info("Storage directory " + dataDir
+        LOG.info("Storage directory with location " + location
-    File volume = location.getFile();
-    if (containsStorageDir(volume)) {
+    if (containsStorageDir(location)) {
-        datanode, nsInfos.get(0), volume, location,
-        StartupOption.HOTSWAP, null);
+        datanode, nsInfos.get(0), location, StartupOption.HOTSWAP, null);
-      List<File> bpDataDirs = Lists.newArrayList();
-      bpDataDirs.add(BlockPoolSliceStorage.getBpRoot(
-          nsInfo.getBlockPoolID(), new File(volume, STORAGE_DIR_CURRENT)));
-      makeBlockPoolDataDir(bpDataDirs, null);
+      location.makeBlockPoolDir(nsInfo.getBlockPoolID(), null);
-          nsInfo, bpDataDirs, location, StartupOption.HOTSWAP,
-          null, datanode.getConf());
+          nsInfo, location, StartupOption.HOTSWAP, null, datanode.getConf());
-      File root = dataDir.getFile();
-      if (!containsStorageDir(root)) {
+      if (!containsStorageDir(dataDir)) {
-              datanode, nsInfo, root, dataDir, startOpt, callables);
+              datanode, nsInfo, dataDir, startOpt, callables);
-      final File curDir = new File(dataDir.getFile(), STORAGE_DIR_CURRENT);
-      List<File> bpDataDirs = new ArrayList<File>();
-      bpDataDirs.add(BlockPoolSliceStorage.getBpRoot(bpid, curDir));
+      dataDir.makeBlockPoolDir(bpid, null);
-        makeBlockPoolDataDir(bpDataDirs, null);
-
-            nsInfo, bpDataDirs, dataDir, startOpt,
-            callables, datanode.getConf());
+            nsInfo, dataDir, startOpt, callables, datanode.getConf());
-  /**
-   * Create physical directory for block pools on the data node
-   * 
-   * @param dataDirs
-   *          List of data directories
-   * @param conf
-   *          Configuration instance to use.
-   * @throws IOException on errors
-   */
-  static void makeBlockPoolDataDir(Collection<File> dataDirs,
-      Configuration conf) throws IOException {
-    if (conf == null)
-      conf = new HdfsConfiguration();
-
-    LocalFileSystem localFS = FileSystem.getLocal(conf);
-    FsPermission permission = new FsPermission(conf.get(
-        DFSConfigKeys.DFS_DATANODE_DATA_DIR_PERMISSION_KEY,
-        DFSConfigKeys.DFS_DATANODE_DATA_DIR_PERMISSION_DEFAULT));
-    for (File data : dataDirs) {
-      try {
-        DiskChecker.checkDir(localFS, new Path(data.toURI()), permission);
-      } catch ( IOException e ) {
-        LOG.warn("Invalid directory in: " + data.getCanonicalPath() + ": "
-            + e.getMessage());
-      }
-    }
-  }
-
