Merge trunk into HA branch.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1203781 13f79535-47bb-0310-9956-ffa450edef68

-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_BLOCKREPORT_INITIAL_DELAY_DEFAULT;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_BLOCKREPORT_INITIAL_DELAY_KEY;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_DEFAULT;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_CLIENT_SOCKET_TIMEOUT_KEY;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_CLIENT_WRITE_PACKET_SIZE_DEFAULT;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_CLIENT_WRITE_PACKET_SIZE_KEY;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_SOCKET_WRITE_TIMEOUT_KEY;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_SYNCONCLOSE_DEFAULT;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_SYNCONCLOSE_KEY;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_TRANSFERTO_ALLOWED_DEFAULT;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_TRANSFERTO_ALLOWED_KEY;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT;
-import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY;
-import org.apache.hadoop.hdfs.DFSConfigKeys;
+import org.apache.hadoop.hdfs.server.common.StorageInfo;
-        BPOfferService bpos = new BPOfferService(isa);
+        BPOfferService bpos = new BPOfferService(isa, DataNode.this);
-            BPOfferService bpos = new BPOfferService(nnaddr);
+            BPOfferService bpos = new BPOfferService(nnaddr, DataNode.this);
-
-          for (BPOfferService bpos : toShutdown) {
-            remove(bpos);
-          }
+        
+        // stoping the BPOSes causes them to call remove() on their own when they
+        // clean up.
+        
-  long blockReportInterval;
-  boolean resetBlockReportTime = true;
-  long deleteReportInterval;
-  long lastDeletedReport = 0;
-  long initialBlockReportDelay = DFS_BLOCKREPORT_INTERVAL_MSEC_DEFAULT * 1000L;
-  long heartBeatInterval;
+  private DNConf dnConf;
-  int socketTimeout;
-  int socketWriteTimeout = 0;  
-  boolean transferToAllowed = true;
-  private boolean dropCacheBehindWrites = false;
-  private boolean syncBehindWrites = false;
-  private boolean dropCacheBehindReads = false;
-  private long readaheadLength = 0;
-  int writePacketSize = 0;
-  boolean syncOnClose;
-  private void initConfig(Configuration conf) {
-    this.socketTimeout =  conf.getInt(DFS_CLIENT_SOCKET_TIMEOUT_KEY,
-                                      HdfsServerConstants.READ_TIMEOUT);
-    this.socketWriteTimeout = conf.getInt(DFS_DATANODE_SOCKET_WRITE_TIMEOUT_KEY,
-                                          HdfsServerConstants.WRITE_TIMEOUT);
-    /* Based on results on different platforms, we might need set the default 
-     * to false on some of them. */
-    this.transferToAllowed = conf.getBoolean(
-        DFS_DATANODE_TRANSFERTO_ALLOWED_KEY,
-        DFS_DATANODE_TRANSFERTO_ALLOWED_DEFAULT);
-    this.writePacketSize = conf.getInt(DFS_CLIENT_WRITE_PACKET_SIZE_KEY, 
-                                       DFS_CLIENT_WRITE_PACKET_SIZE_DEFAULT);
-
-    this.readaheadLength = conf.getLong(
-        DFSConfigKeys.DFS_DATANODE_READAHEAD_BYTES_KEY,
-        DFSConfigKeys.DFS_DATANODE_READAHEAD_BYTES_DEFAULT);
-    this.dropCacheBehindWrites = conf.getBoolean(
-        DFSConfigKeys.DFS_DATANODE_DROP_CACHE_BEHIND_WRITES_KEY,
-        DFSConfigKeys.DFS_DATANODE_DROP_CACHE_BEHIND_WRITES_DEFAULT);
-    this.syncBehindWrites = conf.getBoolean(
-        DFSConfigKeys.DFS_DATANODE_SYNC_BEHIND_WRITES_KEY,
-        DFSConfigKeys.DFS_DATANODE_SYNC_BEHIND_WRITES_DEFAULT);
-    this.dropCacheBehindReads = conf.getBoolean(
-        DFSConfigKeys.DFS_DATANODE_DROP_CACHE_BEHIND_READS_KEY,
-        DFSConfigKeys.DFS_DATANODE_DROP_CACHE_BEHIND_READS_DEFAULT);
-
-    this.blockReportInterval = conf.getLong(DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,
-        DFS_BLOCKREPORT_INTERVAL_MSEC_DEFAULT);
-    this.initialBlockReportDelay = conf.getLong(
-        DFS_BLOCKREPORT_INITIAL_DELAY_KEY,
-        DFS_BLOCKREPORT_INITIAL_DELAY_DEFAULT) * 1000L;
-    if (this.initialBlockReportDelay >= blockReportInterval) {
-      this.initialBlockReportDelay = 0;
-      LOG.info("dfs.blockreport.initialDelay is greater than " +
-        "dfs.blockreport.intervalMsec." + " Setting initial delay to 0 msec:");
-    }
-    this.heartBeatInterval = conf.getLong(DFS_HEARTBEAT_INTERVAL_KEY,
-        DFS_HEARTBEAT_INTERVAL_DEFAULT) * 1000L;
-
-    this.deleteReportInterval = 100 * heartBeatInterval;
-    // do we need to sync block file contents to disk when blockfile is closed?
-    this.syncOnClose = conf.getBoolean(DFS_DATANODE_SYNCONCLOSE_KEY, 
-                                       DFS_DATANODE_SYNCONCLOSE_DEFAULT);
-  }
-  
+    assert data != null;
-      ss = (socketWriteTimeout > 0) ? 
+      ss = (dnConf.socketWriteTimeout > 0) ? 
-  class BPOfferService implements Runnable {
+  static class BPOfferService implements Runnable {
+    long lastDeletedReport = 0;
+
+    boolean resetBlockReportTime = true;
+
-    private boolean isBlockTokenInitialized = false;
+    private final DataNode dn;
+    private final DNConf dnConf;
-    BPOfferService(InetSocketAddress isa) {
-      this.bpRegistration = new DatanodeRegistration(getMachineName());
-      bpRegistration.setInfoPort(infoServer.getPort());
-      bpRegistration.setIpcPort(getIpcPort());
-      this.nnAddr = isa;
+    BPOfferService(InetSocketAddress nnAddr, DataNode dn) {
+      this.dn = dn;
+      this.bpRegistration = dn.createRegistration();
+      this.nnAddr = nnAddr;
+      this.dnConf = dn.getDnConf();
-      blockPoolManager.addBlockPool(this);
-      while (shouldRun && shouldServiceRun) {
+      while (dn.shouldRun && shouldServiceRun) {
-    void setupBP(Configuration conf, AbstractList<File> dataDirs) 
+    void setupBP(Configuration conf) 
-      synchronized(DataNode.this) {
-        // we do not allow namenode from different cluster to register
-        if(clusterId != null && !clusterId.equals(nsInfo.clusterID)) {
-          throw new IOException(
-              "cannot register with the namenode because clusterid do not match:"
-              + " nn=" + nsInfo.getBlockPoolID() + "; nn cid=" + nsInfo.clusterID + 
-              ";dn cid=" + clusterId);
-        }
-
-        setupBPStorage();
-
-        setClusterId(nsInfo.clusterID);
-      }
-    
-      initPeriodicScanners(conf);
-    }
-    
-    void setupBPStorage() throws IOException {
-      StartupOption startOpt = getStartupOption(conf);
-      assert startOpt != null : "Startup option must be set.";
-
-      boolean simulatedFSDataset = conf.getBoolean(
-          DFS_DATANODE_SIMULATEDDATASTORAGE_KEY,
-          DFS_DATANODE_SIMULATEDDATASTORAGE_DEFAULT);
+      dn.initBlockPool(this, nsInfo);
-      if (simulatedFSDataset) {
-        initFsDataSet(conf, dataDirs);
-        bpRegistration.setStorageID(getStorageId()); //same as DN
+      bpRegistration.setStorageID(dn.getStorageId());
+      StorageInfo storageInfo = dn.storage.getBPStorage(blockPoolId);
+      if (storageInfo == null) {
+        // it's null in the case of SimulatedDataSet
-        bpRegistration.storageInfo.namespaceID = bpNSInfo.namespaceID;
-        bpRegistration.storageInfo.clusterID = bpNSInfo.clusterID;
+        bpRegistration.setStorageInfo(nsInfo);
-        // read storage info, lock data dirs and transition fs state if necessary          
-        storage.recoverTransitionRead(DataNode.this, blockPoolId, bpNSInfo,
-            dataDirs, startOpt);
-        LOG.info("setting up storage: nsid=" + storage.namespaceID + ";bpid="
-            + blockPoolId + ";lv=" + storage.layoutVersion + ";nsInfo="
-            + bpNSInfo);
-
-        bpRegistration.setStorageID(getStorageId());
-        bpRegistration.setStorageInfo(storage.getBPStorage(blockPoolId));
-        initFsDataSet(conf, dataDirs);
+        bpRegistration.setStorageInfo(storageInfo);
-      data.addBlockPool(blockPoolId, conf);
-
+    
-        - ( blockReportInterval - DFSUtil.getRandom().nextInt((int)(delay)));
+        - ( dnConf.blockReportInterval - DFSUtil.getRandom().nextInt((int)(delay)));
-        lastBlockReport = lastHeartbeat - blockReportInterval;
+        lastBlockReport = lastHeartbeat - dnConf.blockReportInterval;
-      if (startTime - lastBlockReport > blockReportInterval) {
+      if (startTime - lastBlockReport > dnConf.blockReportInterval) {
-        BlockListAsLongs bReport = data.getBlockReport(blockPoolId);
+        BlockListAsLongs bReport = dn.data.getBlockReport(blockPoolId);
-        metrics.addBlockReport(brSendCost);
+        dn.metrics.addBlockReport(brSendCost);
-          lastBlockReport = startTime - DFSUtil.getRandom().nextInt((int)(blockReportInterval));
+          lastBlockReport = startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));
-          blockReportInterval * blockReportInterval;
+          dnConf.blockReportInterval * dnConf.blockReportInterval;
-          data.getCapacity(),
-          data.getDfsUsed(),
-          data.getRemaining(),
-          data.getBlockPoolUsed(blockPoolId),
-          xmitsInProgress.get(),
-          getXceiverCount(), data.getNumFailedVolumes());
+          dn.data.getCapacity(),
+          dn.data.getDfsUsed(),
+          dn.data.getRemaining(),
+          dn.data.getBlockPoolUsed(blockPoolId),
+          dn.xmitsInProgress.get(),
+          dn.getXceiverCount(), dn.data.getNumFailedVolumes());
-      
-      blockPoolManager.remove(this);
-      if (blockScanner != null) {
-        blockScanner.removeBlockPool(this.getBlockPoolId());
-      }
-    
-      if (data != null) { 
-        data.shutdownBlockPool(this.getBlockPoolId());
-      }
-
-      if (storage != null) {
-        storage.removeBlockPoolStorage(this.getBlockPoolId());
-      }
+      dn.shutdownBlockPool(this);
-          + deleteReportInterval + " msec " + " BLOCKREPORT_INTERVAL of "
-          + blockReportInterval + "msec" + " Initial delay: "
-          + initialBlockReportDelay + "msec" + "; heartBeatInterval="
-          + heartBeatInterval);
+          + dnConf.deleteReportInterval + " msec " + " BLOCKREPORT_INTERVAL of "
+          + dnConf.blockReportInterval + "msec" + " Initial delay: "
+          + dnConf.initialBlockReportDelay + "msec" + "; heartBeatInterval="
+          + dnConf.heartBeatInterval);
-      while (shouldRun && shouldServiceRun) {
+      while (dn.shouldRun && shouldServiceRun) {
-          if (startTime - lastHeartbeat > heartBeatInterval) {
+          if (startTime - lastHeartbeat > dnConf.heartBeatInterval) {
-            if (!heartbeatsDisabledForTests) {
+            if (!dn.heartbeatsDisabledForTests) {
-              metrics.addHeartbeat(now() - startTime);
+              dn.metrics.addHeartbeat(now() - startTime);
-              || (startTime - lastDeletedReport > deleteReportInterval)) {
+              || (startTime - lastDeletedReport > dnConf.deleteReportInterval)) {
-          if (blockScanner != null) {
-            blockScanner.addBlockPool(this.blockPoolId);
+          if (dn.blockScanner != null) {
+            dn.blockScanner.addBlockPool(this.blockPoolId);
-          long waitTime = heartBeatInterval - 
+          long waitTime = dnConf.heartBeatInterval - 
-            long sleepTime = Math.min(1000, heartBeatInterval);
+            long sleepTime = Math.min(1000, dnConf.heartBeatInterval);
-      while(shouldRun && shouldServiceRun) {
+      while(dn.shouldRun && shouldServiceRun) {
-          NetUtils.getHostname();
-          hostName = bpRegistration.getHost();
-
-      if (storage.getStorageID().equals("")) {
-        storage.setStorageID(bpRegistration.getStorageID());
-        storage.writeAll();
-        LOG.info("New storage id " + bpRegistration.getStorageID()
-            + " is assigned to data-node " + bpRegistration.getName());
-      } else if(!storage.getStorageID().equals(bpRegistration.getStorageID())) {
-        throw new IOException("Inconsistent storage IDs. Name-node returned "
-            + bpRegistration.getStorageID() 
-            + ". Expecting " + storage.getStorageID());
-      }
-
-      if (!isBlockTokenInitialized) {
-        /* first time registering with NN */
-        ExportedBlockKeys keys = bpRegistration.exportedKeys;
-        isBlockTokenEnabled = keys.isBlockTokenEnabled();
-        if (isBlockTokenEnabled) {
-          long blockKeyUpdateInterval = keys.getKeyUpdateInterval();
-          long blockTokenLifetime = keys.getTokenLifetime();
-          LOG.info("Block token params received from NN: for block pool " +
-              blockPoolId + " keyUpdateInterval="
-              + blockKeyUpdateInterval / (60 * 1000)
-              + " min(s), tokenLifetime=" + blockTokenLifetime / (60 * 1000)
-              + " min(s)");
-          final BlockTokenSecretManager secretMgr = 
-            new BlockTokenSecretManager(false, 0, blockTokenLifetime);
-          blockPoolTokenSecretManager.addBlockPool(blockPoolId, secretMgr);
-        }
-        isBlockTokenInitialized = true;
-      }
-
-      if (isBlockTokenEnabled) {
-        blockPoolTokenSecretManager.setKeys(blockPoolId,
-            bpRegistration.exportedKeys);
-        bpRegistration.exportedKeys = ExportedBlockKeys.DUMMY_KEYS;
-      }
+      
+      dn.bpRegistrationSucceeded(bpRegistration, blockPoolId);
-      scheduleBlockReport(initialBlockReportDelay);
+      scheduleBlockReport(dnConf.initialBlockReportDelay);
-      LOG.info(bpRegistration + "In BPOfferService.run, data = " + data
+      LOG.info(bpRegistration + "In BPOfferService.run, data = " + dn.data
-          setupBP(conf, dataDirs);
+          setupBP(dn.conf);
-        while (shouldRun && shouldServiceRun) {
+        while (dn.shouldRun && shouldServiceRun) {
-            if (shouldRun && shouldServiceRun) {
+            if (dn.shouldRun && shouldServiceRun) {
-            + blockPoolId);
+            + blockPoolId + " thread " + Thread.currentThread().getId());
-        transferBlocks(bcmd.getBlockPoolId(), bcmd.getBlocks(), bcmd.getTargets());
-        metrics.incrBlocksReplicated(bcmd.getBlocks().length);
+        dn.transferBlocks(bcmd.getBlockPoolId(), bcmd.getBlocks(), bcmd.getTargets());
+        dn.metrics.incrBlocksReplicated(bcmd.getBlocks().length);
-          if (blockScanner != null) {
-            blockScanner.deleteBlocks(bcmd.getBlockPoolId(), toDelete);
+          if (dn.blockScanner != null) {
+            dn.blockScanner.deleteBlocks(bcmd.getBlockPoolId(), toDelete);
-          data.invalidate(bcmd.getBlockPoolId(), toDelete);
+          dn.data.invalidate(bcmd.getBlockPoolId(), toDelete);
-          checkDiskError();
+          dn.checkDiskError();
-        metrics.incrBlocksRemoved(toDelete.length);
+        dn.metrics.incrBlocksRemoved(toDelete.length);
-        if (shouldRun && shouldServiceRun) {
+        if (dn.shouldRun && shouldServiceRun) {
-        storage.finalizeUpgrade(((FinalizeCommand) cmd)
+        dn.storage.finalizeUpgrade(((FinalizeCommand) cmd)
-        recoverBlocks(((BlockRecoveryCommand)cmd).getRecoveringBlocks());
+        dn.recoverBlocks(((BlockRecoveryCommand)cmd).getRecoveringBlocks());
-        if (isBlockTokenEnabled) {
-          blockPoolTokenSecretManager.setKeys(blockPoolId, 
+        if (dn.isBlockTokenEnabled) {
+          dn.blockPoolTokenSecretManager.setKeys(blockPoolId, 
-                       (DataXceiverServer) dataXceiverServer.getRunnable();
+                       (DataXceiverServer) dn.dataXceiverServer.getRunnable();
-          new UpgradeManagerDatanode(DataNode.this, blockPoolId);
+          new UpgradeManagerDatanode(dn, blockPoolId);
+    this.dnConf = new DNConf(conf);
-    initConfig(conf);
+  /**
+   * Check that the registration returned from a NameNode is consistent
+   * with the information in the storage. If the storage is fresh/unformatted,
+   * sets the storage ID based on this registration.
+   * Also updates the block pool's state in the secret manager.
+   */
+  private synchronized void bpRegistrationSucceeded(DatanodeRegistration bpRegistration,
+      String blockPoolId)
+      throws IOException {
+    hostName = bpRegistration.getHost();
+
+    if (storage.getStorageID().equals("")) {
+      // This is a fresh datanode -- take the storage ID provided by the
+      // NN and persist it.
+      storage.setStorageID(bpRegistration.getStorageID());
+      storage.writeAll();
+      LOG.info("New storage id " + bpRegistration.getStorageID()
+          + " is assigned to data-node " + bpRegistration.getName());
+    } else if(!storage.getStorageID().equals(bpRegistration.getStorageID())) {
+      throw new IOException("Inconsistent storage IDs. Name-node returned "
+          + bpRegistration.getStorageID() 
+          + ". Expecting " + storage.getStorageID());
+    }
+    
+    registerBlockPoolWithSecretManager(bpRegistration, blockPoolId);
+  }
+  
+  /**
+   * After the block pool has contacted the NN, registers that block pool
+   * with the secret manager, updating it with the secrets provided by the NN.
+   * @param bpRegistration
+   * @param blockPoolId
+   * @throws IOException
+   */
+  private void registerBlockPoolWithSecretManager(DatanodeRegistration bpRegistration,
+      String blockPoolId) throws IOException {
+    ExportedBlockKeys keys = bpRegistration.exportedKeys;
+    isBlockTokenEnabled = keys.isBlockTokenEnabled();
+    // TODO should we check that all federated nns are either enabled or
+    // disabled?
+    if (!isBlockTokenEnabled) return;
+    
+    if (!blockPoolTokenSecretManager.isBlockPoolRegistered(blockPoolId)) {
+      long blockKeyUpdateInterval = keys.getKeyUpdateInterval();
+      long blockTokenLifetime = keys.getTokenLifetime();
+      LOG.info("Block token params received from NN: for block pool " +
+          blockPoolId + " keyUpdateInterval="
+          + blockKeyUpdateInterval / (60 * 1000)
+          + " min(s), tokenLifetime=" + blockTokenLifetime / (60 * 1000)
+          + " min(s)");
+      final BlockTokenSecretManager secretMgr = 
+        new BlockTokenSecretManager(false, 0, blockTokenLifetime);
+      blockPoolTokenSecretManager.addBlockPool(blockPoolId, secretMgr);
+    }
+    
+    blockPoolTokenSecretManager.setKeys(blockPoolId,
+        bpRegistration.exportedKeys);
+    bpRegistration.exportedKeys = ExportedBlockKeys.DUMMY_KEYS;
+  }
+
+  /**
+   * Remove the given block pool from the block scanner, dataset, and storage.
+   */
+  private void shutdownBlockPool(BPOfferService bpos) {
+    blockPoolManager.remove(bpos);
+
+    String bpId = bpos.getBlockPoolId();
+    if (blockScanner != null) {
+      blockScanner.removeBlockPool(bpId);
+    }
+  
+    if (data != null) { 
+      data.shutdownBlockPool(bpId);
+    }
+
+    if (storage != null) {
+      storage.removeBlockPoolStorage(bpId);
+    }
+  }
+
+  void initBlockPool(BPOfferService bpOfferService,
+      NamespaceInfo nsInfo) throws IOException {
+    String blockPoolId = nsInfo.getBlockPoolID();
+
+    blockPoolManager.addBlockPool(bpOfferService);
+
+    synchronized (this) {
+      // we do not allow namenode from different cluster to register
+      if(clusterId != null && !clusterId.equals(nsInfo.clusterID)) {
+        throw new IOException(
+            "cannot register with the namenode because clusterid do not match:"
+            + " nn=" + nsInfo.getBlockPoolID() + "; nn cid=" + nsInfo.clusterID + 
+            ";dn cid=" + clusterId);
+      }
+
+      setClusterId(nsInfo.clusterID);
+    }
+    
+    StartupOption startOpt = getStartupOption(conf);
+    assert startOpt != null : "Startup option must be set.";
+
+    boolean simulatedFSDataset = conf.getBoolean(
+        DFS_DATANODE_SIMULATEDDATASTORAGE_KEY,
+        DFS_DATANODE_SIMULATEDDATASTORAGE_DEFAULT);
+    
+    if (!simulatedFSDataset) {
+      // read storage info, lock data dirs and transition fs state if necessary          
+      storage.recoverTransitionRead(DataNode.this, blockPoolId, nsInfo,
+          dataDirs, startOpt);
+      StorageInfo bpStorage = storage.getBPStorage(blockPoolId);
+      LOG.info("setting up storage: nsid=" +
+          bpStorage.getNamespaceID() + ";bpid="
+          + blockPoolId + ";lv=" + storage.getLayoutVersion() +
+          ";nsInfo=" + nsInfo);
+    }
+    initFsDataSet();
+    initPeriodicScanners(conf);    
+    data.addBlockPool(nsInfo.getBlockPoolID(), conf);
+  }
+
+  private DatanodeRegistration createRegistration() {
+    DatanodeRegistration reg = new DatanodeRegistration(getMachineName());
+    reg.setInfoPort(infoServer.getPort());
+    reg.setIpcPort(getIpcPort());
+    return reg;
+  }
+
-  private synchronized void initFsDataSet(Configuration conf,
-      AbstractList<File> dataDirs) throws IOException {
+  private synchronized void initFsDataSet() throws IOException {
-    return (socketWriteTimeout > 0) ? 
+    return (dnConf.socketWriteTimeout > 0) ? 
-        NetUtils.connect(sock, curTarget, socketTimeout);
-        sock.setSoTimeout(targets.length * socketTimeout);
+        NetUtils.connect(sock, curTarget, dnConf.socketTimeout);
+        sock.setSoTimeout(targets.length * dnConf.socketTimeout);
-        long writeTimeout = socketWriteTimeout + 
+        long writeTimeout = dnConf.socketWriteTimeout + 
-                socketTimeout);
+                dnConf.socketTimeout);
-
-  long getReadaheadLength() {
-    return readaheadLength;
-  }
-
-  boolean shouldDropCacheBehindWrites() {
-    return dropCacheBehindWrites;
-  }
-
-  boolean shouldDropCacheBehindReads() {
-    return dropCacheBehindReads;
-  }
-  boolean shouldSyncBehindWrites() {
-    return syncBehindWrites;
+  DNConf getDnConf() {
+    return dnConf;
