YARN-6100. Improve YARN webservice to output aggregated container logs. Contributed by Xuan Gong.

+import java.nio.charset.Charset;
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState;
+import org.apache.hadoop.yarn.util.Times;
+  private static final Log LOG = LogFactory.getLog(NMWebServices.class);
-  public Response getLogs(@PathParam("containerid") String containerIdStr,
+  public Response getLogs(
+      @PathParam("containerid") final String containerIdStr,
-    ContainerId containerId;
+    ContainerId tempContainerId;
-      containerId = ContainerId.fromString(containerIdStr);
+      tempContainerId = ContainerId.fromString(containerIdStr);
-    
+    final ContainerId containerId = tempContainerId;
+    boolean tempIsRunning = false;
+    // check what is the status for container
+    try {
+      Container container = nmContext.getContainers().get(containerId);
+      tempIsRunning = (container.getContainerState() == ContainerState.RUNNING);
+    } catch (Exception ex) {
+      // This NM does not have this container any more. We
+      // assume the container has already finished.
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Can not find the container:" + containerId
+            + " in this node.");
+      }
+    }
+    final boolean isRunning = tempIsRunning;
+    final String lastModifiedTime = Times.format(logFile.lastModified());
+    final String outputFileName = filename;
-            long toSkip = 0;
-            long totalBytesToRead = fileLength;
-            long skipAfterRead = 0;
-            if (bytes < 0) {
-              long absBytes = Math.abs(bytes);
-              if (absBytes < fileLength) {
-                toSkip = fileLength - absBytes;
-                totalBytesToRead = absBytes;
-              }
-              org.apache.hadoop.io.IOUtils.skipFully(fis, toSkip);
+            LogToolUtils.outputContainerLog(containerId.toString(),
+                nmContext.getNodeId().toString(), outputFileName, fileLength,
+                bytes, lastModifiedTime, fis, os, buf, ContainerLogType.LOCAL);
+            StringBuilder sb = new StringBuilder();
+            String endOfFile = "End of LogFile:" + outputFileName;
+            sb.append(endOfFile + ".");
+            if (isRunning) {
+              sb.append("This log file belongs to a running container ("
+                  + containerIdStr + ") and so may not be complete." + "\n");
-              if (bytes < fileLength) {
-                totalBytesToRead = bytes;
-                skipAfterRead = fileLength - bytes;
+              sb.append("\n");
+            }
+            sb.append(StringUtils.repeat("*", endOfFile.length() + 50)
+                + "\n\n");
+            os.write(sb.toString().getBytes(Charset.forName("UTF-8")));
+            // If we have aggregated logs for this container,
+            // output the aggregation logs as well.
+            ApplicationId appId = containerId.getApplicationAttemptId()
+                .getApplicationId();
+            Application app = nmContext.getApplications().get(appId);
+            String appOwner = app == null ? null : app.getUser();
+            try {
+              LogToolUtils.outputAggregatedContainerLog(nmContext.getConf(),
+                  appId, appOwner, containerId.toString(),
+                  nmContext.getNodeId().toString(), outputFileName, bytes,
+                  os, buf);
+            } catch (Exception ex) {
+              // Something wrong when we try to access the aggregated log.
+              if (LOG.isDebugEnabled()) {
+                LOG.debug("Can not access the aggregated log for "
+                    + "the container:" + containerId);
+                LOG.debug(ex.getMessage());
-
-            long curRead = 0;
-            long pendingRead = totalBytesToRead - curRead;
-            int toRead = pendingRead > buf.length ? buf.length
-                : (int) pendingRead;
-            int len = fis.read(buf, 0, toRead);
-            while (len != -1 && curRead < totalBytesToRead) {
-              os.write(buf, 0, len);
-              curRead += len;
-
-              pendingRead = totalBytesToRead - curRead;
-              toRead = pendingRead > buf.length ? buf.length
-                  : (int) pendingRead;
-              len = fis.read(buf, 0, toRead);
-            }
-            org.apache.hadoop.io.IOUtils.skipFully(fis, skipAfterRead);
-            os.flush();
