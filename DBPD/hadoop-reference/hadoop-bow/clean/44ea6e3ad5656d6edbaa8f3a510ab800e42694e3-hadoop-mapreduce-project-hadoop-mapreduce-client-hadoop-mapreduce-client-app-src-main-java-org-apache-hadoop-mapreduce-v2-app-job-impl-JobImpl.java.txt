Back out revision 1503499 for MAPREDUCE-5317. Stale files left behind for failed jobs


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1503744 13f79535-47bb-0310-9956-ffa450edef68

-import java.util.concurrent.ScheduledFuture;
-import java.util.concurrent.ScheduledThreadPoolExecutor;
-import java.util.concurrent.TimeUnit;
-                  JobStateInternal.COMMITTING, JobStateInternal.FAIL_WAIT,
-                  JobStateInternal.FAIL_ABORT),
+                  JobStateInternal.COMMITTING, JobStateInternal.FAIL_ABORT),
-          // Transitions from FAIL_WAIT state
-          .addTransition(JobStateInternal.FAIL_WAIT,
-              JobStateInternal.FAIL_WAIT,
-              JobEventType.JOB_DIAGNOSTIC_UPDATE,
-              DIAGNOSTIC_UPDATE_TRANSITION)
-          .addTransition(JobStateInternal.FAIL_WAIT,
-              JobStateInternal.FAIL_WAIT,
-              JobEventType.JOB_COUNTER_UPDATE, COUNTER_UPDATE_TRANSITION)
-          .addTransition(JobStateInternal.FAIL_WAIT,
-              EnumSet.of(JobStateInternal.FAIL_WAIT, JobStateInternal.FAIL_ABORT),
-              JobEventType.JOB_TASK_COMPLETED, 
-              new JobFailWaitTransition())
-          .addTransition(JobStateInternal.FAIL_WAIT,
-              JobStateInternal.FAIL_ABORT, JobEventType.JOB_FAIL_WAIT_TIMEDOUT, 
-              new JobFailWaitTimedOutTransition())
-          .addTransition(JobStateInternal.FAIL_WAIT, JobStateInternal.KILLED,
-              JobEventType.JOB_KILL,
-              new KilledDuringAbortTransition())
-          .addTransition(JobStateInternal.FAIL_WAIT,
-              JobStateInternal.ERROR, JobEventType.INTERNAL_ERROR,
-              INTERNAL_ERROR_TRANSITION)
-          // Ignore-able events
-          .addTransition(JobStateInternal.FAIL_WAIT,
-              JobStateInternal.FAIL_WAIT,
-              EnumSet.of(JobEventType.JOB_UPDATED_NODES,
-                  JobEventType.JOB_TASK_ATTEMPT_COMPLETED,
-                  JobEventType.JOB_MAP_TASK_RESCHEDULED,
-                  JobEventType.JOB_TASK_ATTEMPT_FETCH_FAILURE,
-                  JobEventType.JOB_AM_REBOOT))
-
-          //Transitions from FAIL_ABORT state
+          // Transitions from FAIL_ABORT state
-                  JobEventType.JOB_AM_REBOOT,
-                  JobEventType.JOB_FAIL_WAIT_TIMEDOUT))
+                  JobEventType.JOB_AM_REBOOT))
-  //Executor used for running future tasks. Setting thread pool size to 1
-  private ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(1);
-  private ScheduledFuture failWaitTriggerScheduledFuture;
-
-    case FAIL_WAIT:
-
-  //This transition happens when a job is to be failed. It waits for all the
-  //tasks to finish / be killed.
-  private static class JobFailWaitTransition
-  implements MultipleArcTransition<JobImpl, JobEvent, JobStateInternal> {
-    @Override
-    public JobStateInternal transition(JobImpl job, JobEvent event) {
-      if(!job.failWaitTriggerScheduledFuture.isCancelled()) {
-        for(Task task: job.tasks.values()) {
-          if(!task.isFinished()) {
-            return JobStateInternal.FAIL_WAIT;
-          }
-        }
-      }
-      //Finished waiting. All tasks finished / were killed
-      job.failWaitTriggerScheduledFuture.cancel(false);
-      job.eventHandler.handle(new CommitterJobAbortEvent(job.jobId,
-        job.jobContext, org.apache.hadoop.mapreduce.JobStatus.State.FAILED));
-      return JobStateInternal.FAIL_ABORT;
-    }
-  }
-
-  //This transition happens when a job to be failed times out while waiting on
-  //tasks that had been sent the KILL signal. It is triggered by a
-  //ScheduledFuture task queued in the executor.
-  private static class JobFailWaitTimedOutTransition
-  implements SingleArcTransition<JobImpl, JobEvent> {
-    @Override
-    public void transition(JobImpl job, JobEvent event) {
-      LOG.info("Timeout expired in FAIL_WAIT waiting for tasks to get killed."
-        + " Going to fail job anyway");
-      job.failWaitTriggerScheduledFuture.cancel(false);
-      job.eventHandler.handle(new CommitterJobAbortEvent(job.jobId,
-        job.jobContext, org.apache.hadoop.mapreduce.JobStatus.State.FAILED));
-    }
-  }
-
+    
-    //This class is used to queue a ScheduledFuture to send an event to a job
-    //after some delay. This can be used to wait for maximum amount of time
-    //before proceeding anyway. e.g. When a job is waiting in FAIL_WAIT for
-    //all tasks to be killed.
-    static class TriggerScheduledFuture implements Runnable {
-      JobEvent toSend;
-      JobImpl job;
-      TriggerScheduledFuture(JobImpl job, JobEvent toSend) {
-        this.toSend = toSend;
-        this.job = job;
-      }
-      public void run() {
-        LOG.info("Sending event " + toSend + " to " + job.getID());
-        job.getEventHandler().handle(toSend);
-      }
-    }
-
-
-        //Send kill signal to all unfinished tasks here.
-        boolean allDone = true;
-        for (Task task : job.tasks.values()) {
-          if(!task.isFinished()) {
-            allDone = false;
-            job.eventHandler.handle(
-              new TaskEvent(task.getID(), TaskEventType.T_KILL));
-          }
-        }
-
-        //If all tasks are already done, we should go directly to FAIL_ABORT
-        if(allDone) {
-          return JobStateInternal.FAIL_ABORT;
-        }
-
-        //Set max timeout to wait for the tasks to get killed
-        job.failWaitTriggerScheduledFuture = job.executor.schedule(
-          new TriggerScheduledFuture(job, new JobEvent(job.getID(),
-            JobEventType.JOB_FAIL_WAIT_TIMEDOUT)), job.conf.getInt(
-                MRJobConfig.MR_AM_COMMITTER_CANCEL_TIMEOUT_MS,
-                MRJobConfig.DEFAULT_MR_AM_COMMITTER_CANCEL_TIMEOUT_MS),
-                TimeUnit.MILLISECONDS);
-        return JobStateInternal.FAIL_WAIT;
+        job.eventHandler.handle(new CommitterJobAbortEvent(job.jobId,
+            job.jobContext,
+            org.apache.hadoop.mapreduce.JobStatus.State.FAILED));
+        return JobStateInternal.FAIL_ABORT;
