HDDS-1312. Add more unit tests to verify BlockOutputStream functionalities. Contributed by Shashikant Banerjee.

+  private long offset;
+    offset = 0;
+  @VisibleForTesting
+  public int getRetryCount() {
+    return retryCount;
+  }
+          offset += writeLen;
-        writeLen = retry ? (int) len :
-            (int) (current.getWrittenDataLength() - currentPos);
+        int dataWritten  = (int) (current.getWrittenDataLength() - currentPos);
+        writeLen = retry ? (int) len : dataWritten;
+        // In retry path, the data written is already accounted in offset.
+        if (!retry) {
+          offset += writeLen;
+        }
+   * @param streamIndex index of the stream
-      PipelineID pipelineId) {
-    // currentStreamIndex < streamEntries.size() signifies that, there are still
+      PipelineID pipelineId, int streamIndex) {
+    // streamIndex < streamEntries.size() signifies that, there are still
-    if (currentStreamIndex < streamEntries.size()) {
+
+    // This will be called only to discard the next subsequent unused blocks
+    // in the sreamEntryList.
+    if (streamIndex < streamEntries.size()) {
-          streamEntries.listIterator(currentStreamIndex);
+          streamEntries.listIterator(streamIndex);
+        Preconditions.checkArgument(streamEntry.getCurrentPosition() == 0);
-            && streamEntry.getBlockID().getContainerID() == containerID))
-            && streamEntry.getCurrentPosition() == 0) {
+            && streamEntry.getBlockID().getContainerID() == containerID))) {
-    boolean retryFailure = checkForRetryFailure(exception);
+    boolean retryFailure = checkForRetryFailure(t);
-    Preconditions.checkArgument(
-        streamEntry.getWrittenDataLength() - totalSuccessfulFlushedData
-            == bufferedDataLen);
+    Preconditions.checkArgument(offset - getKeyLength() == bufferedDataLen);
-    if (checkIfContainerIsClosed(t)) {
+    if (closedContainerException) {
-    if (bufferedDataLen > 0) {
-      // If the data is still cached in the underlying stream, we need to
-      // allocate new block and write this data in the datanode.
-      currentStreamIndex += 1;
-      handleRetry(exception, bufferedDataLen);
-    }
-    if (totalSuccessfulFlushedData == 0) {
-      streamEntries.remove(streamIndex);
-      currentStreamIndex -= 1;
-    }
+    // discard all sunsequent blocks the containers and pipelines which
+    // are in the exclude list so that, the very next retry should never
+    // write data on the  closed container/pipeline
-          null);
+          null, streamIndex + 1);
-      discardPreallocatedBlocks(-1, pipelineId);
+      discardPreallocatedBlocks(-1, pipelineId, streamIndex + 1);
+    }
+    if (bufferedDataLen > 0) {
+      // If the data is still cached in the underlying stream, we need to
+      // allocate new block and write this data in the datanode.
+      currentStreamIndex += 1;
+      handleRetry(exception, bufferedDataLen);
+      // reset the retryCount after handling the exception
+      retryCount = 0;
+    }
+    if (totalSuccessfulFlushedData == 0) {
+      streamEntries.remove(streamIndex);
+      currentStreamIndex -= 1;
-        keyArgs.setDataSize(getKeyLength());
+        long length = getKeyLength();
+        Preconditions.checkArgument(offset == length);
+        keyArgs.setDataSize(length);
