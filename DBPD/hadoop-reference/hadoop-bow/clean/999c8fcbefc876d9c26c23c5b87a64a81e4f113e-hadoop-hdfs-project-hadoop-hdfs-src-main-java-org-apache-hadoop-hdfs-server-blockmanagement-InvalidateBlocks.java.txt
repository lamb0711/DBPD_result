HDFS-10999. Introduce separate stats for Replicated and Erasure Coded Blocks apart from the current Aggregated stats. (Manoj Govindassamy via lei)

+import java.util.HashSet;
+import java.util.Map.Entry;
+import java.util.concurrent.atomic.LongAdder;
-  /** Mapping: DatanodeInfo -> Collection of Blocks */
-  private final Map<DatanodeInfo, LightWeightHashSet<Block>> node2blocks =
-      new HashMap<DatanodeInfo, LightWeightHashSet<Block>>();
-  /** The total number of blocks in the map. */
-  private long numBlocks = 0L;
-
+  private final Map<DatanodeInfo, LightWeightHashSet<Block>>
+      nodeToBlocks = new HashMap<>();
+  private final Map<DatanodeInfo, LightWeightHashSet<Block>>
+      nodeToECBlockGroups = new HashMap<>();
+  private final LongAdder numBlocks = new LongAdder();
+  private final LongAdder numECBlockGroups = new LongAdder();
-  /** @return the number of blocks to be invalidated . */
-  synchronized long numBlocks() {
-    return numBlocks;
+  /**
+   * @return The total number of blocks to be invalidated.
+   */
+  long numBlocks() {
+    return getECBlockGroupsStat() + getBlocksStat();
+   * @return The total number of blocks of type
+   * {@link org.apache.hadoop.hdfs.protocol.BlockType#CONTIGUOUS}
+   * to be invalidated.
+   */
+  long getBlocksStat() {
+    return numBlocks.longValue();
+  }
+
+  /**
+   * @return The total number of blocks of type
+   * {@link org.apache.hadoop.hdfs.protocol.BlockType#STRIPED}
+   * to be invalidated.
+   */
+  long getECBlockGroupsStat() {
+    return numECBlockGroups.longValue();
+  }
+
+  private LightWeightHashSet<Block> getBlocksSet(final DatanodeInfo dn) {
+    if (nodeToBlocks.containsKey(dn)) {
+      return nodeToBlocks.get(dn);
+    }
+    return null;
+  }
+
+  private LightWeightHashSet<Block> getECBlockGroupsSet(final DatanodeInfo dn) {
+    if (nodeToECBlockGroups.containsKey(dn)) {
+      return nodeToECBlockGroups.get(dn);
+    }
+    return null;
+  }
+
+  private LightWeightHashSet<Block> getBlocksSet(final DatanodeInfo dn,
+      final Block block) {
+    if (BlockIdManager.isStripedBlockID(block.getBlockId())) {
+      return getECBlockGroupsSet(dn);
+    } else {
+      return getBlocksSet(dn);
+    }
+  }
+
+  private void putBlocksSet(final DatanodeInfo dn, final Block block,
+      final LightWeightHashSet set) {
+    if (BlockIdManager.isStripedBlockID(block.getBlockId())) {
+      assert getECBlockGroupsSet(dn) == null;
+      nodeToECBlockGroups.put(dn, set);
+    } else {
+      assert getBlocksSet(dn) == null;
+      nodeToBlocks.put(dn, set);
+    }
+  }
+
+  private long getBlockSetsSize(final DatanodeInfo dn) {
+    LightWeightHashSet<Block> replicaBlocks = getBlocksSet(dn);
+    LightWeightHashSet<Block> stripedBlocks = getECBlockGroupsSet(dn);
+    return ((replicaBlocks == null ? 0 : replicaBlocks.size()) +
+        (stripedBlocks == null ? 0 : stripedBlocks.size()));
+  }
+
+
+  /**
-    final LightWeightHashSet<Block> s = node2blocks.get(dn);
+    final LightWeightHashSet<Block> s = getBlocksSet(dn, block);
-   * Add a block to the block collection
-   * which will be invalidated on the specified datanode.
+   * Add a block to the block collection which will be
+   * invalidated on the specified datanode.
-    LightWeightHashSet<Block> set = node2blocks.get(datanode);
+    LightWeightHashSet<Block> set = getBlocksSet(datanode, block);
-      set = new LightWeightHashSet<Block>();
-      node2blocks.put(datanode, set);
+      set = new LightWeightHashSet<>();
+      putBlocksSet(datanode, block, set);
-      numBlocks++;
+      if (BlockIdManager.isStripedBlockID(block.getBlockId())) {
+        numECBlockGroups.increment();
+      } else {
+        numBlocks.increment();
+      }
-    final LightWeightHashSet<Block> blocks = node2blocks.remove(dn);
-    if (blocks != null) {
-      numBlocks -= blocks.size();
+    LightWeightHashSet<Block> replicaBlockSets = nodeToBlocks.remove(dn);
+    if (replicaBlockSets != null) {
+      numBlocks.add(replicaBlockSets.size() * -1);
+    }
+    LightWeightHashSet<Block> blockGroupSets = nodeToECBlockGroups.remove(dn);
+    if (blockGroupSets != null) {
+      numECBlockGroups.add(blockGroupSets.size() * -1);
-    final LightWeightHashSet<Block> v = node2blocks.get(dn);
+    final LightWeightHashSet<Block> v = getBlocksSet(dn, block);
-      numBlocks--;
-      if (v.isEmpty()) {
-        node2blocks.remove(dn);
+      if (BlockIdManager.isStripedBlockID(block.getBlockId())) {
+        numECBlockGroups.decrement();
+      } else {
+        numBlocks.decrement();
+      }
+      if (v.isEmpty() && getBlockSetsSize(dn) == 0) {
+        remove(dn);
-  /** Print the contents to out. */
-  synchronized void dump(final PrintWriter out) {
-    final int size = node2blocks.values().size();
-    out.println("Metasave: Blocks " + numBlocks 
-        + " waiting deletion from " + size + " datanodes.");
-    if (size == 0) {
-      return;
-    }
-
-    for(Map.Entry<DatanodeInfo, LightWeightHashSet<Block>> entry : node2blocks.entrySet()) {
+  private void dumpBlockSet(final Map<DatanodeInfo,
+      LightWeightHashSet<Block>> nodeToBlocksMap, final PrintWriter out) {
+    for(Entry<DatanodeInfo, LightWeightHashSet<Block>> entry :
+        nodeToBlocksMap.entrySet()) {
-      if (blocks.size() > 0) {
+      if (blocks != null && blocks.size() > 0) {
+  /** Print the contents to out. */
+  synchronized void dump(final PrintWriter out) {
+    final int size = nodeToBlocks.values().size() +
+        nodeToECBlockGroups.values().size();
+    out.println("Metasave: Blocks " + numBlocks()
+        + " waiting deletion from " + size + " datanodes.");
+    if (size == 0) {
+      return;
+    }
+    dumpBlockSet(nodeToBlocks, out);
+    dumpBlockSet(nodeToECBlockGroups, out);
+  }
-    return new ArrayList<DatanodeInfo>(node2blocks.keySet());
+    HashSet<DatanodeInfo> set = new HashSet<>();
+    set.addAll(nodeToBlocks.keySet());
+    set.addAll(nodeToECBlockGroups.keySet());
+    return new ArrayList<>(set);
+  /**
+   * Get blocks to invalidate by limit as blocks that can be sent in one
+   * message is limited.
+   * @return the remaining limit
+   */
+  private int getBlocksToInvalidateByLimit(LightWeightHashSet<Block> blockSet,
+      List<Block> toInvalidate, LongAdder statsAdder, int limit) {
+    assert blockSet != null;
+    int remainingLimit = limit;
+    List<Block> polledBlocks = blockSet.pollN(limit);
+    remainingLimit -= polledBlocks.size();
+    toInvalidate.addAll(polledBlocks);
+    statsAdder.add(polledBlocks.size() * -1);
+    return remainingLimit;
+  }
+
-    final LightWeightHashSet<Block> set = node2blocks.get(dn);
-    if (set == null) {
-      return null;
+
+    int remainingLimit = blockInvalidateLimit;
+    final List<Block> toInvalidate = new ArrayList<>();
+
+    if (nodeToBlocks.get(dn) != null) {
+      remainingLimit = getBlocksToInvalidateByLimit(nodeToBlocks.get(dn),
+          toInvalidate, numBlocks, remainingLimit);
-
-    // # blocks that can be sent in one message is limited
-    final int limit = blockInvalidateLimit;
-    final List<Block> toInvalidate = set.pollN(limit);
-
-    // If we send everything in this message, remove this node entry
-    if (set.isEmpty()) {
+    if ((remainingLimit > 0) && (nodeToECBlockGroups.get(dn) != null)) {
+      getBlocksToInvalidateByLimit(nodeToECBlockGroups.get(dn),
+          toInvalidate, numECBlockGroups, remainingLimit);
+    }
+    if (toInvalidate.size() > 0 && getBlockSetsSize(dn) == 0) {
-
-    numBlocks -= toInvalidate.size();
-    node2blocks.clear();
-    numBlocks = 0;
+    nodeToBlocks.clear();
+    nodeToECBlockGroups.clear();
+    numBlocks.reset();
+    numECBlockGroups.reset();
