MAPREDUCE-2365. Add counters to track bytes (read,written) via File(Input,Output)Format. Contributed by Siddharth Seth. 


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1146515 13f79535-47bb-0310-9956-ffa450edef68

-import org.apache.hadoop.fs.LocalDirAllocator;
+import org.apache.hadoop.fs.FileSystem.Statistics;
-import org.apache.hadoop.mapreduce.MRConfig;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter;
-    private Counters.Counter inputByteCounter;
+    private Counters.Counter fileInputByteCounter;
-    private long beforePos = -1;
-    private long afterPos = -1;
+    private long bytesInPrev = -1;
+    private long bytesInCurr = -1;
+    private final Statistics fsStats;
-    TrackedRecordReader(RecordReader<K,V> raw, TaskReporter reporter) 
+    TrackedRecordReader(TaskReporter reporter, JobConf job) 
-      rawIn = raw;
-      inputByteCounter = reporter.getCounter(
-                           FileInputFormat.COUNTER_GROUP,
-                           FileInputFormat.BYTES_READ);
+      fileInputByteCounter = reporter.getCounter(FileInputFormatCounter.BYTES_READ);
+      
+      Statistics matchedStats = null;
+      if (this.reporter.getInputSplit() instanceof FileSplit) {
+        matchedStats = getFsStatistics(((FileSplit) this.reporter
+            .getInputSplit()).getPath(), job);
+      }
+      fsStats = matchedStats;
+
+      bytesInPrev = getInputBytes(fsStats);
+      rawIn = job.getInputFormat().getRecordReader(reporter.getInputSplit(),
+          job, reporter);
+      bytesInCurr = getInputBytes(fsStats);
+      fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
-      inputByteCounter.increment(afterPos - beforePos);
-      beforePos = getPos();
+      bytesInPrev = getInputBytes(fsStats);
-      afterPos = getPos();
+      bytesInCurr = getInputBytes(fsStats);
+      fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
-    public void close() throws IOException { rawIn.close(); }
+
+    public void close() throws IOException {
+      bytesInPrev = getInputBytes(fsStats);
+      rawIn.close();
+      bytesInCurr = getInputBytes(fsStats);
+      fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
+    }
+
+
+    private long getInputBytes(Statistics stats) {
+      return stats == null ? 0 : stats.getBytesRead();
+    }
-    SkippingRecordReader(RecordReader<K,V> raw, TaskUmbilicalProtocol umbilical,
-                         TaskReporter reporter) throws IOException{
-      super(raw, reporter);
+    SkippingRecordReader(TaskUmbilicalProtocol umbilical,
+                         TaskReporter reporter, JobConf job) throws IOException{
+      super(reporter, job);
-    RecordReader<INKEY,INVALUE> rawIn =                  // open input
-      job.getInputFormat().getRecordReader(inputSplit, job, reporter);
-        new SkippingRecordReader<INKEY,INVALUE>(rawIn, umbilical, reporter) :
-        new TrackedRecordReader<INKEY,INVALUE>(rawIn, reporter);
+        new SkippingRecordReader<INKEY,INVALUE>(umbilical, reporter, job) :
+          new TrackedRecordReader<INKEY,INVALUE>(reporter, job);
+    private final org.apache.hadoop.mapreduce.Counter fileInputByteCounter;
+    private final Statistics fsStats;
-    NewTrackingRecordReader(org.apache.hadoop.mapreduce.RecordReader<K,V> real,
-                            TaskReporter reporter) {
-      this.real = real;
+    NewTrackingRecordReader(org.apache.hadoop.mapreduce.InputSplit split,
+        org.apache.hadoop.mapreduce.InputFormat<K, V> inputFormat,
+        TaskReporter reporter,
+        org.apache.hadoop.mapreduce.TaskAttemptContext taskContext)
+        throws InterruptedException, IOException {
-      this.inputRecordCounter = reporter.getCounter(TaskCounter.MAP_INPUT_RECORDS);
+      this.inputRecordCounter = reporter
+          .getCounter(TaskCounter.MAP_INPUT_RECORDS);
+      this.fileInputByteCounter = reporter
+          .getCounter(FileInputFormatCounter.BYTES_READ);
+
+      Statistics matchedStats = null;
+      if (split instanceof org.apache.hadoop.mapreduce.lib.input.FileSplit) {
+        matchedStats = getFsStatistics(((org.apache.hadoop.mapreduce.lib.input.FileSplit) split)
+            .getPath(), taskContext.getConfiguration());
+      }
+      fsStats = matchedStats;
+
+      long bytesInPrev = getInputBytes(fsStats);
+      this.real = inputFormat.createRecordReader(split, taskContext);
+      long bytesInCurr = getInputBytes(fsStats);
+      fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
+      long bytesInPrev = getInputBytes(fsStats);
+      long bytesInCurr = getInputBytes(fsStats);
+      fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
+      long bytesInPrev = getInputBytes(fsStats);
+      long bytesInCurr = getInputBytes(fsStats);
+      fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
+      long bytesInPrev = getInputBytes(fsStats);
+      long bytesInCurr = getInputBytes(fsStats);
+      fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
+
+    private long getInputBytes(Statistics stats) {
+      return stats == null ? 0 : stats.getBytesRead();
+    }
+    private final Counters.Counter fileOutputByteCounter; 
+    private final Statistics fsStats;
+      mapOutputRecordCounter = reporter
+          .getCounter(TaskCounter.MAP_OUTPUT_RECORDS);
+      fileOutputByteCounter = reporter
+          .getCounter(FileOutputFormatCounter.BYTES_WRITTEN);
+
+      Statistics matchedStats = null;
+      if (outputFormat instanceof org.apache.hadoop.mapreduce.lib.output.FileOutputFormat) {
+        matchedStats = getFsStatistics(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
+            .getOutputPath(taskContext), taskContext.getConfiguration());
+      }
+      fsStats = matchedStats;
+
+      long bytesOutPrev = getOutputBytes(fsStats);
-      mapOutputRecordCounter = 
-        reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);
+      long bytesOutCurr = getOutputBytes(fsStats);
+      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+      long bytesOutPrev = getOutputBytes(fsStats);
+      long bytesOutCurr = getOutputBytes(fsStats);
+      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+        long bytesOutPrev = getOutputBytes(fsStats);
+        long bytesOutCurr = getOutputBytes(fsStats);
+        fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+    
+    private long getOutputBytes(Statistics stats) {
+      return stats == null ? 0 : stats.getBytesWritten();
+    }
-          (inputFormat.createRecordReader(split, taskContext), reporter);
+        (split, inputFormat, reporter, taskContext);
+    private final Counters.Counter fileOutputByteCounter;
+    private final Statistics fsStats;
-      out = job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter);
-
+      OutputFormat<K, V> outputFormat = job.getOutputFormat();   
+      
+      fileOutputByteCounter = reporter
+          .getCounter(FileOutputFormatCounter.BYTES_WRITTEN);
+
+      Statistics matchedStats = null;
+      if (outputFormat instanceof FileOutputFormat) {
+        matchedStats = getFsStatistics(FileOutputFormat.getOutputPath(job), job);
+      }
+      fsStats = matchedStats;
+
+      long bytesOutPrev = getOutputBytes(fsStats);
+      out = job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter);
+      long bytesOutCurr = getOutputBytes(fsStats);
+      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+        long bytesOutPrev = getOutputBytes(fsStats);
+        long bytesOutCurr = getOutputBytes(fsStats);
+        fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+      long bytesOutPrev = getOutputBytes(fsStats);
+      long bytesOutCurr = getOutputBytes(fsStats);
+      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
-    
+
+    private long getOutputBytes(Statistics stats) {
+      return stats == null ? 0 : stats.getBytesWritten();
+    }
+    final Counters.Counter fileOutputByteCounter;
+      fileOutputByteCounter = reporter
+          .getCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES);
+      Path outputPath = mapOutputFile.getOutputFile();
+      fileOutputByteCounter.increment(rfs.getFileStatus(outputPath).getLen());
