HDFS-13958. Miscellaneous Improvements for FsVolumeSpi. Contributed by BELUGA BEHR.

+import java.util.Collection;
-import java.util.LinkedList;
-import org.apache.hadoop.hdfs.server.datanode.FileIoProvider;
-import org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader;
-import org.apache.hadoop.hdfs.server.datanode.checker.VolumeCheckResult;
-import org.apache.hadoop.hdfs.server.datanode.fsdataset.DataNodeVolumeMetrics;
-import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;
-import org.apache.hadoop.hdfs.server.datanode.FinalizedReplica;
-import org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten;
-import org.apache.hadoop.hdfs.server.datanode.DataStorage;
-import org.apache.hadoop.hdfs.server.datanode.DatanodeUtil;
-import org.apache.hadoop.hdfs.server.datanode.LocalReplica;
-import org.apache.hadoop.hdfs.server.datanode.ReplicaInfo;
-import org.apache.hadoop.util.DataChecksum;
-import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;
-import org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder;
-import org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline;
-import org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline;
-import org.apache.hadoop.hdfs.server.datanode.StorageLocation;
-import org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.BlockDirFilter;
+import org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader;
+import org.apache.hadoop.hdfs.server.datanode.DataStorage;
+import org.apache.hadoop.hdfs.server.datanode.DatanodeUtil;
+import org.apache.hadoop.hdfs.server.datanode.FileIoProvider;
+import org.apache.hadoop.hdfs.server.datanode.FinalizedReplica;
+import org.apache.hadoop.hdfs.server.datanode.LocalReplica;
+import org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline;
+import org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten;
+import org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder;
+import org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline;
+import org.apache.hadoop.hdfs.server.datanode.ReplicaInfo;
+import org.apache.hadoop.hdfs.server.datanode.StorageLocation;
+import org.apache.hadoop.hdfs.server.datanode.checker.VolumeCheckResult;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.DataNodeVolumeMetrics;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;
+import org.apache.hadoop.util.DataChecksum;
+import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;
- * 
+ *
-      if (FsDatasetImpl.LOG.isDebugEnabled()) {
-        FsDatasetImpl.LOG.debug(String.format(
-            "The reference count for %s is %d, wait to be 0.",
-            this, reference.getReferenceCount()));
-      }
+      FsDatasetImpl.LOG.debug("The reference count for {} is {}, wait to be 0.",
+          this, reference.getReferenceCount());
-  
+
-    for(BlockPoolSlice s : bpSlices.values()) {
+    for (BlockPoolSlice s : bpSlices.values()) {
-  
+
-   * 
+   *
-    if (configuredCapacity < 0) {
+    if (configuredCapacity < 0L) {
-      return remaining > 0 ? remaining : 0;
+      return Math.max(remaining, 0L);
-
-  /*
+  /**
-   * for non-HDFS and space reserved for RBW
-   * 
+   * for non-HDFS and space reserved for RBW.
+   *
-    return (remaining > 0) ? remaining : 0;
+    return Math.max(remaining, 0L);
-    if (actualNonDfsUsed < actualReserved) {
-      return 0L;
-    }
-    return actualNonDfsUsed - actualReserved;
+    long nonDfsUsed = actualNonDfsUsed - actualReserved;
+    return Math.max(nonDfsUsed, 0L);
-        LOG.error("Unable to get disk statistics for volume " + this);
+        LOG.error("Unable to get disk statistics for volume {}", this, e);
-   * Make a deep copy of the list of currently active BPIDs
+   * Make a deep copy of the list of currently active BPIDs.
-    return bpSlices.keySet().toArray(new String[bpSlices.keySet().size()]);   
+    return bpSlices.keySet().toArray(new String[0]);
-    if (bytesToReserve != 0) {
+    if (bytesToReserve != 0L) {
-    if (bytesToRelease != 0) {
-
+    if (bytesToRelease != 0L) {
-        if (newReservation < 0) {
-          // Failsafe, this should never occur in practice, but if it does we
-          // don't want to start advertising more space than we have available.
-          newReservation = 0;
-        }
+
+        // Fail-safe, this should never be less than zero in practice, but if it
+        // does, do not advertise more space than is have available.
+        newReservation = Math.max(newReservation, 0L);
-      if (children.size() == 0) {
+      if (children.isEmpty()) {
-      if (nextSubDir == null) {
-        LOG.trace("getNextSubDir({}, {}): no more subdirectories found in {}",
-            storageID, bpid, dir.getAbsolutePath());
-      } else {
-        LOG.trace("getNextSubDir({}, {}): picking next subdirectory {} " +
-            "within {}", storageID, bpid, nextSubDir, dir.getAbsolutePath());
-      }
+      LOG.trace("getNextSubDir({}, {}): picking next subdirectory {} within {}",
+          storageID, bpid, nextSubDir, dir.getAbsolutePath());
-      if (entries.size() == 0) {
+      if (entries.isEmpty()) {
+        LOG.trace("getSubdirEntries({}, {}): no entries found in {}", storageID,
+            bpid, dir.getAbsolutePath());
-      }
-      if (entries == null) {
-        LOG.trace("getSubdirEntries({}, {}): no entries found in {}",
-            storageID, bpid, dir.getAbsolutePath());
-      } else {
-        LOG.trace("getSubdirEntries({}, {}): listed {} entries in {}", 
+        LOG.trace("getSubdirEntries({}, {}): listed {} entries in {}",
-      LOG.trace("load({}, {}): loaded iterator {} from {}: {}", storageID,
-          bpid, name, file.getAbsoluteFile(),
-          WRITER.writeValueAsString(state));
+      if (LOG.isTraceEnabled()) {
+        LOG.trace("load({}, {}): loaded iterator {} from {}: {}", storageID,
+            bpid, name, file.getAbsoluteFile(),
+            WRITER.writeValueAsString(state));
+      }
-    byte[] checksum = null;
+    final byte[] checksum;
-    if (replicaInfo.getState() == ReplicaState.FINALIZED) {
-      FinalizedReplica finalized = (FinalizedReplica)replicaInfo;
+    switch (replicaInfo.getState()) {
+    case FINALIZED:
+      FinalizedReplica finalized = (FinalizedReplica) replicaInfo;
-    } else if (replicaInfo.getState() == ReplicaState.RBW) {
-      ReplicaBeingWritten rbw = (ReplicaBeingWritten)replicaInfo;
+      break;
+    case RBW:
+      ReplicaBeingWritten rbw = (ReplicaBeingWritten) replicaInfo;
+      break;
+    default:
+      checksum = null;
+      break;
-    for(BlockPoolSlice s : bpSlices.values()) {
+    for (BlockPoolSlice s : bpSlices.values()) {
-    
+
-                    final RamDiskReplicaTracker ramDiskReplicaMap)
-      throws IOException {
-    for(BlockPoolSlice s : bpSlices.values()) {
+      final RamDiskReplicaTracker ramDiskReplicaMap) throws IOException {
+    for (BlockPoolSlice s : bpSlices.values()) {
-  
+
-                    final RamDiskReplicaTracker ramDiskReplicaMap)
-      throws IOException {
+      final RamDiskReplicaTracker ramDiskReplicaMap) throws IOException {
-    long numBlocks = 0;
+    long numBlocks = 0L;
-      bp = new BlockPoolSlice(bpid, this, bpdir, c, new Timer());
-    } else {
-      bp = new BlockPoolSlice(bpid, this, bpdir, c, timer);
+      timer = new Timer();
+    bp = new BlockPoolSlice(bpid, this, bpdir, c, timer);
-  
+
-  
+
-    File tmpDir = new File(bpDir, DataStorage.STORAGE_DIR_TMP); 
+    File tmpDir = new File(bpDir, DataStorage.STORAGE_DIR_TMP);
-  
+
-  
+
-
-  public LinkedList<ScanInfo> compileReport(String bpid,
-      LinkedList<ScanInfo> report, ReportCompiler reportCompiler)
-      throws InterruptedException, IOException {
-    return compileReport(getFinalizedDir(bpid),
-        getFinalizedDir(bpid), report, reportCompiler);
+  public void compileReport(String bpid, Collection<ScanInfo> report,
+      ReportCompiler reportCompiler) throws InterruptedException, IOException {
+    compileReport(getFinalizedDir(bpid), getFinalizedDir(bpid), report,
+        reportCompiler);
-  private LinkedList<ScanInfo> compileReport(File bpFinalizedDir,
-      File dir, LinkedList<ScanInfo> report, ReportCompiler reportCompiler)
-        throws InterruptedException {
+  /**
+   * Filter for block file names stored on the file system volumes.
+   */
+  public enum BlockDirFilter implements FilenameFilter {
+    INSTANCE;
+
+    @Override
+    public boolean accept(File dir, String name) {
+      return name.startsWith(DataStorage.BLOCK_SUBDIR_PREFIX)
+          || name.startsWith(DataStorage.STORAGE_DIR_FINALIZED)
+          || name.startsWith(Block.BLOCK_FILE_PREFIX);
+    }
+  }
+
+  private void compileReport(File bpFinalizedDir, File dir,
+      Collection<ScanInfo> report, ReportCompiler reportCompiler)
+      throws InterruptedException {
-      fileNames = fileIoProvider.listDirectory(
-          this, dir, BlockDirFilter.INSTANCE);
+      fileNames =
+          fileIoProvider.listDirectory(this, dir, BlockDirFilter.INSTANCE);
-      LOG.warn("Exception occurred while compiling report: ", ioe);
+      LOG.warn("Exception occurred while compiling report", ioe);
-      return report;
+      return;
-    return report;
