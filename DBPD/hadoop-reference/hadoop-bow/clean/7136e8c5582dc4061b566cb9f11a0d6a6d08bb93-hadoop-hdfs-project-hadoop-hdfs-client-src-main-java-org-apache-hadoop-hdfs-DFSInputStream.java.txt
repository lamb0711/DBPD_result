HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.

-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.UnresolvedLinkException;
+import javax.annotation.Nonnull;
+
- * DFSInputStream provides bytes from a named file.  It handles 
+ * DFSInputStream provides bytes from a named file.  It handles
-implements ByteBufferReadable, CanSetDropBehind, CanSetReadahead,
+    implements ByteBufferReadable, CanSetDropBehind, CanSetReadahead,
-   * 
+   *
-      extendedReadBuffers = new IdentityHashStore<ByteBuffer, Object>(0);
+      extendedReadBuffers = new IdentityHashStore<>(0);
-    
+
-    
+
-    
+
-  
+
-  /* XXX Use of CocurrentHashMap is temp fix. Need to fix 
+  /* XXX Use of CocurrentHashMap is temp fix. Need to fix
-             new ConcurrentHashMap<DatanodeInfo, DatanodeInfo>();
+             new ConcurrentHashMap<>();
-  
+
-      LocatedBlocks locatedBlocks) throws IOException, UnresolvedLinkException {
+      LocatedBlocks locatedBlocks) throws IOException {
-  void openInfo(boolean refreshLocatedBlocks) throws IOException,
-      UnresolvedLinkException {
+  void openInfo(boolean refreshLocatedBlocks) throws IOException {
-        lastBlockBeingWrittenLength = len; 
+        lastBlockBeingWrittenLength = len;
-    
+
-      
+
-        
+
-        
+
-          (((RemoteException) ioe).unwrapRemoteException() instanceof
-            ReplicaNotFoundException)) {
+            (((RemoteException) ioe).unwrapRemoteException() instanceof
+                ReplicaNotFoundException)) {
-        
+
-  
+
-   * Returns the block containing the target position. 
+   * Returns the block containing the target position.
-   * 
+   *
-        blocks = new ArrayList<LocatedBlock>(1);
+        blocks = new ArrayList<>(1);
-         blocks.add(locatedBlocks.getLastLocatedBlock());
+        blocks.add(locatedBlocks.getLastLocatedBlock());
-      List<LocatedBlock> blockRange = new ArrayList<LocatedBlock>();
+      List<LocatedBlock> blockRange = new ArrayList<>();
-    DatanodeInfo chosenNode = null;
+    DatanodeInfo chosenNode;
-    
+
-            + ", add to deadNodes and continue. " + ex, ex);
+              + ", add to deadNodes and continue. " + ex, ex);
-    public int doRead(BlockReader blockReader, int off, int len)
-        throws ChecksumException, IOException;
+    int doRead(BlockReader blockReader, int off, int len)
+        throws IOException;
-    public int copyFrom(ByteBuffer src, int offset, int length);
+    int copyFrom(ByteBuffer src, int offset, int length);
-  
+
-          throws ChecksumException, IOException {
+        throws IOException {
-        throws ChecksumException, IOException {
+        throws IOException {
-      } 
+      }
-   */ 
+   */
-    
+
-            + " at " + ce.getPos());        
+            + " at " + ce.getPos());
-      boolean sourceFound = false;
+      boolean sourceFound;
-         */ 
+         */
-    Map<ExtendedBlock,Set<DatanodeInfo>> corruptedBlockMap 
-      = new HashMap<ExtendedBlock, Set<DatanodeInfo>>();
+    Map<ExtendedBlock,Set<DatanodeInfo>> corruptedBlockMap = new HashMap<>();
-          
+
-          throw ce;            
+          throw ce;
-          reportCheckSumFailure(corruptedBlockMap, 
+          reportCheckSumFailure(corruptedBlockMap,
-  public synchronized int read(final byte buf[], int off, int len) throws IOException {
+  public synchronized int read(@Nonnull final byte buf[], int off, int len)
+      throws IOException {
-    TraceScope scope =
-        dfsClient.newPathTraceScope("DFSInputStream#byteArrayRead", src);
-    try {
+    try (TraceScope ignored =
+             dfsClient.newPathTraceScope("DFSInputStream#byteArrayRead", src)) {
-    } finally {
-      scope.close();
-    TraceScope scope =
-        dfsClient.newPathTraceScope("DFSInputStream#byteBufferRead", src);
-    try {
+    try (TraceScope ignored =
+             dfsClient.newPathTraceScope("DFSInputStream#byteBufferRead", src)){
-    } finally {
-      scope.close();
-    Set<DatanodeInfo> dnSet = null;
+    Set<DatanodeInfo> dnSet;
-      dnSet = new HashSet<DatanodeInfo>();
+      dnSet = new HashSet<>();
-          deadNodes, ignoredNodes);
+            deadNodes, ignoredNodes);
-          // expanded to 9000ms. 
+          // expanded to 9000ms.
-        } catch (InterruptedException iex) {
+        } catch (InterruptedException ignored) {
-        TraceScope scope = dfsClient.getTracer().
-            newScope("hedgedRead" + hedgedReadId, parentSpanId);
-        try {
+        try (TraceScope ignored = dfsClient.getTracer().
+            newScope("hedgedRead" + hedgedReadId, parentSpanId)) {
-        } finally {
-          scope.close();
-    ArrayList<Future<ByteBuffer>> futures = new ArrayList<Future<ByteBuffer>>();
+    ArrayList<Future<ByteBuffer>> futures = new ArrayList<>();
-        new ExecutorCompletionService<ByteBuffer>(
-        dfsClient.getHedgedReadsThreadPool());
-    ArrayList<DatanodeInfo> ignored = new ArrayList<DatanodeInfo>();
-    ByteBuffer bb = null;
+        new ExecutorCompletionService<>(dfsClient.getHedgedReadsThreadPool());
+    ArrayList<DatanodeInfo> ignored = new ArrayList<>();
+    ByteBuffer bb;
-          continue; // no need to refresh block locations
-        } catch (InterruptedException e) {
+          // continue; no need to refresh block locations
+        } catch (InterruptedException | ExecutionException e) {
-        } catch (ExecutionException e) {
-          // Ignore already logged in the call.
-    } catch (ExecutionException e) {
-      // already logged in the Callable
-      futures.remove(future);
-    } catch (CancellationException ce) {
+    } catch (ExecutionException | CancellationException e) {
-   * 
+   *
-   * 
+   *
-   * 
+   *
-    TraceScope scope = dfsClient.
-        newPathTraceScope("DFSInputStream#byteArrayPread", src);
-    try {
+    try (TraceScope ignored = dfsClient.
+        newPathTraceScope("DFSInputStream#byteArrayPread", src)) {
-    } finally {
-      scope.close();
-    
+
-    Map<ExtendedBlock,Set<DatanodeInfo>> corruptedBlockMap 
-      = new HashMap<ExtendedBlock, Set<DatanodeInfo>>();
+    Map<ExtendedBlock,Set<DatanodeInfo>> corruptedBlockMap = new HashMap<>();
-  
+
-   * namenode. 
+   * namenode.
-      Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap, 
+      Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap,
-            // most likely a bug. 
-            String errMsg = "BlockReader failed to seek to " + 
+            // most likely a bug.
+            String errMsg = "BlockReader failed to seek to " +
-  
+
-   * a node other than the current node is found, then returns true. 
+   * a node other than the current node is found, then returns true.
-      /* remove it from deadNodes. blockSeekTo could have cleared 
+      /* remove it from deadNodes. blockSeekTo could have cleared
-      
+
-    // Close the current block reader so that the new caching settings can 
+    // Close the current block reader so that the new caching settings can
-    ByteBuffer.allocateDirect(0).asReadOnlyBuffer();
+      ByteBuffer.allocateDirect(0).asReadOnlyBuffer();
-      int maxLength, EnumSet<ReadOption> opts) 
+      int maxLength, EnumSet<ReadOption> opts)
