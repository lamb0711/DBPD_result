HDFS-11148. Update DataNode to use StorageLocationChecker at startup.

+import org.apache.hadoop.util.DiskChecker.DiskErrorException;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
-  public StorageLocationChecker(Configuration conf, Timer timer) {
+  public StorageLocationChecker(Configuration conf, Timer timer)
+      throws DiskErrorException {
-        DFSConfigKeys.DFS_DATANODE_DISK_CHECK_TIMEOUT_KEY,
-        DFSConfigKeys.DFS_DATANODE_DISK_CHECK_TIMEOUT_DEFAULT,
+        DFS_DATANODE_DISK_CHECK_TIMEOUT_KEY,
+        DFS_DATANODE_DISK_CHECK_TIMEOUT_DEFAULT,
+    if (maxAllowedTimeForCheckMs <= 0) {
+      throw new DiskErrorException("Invalid value configured for "
+          + DFS_DATANODE_DISK_CHECK_TIMEOUT_KEY + " - "
+          + maxAllowedTimeForCheckMs + " (should be > 0)");
+    }
+
+    if (maxVolumeFailuresTolerated < 0) {
+      throw new DiskErrorException("Invalid value configured for "
+          + DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY + " - "
+          + maxVolumeFailuresTolerated + " (should be non-negative)");
+    }
+
+   * StorageLocations are returned in the same order as the input
+   * for compatibility with existing unit tests.
+   *
-    final ArrayList<StorageLocation> goodLocations = new ArrayList<>();
+    final HashMap<StorageLocation, Boolean> goodLocations =
+        new LinkedHashMap<>();
+      goodLocations.put(location, true);
+    if (maxVolumeFailuresTolerated >= dataDirs.size()) {
+      throw new DiskErrorException("Invalid value configured for "
+          + DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY + " - "
+          + maxVolumeFailuresTolerated + ". Value configured is >= "
+          + "to the number of configured volumes (" + dataDirs.size() + ").");
+    }
+
-          goodLocations.add(entry.getKey());
+          goodLocations.remove(location);
-          goodLocations.add(entry.getKey());
+        goodLocations.remove(location);
-    return goodLocations;
+    return new ArrayList<>(goodLocations.keySet());
