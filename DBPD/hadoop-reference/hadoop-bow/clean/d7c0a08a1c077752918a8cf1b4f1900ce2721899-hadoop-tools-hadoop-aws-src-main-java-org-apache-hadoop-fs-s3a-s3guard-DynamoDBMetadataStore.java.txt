HADOOP-15426 Make S3guard client resilient to DDB throttle events and network failures (Contributed by Steve Loughran)

+import java.util.concurrent.atomic.AtomicLong;
+import com.amazonaws.AmazonServiceException;
+import org.apache.hadoop.fs.s3a.AWSServiceThrottledException;
-import org.apache.hadoop.fs.s3a.S3ARetryPolicy;
-  /** Initial delay for retries when batched operations get throttled by
-   * DynamoDB. Value is {@value} msec. */
-  public static final long MIN_RETRY_SLEEP_MSEC = 100;
-
+  @VisibleForTesting
+  static final String HINT_DDB_IOPS_TOO_LOW
+      = " This may be because the write threshold of DynamoDB is set too low.";
+
+  @VisibleForTesting
+  static final String THROTTLING = "Throttling";
+
-  private RetryPolicy dataAccessRetryPolicy;
+  /**
+   * This policy is mostly for batched writes, not for processing
+   * exceptions in invoke() calls.
+   * It also has a role purpose in {@link #getVersionMarkerItem()};
+   * look at that method for the details.
+   */
+  private RetryPolicy batchWriteRetryPolicy;
+
-  /** Data access can have its own policies. */
-  private Invoker dataAccess;
+  /** Invoker for read operations. */
+  private Invoker readOp;
+
+  /** Invoker for write operations. */
+  private Invoker writeOp;
+
+  private final AtomicLong readThrottleEvents = new AtomicLong(0);
+  private final AtomicLong writeThrottleEvents = new AtomicLong(0);
+  private final AtomicLong batchWriteCapacityExceededEvents = new AtomicLong(0);
-    owner = (S3AFileSystem) fs;
-    instrumentation = owner.getInstrumentation().getS3GuardInstrumentation();
+    bindToOwnerFilesystem((S3AFileSystem) fs);
-    conf = owner.getConf();
-    username = owner.getUsername();
-    invoker = new Invoker(new S3ARetryPolicy(conf),
+    invoker = new Invoker(new S3GuardDataAccessRetryPolicy(conf),
+   * Declare that this table is owned by the specific S3A FS instance.
+   * This will bind some fields to the values provided by the owner,
+   * including wiring up the instrumentation.
+   * @param fs owner filesystem
+   */
+  @VisibleForTesting
+  void bindToOwnerFilesystem(final S3AFileSystem fs) {
+    owner = fs;
+    conf = owner.getConf();
+    instrumentation = owner.getInstrumentation().getS3GuardInstrumentation();
+    username = owner.getUsername();
+  }
+
+  /**
-   * between each attempt of {@link #MIN_RETRY_SLEEP_MSEC} milliseconds.
+   * between each attempt of {@link Constants#S3GUARD_DDB_THROTTLE_RETRY_INTERVAL}
+   * milliseconds.
-    int maxRetries = config.getInt(S3GUARD_DDB_MAX_RETRIES,
-        S3GUARD_DDB_MAX_RETRIES_DEFAULT);
-    dataAccessRetryPolicy = RetryPolicies
-        .exponentialBackoffRetry(maxRetries, MIN_RETRY_SLEEP_MSEC,
+    batchWriteRetryPolicy = RetryPolicies
+        .exponentialBackoffRetry(
+            config.getInt(S3GUARD_DDB_MAX_RETRIES,
+                S3GUARD_DDB_MAX_RETRIES_DEFAULT),
+            conf.getTimeDuration(S3GUARD_DDB_THROTTLE_RETRY_INTERVAL,
+                S3GUARD_DDB_THROTTLE_RETRY_INTERVAL_DEFAULT,
+                TimeUnit.MILLISECONDS),
-    dataAccess = new Invoker(dataAccessRetryPolicy, this::retryEvent);
+    final RetryPolicy throttledRetryRetryPolicy
+        = new S3GuardDataAccessRetryPolicy(config);
+    readOp = new Invoker(throttledRetryRetryPolicy, this::readRetryEvent);
+    writeOp = new Invoker(throttledRetryRetryPolicy, this::writeRetryEvent);
-      invoker.retry("Put tombstone", path.toString(), idempotent,
+      writeOp.retry(
+          "Put tombstone",
+          path.toString(),
+          idempotent,
-      invoker.retry("Delete key", path.toString(), idempotent,
+      writeOp.retry(
+          "Delete key",
+          path.toString(),
+          idempotent,
-  @Retries.OnceRaw
-  private Item getConsistentItem(PrimaryKey key) {
+  /**
+   * Get a consistent view of an item.
+   * @param path path to look up in the database
+   * @param path entry
+   * @return the result
+   * @throws IOException failure
+   */
+  @Retries.RetryTranslated
+  private Item getConsistentItem(final Path path) throws IOException {
+    PrimaryKey key = pathToKey(path);
-    return table.getItem(spec);
+    return readOp.retry("get",
+        path.toString(),
+        true,
+        () -> table.getItem(spec));
-  @Retries.OnceTranslated
+  @Retries.RetryTranslated
-  @Retries.OnceTranslated
+  @Retries.RetryTranslated
-    return Invoker.once("get", path.toString(),
-        () -> innerGet(path, wantEmptyDirectoryFlag));
+    return innerGet(path, wantEmptyDirectoryFlag);
-   * @throws AmazonClientException dynamo DB level problem
-  @Retries.OnceRaw
+  @Retries.RetryTranslated
-      final Item item = getConsistentItem(pathToKey(path));
+      final Item item = getConsistentItem(path);
-        final ItemCollection<QueryOutcome> items = table.query(spec);
-        boolean hasChildren = items.iterator().hasNext();
+        boolean hasChildren = readOp.retry("get/hasChildren",
+            path.toString(),
+            true,
+            () -> table.query(spec).iterator().hasNext());
-  @Retries.OnceTranslated
+  @Retries.RetryTranslated
-    return Invoker.once("listChildren", path.toString(),
+    return readOp.retry(
+        "listChildren",
+        path.toString(),
+        true,
-  @Retries.OnceTranslated
+  @Retries.RetryTranslated
-    Invoker.once("move", tableName,
-        () -> processBatchWriteRequest(null, pathMetadataToItem(newItems)));
+    processBatchWriteRequest(null, pathMetadataToItem(newItems));
-   * The retry logic here is limited to repeating the write operations
-   * until all items have been written; there is no other attempt
-   * at recovery/retry. Throttling is handled internally.
+   * As well as retrying on the operation invocation, incomplete
+   * batches are retried until all have been deleted.
+   * @return the number of iterations needed to complete the call.
-  @Retries.OnceRaw("Outstanding batch items are updated with backoff")
-  private void processBatchWriteRequest(PrimaryKey[] keysToDelete,
+  @Retries.RetryTranslated("Outstanding batch items are updated with backoff")
+  private int processBatchWriteRequest(PrimaryKey[] keysToDelete,
+    int batches = 0;
-      BatchWriteItemOutcome res = dynamoDB.batchWriteItem(writeItems);
+      // if there's a retry and another process updates things then it's not
+      // quite idempotent, but this was the case anyway
+      batches++;
+      BatchWriteItemOutcome res = writeOp.retry(
+          "batch write",
+          "",
+          true,
+          () -> dynamoDB.batchWriteItem(writeItems));
-        retryBackoff(retryCount++);
-        res = dynamoDB.batchWriteItemUnprocessed(unprocessed);
+        batchWriteCapacityExceededEvents.incrementAndGet();
+        batches++;
+        retryBackoffOnBatchWrite(retryCount++);
+        // use a different reference to keep the compiler quiet
+        final Map<String, List<WriteRequest>> upx = unprocessed;
+        res = writeOp.retry(
+            "batch write",
+            "",
+            true,
+            () -> dynamoDB.batchWriteItemUnprocessed(upx));
+    return batches;
+   *
-  private void retryBackoff(int retryCount) throws IOException {
+  private void retryBackoffOnBatchWrite(int retryCount) throws IOException {
-      RetryPolicy.RetryAction action = dataAccessRetryPolicy.shouldRetry(null,
+      RetryPolicy.RetryAction action = batchWriteRetryPolicy.shouldRetry(
+          null,
-        throw new IOException(
-            String.format("Max retries exceeded (%d) for DynamoDB. This may be"
-                    + " because write threshold of DynamoDB is set too low.",
-                retryCount));
+        // Create an AWSServiceThrottledException, with a fake inner cause
+        // which we fill in to look like a real exception so
+        // error messages look sensible
+        AmazonServiceException cause = new AmazonServiceException(
+            "Throttling");
+        cause.setServiceName("S3Guard");
+        cause.setStatusCode(AWSServiceThrottledException.STATUS_CODE);
+        cause.setErrorCode(THROTTLING);  // used in real AWS errors
+        cause.setErrorType(AmazonServiceException.ErrorType.Service);
+        cause.setErrorMessage(THROTTLING);
+        cause.setRequestId("n/a");
+        throw new AWSServiceThrottledException(
+            String.format("Max retries during batch write exceeded"
+                    + " (%d) for DynamoDB."
+                    + HINT_DDB_IOPS_TOO_LOW,
+                retryCount),
+            cause);
-      throw new IOException("Unexpected exception", e);
+      throw new IOException("Unexpected exception " + e, e);
-  @Retries.OnceRaw
+  @Retries.RetryTranslated
-  @Retries.OnceRaw
+  @Retries.RetryTranslated
-  @Retries.OnceRaw
-  private Collection<DDBPathMetadata> fullPathsToPut(DDBPathMetadata meta)
+  @VisibleForTesting
+  @Retries.RetryTranslated
+  Collection<DDBPathMetadata> fullPathsToPut(DDBPathMetadata meta)
-      final Item item = getConsistentItem(pathToKey(path));
+      final Item item = getConsistentItem(path);
-  @Retries.OnceTranslated("retry(listFullPaths); once(batchWrite)")
+  @Retries.RetryTranslated
-    final Collection<DDBPathMetadata> metasToPut = invoker.retry(
-        "paths to put", path.toString(), true,
-        () -> fullPathsToPut(ddbPathMeta));
+    final Collection<DDBPathMetadata> metasToPut = fullPathsToPut(ddbPathMeta);
-    Invoker.once("put", path.toString(),
-        () -> processBatchWriteRequest(null, pathMetadataToItem(metasToPut)));
+    processBatchWriteRequest(null, pathMetadataToItem(metasToPut));
-}
+  }
-  @Retries.OnceTranslated
+  @Retries.RetryTranslated
-      table.delete();
+      invoker.retry("delete", null, true,
+          () -> table.delete());
-    } catch (ResourceNotFoundException rnfe) {
-      LOG.info("ResourceNotFoundException while deleting DynamoDB table {} in "
+    } catch (FileNotFoundException rnfe) {
+      LOG.info("FileNotFoundException while deleting DynamoDB table {} in "
-    } catch (AmazonClientException e) {
-      throw translateException("destroy", tableName, e);
-  @Retries.OnceRaw
+  @Retries.RetryTranslated
-      String keyPrefix) {
+      String keyPrefix) throws IOException {
-    return table.scan(filterExpression, projectionExpression, null, map);
+    return readOp.retry(
+        "scan",
+        keyPrefix,
+        true,
+        () -> table.scan(filterExpression, projectionExpression, null, map));
-  @Retries.OnceRaw("once(batchWrite)")
+  @Retries.RetryTranslated
+  /**
+   * Prune files, in batches. There's a sleep between each batch.
+   * @param modTime Oldest modification time to allow
+   * @param keyPrefix The prefix for the keys that should be removed
+   * @throws IOException Any IO/DDB failure.
+   * @throws InterruptedIOException if the prune was interrupted
+   */
-  @Retries.OnceRaw("once(batchWrite)")
+  @Retries.RetryTranslated
-      int delay = conf.getInt(S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,
-          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT);
-      Set<Path> parentPathSet =  new HashSet<>();
+      long delay = conf.getTimeDuration(
+          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,
+          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,
+          TimeUnit.MILLISECONDS);
+      Set<Path> parentPathSet = new HashSet<>();
-      if (deletionBatch.size() > 0) {
+      // final batch of deletes
+      if (!deletionBatch.isEmpty()) {
-   * process, we sleep and retry a limited times before we fail to get it.
-   * This does not include handling any failure other than "item not found",
-   * so this method is tagged as "OnceRaw"
+   * process, we sleep and retry a limited number times if the lookup returns
+   * with a null value.
+   * DDB throttling is always retried.
-  @Retries.OnceRaw
-  private Item getVersionMarkerItem() throws IOException {
+  @VisibleForTesting
+  @Retries.RetryTranslated
+  Item getVersionMarkerItem() throws IOException {
-    Item versionMarker = table.getItem(versionMarkerKey);
+    // look for a version marker, with usual throttling/failure retries.
+    Item versionMarker = queryVersionMarker(versionMarkerKey);
+      // The marker was null.
+      // Two possibilities
+      // 1. This isn't a S3Guard table.
+      // 2. This is a S3Guard table in construction; another thread/process
+      //    is about to write/actively writing the version marker.
+      // So that state #2 is handled, batchWriteRetryPolicy is used to manage
+      // retries.
+      // This will mean that if the cause is actually #1, failure will not
+      // be immediate. As this will ultimately result in a failure to
+      // init S3Guard and the S3A FS, this isn't going to be a performance
+      // bottleneck -simply a slightly slower failure report than would otherwise
+      // be seen.
+      // "if your settings are broken, performance is not your main issue"
-        RetryPolicy.RetryAction action = dataAccessRetryPolicy.shouldRetry(null,
+        RetryPolicy.RetryAction action = batchWriteRetryPolicy.shouldRetry(null,
-        throw new IOException("initTable: Unexpected exception", e);
+        throw new IOException("initTable: Unexpected exception " + e, e);
-      versionMarker = table.getItem(versionMarkerKey);
+      versionMarker = queryVersionMarker(versionMarkerKey);
+   * Issue the query to get the version marker, with throttling for overloaded
+   * DDB tables.
+   * @param versionMarkerKey key to look up
+   * @return the marker
+   * @throws IOException failure
+   */
+  @Retries.RetryTranslated
+  private Item queryVersionMarker(final PrimaryKey versionMarkerKey)
+      throws IOException {
+    return readOp.retry("getVersionMarkerItem",
+        VERSION_MARKER, true,
+        () -> table.getItem(versionMarkerKey));
+  }
+
+  /**
-  PutItemOutcome putItem(Item item) {
+  private PutItemOutcome putItem(Item item) {
+  public String getTableName() {
+    return tableName;
+  }
+
+  @VisibleForTesting
-    if (dataAccessRetryPolicy != null) {
-      map.put("retryPolicy", dataAccessRetryPolicy.toString());
+    if (batchWriteRetryPolicy != null) {
+      map.put("retryPolicy", batchWriteRetryPolicy.toString());
+   * Callback on a read operation retried.
+   * @param text text of the operation
+   * @param ex exception
+   * @param attempts number of attempts
+   * @param idempotent is the method idempotent (this is assumed to be true)
+   */
+  void readRetryEvent(
+      String text,
+      IOException ex,
+      int attempts,
+      boolean idempotent) {
+    readThrottleEvents.incrementAndGet();
+    retryEvent(text, ex, attempts, true);
+  }
+
+  /**
+   * Callback  on a write operation retried.
+   * @param text text of the operation
+   * @param ex exception
+   * @param attempts number of attempts
+   * @param idempotent is the method idempotent (this is assumed to be true)
+   */
+  void writeRetryEvent(
+      String text,
+      IOException ex,
+      int attempts,
+      boolean idempotent) {
+    writeThrottleEvents.incrementAndGet();
+    retryEvent(text, ex, attempts, idempotent);
+  }
+
+  /**
+  /**
+   * Get the count of read throttle events.
+   * @return the current count of read throttle events.
+   */
+  @VisibleForTesting
+  public long getReadThrottleEventCount() {
+    return readThrottleEvents.get();
+  }
+
+  /**
+   * Get the count of write throttle events.
+   * @return the current count of write throttle events.
+   */
+  @VisibleForTesting
+  public long getWriteThrottleEventCount() {
+    return writeThrottleEvents.get();
+  }
+
+  @VisibleForTesting
+  public long getBatchWriteCapacityExceededCount() {
+    return batchWriteCapacityExceededEvents.get();
+  }
+
+  @VisibleForTesting
+  public Invoker getInvoker() {
+    return invoker;
+  }
