HDFS-4988. Datanode must support all the volumes as individual storages.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1526969 13f79535-47bb-0310-9956-ffa450edef68

-import org.apache.hadoop.hdfs.DFSUtil;
+import org.apache.hadoop.hdfs.server.common.Storage;
-  final ReplicaMap volumeMap;
+  // TODO HDFS-2832: Consider removing duplicated block info from these
+  //                 two maps. This might require some refactoring
+  //                 rewrite of FsDatasetImpl.
+  final ReplicaMap volumeMap;
+  final Map<FsVolumeImpl, ReplicaMap> perVolumeReplicaMap;
+
+
-      final File dir = storage.getStorageDir(idx).getCurrentDir();
+      Storage.StorageDirectory sd = storage.getStorageDir(idx);
+      final File dir = sd.getCurrentDir();
-      volArray.add(new FsVolumeImpl(this, storage.getStorageID(), dir, conf,
+      volArray.add(new FsVolumeImpl(this, sd.getStorageUuid(), dir, conf,
+    perVolumeReplicaMap = new HashMap<FsVolumeImpl, ReplicaMap>();
-    volumes.getVolumeMap(volumeMap);
+    volumes.initializeReplicaMaps(perVolumeReplicaMap, volumeMap, this);
-    registerMBean(storage.getStorageID());
+    registerMBean(datanode.getDatanodeUuid());
-      if (LOG.isDebugEnabled()) {
-        LOG.debug("b=" + b + ", volumeMap=" + volumeMap);
-      }
+    perVolumeReplicaMap.get(v).add(bpid, newReplicaInfo);
+    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);
+    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), rbw);
+    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);
+    perVolumeReplicaMap.get(newReplicaInfo.getVolume()).add(bpid, newReplicaInfo);
+      perVolumeReplicaMap.get((FsVolumeImpl) replicaInfo.getVolume())
+          .remove(b.getBlockPoolId(), b.getLocalBlock());
-  /**
-   * Generates a block report from the in-memory block map.
-   */
-  @Override // FsDatasetSpi
-  public BlockListAsLongs getBlockReport(String bpid) {
-    int size =  volumeMap.size(bpid);
+  private BlockListAsLongs getBlockReportWithReplicaMap(
+      String bpid, ReplicaMap rMap) {
+    int size =  rMap.size(bpid);
-      for (ReplicaInfo b : volumeMap.replicas(bpid)) {
+      for (ReplicaInfo b : rMap.replicas(bpid)) {
+   * Generates a block report from the in-memory block map.
+   */
+  @Override // FsDatasetSpi
+  public BlockListAsLongs getBlockReport(String bpid) {
+    return getBlockReportWithReplicaMap(bpid, volumeMap);
+  }
+
+  @Override
+  public Map<String, BlockListAsLongs> getBlockReports(String bpid) {
+    Map<String, BlockListAsLongs> blockReportMap =
+        new HashMap<String, BlockListAsLongs>();
+
+    for (FsVolumeImpl v : getVolumes()) {
+      ReplicaMap rMap = perVolumeReplicaMap.get(v);
+      BlockListAsLongs blockList = getBlockReportWithReplicaMap(bpid, rMap);
+      blockReportMap.put(v.getStorageID(), blockList);
+    }
+
+    return blockReportMap;
+  }
+
+  /**
+        perVolumeReplicaMap.get(v).remove(bpid, invalidBlks[i]);
+              perVolumeReplicaMap.get(fv).remove(bpid, b.getBlockId());
-   *        "hadoop:service=DataNode,name=FSDatasetState-<storageid>"
+   *        "hadoop:service=DataNode,name=FSDatasetState-<datanodeUuid>"
-  void registerMBean(final String storageId) {
+  void registerMBean(final String datanodeUuid) {
-    StandardMBean bean;
-    String storageName;
-    if (storageId == null || storageId.equals("")) {// Temp fix for the uninitialized storage
-      storageName = "UndefinedStorageId" + DFSUtil.getRandom().nextInt();
-    } else {
-      storageName = storageId;
-    }
-      bean = new StandardMBean(this,FSDatasetMBean.class);
-      mbeanName = MBeans.register("DataNode", "FSDatasetState-" + storageName, bean);
+      StandardMBean bean = new StandardMBean(this,FSDatasetMBean.class);
+      mbeanName = MBeans.register("DataNode", "FSDatasetState-" + datanodeUuid, bean);
+          perVolumeReplicaMap.get((FsVolumeImpl) memBlockInfo.getVolume())
+              .remove(bpid, blockId);
+        perVolumeReplicaMap.get((FsVolumeImpl) memBlockInfo.getVolume()).
+            remove(bpid, diskBlockInfo);
-    volumes.getVolumeMap(bpid, volumeMap);
+    volumes.getAllVolumesMap(bpid, volumeMap);
+
+    // TODO: Avoid the double scan.
+    for (FsVolumeImpl v : getVolumes()) {
+      ReplicaMap rMap = perVolumeReplicaMap.get(v);
+      rMap.initBlockPool(bpid);
+      volumes.getVolumeMap(bpid, v, rMap);
+    }
-  @Override
-  public String[] getBlockPoolList() {
-    return volumeMap.getBlockPoolList();
-  }
-  
