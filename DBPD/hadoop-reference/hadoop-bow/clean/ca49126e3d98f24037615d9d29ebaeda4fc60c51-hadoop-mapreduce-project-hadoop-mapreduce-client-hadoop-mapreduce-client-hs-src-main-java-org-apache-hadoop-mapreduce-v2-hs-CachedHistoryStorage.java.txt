Merge trunk into auto-HA branch


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3042@1327724 13f79535-47bb-0310-9956-ffa450edef68

-import org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.MetaInfo;
+import org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.HistoryFileInfo;
-  private Job loadJob(MetaInfo metaInfo) {
+  private Job loadJob(HistoryFileInfo fileInfo) {
-      Job job = hsManager.loadJob(metaInfo);
+      Job job = fileInfo.loadJob();
+      // We can clobber results here, but that should be OK, because it only
+      // means that we may have two identical copies of the same job floating
+      // around for a while.
-          "Could not find/load job: " + metaInfo.getJobId(), e);
+          "Could not find/load job: " + fileInfo.getJobId(), e);
-  public synchronized Job getFullJob(JobId jobId) {
+  public Job getFullJob(JobId jobId) {
-      Job result = loadedJobCache.get(jobId);
-      if (result == null) {
-        MetaInfo metaInfo = hsManager.getMetaInfo(jobId);
-        if (metaInfo != null) {
-          result = loadJob(metaInfo);
+      HistoryFileInfo fileInfo = hsManager.getFileInfo(jobId);
+      Job result = null;
+      if (fileInfo != null) {
+        result = loadedJobCache.get(jobId);
+        if (result == null) {
+          result = loadJob(fileInfo);
+        } else if(fileInfo.isDeleted()) {
+          loadedJobCache.remove(jobId);
+          result = null;
+      } else {
+        loadedJobCache.remove(jobId);
-      for (MetaInfo mi : hsManager.getAllMetaInfo()) {
+      for (HistoryFileInfo mi : hsManager.getAllFileInfo()) {
-      LOG.warn("Error trying to scan for all MetaInfos", e);
+      LOG.warn("Error trying to scan for all FileInfos", e);
-  public void jobRemovedFromHDFS(JobId jobId) {
-    loadedJobCache.remove(jobId);
-  }
-
-  @Override
+
