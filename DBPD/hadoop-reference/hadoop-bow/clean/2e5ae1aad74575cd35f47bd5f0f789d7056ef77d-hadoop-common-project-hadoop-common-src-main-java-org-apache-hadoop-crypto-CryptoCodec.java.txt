HADOOP-10628. Javadoc and few code style improvement for Crypto input and output streams. (yliu via clamb)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1598429 13f79535-47bb-0310-9956-ffa450edef68

-   * Get block size of a block cipher.
+   * Get the block size of a block cipher.
-   * @return int block size
+   * @return int the block size
-   * Get a {@link #org.apache.hadoop.crypto.Encryptor}. 
-   * @return Encryptor
+   * Get an {@link #org.apache.hadoop.crypto.Encryptor}. 
+   * @return Encryptor the encryptor
-   * @return Decryptor
+   * @return Decryptor the decryptor
-   * This interface is only for Counter (CTR) mode. Typically calculating 
-   * IV(Initialization Vector) is up to Encryptor or Decryptor, for 
-   * example {@link #javax.crypto.Cipher} will maintain encryption context 
-   * internally when do encryption/decryption continuously using its 
+   * This interface is only for Counter (CTR) mode. Generally the Encryptor
+   * or Decryptor calculates the IV and maintain encryption context internally. 
+   * For example a {@link #javax.crypto.Cipher} will maintain its encryption 
+   * context internally when we do encryption/decryption using the 
-   * In Hadoop, multiple nodes may read splits of a file, so decrypting of 
-   * file is not continuous, even for encrypting may be not continuous. For 
-   * each part, we need to calculate the counter through file position.
+   * Encryption/Decryption is not always on the entire file. For example,
+   * in Hadoop, a node may only decrypt a portion of a file (i.e. a split).
+   * In these situations, the counter is derived from the file position.
-   * Typically IV for a file position is produced by combining initial IV and 
-   * the counter using any lossless operation (concatenation, addition, or XOR).
+   * The IV can be calculated by combining the initial IV and the counter with 
+   * a lossless operation (concatenation, addition, or XOR).
