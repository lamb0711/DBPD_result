HADOOP-10809. hadoop-azure: page blob support. Contributed by Dexter Bradshaw, Mostafa Elhemali, Eric Hanson, and Mike Liddell.

-import java.io.ByteArrayInputStream;
+import java.io.UnsupportedEncodingException;
+import java.net.URLDecoder;
+import java.net.URLEncoder;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Locale;
+import java.util.Set;
+import org.apache.commons.lang.StringUtils;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.azure.StorageInterface.CloudBlobWrapper;
+import org.apache.hadoop.fs.azure.StorageInterface.CloudPageBlobWrapper;
+import org.apache.hadoop.fs.azure.StorageInterfaceImpl.CloudPageBlobWrapperImpl;
-
+  /**
+   * Configuration key to indicate the set of directories in WASB where we
+   * should store files as page blobs instead of block blobs.
+   *
+   * Entries should be plain directory names (i.e. not URIs) with no leading or
+   * trailing slashes. Delimit the entries with commas.
+   */
+  public static final String KEY_PAGE_BLOB_DIRECTORIES =
+      "fs.azure.page.blob.dir";
+  /**
+   * The set of directories where we should store files as page blobs.
+   */
+  private Set<String> pageBlobDirs;
+  
+  /**
+   * Configuration key to indicate the set of directories in WASB where
+   * we should do atomic folder rename synchronized with createNonRecursive.
+   */
+  public static final String KEY_ATOMIC_RENAME_DIRECTORIES =
+      "fs.azure.atomic.rename.dir";
+
+  /**
+   * The set of directories where we should apply atomic folder rename
+   * synchronized with createNonRecursive.
+   */
+  private Set<String> atomicRenameDirs;
+
+  //
+  //
+
+
-  private static final JSON PERMISSION_JSON_SERIALIZER = createPermissionJsonSerializer();
+  private final static JSON PERMISSION_JSON_SERIALIZER = createPermissionJsonSerializer();
-  
+
-   * @param uri
-   *          - URI for target storage blob.
-   * @param conf
-   *          - reference to configuration object.
+   * @param uri - URI for target storage blob.
+   * @param conf - reference to configuration object.
+   * @param instrumentation - the metrics source that will keep track of operations here.
-   * @throws IllegalArgumentException
-   *           if URI or job object is null, or invalid scheme.
+   * @throws IllegalArgumentException if URI or job object is null, or invalid scheme.
-  public void initialize(URI uri, Configuration conf, AzureFileSystemInstrumentation instrumentation) throws AzureException {
-
-    if (null == this.storageInteractionLayer) {
-      this.storageInteractionLayer = new StorageInterfaceImpl();
+  public void initialize(URI uri, Configuration conf, AzureFileSystemInstrumentation instrumentation)
+      throws IllegalArgumentException, AzureException, IOException  {
+    
+    if (null == instrumentation) {
+      throw new IllegalArgumentException("Null instrumentation");
+
+    // Extract the directories that should contain page blobs
+    pageBlobDirs = getDirectorySet(KEY_PAGE_BLOB_DIRECTORIES);
+    LOG.debug("Page blob directories:  " + setToString(pageBlobDirs));
+
+    // Extract directories that should have atomic rename applied.
+    atomicRenameDirs = getDirectorySet(KEY_ATOMIC_RENAME_DIRECTORIES);
+    String hbaseRoot;
+    try {
+
+      // Add to this the hbase root directory, or /hbase is that is not set.
+      hbaseRoot = verifyAndConvertToStandardFormat(
+          sessionConfiguration.get("hbase.rootdir", "hbase"));
+      atomicRenameDirs.add(hbaseRoot);
+    } catch (URISyntaxException e) {
+      LOG.warn("Unable to initialize HBase root as an atomic rename directory.");
+    }
+    LOG.debug("Atomic rename directories:  " + setToString(atomicRenameDirs));
+  }
+
+  /**
+   * Helper to format a string for log output from Set<String>
+   */
+  private String setToString(Set<String> set) {
+    StringBuilder sb = new StringBuilder();
+    int i = 1;
+    for (String s : set) {
+      sb.append("/" + s);
+      if (i != set.size()) {
+        sb.append(", ");
+      }
+      i++;
+    }
+    return sb.toString();
-    // Check if authority container the delimiter separating the account name
-    // from the
+    // Check if authority container the delimiter separating the account name from the
-      // The authority does not have a container name. Use the default container
-      // by
+      // The authority does not have a container name. Use the default container by
-    if (sessionScheme != null
-        && (sessionScheme.equalsIgnoreCase("asvs") || sessionScheme
-            .equalsIgnoreCase("wasbs"))) {
+    if (sessionScheme != null &&
+        (sessionScheme.equalsIgnoreCase("asvs") ||
+         sessionScheme.equalsIgnoreCase("wasbs"))) {
-    minBackoff = sessionConfiguration.getInt(KEY_MIN_BACKOFF_INTERVAL,
-        DEFAULT_MIN_BACKOFF_INTERVAL);
+    //
+    minBackoff = sessionConfiguration.getInt(
+        KEY_MIN_BACKOFF_INTERVAL, DEFAULT_MIN_BACKOFF_INTERVAL);
-    maxBackoff = sessionConfiguration.getInt(KEY_MAX_BACKOFF_INTERVAL,
-        DEFAULT_MAX_BACKOFF_INTERVAL);
+    maxBackoff = sessionConfiguration.getInt(
+        KEY_MAX_BACKOFF_INTERVAL, DEFAULT_MAX_BACKOFF_INTERVAL);
-    deltaBackoff = sessionConfiguration.getInt(KEY_BACKOFF_INTERVAL,
-        DEFAULT_BACKOFF_INTERVAL);
+    deltaBackoff = sessionConfiguration.getInt(
+        KEY_BACKOFF_INTERVAL, DEFAULT_BACKOFF_INTERVAL);
-    maxRetries = sessionConfiguration.getInt(KEY_MAX_IO_RETRIES,
-        DEFAULT_MAX_RETRY_ATTEMPTS);
+    maxRetries = sessionConfiguration.getInt(
+        KEY_MAX_IO_RETRIES, DEFAULT_MAX_RETRY_ATTEMPTS);
-    storageInteractionLayer.setRetryPolicyFactory(new RetryExponentialRetry(
-        minBackoff, deltaBackoff, maxBackoff, maxRetries));
+    storageInteractionLayer.setRetryPolicyFactory(
+          new RetryExponentialRetry(minBackoff, deltaBackoff, maxBackoff, maxRetries));
+
+    URI blobEndPoint;
-      CloudStorageAccount account = CloudStorageAccount
-          .getDevelopmentStorageAccount();
+      CloudStorageAccount account =
+          CloudStorageAccount.getDevelopmentStorageAccount();
-      URI blobEndPoint = new URI(getHTTPScheme() + "://" + accountName);
+      blobEndPoint = new URI(getHTTPScheme() + "://" +
+          accountName);
-  private void createAzureStorageSession() throws AzureException {
+  private void createAzureStorageSession ()
+      throws AzureException, IOException {
+   * Trims a suffix/prefix from the given string. For example if
+   * s is given as "/xy" and toTrim is "/", this method returns "xy"
+   */
+  private static String trim(String s, String toTrim) {
+    return StringUtils.removeEnd(StringUtils.removeStart(s, toTrim),
+        toTrim);
+  }
+
+  /**
+   * Checks if the given rawDir belongs to this account/container, and
+   * if so returns the canonicalized path for it. Otherwise return null.
+   */
+  private String verifyAndConvertToStandardFormat(String rawDir) throws URISyntaxException {
+    URI asUri = new URI(rawDir);
+    if (asUri.getAuthority() == null 
+        || asUri.getAuthority().toLowerCase(Locale.US).equalsIgnoreCase(
+        		sessionUri.getAuthority().toLowerCase(Locale.US))) {
+      // Applies to me.
+      return trim(asUri.getPath(), "/");
+    } else {
+      // Doen't apply to me.
+      return null;
+    }
+  }
+
+  /**
+   * Take a comma-separated list of directories from a configuration variable
+   * and transform it to a set of directories.
+   */
+  private Set<String> getDirectorySet(final String configVar)
+      throws AzureException {
+    String[] rawDirs = sessionConfiguration.getStrings(configVar, new String[0]);
+    Set<String> directorySet = new HashSet<String>();
+    for (String currentDir : rawDirs) {
+      String myDir;
+      try {
+        myDir = verifyAndConvertToStandardFormat(currentDir);
+      } catch (URISyntaxException ex) {
+        throw new AzureException(String.format(
+            "The directory %s specified in the configuration entry %s is not" +
+            " a valid URI.",
+            currentDir, configVar));
+      }
+      if (myDir != null) {
+        directorySet.add(myDir);
+      }
+    }
+    return directorySet;
+  }
+
+  /**
+   * Checks if the given key in Azure Storage should be stored as a page
+   * blob instead of block blob.
+   * @throws URISyntaxException
+   */
+  public boolean isPageBlobKey(String key) {
+    return isKeyForDirectorySet(key, pageBlobDirs);
+  }
+
+  /**
+   * Checks if the given key in Azure storage should have synchronized
+   * atomic folder rename createNonRecursive implemented.
+   */
+  @Override
+  public boolean isAtomicRenameKey(String key) {
+    return isKeyForDirectorySet(key, atomicRenameDirs);
+  }
+
+  public boolean isKeyForDirectorySet(String key, Set<String> dirSet) {
+    String defaultFS = FileSystem.getDefaultUri(sessionConfiguration).toString();
+    for (String dir : dirSet) {
+      if (dir.isEmpty() ||
+          key.startsWith(dir + "/")) {
+        return true;
+      }
+
+      // Allow for blob directories with paths relative to the default file
+      // system.
+      //
+      try {
+        URI uriPageBlobDir = new URI (dir);
+        if (null == uriPageBlobDir.getAuthority()) {
+          // Concatenate the default file system prefix with the relative
+          // page blob directory path.
+          //
+          if (key.startsWith(trim(defaultFS, "/") + "/" + dir + "/")){
+            return true;
+          }
+        }
+      } catch (URISyntaxException e) {
+        LOG.info(String.format(
+                   "URI syntax error creating URI for %s", dir));
+      }
+    }
+    return false;
+  }
+
+  
+  
+  /**
-    options.setRetryPolicyFactory(new RetryExponentialRetry(minBackoff,
-        deltaBackoff, maxBackoff, maxRetries));
+    options.setRetryPolicyFactory(
+          new RetryExponentialRetry(minBackoff, deltaBackoff, maxBackoff, maxRetries));
-  public DataOutputStream storefile(String key,
-      PermissionStatus permissionStatus) throws AzureException {
+  public DataOutputStream storefile(String key, PermissionStatus permissionStatus)
+      throws AzureException {
-       * Note: Windows Azure Blob Storage does not allow the creation of
-       * arbitrary directory paths under the default $root directory. This is by
-       * design to eliminate ambiguity in specifying a implicit blob address. A
-       * blob in the $root container cannot include a / in its name and must be
-       * careful not to include a trailing '/' when referencing blobs in the
-       * $root container. A '/; in the $root container permits ambiguous blob
-       * names as in the following example involving two containers $root and
-       * mycontainer: http://myaccount.blob.core.windows.net/$root
-       * http://myaccount.blob.core.windows.net/mycontainer If the URL
-       * "mycontainer/somefile.txt were allowed in $root then the URL:
-       * http://myaccount.blob.core.windows.net/mycontainer/myblob.txt could
-       * mean either: (1) container=mycontainer; blob=myblob.txt (2)
-       * container=$root; blob=mycontainer/myblob.txt
+       * Note: Windows Azure Blob Storage does not allow the creation of arbitrary directory
+       *      paths under the default $root directory.  This is by design to eliminate
+       *      ambiguity in specifying a implicit blob address. A blob in the $root conatiner
+       *      cannot include a / in its name and must be careful not to include a trailing
+       *      '/' when referencing  blobs in the $root container.
+       *      A '/; in the $root container permits ambiguous blob names as in the following
+       *      example involving two containers $root and mycontainer:
+       *                http://myaccount.blob.core.windows.net/$root
+       *                http://myaccount.blob.core.windows.net/mycontainer
+       *      If the URL "mycontainer/somefile.txt were allowed in $root then the URL:
+       *                http://myaccount.blob.core.windows.net/mycontainer/myblob.txt
+       *      could mean either:
+       *        (1) container=mycontainer; blob=myblob.txt
+       *        (2) container=$root; blob=mycontainer/myblob.txt
-      // Get the block blob reference from the store's container and
+      // Get the blob reference from the store's container and
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
-      OutputStream outputStream = blob.openOutputStream(getUploadOptions(),
-          getInstrumentedContext());
-
-      // Return to caller with DataOutput stream.
-      DataOutputStream dataOutStream = new DataOutputStream(outputStream);
+      //
+      OutputStream outputStream = openOutputStream(blob);
+      DataOutputStream dataOutStream = new SyncableDataOutputStream(outputStream);
+   * Opens a new output stream to the given blob (page or block blob)
+   * to populate it from scratch with data.
+   */
+  private OutputStream openOutputStream(final CloudBlobWrapper blob)
+      throws StorageException {
+    if (blob instanceof CloudPageBlobWrapperImpl){
+      return new PageBlobOutputStream(
+          (CloudPageBlobWrapper)blob, getInstrumentedContext(), sessionConfiguration);
+    } else {
+
+      // Handle both ClouldBlockBlobWrapperImpl and (only for the test code path)
+      // MockCloudBlockBlobWrapper.
+      return ((CloudBlockBlobWrapper) blob).openOutputStream(getUploadOptions(),
+                getInstrumentedContext());
+    }
+  }
+
+  /**
+   * Opens a new input stream for the given blob (page or block blob)
+   * to read its data.
+   */
+  private InputStream openInputStream(CloudBlobWrapper blob)
+      throws StorageException, IOException {
+    if (blob instanceof CloudBlockBlobWrapper) {
+      return blob.openInputStream(getDownloadOptions(),
+          getInstrumentedContext(isConcurrentOOBAppendAllowed()));
+    } else {
+      return new PageBlobInputStream(
+          (CloudPageBlobWrapper) blob, getInstrumentedContext(
+              isConcurrentOOBAppendAllowed()));
+    }
+  }
+
+  /**
-  private static void storeMetadataAttribute(CloudBlockBlobWrapper blob,
+  private static void storeMetadataAttribute(CloudBlobWrapper blob,
-  private static String getMetadataAttribute(CloudBlockBlobWrapper blob,
+  private static String getMetadataAttribute(CloudBlobWrapper blob,
-  private static void removeMetadataAttribute(CloudBlockBlobWrapper blob,
+  private static void removeMetadataAttribute(CloudBlobWrapper blob,
-  private void storePermissionStatus(CloudBlockBlobWrapper blob,
+  private static void storePermissionStatus(CloudBlobWrapper blob,
-  private PermissionStatus getPermissionStatus(CloudBlockBlobWrapper blob) {
+  private PermissionStatus getPermissionStatus(CloudBlobWrapper blob) {
-      return PermissionStatusJsonSerializer
-          .fromJSONString(permissionMetadataValue);
+      return PermissionStatusJsonSerializer.fromJSONString(
+          permissionMetadataValue);
-  private static void storeFolderAttribute(CloudBlockBlobWrapper blob) {
+  private static void storeFolderAttribute(CloudBlobWrapper blob) {
-  private static void storeLinkAttribute(CloudBlockBlobWrapper blob,
-      String linkTarget) {
-    storeMetadataAttribute(blob, LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY,
-        linkTarget);
+  private static void storeLinkAttribute(CloudBlobWrapper blob,
+      String linkTarget) throws UnsupportedEncodingException {
+    // We have to URL encode the link attribute as the link URI could
+    // have URI special characters which unless encoded will result
+    // in 403 errors from the server. This is due to metadata properties
+    // being sent in the HTTP header of the request which is in turn used
+    // on the server side to authorize the request.
+    String encodedLinkTarget = null;
+    if (linkTarget != null) {
+      encodedLinkTarget = URLEncoder.encode(linkTarget, "UTF-8");
+    }
+    storeMetadataAttribute(blob,
+        LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY,
+        encodedLinkTarget);
-  private static String getLinkAttributeValue(CloudBlockBlobWrapper blob) {
-    return getMetadataAttribute(blob,
+  private static String getLinkAttributeValue(CloudBlobWrapper blob)
+      throws UnsupportedEncodingException {
+    String encodedLinkTarget = getMetadataAttribute(blob,
+    String linkTarget = null;
+    if (encodedLinkTarget != null) {
+      linkTarget = URLDecoder.decode(encodedLinkTarget, "UTF-8");
+    }
+    return linkTarget;
-  private static boolean retrieveFolderAttribute(CloudBlockBlobWrapper blob) {
+  private static boolean retrieveFolderAttribute(CloudBlobWrapper blob) {
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
-      blob.upload(new ByteArrayInputStream(new byte[0]),
-          getInstrumentedContext());
+      openOutputStream(blob).close();
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
-      blob.upload(new ByteArrayInputStream(new byte[0]),
-          getInstrumentedContext());
+      openOutputStream(blob).close();
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
+        null, false,
+        includeMetadata ?
+            EnumSet.of(BlobListingDetails.METADATA) :
+              EnumSet.noneOf(BlobListingDetails.class),
-        false,
-        includeMetadata ? EnumSet.of(BlobListingDetails.METADATA) : EnumSet
-            .noneOf(BlobListingDetails.class), null, getInstrumentedContext());
+              getInstrumentedContext());
-    return rootDirectory.listBlobs(
-        aPrefix,
+    Iterable<ListBlobItem> list = rootDirectory.listBlobs(aPrefix,
-        includeMetadata ? EnumSet.of(BlobListingDetails.METADATA) : EnumSet
-            .noneOf(BlobListingDetails.class), null, getInstrumentedContext());
+        includeMetadata ?
+            EnumSet.of(BlobListingDetails.METADATA) :
+              EnumSet.noneOf(BlobListingDetails.class),
+              null,
+              getInstrumentedContext());
+    return list;
-  private Iterable<ListBlobItem> listRootBlobs(String aPrefix,
-      boolean useFlatBlobListing, EnumSet<BlobListingDetails> listingDetails,
-      BlobRequestOptions options, OperationContext opContext)
-      throws StorageException, URISyntaxException {
+  private Iterable<ListBlobItem> listRootBlobs(String aPrefix, boolean useFlatBlobListing,
+      EnumSet<BlobListingDetails> listingDetails, BlobRequestOptions options,
+      OperationContext opContext) throws StorageException, URISyntaxException {
-    CloudBlobDirectoryWrapper directory = this.container
-        .getDirectoryReference(aPrefix);
-    return directory.listBlobs(null, useFlatBlobListing, listingDetails,
-        options, opContext);
+    CloudBlobDirectoryWrapper directory =  this.container.getDirectoryReference(aPrefix);
+    return directory.listBlobs(
+        null,
+        useFlatBlobListing,
+        listingDetails,
+        options,
+        opContext);
-  private CloudBlockBlobWrapper getBlobReference(String aKey)
+  private CloudBlobWrapper getBlobReference(String aKey)
-    CloudBlockBlobWrapper blob = this.container.getBlockBlobReference(aKey);
-
+    CloudBlobWrapper blob = null;
+    if (isPageBlobKey(aKey)) {
+      blob = this.container.getPageBlobReference(aKey);
+    } else {
+      blob = this.container.getBlockBlobReference(aKey);
+    }
-    // Return with block blob.
-  private String normalizeKey(CloudBlockBlobWrapper blob) {
+  private String normalizeKey(CloudBlobWrapper blob) {
-    // Bind operation context to receive send request callbacks on this
-    // operation.
-    // If reads concurrent to OOB writes are allowed, the interception will
-    // reset the conditional header on all Azure blob storage read requests.
+    // Bind operation context to receive send request callbacks on this operation.
+    // If reads concurrent to OOB writes are allowed, the interception will reset
+    // the conditional header on all Azure blob storage read requests.
-      operationContext = testHookOperationContext
-          .modifyOperationContext(operationContext);
+      operationContext =
+          testHookOperationContext.modifyOperationContext(operationContext);
-    
+
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
-              properties.getLength(), properties.getLastModified().getTime(),
+              getDataLength(blob, properties),
+              properties.getLastModified().getTime(),
-      Iterable<ListBlobItem> objects = listRootBlobs(key, true,
-          EnumSet.of(BlobListingDetails.METADATA), null,
+      //
+      Iterable<ListBlobItem> objects =
+          listRootBlobs(
+              key,
+              true,
+              EnumSet.of(BlobListingDetails.METADATA),
+              null,
-        if (blobItem instanceof CloudBlockBlobWrapper) {
+        if (blobItem instanceof CloudBlockBlobWrapper
+            || blobItem instanceof CloudPageBlobWrapper) {
-          blob = (CloudBlockBlobWrapper) blobItem;
+          blob = (CloudBlobWrapper) blobItem;
-  public DataInputStream retrieve(String key) throws AzureException {
-    InputStream inStream = null;
-    BufferedInputStream inBufStream = null;
-    try {
+  public DataInputStream retrieve(String key) throws AzureException, IOException {
-        CloudBlockBlobWrapper blob = getBlobReference(key);
-        inStream = blob.openInputStream(getDownloadOptions(),
-            getInstrumentedContext(isConcurrentOOBAppendAllowed()));
-
-        inBufStream = new BufferedInputStream(inStream);
+        CloudBlobWrapper blob = getBlobReference(key);
+      BufferedInputStream inBufStream = new BufferedInputStream(
+          openInputStream(blob));
-      }
-      catch (Exception e){
-        // close the streams on error.
-        // We use nested try-catch as stream.close() can throw IOException.
-        if(inBufStream != null){
-          inBufStream.close();
-        }
-        if(inStream != null){
-          inStream.close();
-        }
-        throw e;
-      }
-      throws AzureException {
-
-    InputStream in = null;
-    DataInputStream inDataStream = null;
-    try {
+      throws AzureException, IOException {
-        CloudBlockBlobWrapper blob = getBlobReference(key);
+        CloudBlobWrapper blob = getBlobReference(key);
-        in = blob.openInputStream(getDownloadOptions(),
-            getInstrumentedContext(isConcurrentOOBAppendAllowed()));
+        InputStream in = blob.openInputStream(
+          getDownloadOptions(), getInstrumentedContext(isConcurrentOOBAppendAllowed()));
-        inDataStream = new DataInputStream(in);
-        long skippedBytes = inDataStream.skip(startByteOffset);
-        if (skippedBytes != startByteOffset) {
-          throw new IOException("Couldn't skip the requested number of bytes");
-        }
+	    DataInputStream inDataStream = new DataInputStream(in);
+	    
+	    // Skip bytes and ignore return value. This is okay
+	    // because if you try to skip too far you will be positioned
+	    // at the end and reads will not return data.
+	    inDataStream.skip(startByteOffset);
-      }
-      catch (Exception e){
-        // close the streams on error.
-        // We use nested try-catch as stream.close() can throw IOException.
-        if(inDataStream != null){
-          inDataStream.close();
-        }
-        if(in != null){
-          in.close();
-        }
-        throw e;
-      }
-        if (0 < maxListingCount && fileMetadata.size() >= maxListingCount) {
+        if (0 < maxListingCount
+            && fileMetadata.size() >= maxListingCount) {
-        if (blobItem instanceof CloudBlockBlobWrapper) {
+        if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {
-          CloudBlockBlobWrapper blob = (CloudBlockBlobWrapper) blobItem;
+          CloudBlobWrapper blob = (CloudBlobWrapper) blobItem;
-            metadata = new FileMetadata(blobKey, properties.getLastModified()
-                .getTime(), getPermissionStatus(blob),
+            metadata = new FileMetadata(blobKey,
+                properties.getLastModified().getTime(),
+                getPermissionStatus(blob),
-            metadata = new FileMetadata(blobKey, properties.getLength(),
+            metadata = new FileMetadata(
+                blobKey,
+                getDataLength(blob, properties),
-      return new PartialListing(priorLastKey,
+      PartialListing listing = new PartialListing(priorLastKey,
-          0 == fileMetadata.size() ? new String[] {} : new String[] { prefix });
+          0 == fileMetadata.size() ? new String[] {}
+      : new String[] { prefix });
+      return listing;
-    LinkedList<Iterator<ListBlobItem>> dirIteratorStack = new LinkedList<Iterator<ListBlobItem>>();
+    //
+    AzureLinkedStack<Iterator<ListBlobItem>> dirIteratorStack =
+        new AzureLinkedStack<Iterator<ListBlobItem>>();
-        if (blobItem instanceof CloudBlockBlobWrapper) {
+        //
+        if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {
-          CloudBlockBlobWrapper blob = (CloudBlockBlobWrapper) blobItem;
+          CloudBlobWrapper blob = (CloudBlobWrapper) blobItem;
-            metadata = new FileMetadata(blobKey, properties.getLastModified()
-                .getTime(), getPermissionStatus(blob),
+            metadata = new FileMetadata(blobKey,
+                properties.getLastModified().getTime(),
+                getPermissionStatus(blob),
-            metadata = new FileMetadata(blobKey, properties.getLength(),
+            metadata = new FileMetadata(
+                blobKey,
+                getDataLength(blob, properties),
-              FileMetadata directoryMetadata = new FileMetadata(dirKey, 0,
+              //
+              FileMetadata directoryMetadata = new FileMetadata(dirKey,
+                  0,
-   * Deletes the given blob, taking special care that if we get a blob-not-found
-   * exception upon retrying the operation, we just swallow the error since what
-   * most probably happened is that the first operation succeeded on the server.
-   * 
-   * @param blob
-   *          The blob to delete.
+   * Return the actual data length of the blob with the specified properties.
+   * If it is a page blob, you can't rely on the length from the properties
+   * argument and you must get it from the file. Otherwise, you can.
+   */
+  private long getDataLength(CloudBlobWrapper blob, BlobProperties properties)
+    throws AzureException {
+    if (blob instanceof CloudPageBlobWrapper) {
+      try {
+        return PageBlobInputStream.getPageBlobSize((CloudPageBlobWrapper) blob,
+            getInstrumentedContext(
+                isConcurrentOOBAppendAllowed()));
+      } catch (Exception e) {
+        throw new AzureException(
+            "Unexpected exception getting page blob actual data size.", e);
+      }
+    }
+    return properties.getLength();
+  }
+
+  /**
+   * Deletes the given blob, taking special care that if we get a
+   * blob-not-found exception upon retrying the operation, we just
+   * swallow the error since what most probably happened is that
+   * the first operation succeeded on the server.
+   * @param blob The blob to delete.
+   * @param leaseID A string identifying the lease, or null if no
+   *        lease is to be used.
-  private void safeDelete(CloudBlockBlobWrapper blob) throws StorageException {
+  private void safeDelete(CloudBlobWrapper blob, SelfRenewingLease lease) throws StorageException {
-      blob.delete(operationContext);
+      blob.delete(operationContext, lease);
-      if (e.getErrorCode() != null && e.getErrorCode().equals("BlobNotFound")
-          && operationContext.getRequestResults().size() > 1
-          && operationContext.getRequestResults().get(0).getException() != null) {
+      if (e.getErrorCode() != null &&
+          e.getErrorCode().equals("BlobNotFound") &&
+          operationContext.getRequestResults().size() > 1 &&
+          operationContext.getRequestResults().get(0).getException() != null) {
+    } finally {
+      if (lease != null) {
+        lease.free();
+      }
-  public void delete(String key) throws IOException {
+  public void delete(String key, SelfRenewingLease lease) throws IOException {
-      // Get the blob reference an delete it.
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      // Get the blob reference and delete it.
+      CloudBlobWrapper blob = getBlobReference(key);
-        safeDelete(blob);
+        safeDelete(blob, lease);
+  public void delete(String key) throws IOException {
+    delete(key, null);
+  }
+
+  @Override
+    rename(srcKey, dstKey, false, null);
+  }
+
+  @Override
+  public void rename(String srcKey, String dstKey, boolean acquireLease,
+      SelfRenewingLease existingLease) throws IOException {
+    if (acquireLease && existingLease != null) {
+      throw new IOException("Cannot acquire new lease if one already exists.");
+    }
+
-      CloudBlockBlobWrapper srcBlob = getBlobReference(srcKey);
+      //
+      CloudBlobWrapper srcBlob = getBlobReference(srcKey);
-        throw new AzureException("Source blob " + srcKey + " does not exist.");
+        throw new AzureException ("Source blob " + srcKey +
+            " does not exist.");
+      }
+
+      /**
+       * Conditionally get a lease on the source blob to prevent other writers
+       * from changing it. This is used for correctness in HBase when log files
+       * are renamed. It generally should do no harm other than take a little
+       * more time for other rename scenarios. When the HBase master renames a
+       * log file folder, the lease locks out other writers.  This
+       * prevents a region server that the master thinks is dead, but is still
+       * alive, from committing additional updates.  This is different than
+       * when HBase runs on HDFS, where the region server recovers the lease
+       * on a log file, to gain exclusive access to it, before it splits it.
+       */
+      SelfRenewingLease lease = null;
+      if (acquireLease) {
+        lease = srcBlob.acquireLease();
+      } else if (existingLease != null) {
+        lease = existingLease;
-      CloudBlockBlobWrapper dstBlob = getBlobReference(dstKey);
+      //
+      CloudBlobWrapper dstBlob = getBlobReference(dstKey);
+
+      // TODO: Remove at the time when we move to Azure Java SDK 1.2+.
+      // This is the workaround provided by Azure Java SDK team to
+      // mitigate the issue with un-encoded x-ms-copy-source HTTP
+      // request header. Azure sdk version before 1.2+ does not encode this
+      // header what causes all URIs that have special (category "other")
+      // characters in the URI not to work with startCopyFromBlob when
+      // specified as source (requests fail with HTTP 403).
+      URI srcUri = new URI(srcBlob.getUri().toASCIIString());
-      dstBlob.startCopyFromBlob(srcBlob, getInstrumentedContext());
+      dstBlob.startCopyFromBlob(srcUri, getInstrumentedContext());
-      safeDelete(srcBlob);
+      safeDelete(srcBlob, lease);
-  private void waitForCopyToComplete(CloudBlockBlobWrapper blob,
-      OperationContext opContext) throws AzureException {
+  private void waitForCopyToComplete(CloudBlobWrapper blob, OperationContext opContext){
-    int exceptionCount = 0;
-      } catch (StorageException se) {
-        exceptionCount++;
-        if(exceptionCount > 10){
-          throw new AzureException("Too many storage exceptions during waitForCopyToComplete", se);
+      catch (StorageException se){
-      // test for null because mocked filesystem doesn't know about copystates
-      // yet.
-      copyInProgress = (blob.getCopyState() != null && blob.getCopyState()
-          .getStatus() == CopyStatus.PENDING);
+      // test for null because mocked filesystem doesn't know about copystates yet.
+      copyInProgress = (blob.getCopyState() != null && blob.getCopyState().getStatus() == CopyStatus.PENDING);
-        } catch (InterruptedException ie) {
-          Thread.currentThread().interrupt();
+          }
+          catch (InterruptedException ie){
+            //ignore
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
+  /**
+   * Get a lease on the blob identified by key. This lease will be renewed
+   * indefinitely by a background thread.
+   */
-  public void updateFolderLastModifiedTime(String key, Date lastModified)
-      throws AzureException {
+  public SelfRenewingLease acquireLease(String key) throws AzureException {
+    LOG.debug("acquiring lease on " + key);
-      CloudBlockBlobWrapper blob = getBlobReference(key);
-      blob.getProperties().setLastModified(lastModified);
-      blob.uploadProperties(getInstrumentedContext());
-    } catch (Exception e) {
-      // Caught exception while attempting update the properties. Re-throw as an
+      CloudBlobWrapper blob = getBlobReference(key);
+      return blob.acquireLease();
+    }
+    catch (Exception e) {
+
+      // Caught exception while attempting to get lease. Re-throw as an
-  public void updateFolderLastModifiedTime(String key) throws AzureException {
+  public void updateFolderLastModifiedTime(String key, Date lastModified,
+      SelfRenewingLease folderLease)
+      throws AzureException {
+    try {
+      checkContainer(ContainerAccessType.ReadThenWrite);
+      CloudBlobWrapper blob = getBlobReference(key);
+      blob.getProperties().setLastModified(lastModified);
+      blob.uploadProperties(getInstrumentedContext(), folderLease);
+    } catch (Exception e) {
+
+      // Caught exception while attempting to update the properties. Re-throw as an
+      // Azure storage exception.
+      throw new AzureException(e);
+    }
+  }
+
+  @Override
+  public void updateFolderLastModifiedTime(String key,
+      SelfRenewingLease folderLease) throws AzureException {
-    updateFolderLastModifiedTime(key, lastModified);
+    updateFolderLastModifiedTime(key, lastModified, folderLease);
