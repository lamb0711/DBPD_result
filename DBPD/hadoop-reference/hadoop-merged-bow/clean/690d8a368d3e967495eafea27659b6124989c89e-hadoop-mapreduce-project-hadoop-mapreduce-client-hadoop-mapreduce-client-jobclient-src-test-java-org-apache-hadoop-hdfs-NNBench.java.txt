MAPREDUCE-6663. [NNBench] Refactor nnbench as a Tool implementation. Contributed by Brahma Reddy Battula.

-import java.net.InetAddress;
+import org.apache.hadoop.HadoopIllegalArgumentException;
+import org.apache.hadoop.io.SequenceFile.Writer;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
-public class NNBench {
+public class NNBench extends Configured implements Tool {
-  protected static String CONTROL_DIR_NAME = "control";
-  protected static String OUTPUT_DIR_NAME = "output";
-  protected static String DATA_DIR_NAME = "data";
-  protected static final String DEFAULT_RES_FILE_NAME = "NNBench_results.log";
-  protected static final String NNBENCH_VERSION = "NameNode Benchmark 0.4";
-  
-  public static String operation = "none";
-  public static long numberOfMaps = 1l; // default is 1
-  public static long numberOfReduces = 1l; // default is 1
-  public static long startTime = 
+  private static String CONTROL_DIR_NAME = "control";
+  private static String OUTPUT_DIR_NAME = "output";
+  private static String DATA_DIR_NAME = "data";
+  static final String DEFAULT_RES_FILE_NAME = "NNBench_results.log";
+  private static final String NNBENCH_VERSION = "NameNode Benchmark 0.4";
+
+  private String operation = "none";
+  private long numberOfMaps = 1l; // default is 1
+  private long numberOfReduces = 1l; // default is 1
+  private long startTime =
-  public static long blockSize = 1l; // default is 1
-  public static int bytesToWrite = 0; // default is 0
-  public static long bytesPerChecksum = 1l; // default is 1
-  public static long numberOfFiles = 1l; // default is 1
-  public static short replicationFactorPerFile = 1; // default is 1
-  public static String baseDir = "/benchmarks/NNBench";  // default
-  public static boolean readFileAfterOpen = false; // default is to not read
-  
+  private long blockSize = 1l; // default is 1
+  private int bytesToWrite = 0; // default is 0
+  private long bytesPerChecksum = 1l; // default is 1
+  private long numberOfFiles = 1l; // default is 1
+  private short replicationFactorPerFile = 1; // default is 1
+  private String baseDir = "/benchmarks/NNBench";  // default
+  private boolean readFileAfterOpen = false; // default is to not read
+  private boolean isHelpMessage = false;
+  private static final int MAX_OPERATION_EXCEPTIONS = 1000;
-
-  private static Configuration config = new Configuration();
-  private static void cleanupBeforeTestrun() throws IOException {
-    FileSystem tempFS = FileSystem.get(config);
+  private void cleanupBeforeTestrun() throws IOException {
+    FileSystem tempFS = FileSystem.get(getConf());
-  private static void createControlFiles() throws IOException {
-    FileSystem tempFS = FileSystem.get(config);
+  private void createControlFiles() throws IOException {
-        writer = SequenceFile.createWriter(tempFS, config, filePath, Text.class, 
-                LongWritable.class, CompressionType.NONE);
+        writer = SequenceFile.createWriter(getConf(), Writer.file(filePath),
+            Writer.keyClass(Text.class), Writer.valueClass(LongWritable.class),
+            Writer.compression(CompressionType.NONE));
-  public static void checkArgs(final int index, final int length) {
+  private static void checkArgs(final int index, final int length) {
-      System.exit(-1);
+      throw new HadoopIllegalArgumentException("Not enough arguments");
+   *  @param args array of command line's parameters to be parsed
-   * @param args array of command line's parameters to be parsed
-  public static void parseInputs(final String[] args) {
+  private void parseInputs(final String[] args) {
-      System.exit(-1);
+      throw new HadoopIllegalArgumentException("Give valid inputs");
-        System.exit(-1);
+        isHelpMessage = true;
-    config.set("test.nnbench.operation", operation);
-    config.setLong("test.nnbench.maps", numberOfMaps);
-    config.setLong("test.nnbench.reduces", numberOfReduces);
-    config.setLong("test.nnbench.starttime", startTime);
-    config.setLong("test.nnbench.blocksize", blockSize);
-    config.setInt("test.nnbench.bytestowrite", bytesToWrite);
-    config.setLong("test.nnbench.bytesperchecksum", bytesPerChecksum);
-    config.setLong("test.nnbench.numberoffiles", numberOfFiles);
-    config.setInt("test.nnbench.replicationfactor", 
+    getConf().set("test.nnbench.operation", operation);
+    getConf().setLong("test.nnbench.maps", numberOfMaps);
+    getConf().setLong("test.nnbench.reduces", numberOfReduces);
+    getConf().setLong("test.nnbench.starttime", startTime);
+    getConf().setLong("test.nnbench.blocksize", blockSize);
+    getConf().setInt("test.nnbench.bytestowrite", bytesToWrite);
+    getConf().setLong("test.nnbench.bytesperchecksum", bytesPerChecksum);
+    getConf().setLong("test.nnbench.numberoffiles", numberOfFiles);
+    getConf().setInt("test.nnbench.replicationfactor",
-    config.set("test.nnbench.basedir", baseDir);
-    config.setBoolean("test.nnbench.readFileAfterOpen", readFileAfterOpen);
+    getConf().set("test.nnbench.basedir", baseDir);
+    getConf().setBoolean("test.nnbench.readFileAfterOpen", readFileAfterOpen);
-    config.set("test.nnbench.datadir.name", DATA_DIR_NAME);
-    config.set("test.nnbench.outputdir.name", OUTPUT_DIR_NAME);
-    config.set("test.nnbench.controldir.name", CONTROL_DIR_NAME);
+    getConf().set("test.nnbench.datadir.name", DATA_DIR_NAME);
+    getConf().set("test.nnbench.outputdir.name", OUTPUT_DIR_NAME);
+    getConf().set("test.nnbench.controldir.name", CONTROL_DIR_NAME);
-   * 
-  private static void analyzeResults() throws IOException {
-    final FileSystem fs = FileSystem.get(config);
+  private int analyzeResults() throws IOException {
+    final FileSystem fs = FileSystem.get(getConf());
-      DataInputStream in;
-      in = new DataInputStream(fs.open(reduceFile));
+      try (DataInputStream in = new DataInputStream(fs.open(reduceFile));
+          BufferedReader lines =
+              new BufferedReader(new InputStreamReader(in))) {
-      BufferedReader lines;
-      lines = new BufferedReader(new InputStreamReader(in));
-
-      String line;
-      while ((line = lines.readLine()) != null) {
-        StringTokenizer tokens = new StringTokenizer(line, " \t\n\r\f%;");
-        String attr = tokens.nextToken();
-        if (attr.endsWith(":totalTimeAL1")) {
-          totalTimeAL1 = Long.parseLong(tokens.nextToken());
-        } else if (attr.endsWith(":totalTimeAL2")) {
-          totalTimeAL2 = Long.parseLong(tokens.nextToken());
-        } else if (attr.endsWith(":totalTimeTPmS")) {
-          totalTimeTPmS = Long.parseLong(tokens.nextToken());
-        } else if (attr.endsWith(":latemaps")) {
-          lateMaps = Long.parseLong(tokens.nextToken());
-        } else if (attr.endsWith(":numOfExceptions")) {
-          numOfExceptions = Long.parseLong(tokens.nextToken());
-        } else if (attr.endsWith(":successfulFileOps")) {
-          successfulFileOps = Long.parseLong(tokens.nextToken());
-        } else if (attr.endsWith(":mapStartTimeTPmS")) {
-          mapStartTimeTPmS = Long.parseLong(tokens.nextToken());
-        } else if (attr.endsWith(":mapEndTimeTPmS")) {
-          mapEndTimeTPmS = Long.parseLong(tokens.nextToken());
+        String line;
+        while ((line = lines.readLine()) != null) {
+          StringTokenizer tokens = new StringTokenizer(line, " \t\n\r\f%;");
+          String attr = tokens.nextToken();
+          if (attr.endsWith(":totalTimeAL1")) {
+            totalTimeAL1 = Long.parseLong(tokens.nextToken());
+          } else if (attr.endsWith(":totalTimeAL2")) {
+            totalTimeAL2 = Long.parseLong(tokens.nextToken());
+          } else if (attr.endsWith(":totalTimeTPmS")) {
+            totalTimeTPmS = Long.parseLong(tokens.nextToken());
+          } else if (attr.endsWith(":latemaps")) {
+            lateMaps = Long.parseLong(tokens.nextToken());
+          } else if (attr.endsWith(":numOfExceptions")) {
+            numOfExceptions = Long.parseLong(tokens.nextToken());
+          } else if (attr.endsWith(":successfulFileOps")) {
+            successfulFileOps = Long.parseLong(tokens.nextToken());
+          } else if (attr.endsWith(":mapStartTimeTPmS")) {
+            mapStartTimeTPmS = Long.parseLong(tokens.nextToken());
+          } else if (attr.endsWith(":mapEndTimeTPmS")) {
+            mapEndTimeTPmS = Long.parseLong(tokens.nextToken());
+          }
-    PrintStream res = new PrintStream(new FileOutputStream(
-            new File(DEFAULT_RES_FILE_NAME), true));
-    
-    // Write to a file and also dump to log
-    for(int i = 0; i < resultLines.length; i++) {
-      LOG.info(resultLines[i]);
-      res.println(resultLines[i]);
+    try (PrintStream res = new PrintStream(
+        new FileOutputStream(new File(DEFAULT_RES_FILE_NAME), true))) {
+      // Write to a file and also dump to log
+      for (String resultLine : resultLines) {
+        LOG.info(resultLine);
+        res.println(resultLine);
+      }
+    if(numOfExceptions >= MAX_OPERATION_EXCEPTIONS){
+      return -1;
+    }
+    return 0;
-  
+
-  public static void runTests() throws IOException {
-    config.setLong("io.bytes.per.checksum", bytesPerChecksum);
+  private void runTests() throws IOException {
+    getConf().setLong("io.bytes.per.checksum", bytesPerChecksum);
-    JobConf job = new JobConf(config, NNBench.class);
+    JobConf job = new JobConf(getConf(), NNBench.class);
-  public static void validateInputs() {
+  private void validateInputs() {
-      System.exit(-1);
+      throw new HadoopIllegalArgumentException(
+          "Error: Unknown operation: " + operation);
-      System.exit(-1);
+      throw new HadoopIllegalArgumentException(
+          "Error: Number of maps must be a positive number");
-      System.exit(-1);
+      throw new HadoopIllegalArgumentException(
+          "Error: Number of reduces must be a positive number");
-      System.exit(-1);
+      throw new HadoopIllegalArgumentException(
+          "Error: Block size must be a positive number");
-      System.exit(-1);
+      throw new HadoopIllegalArgumentException(
+          "Error: Bytes to write must be a positive number");
-      System.exit(-1);
+      throw new HadoopIllegalArgumentException(
+          "Error: Bytes per checksum must be a positive number");
-      System.exit(-1);
+      throw new HadoopIllegalArgumentException(
+          "Error: Number of files must be a positive number");
-      System.exit(-1);
+      throw new HadoopIllegalArgumentException(
+          "Error: Replication factor must be a positive number");
-      System.err.println("Error: Block Size in bytes must be a multiple of " +
-              "bytes per checksum: ");
+      System.err.println("Error: Block Size in bytes must be a multiple of "
+          + "bytes per checksum: ");
-      System.exit(-1);
+      throw new HadoopIllegalArgumentException(
+          "Error: Block Size in bytes must be a multiple of "
+              + "bytes per checksum:");
-  public static void main(String[] args) throws IOException {
+  public static void main(String[] args) throws Exception {
+    int res = ToolRunner.run(new NNBench(), args);
+    System.exit(res);
+  }
+
+  @Override
+  public int run(String[] args) throws Exception {
-    
+    if (isHelpMessage) {
+      return 0;
+    }
+
-    analyzeResults();
+    return analyzeResults();
-    private String hostName = null;
-    final int MAX_OPERATION_EXCEPTIONS = 1000;
-      
-      try {
-        hostName = InetAddress.getLocalHost().getHostName();
-      } catch(Exception e) {
-        throw new RuntimeException("Error getting hostname", e);
-      }
-      Configuration conf = filesystem.getConf();
+      Configuration conf = getConf();

MOV26 INS26 INS26 INS26 UPD40 INS40 INS40 INS40 INS43 INS43 INS23 MOV23 INS31 INS42 INS42 UPD83 UPD83 UPD83 UPD83 UPD83 UPD83 UPD83 UPD83 UPD83 UPD83 UPD83 UPD83 UPD83 UPD83 UPD83 MOV83 INS39 INS59 INS83 INS83 UPD83 UPD83 UPD39 UPD83 UPD83 MOV29 INS83 INS83 INS39 INS42 INS44 INS43 INS8 INS78 UPD39 UPD42 MOV43 INS42 INS9 INS54 INS25 INS41 INS5 INS42 UPD42 MOV42 INS60 INS21 INS42 UPD42 MOV42 INS25 INS41 INS58 INS8 INS27 INS8 INS34 INS43 INS85 INS39 INS59 INS32 INS42 INS8 MOV32 INS53 INS53 INS32 INS32 INS32 INS32 INS32 INS32 INS32 INS32 INS32 INS32 INS32 INS32 INS32 INS32 INS54 MOV43 MOV59 INS70 INS42 INS42 INS41 INS32 INS53 INS53 INS53 INS53 INS53 INS53 INS53 INS53 INS53 INS42 UPD42 MOV42 INS32 INS42 INS42 INS42 INS41 INS32 INS14 INS14 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 INS32 INS58 INS58 INS8 INS44 INS42 INS8 MOV38 UPD42 MOV42 INS32 INS14 INS14 INS14 INS14 INS14 INS14 INS14 INS14 INS14 INS42 INS42 INS14 INS42 INS34 UPD42 MOV42 INS43 INS45 INS43 INS45 UPD42 MOV42 MOV43 INS59 MOV43 INS59 MOV60 MOV61 MOV43 INS42 MOV21 MOV21 INS42 INS43 INS27 INS43 INS45 INS43 INS45 INS43 INS45 INS43 INS45 INS43 INS45 INS43 INS45 INS43 INS45 INS43 INS27 UPD43 MOV43 INS42 INS42 MOV42 MOV14 MOV42 MOV14 INS42 INS45 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS45 INS45 UPD42 INS42 INS42 INS32 INS32 INS32 INS32 INS32 UPD42 MOV42 UPD42 MOV42 INS42 MOV42 INS42 INS42 MOV57 INS42 INS42 MOV57 INS42 INS42 MOV40 INS7 INS42 INS9 DEL83 DEL83 DEL83 DEL83 DEL83 DEL83 DEL83 DEL83 DEL83 DEL83 DEL83 DEL83 DEL83 DEL43 DEL14 DEL59 DEL23 DEL83 DEL83 DEL42 DEL43 DEL42 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL83 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL38 DEL32 DEL83 DEL59 DEL60 DEL42 DEL7 DEL21 DEL59 DEL60 DEL42 DEL7 DEL21 DEL42 DEL42 DEL2 DEL42 DEL42 DEL2 DEL60 DEL39 DEL42 DEL34 DEL59 DEL58 DEL42 DEL40 DEL27 DEL42 DEL37 DEL8 DEL24 DEL83 DEL42 DEL83 DEL42 DEL42 DEL32 DEL21 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL83 DEL21 DEL83 DEL42 DEL33 DEL59 DEL23 DEL42 DEL42 DEL42 DEL32 DEL42 DEL32 DEL7 DEL21 DEL8 DEL42 DEL44 DEL42 DEL43 DEL45 DEL42 DEL14 DEL53 DEL8 DEL12 DEL54 DEL42