HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68

+import static org.apache.hadoop.hdfs.server.common.Util.now;
+
+import org.apache.hadoop.HadoopIllegalArgumentException;
-  //
-  // Store blocks-->datanodedescriptor(s) map of corrupt replicas
-  //
-  private final CorruptReplicasMap corruptReplicas = new CorruptReplicasMap();
+  /** Store blocks -> datanodedescriptor(s) map of corrupt replicas */
+  final CorruptReplicasMap corruptReplicas = new CorruptReplicasMap();
-  //  The maximum number of replicas allowed for a block
+  /** The maximum number of replicas allowed for a block */
-  //  How many outgoing replication streams a given node should have at one time
+  /** The maximum number of outgoing replication streams
+   *  a given node should have at one time 
+   */
-  // Minimum copies needed or else write is disallowed
+  /** Minimum copies needed or else write is disallowed */
-  // Default number of replicas
+  /** Default number of replicas */
-  // How many entries are returned by getCorruptInodes()
+  /** The maximum number of entries returned by getCorruptInodes() */
-  // variable to enable check for enough racks 
+  /** variable to enable check for enough racks */
-  /**
-   * Last block index used for replication work.
-   */
+  /** Last block index used for replication work. */
-  // for block replicas placement
-  public final BlockPlacementPolicy replicator;
+  /** for block replicas placement */
+  private BlockPlacementPolicy blockplacement;
-    datanodeManager = new DatanodeManager(fsn);
+    datanodeManager = new DatanodeManager(fsn, conf);
-    replicator = BlockPlacementPolicy.getInstance(
+    blockplacement = BlockPlacementPolicy.getInstance(
+  /** @return the BlockPlacementPolicy */
+  public BlockPlacementPolicy getBlockPlacementPolicy() {
+    return blockplacement;
+  }
+
+  /** Set BlockPlacementPolicy */
+  public void setBlockPlacementPolicy(BlockPlacementPolicy newpolicy) {
+    if (newpolicy == null) {
+      throw new HadoopIllegalArgumentException("newpolicy == null");
+    }
+    this.blockplacement = newpolicy;
+  }
+
-  void removeFromInvalidates(String storageID, Block block) {
+  private void removeFromInvalidates(String storageID, Block block) {
-                       replicator.chooseTarget(fileINode, additionalReplRequired,
+                       blockplacement.chooseTarget(fileINode, additionalReplRequired,
-    final DatanodeDescriptor targets[] = replicator.chooseTarget(
+    final DatanodeDescriptor targets[] = blockplacement.chooseTarget(
-  void reportDiff(DatanodeDescriptor dn, 
+  private void reportDiff(DatanodeDescriptor dn, 
-        addedNode, delNodeHint, replicator);
+        addedNode, delNodeHint, blockplacement);
-  public void removeStoredBlock(Block block, DatanodeDescriptor node) {
+  private void removeStoredBlock(Block block, DatanodeDescriptor node) {
-  public void processOverReplicatedBlocksOnReCommission(DatanodeDescriptor srcNode) {
+  private void processOverReplicatedBlocksOnReCommission(
+      final DatanodeDescriptor srcNode) {
-  public boolean isReplicationInProgress(DatanodeDescriptor srcNode) {
+  boolean isReplicationInProgress(DatanodeDescriptor srcNode) {
-  public void removeFromInvalidates(String storageID) {
+  private void removeFromInvalidates(String storageID) {
-  
-  //Returns the number of racks over which a given block is replicated
-  //decommissioning/decommissioned nodes are not counted. corrupt replicas 
-  //are also ignored
-  public int getNumberOfRacks(Block b) {
-    HashSet<String> rackSet = new HashSet<String>(0);
-    Collection<DatanodeDescriptor> corruptNodes = 
-                                  corruptReplicas.getNodes(b);
-    for (Iterator<DatanodeDescriptor> it = blocksMap.nodeIterator(b); 
-         it.hasNext();) {
-      DatanodeDescriptor cur = it.next();
-      if (!cur.isDecommissionInProgress() && !cur.isDecommissioned()) {
-        if ((corruptNodes == null ) || !corruptNodes.contains(cur)) {
-          String rackName = cur.getNetworkLocation();
-          if (!rackSet.contains(rackName)) {
-            rackSet.add(rackName);
-          }
-        }
-      }
-    }
-    return rackSet.size();
-  }
+
+  /**
+   * Change, if appropriate, the admin state of a datanode to 
+   * decommission completed. Return true if decommission is complete.
+   */
+  boolean checkDecommissionStateInternal(DatanodeDescriptor node) {
+    // Check to see if all blocks in this decommissioned
+    // node has reached their target replication factor.
+    if (node.isDecommissionInProgress()) {
+      if (!isReplicationInProgress(node)) {
+        node.setDecommissioned();
+        LOG.info("Decommission complete for node " + node.getName());
+      }
+    }
+    return node.isDecommissioned();
+  }
+
+  /** Start decommissioning the specified datanode. */
+  void startDecommission(DatanodeDescriptor node) throws IOException {
+    if (!node.isDecommissionInProgress() && !node.isDecommissioned()) {
+      LOG.info("Start Decommissioning node " + node.getName() + " with " + 
+          node.numBlocks() +  " blocks.");
+      synchronized (namesystem.heartbeats) {
+        namesystem.updateStats(node, false);
+        node.startDecommission();
+        namesystem.updateStats(node, true);
+      }
+      node.decommissioningStatus.setStartTime(now());
+      
+      // all the blocks that reside on this node have to be replicated.
+      checkDecommissionStateInternal(node);
+    }
+  }
+
+  /** Stop decommissioning the specified datanodes. */
+  void stopDecommission(DatanodeDescriptor node) throws IOException {
+    if (node.isDecommissionInProgress() || node.isDecommissioned()) {
+      LOG.info("Stop Decommissioning node " + node.getName());
+      synchronized (namesystem.heartbeats) {
+        namesystem.updateStats(node, false);
+        node.stopDecommission();
+        namesystem.updateStats(node, true);
+      }
+      processOverReplicatedBlocksOnReCommission(node);
+    }
+  }

INS26 INS26 INS40 INS40 INS31 INS31 INS31 INS31 INS31 INS29 INS29 INS29 INS29 INS29 INS29 INS29 INS29 UPD83 INS29 MOV83 INS43 INS42 INS8 INS29 INS83 UPD39 MOV39 UPD42 MOV42 MOV44 INS8 INS83 INS83 UPD83 UPD83 UPD83 INS29 INS39 INS42 INS44 INS8 INS29 INS39 INS42 INS44 INS43 INS8 INS29 INS39 INS42 INS44 INS43 INS8 INS65 INS65 INS65 INS65 INS65 INS65 INS65 INS65 UPD42 INS65 INS42 INS41 INS65 UPD43 UPD42 INS25 INS21 INS83 INS65 INS43 INS42 INS25 INS41 INS65 INS43 INS42 INS42 INS25 INS65 INS43 INS42 INS42 INS25 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS66 UPD66 INS66 INS66 INS42 INS66 UPD42 INS27 INS8 INS7 INS66 INS66 MOV42 INS32 INS8 INS32 INS66 MOV42 MOV27 INS8 INS66 UPD42 MOV42 INS27 INS8 UPD42 INS42 INS33 INS53 INS22 INS42 UPD42 UPD42 MOV42 UPD42 MOV42 INS25 UPD42 MOV42 UPD42 MOV42 INS21 INS51 INS21 INS21 INS32 INS32 INS21 INS51 INS21 INS42 INS14 INS52 INS42 UPD42 UPD42 INS38 INS8 INS32 INS40 INS8 INS32 INS32 INS42 INS42 UPD42 MOV42 UPD42 MOV42 INS32 INS40 INS8 INS32 INS43 INS45 INS32 INS21 INS21 UPD42 UPD42 INS42 INS42 INS27 INS21 INS21 INS21 INS40 INS42 INS32 INS42 INS42 UPD42 MOV42 UPD42 MOV42 INS27 MOV21 INS21 INS21 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 INS42 INS42 INS32 INS32 INS45 INS32 INS45 INS32 INS45 INS32 INS32 INS32 INS42 INS45 INS32 INS32 INS32 INS42 INS42 UPD42 MOV42 INS42 INS27 INS42 INS42 INS42 INS42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 INS9 INS42 INS42 INS42 INS42 INS42 INS9 INS42 UPD42 MOV42 UPD42 UPD42 UPD42 INS9 INS42 INS42 INS42 INS42 INS42 INS9 INS45 INS32 UPD42 MOV42 UPD42 MOV42 DEL83 DEL83 DEL83 DEL42 DEL43 DEL43 DEL74 DEL42 DEL42 DEL43 DEL42 DEL43 DEL74 DEL34 DEL14 DEL59 DEL60 DEL42 DEL43 DEL74 DEL42 DEL42 DEL32 DEL59 DEL60 DEL42 DEL43 DEL42 DEL43 DEL74 DEL42 DEL32 DEL59 DEL58 DEL32 DEL43 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL42 DEL33 DEL27 DEL36 DEL32 DEL38 DEL27 DEL43 DEL42 DEL32 DEL59 DEL60 DEL32 DEL38 DEL8 DEL25 DEL8 DEL25 DEL8 DEL25 DEL8 DEL24 DEL32 DEL41 DEL8 DEL31