HDFS-7369. Erasure coding: distribute recovery work for striped blocks to DataNode. Contributed by Zhe Zhang.

+import org.apache.hadoop.hdfs.protocol.HdfsConstants;
-    chooseSourceDatanode(block, containingNodes,
+    chooseSourceDatanodes(getStoredBlock(block), containingNodes,
-        UnderReplicatedBlocks.LEVEL);
+        new LinkedList<Short>(), 1, UnderReplicatedBlocks.LEVEL);
-   * Scan blocks in {@link #neededReplications} and assign replication
-   * work to data-nodes they belong to.
+   * Scan blocks in {@link #neededReplications} and assign recovery
+   * (replication or erasure coding) work to data-nodes they belong to.
-  int computeReplicationWork(int blocksToProcess) {
+  int computeBlockRecoveryWork(int blocksToProcess) {
-    return computeReplicationWorkForBlocks(blocksToReplicate);
+    return computeRecoveryWorkForBlocks(blocksToReplicate);
-  /** Replicate a set of blocks
+  /**
+   * Recover a set of blocks to full strength through replication or
+   * erasure coding
-   * @param blocksToReplicate blocks to be replicated, for each priority
+   * @param blocksToRecover blocks to be recovered, for each priority
-  int computeReplicationWorkForBlocks(List<List<BlockInfo>> blocksToReplicate) {
+  int computeRecoveryWorkForBlocks(List<List<BlockInfo>> blocksToRecover) {
-    DatanodeDescriptor srcNode;
-    List<ReplicationWork> work = new LinkedList<ReplicationWork>();
+    List<BlockRecoveryWork> recovWork = new LinkedList<>();
+    // Step 1: categorize at-risk blocks into replication and EC tasks
-        for (int priority = 0; priority < blocksToReplicate.size(); priority++) {
-          for (BlockInfo block : blocksToReplicate.get(priority)) {
+        for (int priority = 0; priority < blocksToRecover.size(); priority++) {
+          for (BlockInfo block : blocksToRecover.get(priority)) {
-            containingNodes = new ArrayList<DatanodeDescriptor>();
-            List<DatanodeStorageInfo> liveReplicaNodes = new ArrayList<DatanodeStorageInfo>();
+            containingNodes = new ArrayList<>();
+            List<DatanodeStorageInfo> liveReplicaNodes = new ArrayList<>();
-            srcNode = chooseSourceDatanode(
+            List<Short> missingBlockIndices = new LinkedList<>();
+            DatanodeDescriptor[] srcNodes;
+            int numSourceNodes = bc.isStriped() ?
+                HdfsConstants.NUM_DATA_BLOCKS : 1;
+            srcNodes = chooseSourceDatanodes(
-                priority);
-            if(srcNode == null) { // block can not be replicated from any node
-              LOG.debug("Block " + block + " cannot be repl from any node");
+                missingBlockIndices, numSourceNodes, priority);
+            if(srcNodes == null || srcNodes.length == 0) {
+              // block can not be replicated from any node
+              LOG.debug("Block " + block + " cannot be recovered " +
+                  "from any node");
-            // liveReplicaNodes can include READ_ONLY_SHARED replicas which are 
+            // liveReplicaNodes can include READ_ONLY_SHARED replicas which are
-      
+
-            work.add(new ReplicationWork(block, bc, srcNode,
-                containingNodes, liveReplicaNodes, additionalReplRequired,
-                priority));
+            if (bc.isStriped()) {
+              ErasureCodingWork ecw = new ErasureCodingWork(block, bc, srcNodes,
+                  containingNodes, liveReplicaNodes, additionalReplRequired,
+                  priority);
+              short[] missingBlockArray = new short[missingBlockIndices.size()];
+              for (int i = 0 ; i < missingBlockIndices.size(); i++) {
+                missingBlockArray[i] = missingBlockIndices.get(i);
+              }
+              ecw.setMissingBlockIndices(missingBlockArray);
+              recovWork.add(ecw);
+            } else {
+              recovWork.add(new ReplicationWork(block, bc, srcNodes,
+                  containingNodes, liveReplicaNodes, additionalReplRequired,
+                  priority));
+            }
+    // Step 2: choose target nodes for each recovery task
-    for(ReplicationWork rw : work){
+    for(BlockRecoveryWork rw : recovWork){
+    // Step 3: add tasks to the DN
-      for(ReplicationWork rw : work){
+      for(BlockRecoveryWork rw : recovWork){
-            if (rw.srcNode.getNetworkLocation().equals(
+            if (rw.srcNodes[0].getNetworkLocation().equals(
-          rw.srcNode.addBlockToBeReplicated(block, targets);
+          if (bc.isStriped()) {
+            assert rw instanceof ErasureCodingWork;
+            assert rw.targets.length > 0;
+            rw.targets[0].getDatanodeDescriptor().addBlockToBeErasureCoded(
+                new ExtendedBlock(namesystem.getBlockPoolId(), block),
+                rw.srcNodes, rw.targets,
+                ((ErasureCodingWork)rw).getMissingBlockIndicies());
+          }
+          else {
+            rw.srcNodes[0].addBlockToBeReplicated(block, targets);
+          }
-      for(ReplicationWork rw : work){
+      for(BlockRecoveryWork rw : recovWork){
-          blockLog.info("BLOCK* ask {} to replicate {} to {}", rw.srcNode,
+          blockLog.info("BLOCK* ask {} to replicate {} to {}", rw.srcNodes,
-   * Parse the data-nodes the block belongs to and choose one,
-   * which will be the replication source.
+   * Parse the data-nodes the block belongs to and choose a certain number
+   * from them to be the recovery sources.
-   * Otherwise we choose a random node among those that did not reach their
-   * replication limits.  However, if the replication is of the highest priority
-   * and all nodes have reached their replication limits, we will choose a
-   * random node despite the replication limit.
+   * Otherwise we randomly choose nodes among those that did not reach their
+   * replication limits. However, if the recovery work is of the highest
+   * priority and all nodes have reached their replication limits, we will
+   * randomly choose the desired number of nodes despite the replication limit.
-   * @param containingNodes List to be populated with nodes found to contain the 
-   *                        given block
-   * @param nodesContainingLiveReplicas List to be populated with nodes found to
-   *                                    contain live replicas of the given block
-   * @param numReplicas NumberReplicas instance to be initialized with the 
-   *                                   counts of live, corrupt, excess, and
-   *                                   decommissioned replicas of the given
-   *                                   block.
+   * @param containingNodes List to be populated with nodes found to contain
+   *                        the given block
+   * @param nodesContainingLiveReplicas List to be populated with nodes found
+   *                                    to contain live replicas of the given
+   *                                    block
+   * @param numReplicas NumberReplicas instance to be initialized with the
+   *                    counts of live, corrupt, excess, and decommissioned
+   *                    replicas of the given block.
+   * @param missingBlockIndices List to be populated with indices of missing
+   *                            blocks in a striped block group or missing
+   *                            replicas of a replicated block
+   * @param numSourceNodes integer specifying the number of source nodes to
+   *                       choose
-   * @return the DatanodeDescriptor of the chosen node from which to replicate
-   *         the given block
+   * @return the array of DatanodeDescriptor of the chosen nodes from which to
+   *         recover the given block
-   @VisibleForTesting
-   DatanodeDescriptor chooseSourceDatanode(Block block,
-       List<DatanodeDescriptor> containingNodes,
-       List<DatanodeStorageInfo>  nodesContainingLiveReplicas,
-       NumberReplicas numReplicas,
-       int priority) {
+  @VisibleForTesting
+  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,
+      List<DatanodeDescriptor> containingNodes,
+      List<DatanodeStorageInfo> nodesContainingLiveReplicas,
+      NumberReplicas numReplicas,
+      List<Short> missingBlockIndices, int numSourceNodes, int priority) {
-    DatanodeDescriptor srcNode = null;
+    LinkedList<DatanodeDescriptor> srcNodes = new LinkedList<>();
-    
+    missingBlockIndices.clear();
+    Set<Short> healthyIndices = new HashSet<>();
+
+      if (block.isStriped()) {
+        healthyIndices.add((short) ((BlockInfoStriped) block).
+            getStorageBlockIndex(storage));
+      }
-      int countableReplica = storage.getState() == State.NORMAL ? 1 : 0; 
+      int countableReplica = storage.getState() == State.NORMAL ? 1 : 0;
-      if (srcNode == null) {
-        srcNode = node;
+      if(srcNodes.size() < numSourceNodes) {
+        srcNodes.add(node);
-      if(ThreadLocalRandom.current().nextBoolean())
-        srcNode = node;
+      if(ThreadLocalRandom.current().nextBoolean()) {
+        int pos = ThreadLocalRandom.current().nextInt(numSourceNodes);
+        if(!srcNodes.get(pos).isDecommissionInProgress()) {
+          srcNodes.set(pos, node);
+        }
+      }
+    }
+    if (block.isStriped()) {
+      for (short i = 0; i < HdfsConstants.NUM_DATA_BLOCKS +
+          HdfsConstants.NUM_PARITY_BLOCKS; i++) {
+        if (!healthyIndices.contains(i)) {
+          missingBlockIndices.add(i);
+        }
+      }
-    return srcNode;
+    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);
-  
+
-   * Periodically calls computeReplicationWork().
+   * Periodically calls computeBlockRecoveryWork().
-    int workFound = this.computeReplicationWork(blocksToProcess);
+    int workFound = this.computeBlockRecoveryWork(blocksToProcess);
-  private static class ReplicationWork {
+  /**
+   * This class is used internally by {@link this#computeRecoveryWorkForBlocks}
+   * to represent a task to recover a block through replication or erasure
+   * coding. Recovery is done by transferring data from {@link srcNodes} to
+   * {@link targets}
+   */
+  private static class BlockRecoveryWork {
+    protected final BlockInfo block;
+    protected final BlockCollection bc;
-    private final BlockInfo block;
-    private final BlockCollection bc;
+    /**
+     * An erasure coding recovery task has multiple source nodes.
+     * A replication task only has 1 source node, stored on top of the array
+     */
+    protected final DatanodeDescriptor[] srcNodes;
+    /** Nodes containing the block; avoid them in choosing new targets */
+    protected final List<DatanodeDescriptor> containingNodes;
+    /** Required by {@link BlockPlacementPolicy#chooseTarget} */
+    protected final List<DatanodeStorageInfo> liveReplicaStorages;
+    protected final int additionalReplRequired;
-    private final DatanodeDescriptor srcNode;
-    private final List<DatanodeDescriptor> containingNodes;
-    private final List<DatanodeStorageInfo> liveReplicaStorages;
-    private final int additionalReplRequired;
+    protected DatanodeStorageInfo[] targets;
+    protected final int priority;
-    private DatanodeStorageInfo targets[];
-    private final int priority;
-
-    public ReplicationWork(BlockInfo block,
+    public BlockRecoveryWork(BlockInfo block,
-        DatanodeDescriptor srcNode,
+        DatanodeDescriptor[] srcNodes,
-      this.srcNode = srcNode;
-      this.srcNode.incrementPendingReplicationWithoutTargets();
+      this.srcNodes = srcNodes;
-    
-    private void chooseTargets(BlockPlacementPolicy blockplacement,
+
+    protected void chooseTargets(BlockPlacementPolicy blockplacement,
+    }
+  }
+
+  private static class ReplicationWork extends BlockRecoveryWork {
+
+    public ReplicationWork(BlockInfo block,
+        BlockCollection bc,
+        DatanodeDescriptor[] srcNodes,
+        List<DatanodeDescriptor> containingNodes,
+        List<DatanodeStorageInfo> liveReplicaStorages,
+        int additionalReplRequired,
+        int priority) {
+      super(block, bc, srcNodes, containingNodes,
+          liveReplicaStorages, additionalReplRequired, priority);
+      LOG.debug("Creating a ReplicationWork to recover " + block);
+    }
+
+    protected void chooseTargets(BlockPlacementPolicy blockplacement,
+        BlockStoragePolicySuite storagePolicySuite,
+        Set<Node> excludedNodes) {
+      assert srcNodes.length > 0
+          : "At least 1 source node should have been selected";
-            additionalReplRequired, srcNode, liveReplicaStorages, false,
+            additionalReplRequired, srcNodes[0], liveReplicaStorages, false,
-        srcNode.decrementPendingReplicationWithoutTargets();
+        srcNodes[0].decrementPendingReplicationWithoutTargets();
+      }
+    }
+  }
+
+  private static class ErasureCodingWork extends BlockRecoveryWork {
+
+    private short[] missingBlockIndicies = null;
+
+    public ErasureCodingWork(BlockInfo block,
+        BlockCollection bc,
+        DatanodeDescriptor[] srcNodes,
+        List<DatanodeDescriptor> containingNodes,
+        List<DatanodeStorageInfo> liveReplicaStorages,
+        int additionalReplRequired,
+        int priority) {
+      super(block, bc, srcNodes, containingNodes,
+          liveReplicaStorages, additionalReplRequired, priority);
+      LOG.debug("Creating an ErasureCodingWork to recover " + block);
+    }
+
+    public short[] getMissingBlockIndicies() {
+      return missingBlockIndicies;
+    }
+
+    public void setMissingBlockIndices(short[] missingBlockIndicies) {
+      this.missingBlockIndicies = missingBlockIndicies;
+    }
+
+    protected void chooseTargets(BlockPlacementPolicy blockplacement,
+        BlockStoragePolicySuite storagePolicySuite,
+        Set<Node> excludedNodes) {
+      try {
+        // TODO: new placement policy for EC considering multiple writers
+        targets = blockplacement.chooseTarget(bc.getName(),
+            additionalReplRequired, srcNodes[0], liveReplicaStorages, false,
+            excludedNodes, block.getNumBytes(),
+            storagePolicySuite.getPolicy(bc.getStoragePolicyID()));
+      } finally {

INS26 INS40 INS55 INS55 UPD42 UPD42 INS5 UPD42 INS44 INS44 INS29 UPD42 INS31 INS83 INS83 INS42 INS43 INS31 MOV31 INS83 INS83 INS42 INS43 INS23 INS31 INS31 INS31 INS31 UPD42 INS65 INS65 MOV43 INS85 UPD43 INS74 INS42 INS39 INS42 INS21 INS60 INS25 INS65 UPD83 UPD83 INS29 UPD83 INS5 INS29 UPD83 INS29 UPD83 UPD83 UPD83 INS5 UPD83 UPD42 INS83 INS39 INS42 MOV44 MOV44 MOV44 INS8 INS42 INS83 INS42 INS44 INS44 INS44 INS44 INS44 INS44 INS44 INS8 UPD83 INS44 INS44 INS44 INS42 INS83 INS5 INS59 INS83 INS42 INS44 INS44 INS44 INS44 INS44 INS44 INS44 INS8 INS83 INS5 INS42 INS8 INS83 INS39 INS42 INS44 INS8 INS83 INS39 INS42 INS44 INS44 INS44 INS8 UPD66 UPD66 UPD66 INS66 UPD42 UPD66 UPD74 UPD42 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 INS66 UPD66 UPD66 UPD66 INS42 INS66 INS66 INS66 INS42 INS66 INS66 UPD66 UPD66 UPD42 INS43 INS43 INS74 INS32 INS74 INS59 INS32 INS8 INS32 UPD66 INS66 INS65 INS66 INS66 INS65 INS66 INS65 INS65 MOV43 INS85 UPD42 INS65 INS65 MOV43 INS85 INS5 UPD42 INS43 INS42 INS43 INS42 INS5 INS42 INS74 INS42 INS74 INS42 INS39 INS42 INS39 INS42 INS46 INS21 INS43 INS42 INS43 INS42 INS74 INS42 INS6 INS39 INS85 INS42 INS33 INS43 INS42 INS43 INS42 INS5 INS42 INS74 INS42 INS74 INS42 INS39 INS42 INS39 INS42 INS46 INS21 INS39 INS85 INS41 INS5 INS42 INS21 INS43 INS42 INS43 INS42 INS74 INS42 INS54 UPD42 INS32 INS14 INS34 UPD42 UPD43 UPD42 UPD43 INS42 INS42 INS43 MOV43 UPD42 INS14 INS42 INS42 INS43 INS43 INS42 INS14 INS25 INS25 INS42 INS42 INS24 INS42 INS42 INS3 INS66 INS42 INS42 INS66 INS66 INS66 INS66 INS65 INS66 MOV43 INS85 INS42 INS42 INS43 INS85 INS43 INS43 INS43 INS43 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS32 INS42 INS42 INS43 INS43 INS27 INS45 INS42 INS42 INS43 INS85 INS43 INS43 INS43 INS43 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS32 INS42 INS39 INS85 INS7 INS42 INS42 INS43 INS43 INS8 INS8 INS42 MOV42 INS74 UPD42 UPD74 UPD42 UPD42 UPD42 INS42 INS74 INS42 INS42 INS74 INS32 INS8 UPD27 MOV32 INS8 INS58 INS27 INS37 INS8 INS5 INS32 UPD42 INS67 UPD42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS27 INS42 INS42 INS40 INS34 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS27 INS22 INS42 INS42 INS42 INS21 INS43 INS43 UPD43 UPD43 INS43 INS43 INS42 INS42 INS21 INS32 INS42 INS60 MOV25 INS39 INS59 INS42 INS27 INS42 INS25 MOV43 INS85 INS42 INS42 INS42 INS42 UPD42 INS45 INS42 INS45 INS42 INS52 INS42 INS7 INS42 INS42 UPD42 UPD42 INS42 INS42 INS32 INS42 INS42 INS32 INS39 INS59 INS38 INS8 INS42 INS34 INS40 INS40 INS38 INS8 INS2 INS42 INS32 INS25 INS42 INS42 INS11 INS42 INS42 INS42 INS42 INS32 INS32 INS21 INS32 INS21 INS2 INS42 INS34 INS42 INS42 INS32 INS42 INS2 INS42 INS9 INS42 INS32 INS32 UPD42 INS32 INS8 INS8 INS39 INS32 INS32 INS42 INS42 INS32 INS42 INS32 INS42 INS42 INS42 INS32 INS42 INS34 INS42 INS42 INS42 INS34 INS42 INS42 INS42 INS42 INS32 UPD42 INS60 INS60 INS60 INS25 INS42 INS42 INS6 INS6 INS21 MOV21 UPD40 INS36 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS74 INS59 INS5 INS59 INS39 INS59 INS27 INS32 INS8 INS8 INS62 INS27 INS32 INS11 INS43 INS43 INS42 INS14 MOV43 INS85 INS42 INS42 INS16 UPD42 INS27 INS27 INS42 INS42 INS60 INS60 INS24 INS21 INS21 MOV21 INS42 INS43 INS40 INS34 INS32 INS42 INS14 INS40 INS40 INS32 INS2 INS43 INS42 UPD74 UPD74 INS42 INS42 INS74 INS32 INS40 INS34 UPD42 INS42 INS42 UPD42 MOV42 MOV33 INS40 INS34 INS43 INS59 INS5 INS59 INS58 INS27 INS37 INS8 INS32 INS32 INS2 INS42 INS2 INS42 INS43 INS32 INS42 INS36 INS42 INS40 INS34 INS42 INS43 INS42 INS42 INS42 INS42 INS14 INS39 INS85 INS42 INS3 INS39 INS59 INS42 INS32 INS42 INS21 INS42 INS42 INS42 INS42 INS42 INS42 UPD42 INS40 INS34 INS40 INS34 INS42 INS42 INS42 INS11 INS42 INS45 UPD45 INS43 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS5 INS32 INS42 INS34 INS42 INS42 INS7 UPD42 INS43 INS42 INS42 INS39 INS85 INS42 INS42 INS2 INS32 INS42 INS42 INS42 INS42 INS42 INS42 DEL42 DEL59 DEL60 DEL42 DEL43 DEL42 DEL43 DEL27 DEL40 DEL40 DEL66 DEL33 DEL42 DEL33 DEL42 DEL42 DEL7 DEL42 DEL42 DEL7 DEL21 DEL42 DEL85 DEL52 DEL42 DEL22 DEL42 DEL32 DEL21 DEL42 DEL42