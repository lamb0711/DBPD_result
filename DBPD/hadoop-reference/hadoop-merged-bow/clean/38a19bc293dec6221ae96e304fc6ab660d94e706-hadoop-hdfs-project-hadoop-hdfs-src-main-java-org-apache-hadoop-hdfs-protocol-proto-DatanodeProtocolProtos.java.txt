HDFS-2642. Protobuf translators for DatanodeProtocol.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68

-    // required .StorageInfoProto storateInfo = 2;
-    boolean hasStorateInfo();
-    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto getStorateInfo();
-    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProtoOrBuilder getStorateInfoOrBuilder();
+    // required .StorageInfoProto storageInfo = 2;
+    boolean hasStorageInfo();
+    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto getStorageInfo();
+    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProtoOrBuilder getStorageInfoOrBuilder();
-    // required .StorageInfoProto storateInfo = 2;
-    public static final int STORATEINFO_FIELD_NUMBER = 2;
-    private org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto storateInfo_;
-    public boolean hasStorateInfo() {
+    // required .StorageInfoProto storageInfo = 2;
+    public static final int STORAGEINFO_FIELD_NUMBER = 2;
+    private org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto storageInfo_;
+    public boolean hasStorageInfo() {
-    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto getStorateInfo() {
-      return storateInfo_;
+    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto getStorageInfo() {
+      return storageInfo_;
-    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProtoOrBuilder getStorateInfoOrBuilder() {
-      return storateInfo_;
+    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProtoOrBuilder getStorageInfoOrBuilder() {
+      return storageInfo_;
-      storateInfo_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.getDefaultInstance();
+      storageInfo_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.getDefaultInstance();
-      if (!hasStorateInfo()) {
+      if (!hasStorageInfo()) {
-      if (!getStorateInfo().isInitialized()) {
+      if (!getStorageInfo().isInitialized()) {
-        output.writeMessage(2, storateInfo_);
+        output.writeMessage(2, storageInfo_);
-          .computeMessageSize(2, storateInfo_);
+          .computeMessageSize(2, storageInfo_);
-      result = result && (hasStorateInfo() == other.hasStorateInfo());
-      if (hasStorateInfo()) {
-        result = result && getStorateInfo()
-            .equals(other.getStorateInfo());
+      result = result && (hasStorageInfo() == other.hasStorageInfo());
+      if (hasStorageInfo()) {
+        result = result && getStorageInfo()
+            .equals(other.getStorageInfo());
-      if (hasStorateInfo()) {
-        hash = (37 * hash) + STORATEINFO_FIELD_NUMBER;
-        hash = (53 * hash) + getStorateInfo().hashCode();
+      if (hasStorageInfo()) {
+        hash = (37 * hash) + STORAGEINFO_FIELD_NUMBER;
+        hash = (53 * hash) + getStorageInfo().hashCode();
-          getStorateInfoFieldBuilder();
+          getStorageInfoFieldBuilder();
-        if (storateInfoBuilder_ == null) {
-          storateInfo_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.getDefaultInstance();
+        if (storageInfoBuilder_ == null) {
+          storageInfo_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.getDefaultInstance();
-          storateInfoBuilder_.clear();
+          storageInfoBuilder_.clear();
-        if (storateInfoBuilder_ == null) {
-          result.storateInfo_ = storateInfo_;
+        if (storageInfoBuilder_ == null) {
+          result.storageInfo_ = storageInfo_;
-          result.storateInfo_ = storateInfoBuilder_.build();
+          result.storageInfo_ = storageInfoBuilder_.build();
-        if (other.hasStorateInfo()) {
-          mergeStorateInfo(other.getStorateInfo());
+        if (other.hasStorageInfo()) {
+          mergeStorageInfo(other.getStorageInfo());
-        if (!hasStorateInfo()) {
+        if (!hasStorageInfo()) {
-        if (!getStorateInfo().isInitialized()) {
+        if (!getStorageInfo().isInitialized()) {
-              if (hasStorateInfo()) {
-                subBuilder.mergeFrom(getStorateInfo());
+              if (hasStorageInfo()) {
+                subBuilder.mergeFrom(getStorageInfo());
-              setStorateInfo(subBuilder.buildPartial());
+              setStorageInfo(subBuilder.buildPartial());
-      // required .StorageInfoProto storateInfo = 2;
-      private org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto storateInfo_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.getDefaultInstance();
+      // required .StorageInfoProto storageInfo = 2;
+      private org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto storageInfo_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.getDefaultInstance();
-          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProtoOrBuilder> storateInfoBuilder_;
-      public boolean hasStorateInfo() {
+          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProtoOrBuilder> storageInfoBuilder_;
+      public boolean hasStorageInfo() {
-      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto getStorateInfo() {
-        if (storateInfoBuilder_ == null) {
-          return storateInfo_;
+      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto getStorageInfo() {
+        if (storageInfoBuilder_ == null) {
+          return storageInfo_;
-          return storateInfoBuilder_.getMessage();
+          return storageInfoBuilder_.getMessage();
-      public Builder setStorateInfo(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto value) {
-        if (storateInfoBuilder_ == null) {
+      public Builder setStorageInfo(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto value) {
+        if (storageInfoBuilder_ == null) {
-          storateInfo_ = value;
+          storageInfo_ = value;
-          storateInfoBuilder_.setMessage(value);
+          storageInfoBuilder_.setMessage(value);
-      public Builder setStorateInfo(
+      public Builder setStorageInfo(
-        if (storateInfoBuilder_ == null) {
-          storateInfo_ = builderForValue.build();
+        if (storageInfoBuilder_ == null) {
+          storageInfo_ = builderForValue.build();
-          storateInfoBuilder_.setMessage(builderForValue.build());
+          storageInfoBuilder_.setMessage(builderForValue.build());
-      public Builder mergeStorateInfo(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto value) {
-        if (storateInfoBuilder_ == null) {
+      public Builder mergeStorageInfo(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto value) {
+        if (storageInfoBuilder_ == null) {
-              storateInfo_ != org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.getDefaultInstance()) {
-            storateInfo_ =
-              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.newBuilder(storateInfo_).mergeFrom(value).buildPartial();
+              storageInfo_ != org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.getDefaultInstance()) {
+            storageInfo_ =
+              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.newBuilder(storageInfo_).mergeFrom(value).buildPartial();
-            storateInfo_ = value;
+            storageInfo_ = value;
-          storateInfoBuilder_.mergeFrom(value);
+          storageInfoBuilder_.mergeFrom(value);
-      public Builder clearStorateInfo() {
-        if (storateInfoBuilder_ == null) {
-          storateInfo_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.getDefaultInstance();
+      public Builder clearStorageInfo() {
+        if (storageInfoBuilder_ == null) {
+          storageInfo_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.getDefaultInstance();
-          storateInfoBuilder_.clear();
+          storageInfoBuilder_.clear();
-      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.Builder getStorateInfoBuilder() {
+      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProto.Builder getStorageInfoBuilder() {
-        return getStorateInfoFieldBuilder().getBuilder();
+        return getStorageInfoFieldBuilder().getBuilder();
-      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProtoOrBuilder getStorateInfoOrBuilder() {
-        if (storateInfoBuilder_ != null) {
-          return storateInfoBuilder_.getMessageOrBuilder();
+      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageInfoProtoOrBuilder getStorageInfoOrBuilder() {
+        if (storageInfoBuilder_ != null) {
+          return storageInfoBuilder_.getMessageOrBuilder();
-          return storateInfo_;
+          return storageInfo_;
-          getStorateInfoFieldBuilder() {
-        if (storateInfoBuilder_ == null) {
-          storateInfoBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          getStorageInfoFieldBuilder() {
+        if (storageInfoBuilder_ == null) {
+          storageInfoBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-                  storateInfo_,
+                  storageInfo_,
-          storateInfo_ = null;
+          storageInfo_ = null;
-        return storateInfoBuilder_;
+        return storageInfoBuilder_;
-    // optional .BlockRecoveryCommndProto recoveryCmd = 4;
+    // optional .BlockRecoveryCommandProto recoveryCmd = 4;
-    org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto getRecoveryCmd();
-    org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProtoOrBuilder getRecoveryCmdOrBuilder();
+    org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto getRecoveryCmd();
+    org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder getRecoveryCmdOrBuilder();
-    // optional .BlockRecoveryCommndProto recoveryCmd = 4;
+    // optional .BlockRecoveryCommandProto recoveryCmd = 4;
-    private org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto recoveryCmd_;
+    private org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto recoveryCmd_;
-    public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto getRecoveryCmd() {
+    public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto getRecoveryCmd() {
-    public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProtoOrBuilder getRecoveryCmdOrBuilder() {
+    public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder getRecoveryCmdOrBuilder() {
-      recoveryCmd_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.getDefaultInstance();
+      recoveryCmd_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance();
-          recoveryCmd_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.getDefaultInstance();
+          recoveryCmd_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance();
-              org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.Builder subBuilder = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.newBuilder();
+              org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder subBuilder = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.newBuilder();
-      // optional .BlockRecoveryCommndProto recoveryCmd = 4;
-      private org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto recoveryCmd_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.getDefaultInstance();
+      // optional .BlockRecoveryCommandProto recoveryCmd = 4;
+      private org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto recoveryCmd_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance();
-          org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto, org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.Builder, org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProtoOrBuilder> recoveryCmdBuilder_;
+          org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto, org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder, org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder> recoveryCmdBuilder_;
-      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto getRecoveryCmd() {
+      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto getRecoveryCmd() {
-      public Builder setRecoveryCmd(org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto value) {
+      public Builder setRecoveryCmd(org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto value) {
-          org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.Builder builderForValue) {
+          org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder builderForValue) {
-      public Builder mergeRecoveryCmd(org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto value) {
+      public Builder mergeRecoveryCmd(org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto value) {
-              recoveryCmd_ != org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.getDefaultInstance()) {
+              recoveryCmd_ != org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance()) {
-              org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.newBuilder(recoveryCmd_).mergeFrom(value).buildPartial();
+              org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.newBuilder(recoveryCmd_).mergeFrom(value).buildPartial();
-          recoveryCmd_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.getDefaultInstance();
+          recoveryCmd_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance();
-      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.Builder getRecoveryCmdBuilder() {
+      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder getRecoveryCmdBuilder() {
-      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProtoOrBuilder getRecoveryCmdOrBuilder() {
+      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder getRecoveryCmdOrBuilder() {
-          org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto, org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.Builder, org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProtoOrBuilder> 
+          org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto, org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder, org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder> 
-              org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto, org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.Builder, org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProtoOrBuilder>(
+              org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto, org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder, org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder>(
-    // required uint32 action = 1;
+    // required .BlockCommandProto.Action action = 1;
-    int getAction();
+    org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Action getAction();
-    // repeated .DatanodeIDsProto targets = 4;
-    java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto> 
+    // repeated .DatanodeInfosProto targets = 4;
+    java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto> 
-    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto getTargets(int index);
+    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto getTargets(int index);
-    java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProtoOrBuilder> 
+    java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProtoOrBuilder> 
-    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProtoOrBuilder getTargetsOrBuilder(
+    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProtoOrBuilder getTargetsOrBuilder(
-      UNKNOWN(0, 0),
-      TRANSFER(1, 1),
-      INVALIDATE(2, 2),
-      SHUTDOWN(3, 3),
+      TRANSFER(0, 1),
+      INVALIDATE(1, 2),
-      public static final int UNKNOWN_VALUE = 0;
-      public static final int SHUTDOWN_VALUE = 3;
-          case 0: return UNKNOWN;
-          case 3: return SHUTDOWN;
-        UNKNOWN, TRANSFER, INVALIDATE, SHUTDOWN, 
+        TRANSFER, INVALIDATE, 
-    // required uint32 action = 1;
+    // required .BlockCommandProto.Action action = 1;
-    private int action_;
+    private org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Action action_;
-    public int getAction() {
+    public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Action getAction() {
-    // repeated .DatanodeIDsProto targets = 4;
+    // repeated .DatanodeInfosProto targets = 4;
-    private java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto> targets_;
-    public java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto> getTargetsList() {
+    private java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto> targets_;
+    public java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto> getTargetsList() {
-    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProtoOrBuilder> 
+    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProtoOrBuilder> 
-    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto getTargets(int index) {
+    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto getTargets(int index) {
-    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProtoOrBuilder getTargetsOrBuilder(
+    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProtoOrBuilder getTargetsOrBuilder(
-      action_ = 0;
+      action_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Action.TRANSFER;
-        output.writeUInt32(1, action_);
+        output.writeEnum(1, action_.getNumber());
-          .computeUInt32Size(1, action_);
+          .computeEnumSize(1, action_.getNumber());
-        result = result && (getAction()
-            == other.getAction());
+        result = result &&
+            (getAction() == other.getAction());
-        hash = (53 * hash) + getAction();
+        hash = (53 * hash) + hashEnum(getAction());
-        action_ = 0;
+        action_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Action.TRANSFER;
-              bitField0_ |= 0x00000001;
-              action_ = input.readUInt32();
+              int rawValue = input.readEnum();
+              org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Action value = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Action.valueOf(rawValue);
+              if (value == null) {
+                unknownFields.mergeVarintField(1, rawValue);
+              } else {
+                bitField0_ |= 0x00000001;
+                action_ = value;
+              }
-              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.Builder subBuilder = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.newBuilder();
+              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.Builder subBuilder = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.newBuilder();
-      // required uint32 action = 1;
-      private int action_ ;
+      // required .BlockCommandProto.Action action = 1;
+      private org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Action action_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Action.TRANSFER;
-      public int getAction() {
+      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Action getAction() {
-      public Builder setAction(int value) {
+      public Builder setAction(org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Action value) {
+        if (value == null) {
+          throw new NullPointerException();
+        }
-        action_ = 0;
+        action_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Action.TRANSFER;
-      // repeated .DatanodeIDsProto targets = 4;
-      private java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto> targets_ =
+      // repeated .DatanodeInfosProto targets = 4;
+      private java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto> targets_ =
-          targets_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto>(targets_);
+          targets_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto>(targets_);
-          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProtoOrBuilder> targetsBuilder_;
+          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProtoOrBuilder> targetsBuilder_;
-      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto> getTargetsList() {
+      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto> getTargetsList() {
-      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto getTargets(int index) {
+      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto getTargets(int index) {
-          int index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto value) {
+          int index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto value) {
-          int index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.Builder builderForValue) {
+          int index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.Builder builderForValue) {
-      public Builder addTargets(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto value) {
+      public Builder addTargets(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto value) {
-          int index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto value) {
+          int index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto value) {
-          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.Builder builderForValue) {
+          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.Builder builderForValue) {
-          int index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.Builder builderForValue) {
+          int index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.Builder builderForValue) {
-          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto> values) {
+          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto> values) {
-      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.Builder getTargetsBuilder(
+      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.Builder getTargetsBuilder(
-      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProtoOrBuilder getTargetsOrBuilder(
+      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProtoOrBuilder getTargetsOrBuilder(
-      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProtoOrBuilder> 
+      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProtoOrBuilder> 
-      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.Builder addTargetsBuilder() {
+      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.Builder addTargetsBuilder() {
-            org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.getDefaultInstance());
+            org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.getDefaultInstance());
-      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.Builder addTargetsBuilder(
+      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.Builder addTargetsBuilder(
-            index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.getDefaultInstance());
+            index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.getDefaultInstance());
-      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.Builder> 
+      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.Builder> 
-          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProtoOrBuilder> 
+          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProtoOrBuilder> 
-              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeIDsProtoOrBuilder>(
+              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.DatanodeInfosProtoOrBuilder>(
-  public interface BlockRecoveryCommndProtoOrBuilder
+  public interface BlockRecoveryCommandProtoOrBuilder
-  public static final class BlockRecoveryCommndProto extends
+  public static final class BlockRecoveryCommandProto extends
-      implements BlockRecoveryCommndProtoOrBuilder {
-    // Use BlockRecoveryCommndProto.newBuilder() to construct.
-    private BlockRecoveryCommndProto(Builder builder) {
+      implements BlockRecoveryCommandProtoOrBuilder {
+    // Use BlockRecoveryCommandProto.newBuilder() to construct.
+    private BlockRecoveryCommandProto(Builder builder) {
-    private BlockRecoveryCommndProto(boolean noInit) {}
+    private BlockRecoveryCommandProto(boolean noInit) {}
-    private static final BlockRecoveryCommndProto defaultInstance;
-    public static BlockRecoveryCommndProto getDefaultInstance() {
+    private static final BlockRecoveryCommandProto defaultInstance;
+    public static BlockRecoveryCommandProto getDefaultInstance() {
-    public BlockRecoveryCommndProto getDefaultInstanceForType() {
+    public BlockRecoveryCommandProto getDefaultInstanceForType() {
-      return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.internal_static_BlockRecoveryCommndProto_descriptor;
+      return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.internal_static_BlockRecoveryCommandProto_descriptor;
-      return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.internal_static_BlockRecoveryCommndProto_fieldAccessorTable;
+      return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.internal_static_BlockRecoveryCommandProto_fieldAccessorTable;
-      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto)) {
+      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto)) {
-      org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto other = (org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto) obj;
+      org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto other = (org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto) obj;
-    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto parseFrom(
+    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
-    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto parseFrom(
+    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
-    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto parseFrom(byte[] data)
+    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(byte[] data)
-    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto parseFrom(
+    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
-    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto parseFrom(java.io.InputStream input)
+    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(java.io.InputStream input)
-    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto parseFrom(
+    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
-    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto parseDelimitedFrom(java.io.InputStream input)
+    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto parseDelimitedFrom(java.io.InputStream input)
-    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto parseDelimitedFrom(
+    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto parseDelimitedFrom(
-    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto parseFrom(
+    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
-    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto parseFrom(
+    public static org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
-    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto prototype) {
+    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto prototype) {
-       implements org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProtoOrBuilder {
+       implements org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder {
-        return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.internal_static_BlockRecoveryCommndProto_descriptor;
+        return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.internal_static_BlockRecoveryCommandProto_descriptor;
-        return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.internal_static_BlockRecoveryCommndProto_fieldAccessorTable;
+        return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.internal_static_BlockRecoveryCommandProto_fieldAccessorTable;
-      // Construct using org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.newBuilder()
+      // Construct using org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.newBuilder()
-        return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.getDescriptor();
+        return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDescriptor();
-      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto getDefaultInstanceForType() {
-        return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.getDefaultInstance();
+      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto getDefaultInstanceForType() {
+        return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance();
-      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto build() {
-        org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto result = buildPartial();
+      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto build() {
+        org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto result = buildPartial();
-      private org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto buildParsed()
+      private org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto buildParsed()
-        org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto result = buildPartial();
+        org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto result = buildPartial();
-      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto buildPartial() {
-        org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto result = new org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto(this);
+      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto buildPartial() {
+        org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto result = new org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto(this);
-        if (other instanceof org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto) {
-          return mergeFrom((org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto)other);
+        if (other instanceof org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto) {
+          return mergeFrom((org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto)other);
-      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto other) {
-        if (other == org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.getDefaultInstance()) return this;
+      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto other) {
+        if (other == org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance()) return this;
-      // @@protoc_insertion_point(builder_scope:BlockRecoveryCommndProto)
+      // @@protoc_insertion_point(builder_scope:BlockRecoveryCommandProto)
-      defaultInstance = new BlockRecoveryCommndProto(true);
+      defaultInstance = new BlockRecoveryCommandProto(true);
-    // @@protoc_insertion_point(class_scope:BlockRecoveryCommndProto)
+    // @@protoc_insertion_point(class_scope:BlockRecoveryCommandProto)
-    // required uint32 action = 1;
+    // required .UpgradeCommandProto.Action action = 1;
-    int getAction();
+    org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Action getAction();
-    // required uint32 action = 1;
+    // required .UpgradeCommandProto.Action action = 1;
-    private int action_;
+    private org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Action action_;
-    public int getAction() {
+    public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Action getAction() {
-      action_ = 0;
+      action_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Action.UNKNOWN;
-        output.writeUInt32(1, action_);
+        output.writeEnum(1, action_.getNumber());
-          .computeUInt32Size(1, action_);
+          .computeEnumSize(1, action_.getNumber());
-        result = result && (getAction()
-            == other.getAction());
+        result = result &&
+            (getAction() == other.getAction());
-        hash = (53 * hash) + getAction();
+        hash = (53 * hash) + hashEnum(getAction());
-        action_ = 0;
+        action_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Action.UNKNOWN;
-              bitField0_ |= 0x00000001;
-              action_ = input.readUInt32();
+              int rawValue = input.readEnum();
+              org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Action value = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Action.valueOf(rawValue);
+              if (value == null) {
+                unknownFields.mergeVarintField(1, rawValue);
+              } else {
+                bitField0_ |= 0x00000001;
+                action_ = value;
+              }
-      // required uint32 action = 1;
-      private int action_ ;
+      // required .UpgradeCommandProto.Action action = 1;
+      private org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Action action_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Action.UNKNOWN;
-      public int getAction() {
+      public org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Action getAction() {
-      public Builder setAction(int value) {
+      public Builder setAction(org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Action value) {
+        if (value == null) {
+          throw new NullPointerException();
+        }
-        action_ = 0;
+        action_ = org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Action.UNKNOWN;
+      public abstract void versionRequest(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionRequestProto request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto> done);
+      
+        public  void versionRequest(
+            com.google.protobuf.RpcController controller,
+            org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionRequestProto request,
+            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto> done) {
+          impl.versionRequest(controller, request, done);
+        }
+        
+        @java.lang.Override
-              return impl.processUpgrade(controller, (org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ProcessUpgradeRequestProto)request);
+              return impl.versionRequest(controller, (org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionRequestProto)request);
-              return impl.reportBadBlocks(controller, (org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ReportBadBlocksRequestProto)request);
+              return impl.processUpgrade(controller, (org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ProcessUpgradeRequestProto)request);
+              return impl.reportBadBlocks(controller, (org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ReportBadBlocksRequestProto)request);
+            case 8:
-              return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ProcessUpgradeRequestProto.getDefaultInstance();
+              return org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionRequestProto.getDefaultInstance();
-              return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ReportBadBlocksRequestProto.getDefaultInstance();
+              return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ProcessUpgradeRequestProto.getDefaultInstance();
+              return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ReportBadBlocksRequestProto.getDefaultInstance();
+            case 8:
-              return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ProcessUpgradeResponseProto.getDefaultInstance();
+              return org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto.getDefaultInstance();
-              return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ReportBadBlocksResponseProto.getDefaultInstance();
+              return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ProcessUpgradeResponseProto.getDefaultInstance();
+              return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ReportBadBlocksResponseProto.getDefaultInstance();
+            case 8:
+    public abstract void versionRequest(
+        com.google.protobuf.RpcController controller,
+        org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionRequestProto request,
+        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto> done);
+    
+          this.versionRequest(controller, (org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionRequestProto)request,
+            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto>specializeCallback(
+              done));
+          return;
+        case 6:
-        case 6:
+        case 7:
-        case 7:
+        case 8:
-          return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ProcessUpgradeRequestProto.getDefaultInstance();
+          return org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionRequestProto.getDefaultInstance();
-          return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ReportBadBlocksRequestProto.getDefaultInstance();
+          return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ProcessUpgradeRequestProto.getDefaultInstance();
+          return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ReportBadBlocksRequestProto.getDefaultInstance();
+        case 8:
-          return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ProcessUpgradeResponseProto.getDefaultInstance();
+          return org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto.getDefaultInstance();
-          return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ReportBadBlocksResponseProto.getDefaultInstance();
+          return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ProcessUpgradeResponseProto.getDefaultInstance();
+          return org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.ReportBadBlocksResponseProto.getDefaultInstance();
+        case 8:
+      public  void versionRequest(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionRequestProto request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto> done) {
+        channel.callMethod(
+          getDescriptor().getMethods().get(5),
+          controller,
+          request,
+          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto.getDefaultInstance(),
+          com.google.protobuf.RpcUtil.generalizeCallback(
+            done,
+            org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto.class,
+            org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto.getDefaultInstance()));
+      }
+      
-          getDescriptor().getMethods().get(5),
+          getDescriptor().getMethods().get(6),
-          getDescriptor().getMethods().get(6),
+          getDescriptor().getMethods().get(7),
-          getDescriptor().getMethods().get(7),
+          getDescriptor().getMethods().get(8),
+      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto versionRequest(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionRequestProto request)
+          throws com.google.protobuf.ServiceException;
+      
+      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto versionRequest(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionRequestProto request)
+          throws com.google.protobuf.ServiceException {
+        return (org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto) channel.callBlockingMethod(
+          getDescriptor().getMethods().get(5),
+          controller,
+          request,
+          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.VersionResponseProto.getDefaultInstance());
+      }
+      
+      
-          getDescriptor().getMethods().get(5),
+          getDescriptor().getMethods().get(6),
-          getDescriptor().getMethods().get(6),
+          getDescriptor().getMethods().get(7),
-          getDescriptor().getMethods().get(7),
+          getDescriptor().getMethods().get(8),
-    internal_static_BlockRecoveryCommndProto_descriptor;
+    internal_static_BlockRecoveryCommandProto_descriptor;
-      internal_static_BlockRecoveryCommndProto_fieldAccessorTable;
+      internal_static_BlockRecoveryCommandProto_fieldAccessorTable;
-      "\030\001 \002(\0132\020.DatanodeIDProto\022&\n\013storateInfo\030" +
+      "\030\001 \002(\0132\020.DatanodeIDProto\022&\n\013storageInfo\030" +
-      ".ExportedBlockKeysProto\"\243\004\n\024DatanodeComm" +
+      ".ExportedBlockKeysProto\"\244\004\n\024DatanodeComm" +
-      "(\0132\022.BlockCommandProto\022.\n\013recoveryCmd\030\004 " +
-      "\001(\0132\031.BlockRecoveryCommndProto\022*\n\013finali",
-      "zeCmd\030\005 \001(\0132\025.FinalizeCommandProto\022,\n\014ke" +
-      "yUpdateCmd\030\006 \001(\0132\026.KeyUpdateCommandProto" +
-      "\022*\n\013registerCmd\030\007 \001(\0132\025.RegisterCommandP" +
-      "roto\022(\n\nupgradeCmd\030\010 \001(\0132\024.UpgradeComman" +
-      "dProto\"\244\001\n\004Type\022\034\n\030BalancerBandwidthComm" +
-      "and\020\000\022\020\n\014BlockCommand\020\001\022\030\n\024BlockRecovery" +
-      "Command\020\002\022\023\n\017FinalizeCommand\020\003\022\024\n\020KeyUpd" +
-      "ateCommand\020\004\022\023\n\017RegisterCommand\020\005\022\022\n\016Upg" +
-      "radeCommand\020\006\"2\n\035BalancerBandwidthComman" +
-      "dProto\022\021\n\tbandwidth\030\001 \002(\004\"\274\001\n\021BlockComma",
-      "ndProto\022\016\n\006action\030\001 \002(\r\022\023\n\013blockPoolId\030\002" +
-      " \002(\t\022\033\n\006blocks\030\003 \003(\0132\013.BlockProto\022\"\n\007tar" +
-      "gets\030\004 \003(\0132\021.DatanodeIDsProto\"A\n\006Action\022" +
-      "\013\n\007UNKNOWN\020\000\022\014\n\010TRANSFER\020\001\022\016\n\nINVALIDATE" +
-      "\020\002\022\014\n\010SHUTDOWN\020\003\"A\n\030BlockRecoveryCommndP" +
-      "roto\022%\n\006blocks\030\001 \003(\0132\025.RecoveringBlockPr" +
-      "oto\"+\n\024FinalizeCommandProto\022\023\n\013blockPool" +
-      "Id\030\001 \002(\t\">\n\025KeyUpdateCommandProto\022%\n\004key" +
-      "s\030\001 \002(\0132\027.ExportedBlockKeysProto\"\026\n\024Regi" +
-      "sterCommandProto\"\212\001\n\023UpgradeCommandProto",
-      "\022\016\n\006action\030\001 \002(\r\022\017\n\007version\030\002 \002(\r\022\025\n\rupg" +
-      "radeStatus\030\003 \002(\r\";\n\006Action\022\013\n\007UNKNOWN\020\000\022" +
-      "\021\n\rREPORT_STATUS\020d\022\021\n\rSTART_UPGRADE\020e\"P\n" +
-      "\034RegisterDatanodeRequestProto\0220\n\014registr" +
-      "ation\030\001 \002(\0132\032.DatanodeRegistrationProto\"" +
-      "Q\n\035RegisterDatanodeResponseProto\0220\n\014regi" +
-      "stration\030\001 \002(\0132\032.DatanodeRegistrationPro" +
-      "to\"\334\001\n\025HeartbeatRequestProto\0220\n\014registra" +
-      "tion\030\001 \002(\0132\032.DatanodeRegistrationProto\022\020" +
-      "\n\010capacity\030\002 \002(\004\022\017\n\007dfsUsed\030\003 \002(\004\022\021\n\trem",
-      "aining\030\004 \002(\004\022\025\n\rblockPoolUsed\030\005 \002(\004\022\027\n\017x" +
-      "mitsInProgress\030\006 \002(\r\022\024\n\014xceiverCount\030\007 \002" +
-      "(\r\022\025\n\rfailedVolumes\030\010 \002(\r\"=\n\026HeartbeatRe" +
-      "sponseProto\022#\n\004cmds\030\001 \003(\0132\025.DatanodeComm" +
-      "andProto\"t\n\027BlockReportRequestProto\0220\n\014r" +
-      "egistration\030\001 \002(\0132\032.DatanodeRegistration" +
-      "Proto\022\023\n\013blockPoolId\030\002 \002(\t\022\022\n\006blocks\030\003 \003" +
-      "(\004B\002\020\001\">\n\030BlockReportResponseProto\022\"\n\003cm" +
-      "d\030\001 \002(\0132\025.DatanodeCommandProto\"O\n\035Receiv" +
-      "edDeletedBlockInfoProto\022\032\n\005block\030\001 \002(\0132\013",
-      ".BlockProto\022\022\n\ndeleteHint\030\002 \001(\t\"\234\001\n#Bloc" +
-      "kReceivedAndDeletedRequestProto\0220\n\014regis" +
-      "tration\030\001 \002(\0132\032.DatanodeRegistrationProt" +
-      "o\022\023\n\013blockPoolId\030\002 \002(\t\022.\n\006blocks\030\003 \003(\0132\036" +
-      ".ReceivedDeletedBlockInfoProto\"&\n$BlockR" +
-      "eceivedAndDeletedResponseProto\"\275\001\n\027Error" +
-      "ReportRequestProto\0220\n\014registartion\030\001 \002(\013" +
-      "2\032.DatanodeRegistrationProto\022\021\n\terrorCod" +
-      "e\030\002 \002(\r\022\013\n\003msg\030\003 \002(\t\"P\n\tErrorCode\022\n\n\006NOT" +
-      "IFY\020\000\022\016\n\nDISK_ERROR\020\001\022\021\n\rINVALID_BLOCK\020\002",
-      "\022\024\n\020FATAL_DISK_ERROR\020\003\"\032\n\030ErrorReportRes" +
-      "ponseProto\"?\n\032ProcessUpgradeRequestProto" +
-      "\022!\n\003cmd\030\001 \001(\0132\024.UpgradeCommandProto\"@\n\033P" +
-      "rocessUpgradeResponseProto\022!\n\003cmd\030\001 \001(\0132" +
-      "\024.UpgradeCommandProto\"A\n\033ReportBadBlocks" +
-      "RequestProto\022\"\n\006blocks\030\001 \003(\0132\022.LocatedBl" +
-      "ockProto\"\036\n\034ReportBadBlocksResponseProto" +
-      "\"\303\001\n&CommitBlockSynchronizationRequestPr" +
-      "oto\022\"\n\005block\030\001 \002(\0132\023.ExtendedBlockProto\022" +
-      "\023\n\013newGenStamp\030\002 \002(\004\022\021\n\tnewLength\030\003 \002(\004\022",
-      "\021\n\tcloseFile\030\004 \002(\010\022\023\n\013deleteBlock\030\005 \002(\010\022" +
-      "%\n\013newTaragets\030\006 \003(\0132\020.DatanodeIDProto\")" +
-      "\n\'CommitBlockSynchronizationResponseProt" +
-      "o2\254\005\n\027DatanodeProtocolService\022Q\n\020registe" +
-      "rDatanode\022\035.RegisterDatanodeRequestProto" +
-      "\032\036.RegisterDatanodeResponseProto\022@\n\rsend" +
-      "Heartbeat\022\026.HeartbeatRequestProto\032\027.Hear" +
-      "tbeatResponseProto\022B\n\013blockReport\022\030.Bloc" +
-      "kReportRequestProto\032\031.BlockReportRespons" +
-      "eProto\022f\n\027blockReceivedAndDeleted\022$.Bloc",
-      "kReceivedAndDeletedRequestProto\032%.BlockR" +
-      "eceivedAndDeletedResponseProto\022B\n\013errorR" +
-      "eport\022\030.ErrorReportRequestProto\032\031.ErrorR" +
-      "eportResponseProto\022K\n\016processUpgrade\022\033.P" +
-      "rocessUpgradeRequestProto\032\034.ProcessUpgra" +
-      "deResponseProto\022N\n\017reportBadBlocks\022\034.Rep" +
-      "ortBadBlocksRequestProto\032\035.ReportBadBloc" +
-      "ksResponseProto\022o\n\032commitBlockSynchroniz" +
-      "ation\022\'.CommitBlockSynchronizationReques" +
-      "tProto\032(.CommitBlockSynchronizationRespo",
-      "nseProtoBE\n%org.apache.hadoop.hdfs.proto" +
-      "col.protoB\026DatanodeProtocolProtos\210\001\001\240\001\001"
+      "(\0132\022.BlockCommandProto\022/\n\013recoveryCmd\030\004 " +
+      "\001(\0132\032.BlockRecoveryCommandProto\022*\n\013final",
+      "izeCmd\030\005 \001(\0132\025.FinalizeCommandProto\022,\n\014k" +
+      "eyUpdateCmd\030\006 \001(\0132\026.KeyUpdateCommandProt" +
+      "o\022*\n\013registerCmd\030\007 \001(\0132\025.RegisterCommand" +
+      "Proto\022(\n\nupgradeCmd\030\010 \001(\0132\024.UpgradeComma" +
+      "ndProto\"\244\001\n\004Type\022\034\n\030BalancerBandwidthCom" +
+      "mand\020\000\022\020\n\014BlockCommand\020\001\022\030\n\024BlockRecover" +
+      "yCommand\020\002\022\023\n\017FinalizeCommand\020\003\022\024\n\020KeyUp" +
+      "dateCommand\020\004\022\023\n\017RegisterCommand\020\005\022\022\n\016Up" +
+      "gradeCommand\020\006\"2\n\035BalancerBandwidthComma" +
+      "ndProto\022\021\n\tbandwidth\030\001 \002(\004\"\276\001\n\021BlockComm",
+      "andProto\022)\n\006action\030\001 \002(\0162\031.BlockCommandP" +
+      "roto.Action\022\023\n\013blockPoolId\030\002 \002(\t\022\033\n\006bloc" +
+      "ks\030\003 \003(\0132\013.BlockProto\022$\n\007targets\030\004 \003(\0132\023" +
+      ".DatanodeInfosProto\"&\n\006Action\022\014\n\010TRANSFE" +
+      "R\020\001\022\016\n\nINVALIDATE\020\002\"B\n\031BlockRecoveryComm" +
+      "andProto\022%\n\006blocks\030\001 \003(\0132\025.RecoveringBlo" +
+      "ckProto\"+\n\024FinalizeCommandProto\022\023\n\013block" +
+      "PoolId\030\001 \002(\t\">\n\025KeyUpdateCommandProto\022%\n" +
+      "\004keys\030\001 \002(\0132\027.ExportedBlockKeysProto\"\026\n\024" +
+      "RegisterCommandProto\"\247\001\n\023UpgradeCommandP",
+      "roto\022+\n\006action\030\001 \002(\0162\033.UpgradeCommandPro" +
+      "to.Action\022\017\n\007version\030\002 \002(\r\022\025\n\rupgradeSta" +
+      "tus\030\003 \002(\r\";\n\006Action\022\013\n\007UNKNOWN\020\000\022\021\n\rREPO" +
+      "RT_STATUS\020d\022\021\n\rSTART_UPGRADE\020e\"P\n\034Regist" +
+      "erDatanodeRequestProto\0220\n\014registration\030\001" +
+      " \002(\0132\032.DatanodeRegistrationProto\"Q\n\035Regi" +
+      "sterDatanodeResponseProto\0220\n\014registratio" +
+      "n\030\001 \002(\0132\032.DatanodeRegistrationProto\"\334\001\n\025" +
+      "HeartbeatRequestProto\0220\n\014registration\030\001 " +
+      "\002(\0132\032.DatanodeRegistrationProto\022\020\n\010capac",
+      "ity\030\002 \002(\004\022\017\n\007dfsUsed\030\003 \002(\004\022\021\n\tremaining\030" +
+      "\004 \002(\004\022\025\n\rblockPoolUsed\030\005 \002(\004\022\027\n\017xmitsInP" +
+      "rogress\030\006 \002(\r\022\024\n\014xceiverCount\030\007 \002(\r\022\025\n\rf" +
+      "ailedVolumes\030\010 \002(\r\"=\n\026HeartbeatResponseP" +
+      "roto\022#\n\004cmds\030\001 \003(\0132\025.DatanodeCommandProt" +
+      "o\"t\n\027BlockReportRequestProto\0220\n\014registra" +
+      "tion\030\001 \002(\0132\032.DatanodeRegistrationProto\022\023" +
+      "\n\013blockPoolId\030\002 \002(\t\022\022\n\006blocks\030\003 \003(\004B\002\020\001\"" +
+      ">\n\030BlockReportResponseProto\022\"\n\003cmd\030\001 \002(\013" +
+      "2\025.DatanodeCommandProto\"O\n\035ReceivedDelet",
+      "edBlockInfoProto\022\032\n\005block\030\001 \002(\0132\013.BlockP" +
+      "roto\022\022\n\ndeleteHint\030\002 \001(\t\"\234\001\n#BlockReceiv" +
+      "edAndDeletedRequestProto\0220\n\014registration" +
+      "\030\001 \002(\0132\032.DatanodeRegistrationProto\022\023\n\013bl" +
+      "ockPoolId\030\002 \002(\t\022.\n\006blocks\030\003 \003(\0132\036.Receiv" +
+      "edDeletedBlockInfoProto\"&\n$BlockReceived" +
+      "AndDeletedResponseProto\"\275\001\n\027ErrorReportR" +
+      "equestProto\0220\n\014registartion\030\001 \002(\0132\032.Data" +
+      "nodeRegistrationProto\022\021\n\terrorCode\030\002 \002(\r" +
+      "\022\013\n\003msg\030\003 \002(\t\"P\n\tErrorCode\022\n\n\006NOTIFY\020\000\022\016",
+      "\n\nDISK_ERROR\020\001\022\021\n\rINVALID_BLOCK\020\002\022\024\n\020FAT" +
+      "AL_DISK_ERROR\020\003\"\032\n\030ErrorReportResponsePr" +
+      "oto\"?\n\032ProcessUpgradeRequestProto\022!\n\003cmd" +
+      "\030\001 \001(\0132\024.UpgradeCommandProto\"@\n\033ProcessU" +
+      "pgradeResponseProto\022!\n\003cmd\030\001 \001(\0132\024.Upgra" +
+      "deCommandProto\"A\n\033ReportBadBlocksRequest" +
+      "Proto\022\"\n\006blocks\030\001 \003(\0132\022.LocatedBlockProt" +
+      "o\"\036\n\034ReportBadBlocksResponseProto\"\303\001\n&Co" +
+      "mmitBlockSynchronizationRequestProto\022\"\n\005" +
+      "block\030\001 \002(\0132\023.ExtendedBlockProto\022\023\n\013newG",
+      "enStamp\030\002 \002(\004\022\021\n\tnewLength\030\003 \002(\004\022\021\n\tclos" +
+      "eFile\030\004 \002(\010\022\023\n\013deleteBlock\030\005 \002(\010\022%\n\013newT" +
+      "aragets\030\006 \003(\0132\020.DatanodeIDProto\")\n\'Commi" +
+      "tBlockSynchronizationResponseProto2\353\005\n\027D" +
+      "atanodeProtocolService\022Q\n\020registerDatano" +
+      "de\022\035.RegisterDatanodeRequestProto\032\036.Regi" +
+      "sterDatanodeResponseProto\022@\n\rsendHeartbe" +
+      "at\022\026.HeartbeatRequestProto\032\027.HeartbeatRe" +
+      "sponseProto\022B\n\013blockReport\022\030.BlockReport" +
+      "RequestProto\032\031.BlockReportResponseProto\022",
+      "f\n\027blockReceivedAndDeleted\022$.BlockReceiv" +
+      "edAndDeletedRequestProto\032%.BlockReceived" +
+      "AndDeletedResponseProto\022B\n\013errorReport\022\030" +
+      ".ErrorReportRequestProto\032\031.ErrorReportRe" +
+      "sponseProto\022=\n\016versionRequest\022\024.VersionR" +
+      "equestProto\032\025.VersionResponseProto\022K\n\016pr" +
+      "ocessUpgrade\022\033.ProcessUpgradeRequestProt" +
+      "o\032\034.ProcessUpgradeResponseProto\022N\n\017repor" +
+      "tBadBlocks\022\034.ReportBadBlocksRequestProto" +
+      "\032\035.ReportBadBlocksResponseProto\022o\n\032commi",
+      "tBlockSynchronization\022\'.CommitBlockSynch" +
+      "ronizationRequestProto\032(.CommitBlockSync" +
+      "hronizationResponseProtoBE\n%org.apache.h" +
+      "adoop.hdfs.protocol.protoB\026DatanodeProto" +
+      "colProtos\210\001\001\240\001\001"
-              new java.lang.String[] { "DatanodeID", "StorateInfo", "Keys", },
+              new java.lang.String[] { "DatanodeID", "StorageInfo", "Keys", },
-          internal_static_BlockRecoveryCommndProto_descriptor =
+          internal_static_BlockRecoveryCommandProto_descriptor =
-          internal_static_BlockRecoveryCommndProto_fieldAccessorTable = new
+          internal_static_BlockRecoveryCommandProto_fieldAccessorTable = new
-              internal_static_BlockRecoveryCommndProto_descriptor,
+              internal_static_BlockRecoveryCommandProto_descriptor,
-              org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.class,
-              org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.Builder.class);
+              org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.class,
+              org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder.class);

MOV31 UPD42 UPD42 UPD43 INS31 MOV31 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD43 UPD43 UPD43 UPD43 UPD43 INS43 UPD74 UPD43 UPD74 UPD43 INS43 INS43 UPD74 UPD74 MOV8 UPD74 MOV8 UPD43 UPD42 UPD43 UPD42 UPD42 UPD42 UPD42 UPD43 UPD43 UPD43 UPD43 MOV44 UPD43 MOV44 UPD43 MOV44 UPD43 MOV44 UPD43 MOV44 UPD43 MOV44 UPD43 UPD43 UPD43 MOV44 UPD43 MOV44 UPD43 INS43 INS43 INS43 INS31 INS83 INS83 INS39 INS42 INS44 INS44 INS44 MOV43 MOV44 MOV43 MOV44 INS31 INS31 INS31 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD40 UPD40 UPD40 UPD40 UPD40 UPD43 UPD74 UPD43 UPD43 UPD43 UPD74 INS40 UPD43 UPD40 UPD76 UPD40 UPD34 UPD34 INS40 INS40 MOV43 UPD43 UPD43 MOV43 UPD76 UPD40 UPD40 INS43 INS43 UPD74 UPD74 UPD74 UPD43 UPD43 UPD43 UPD74 UPD43 UPD43 UPD74 UPD74 UPD42 UPD42 UPD42 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD43 UPD40 UPD43 UPD43 UPD43 UPD43 INS40 INS40 INS40 INS43 INS43 MOV43 MOV43 INS83 INS83 INS39 INS42 INS44 INS44 INS44 INS43 INS42 INS43 INS42 INS74 INS42 MOV21 MOV21 MOV21 MOV49 MOV49 MOV49 INS83 INS39 INS42 INS44 INS44 INS44 INS8 INS83 INS43 INS42 INS44 INS44 INS43 INS83 INS43 INS42 MOV44 INS44 MOV43 MOV8 MOV43 UPD42 MOV44 MOV43 UPD42 MOV44 MOV8 INS44 INS43 INS8 MOV5 UPD42 UPD42 UPD42 UPD42 UPD40 UPD43 UPD43 UPD43 UPD40 UPD43 UPD43 UPD43 UPD40 UPD40 UPD43 UPD43 UPD43 UPD40 UPD43 MOV49 UPD40 UPD40 UPD43 INS40 INS40 INS40 INS43 INS25 UPD43 UPD43 UPD43 UPD43 UPD43 UPD40 UPD43 UPD43 UPD43 UPD43 UPD43 UPD43 UPD74 UPD40 UPD40 UPD76 UPD40 UPD40 UPD43 MOV43 UPD43 UPD43 UPD43 UPD40 UPD40 UPD43 UPD40 UPD40 UPD40 INS60 UPD40 MOV60 UPD40 UPD43 INS40 INS40 INS40 INS43 INS25 MOV21 INS43 INS42 INS43 INS42 INS74 INS42 INS40 INS40 INS43 INS43 INS21 INS41 INS49 MOV32 INS49 MOV49 MOV49 MOV49 MOV49 INS41 MOV49 MOV49 INS49 INS49 MOV53 MOV32 MOV49 MOV49 MOV49 MOV49 MOV49 MOV49 INS41 MOV49 MOV49 MOV53 INS43 INS42 INS43 INS42 INS74 INS42 INS21 INS40 INS43 INS42 INS43 INS42 INS40 INS40 INS43 INS42 INS43 INS42 INS40 INS41 UPD42 UPD42 UPD42 UPD42 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 INS40 INS40 INS27 INS8 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD76 UPD43 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 INS43 MOV59 UPD43 UPD43 UPD40 INS40 INS40 INS27 INS8 INS40 INS40 INS43 INS43 INS40 INS40 INS32 INS34 INS42 INS34 MOV34 INS32 INS34 INS32 UPD34 INS40 INS40 INS43 INS43 INS32 INS40 INS40 INS40 INS40 INS11 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD40 INS40 INS42 INS33 INS53 INS40 UPD43 UPD40 MOV32 MOV32 UPD43 UPD40 UPD40 INS40 UPD40 UPD40 UPD43 UPD43 INS40 INS42 INS33 INS53 INS40 INS40 INS40 INS31 INS52 INS42 INS42 INS11 INS32 INS40 INS42 INS40 UPD42 MOV42 INS40 INS40 INS42 INS42 MOV32 INS42 INS42 INS32 INS32 INS32 INS43 MOV43 MOV43 MOV43 INS32 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 INS45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 INS45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 INS45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 INS45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 INS45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 UPD45 INS45 UPD45 UPD45 UPD45 INS45 UPD45 UPD45 UPD45 UPD45 UPD45 INS45 INS45 UPD45 INS45 UPD45 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 INS32 INS14 UPD40 UPD40 UPD40 UPD43 UPD40 UPD43 UPD40 UPD40 UPD42 UPD42 INS32 INS14 INS78 INS83 INS39 INS42 INS44 INS44 INS44 INS8 MOV43 MOV44 MOV43 MOV44 INS43 INS42 INS40 INS43 INS42 INS42 INS40 INS42 INS40 INS42 INS42 INS57 INS32 INS32 INS42 INS34 UPD34 UPD34 INS40 INS32 MOV32 MOV32 INS42 INS42 INS32 INS42 INS42 MOV32 UPD42 UPD42 UPD42 UPD42 UPD42 UPD40 UPD42 UPD40 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 MOV42 INS42 UPD42 INS32 INS32 INS60 INS60 INS25 INS43 UPD40 UPD40 MOV42 INS42 UPD42 INS32 INS32 INS60 INS60 INS25 INS43 INS40 INS43 INS42 INS43 INS42 INS74 INS42 INS21 MOV41 MOV41 MOV41 MOV49 MOV49 MOV49 INS40 INS40 INS43 INS40 INS42 INS32 INS42 INS40 INS42 INS32 INS42 INS34 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD40 UPD43 UPD40 UPD74 MOV42 INS42 INS42 MOV32 INS39 INS59 INS43 INS59 INS27 INS8 INS8 UPD43 INS42 UPD74 UPD74 UPD43 MOV42 INS42 INS42 MOV32 INS39 INS59 INS43 INS59 INS27 INS8 INS8 INS42 INS40 INS40 INS43 INS43 INS32 INS41 INS49 MOV32 INS49 MOV49 MOV49 MOV49 MOV49 INS41 MOV49 MOV49 INS49 INS49 MOV53 MOV32 MOV49 MOV49 MOV49 MOV49 MOV49 MOV49 INS41 MOV49 MOV49 MOV53 INS40 INS42 INS32 INS42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD40 UPD40 UPD43 UPD43 UPD43 INS42 INS32 INS40 INS42 INS32 INS42 INS33 INS21 MOV21 MOV21 UPD40 UPD43 MOV43 UPD43 UPD43 UPD43 UPD40 INS42 INS32 INS40 INS42 INS32 INS42 INS33 INS21 MOV21 MOV21 INS40 INS40 INS42 INS42 INS42 INS42 INS42 INS32 INS34 INS42 INS34 MOV34 INS32 INS34 INS32 UPD34 INS42 UPD42 UPD42 UPD40 UPD40 UPD40 UPD40 INS42 INS42 INS40 INS42 INS42 INS32 UPD40 UPD40 UPD40 UPD40 UPD40 INS42 INS42 INS40 INS42 INS42 INS32 INS42 INS42 INS42 INS11 INS40 INS42 INS40 UPD42 MOV42 UPD42 INS42 INS42 INS34 INS42 INS42 INS42 INS42 INS34 INS42 INS42 INS43 INS42 MOV5 UPD43 UPD43 UPD42 UPD42 UPD40 INS40 UPD45 UPD40 UPD40 DEL39 DEL42 DEL34 DEL34 DEL72 DEL42 DEL34 DEL34 DEL72 DEL83 DEL83 DEL83 DEL39 DEL42 DEL34 DEL59 DEL23 DEL83 DEL83 DEL83 DEL39 DEL42 DEL34 DEL59 DEL23 DEL42 DEL41 DEL34 DEL34 DEL49 DEL42 DEL41 DEL49 DEL42 DEL42 DEL39 DEL39 DEL34 DEL34 DEL42 DEL42 DEL32 DEL39 DEL39 DEL39 DEL34 DEL40 DEL43 DEL60 DEL39 DEL39 DEL39 DEL34 DEL34 DEL42 DEL42 DEL32 DEL39 DEL39 DEL39 DEL34 DEL49 DEL49 DEL45 DEL45 DEL45 DEL45 DEL45 DEL45 DEL45