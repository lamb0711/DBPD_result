MAPREDUCE-5352. Optimize node local splits generated by CombineFileInputFormat. (sseth)


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1509345 13f79535-47bb-0310-9956-ffa450edef68

+import java.util.Collections;
+import java.util.LinkedHashSet;
+import java.util.Map.Entry;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import com.google.common.collect.HashMultiset;
+import com.google.common.collect.Multiset;
+  private static final Log LOG = LogFactory.getLog(CombineFileInputFormat.class);
+  
+      // If maxSize is not configured, a single split will be generated per
+      // node.
-    HashMap<String, List<OneBlockInfo>> nodeToBlocks = 
-                              new HashMap<String, List<OneBlockInfo>>();
+    HashMap<String, Set<OneBlockInfo>> nodeToBlocks = 
+                              new HashMap<String, Set<OneBlockInfo>>();
-  void createSplits(HashMap<String, List<OneBlockInfo>> nodeToBlocks,
-                     HashMap<OneBlockInfo, String[]> blockToNodes,
-                     HashMap<String, List<OneBlockInfo>> rackToBlocks,
+  void createSplits(Map<String, Set<OneBlockInfo>> nodeToBlocks,
+                     Map<OneBlockInfo, String[]> blockToNodes,
+                     Map<String, List<OneBlockInfo>> rackToBlocks,
-    Set<String> nodes = new HashSet<String>();
-    int numNodes = nodeToBlocks.size();
+    int totalNodes = nodeToBlocks.size();
+    Multiset<String> splitsPerNode = HashMultiset.create();
+    Set<String> completedNodes = new HashSet<String>();
+    
-      int avgSplitsPerNode = maxSize > 0 && numNodes > 0 ?
-                                        ((int) (totalLength/maxSize))/numNodes
-                                        : Integer.MAX_VALUE;
-      int maxSplitsByNodeOnly = (avgSplitsPerNode > 0) ? avgSplitsPerNode : 1;
-      numNodes = 0;
-      // process all nodes and create splits that are local to a node.
-      for (Iterator<Map.Entry<String, List<OneBlockInfo>>> iter = nodeToBlocks
+      // process all nodes and create splits that are local to a node. Generate
+      // one split per node iteration, and walk over nodes multiple times to
+      // distribute the splits across nodes. 
+      for (Iterator<Map.Entry<String, Set<OneBlockInfo>>> iter = nodeToBlocks
-        Map.Entry<String, List<OneBlockInfo>> one = iter.next();
-        nodes.add(one.getKey());
-        List<OneBlockInfo> blocksInNode = one.getValue();
+        Map.Entry<String, Set<OneBlockInfo>> one = iter.next();
+        
+        String node = one.getKey();
+        
+        // Skip the node if it has previously been marked as completed.
+        if (completedNodes.contains(node)) {
+          continue;
+        }
+
+        Set<OneBlockInfo> blocksInCurrentNode = one.getValue();
-        int splitsInNode = 0;
-        for (OneBlockInfo oneblock : blocksInNode) {
-          if (blockToNodes.containsKey(oneblock)) {
-            validBlocks.add(oneblock);
-            blockToNodes.remove(oneblock);
-            curSplitSize += oneblock.length;
+        Iterator<OneBlockInfo> oneBlockIter = blocksInCurrentNode.iterator();
+        while (oneBlockIter.hasNext()) {
+          OneBlockInfo oneblock = oneBlockIter.next();
+          
+          // Remove all blocks which may already have been assigned to other
+          // splits.
+          if(!blockToNodes.containsKey(oneblock)) {
+            oneBlockIter.remove();
+            continue;
+          }
+        
+          validBlocks.add(oneblock);
+          blockToNodes.remove(oneblock);
+          curSplitSize += oneblock.length;
-            // if the accumulated split size exceeds the maximum, then
-            // create this split.
-            if (maxSize != 0 && curSplitSize >= maxSize) {
-              // create an input split and add it to the splits array
-              addCreatedSplit(splits, nodes, validBlocks);
-              totalLength -= curSplitSize;
-              curSplitSize = 0;
-              validBlocks.clear();
-              splitsInNode++;
-              if (splitsInNode == maxSplitsByNodeOnly) {
-                // stop grouping on a node so as not to create
-                // disproportionately more splits on a node because it happens
-                // to have many blocks
-                // consider only these nodes in next round of grouping because
-                // they have leftover blocks that may need to be grouped
-                numNodes++;
-                break;
-              }
+          // if the accumulated split size exceeds the maximum, then
+          // create this split.
+          if (maxSize != 0 && curSplitSize >= maxSize) {
+            // create an input split and add it to the splits array
+            addCreatedSplit(splits, Collections.singleton(node), validBlocks);
+            totalLength -= curSplitSize;
+            curSplitSize = 0;
+
+            splitsPerNode.add(node);
+
+            // Remove entries from blocksInNode so that we don't walk these
+            // again.
+            blocksInCurrentNode.removeAll(validBlocks);
+            validBlocks.clear();
+
+            // Done creating a single split for this node. Move on to the next
+            // node so that splits are distributed across nodes.
+            break;
+          }
+
+        }
+        if (validBlocks.size() != 0) {
+          // This implies that the last few blocks (or all in case maxSize=0)
+          // were not part of a split. The node is complete.
+          
+          // if there were any blocks left over and their combined size is
+          // larger than minSplitNode, then combine them into one split.
+          // Otherwise add them back to the unprocessed pool. It is likely
+          // that they will be combined with other blocks from the
+          // same rack later on.
+          // This condition also kicks in when max split size is not set. All
+          // blocks on a node will be grouped together into a single split.
+          if (minSizeNode != 0 && curSplitSize >= minSizeNode
+              && splitsPerNode.count(node) == 0) {
+            // haven't created any split on this machine. so its ok to add a
+            // smaller one for parallelism. Otherwise group it in the rack for
+            // balanced size create an input split and add it to the splits
+            // array
+            addCreatedSplit(splits, Collections.singleton(node), validBlocks);
+            totalLength -= curSplitSize;
+            splitsPerNode.add(node);
+            // Remove entries from blocksInNode so that we don't walk this again.
+            blocksInCurrentNode.removeAll(validBlocks);
+            // The node is done. This was the last set of blocks for this node.
+          } else {
+            // Put the unplaced blocks back into the pool for later rack-allocation.
+            for (OneBlockInfo oneblock : validBlocks) {
+              blockToNodes.put(oneblock, oneblock.hosts);
+          validBlocks.clear();
+          curSplitSize = 0;
+          completedNodes.add(node);
+        } else { // No in-flight blocks.
+          if (blocksInCurrentNode.size() == 0) {
+            // Node is done. All blocks were fit into node-local splits.
+            completedNodes.add(node);
+          } // else Run through the node again.
-        // if there were any blocks left over and their combined size is
-        // larger than minSplitNode, then combine them into one split.
-        // Otherwise add them back to the unprocessed pool. It is likely
-        // that they will be combined with other blocks from the
-        // same rack later on.
-        if (minSizeNode != 0 && curSplitSize >= minSizeNode
-            && splitsInNode == 0) {
-          // haven't created any split on this machine. so its ok to add a
-          // smaller
-          // one for parallelism. Otherwise group it in the rack for balanced
-          // size
-          // create an input split and add it to the splits array
-          addCreatedSplit(splits, nodes, validBlocks);
-          totalLength -= curSplitSize;
-        } else {
-          for (OneBlockInfo oneblock : validBlocks) {
-            blockToNodes.put(oneblock, oneblock.hosts);
-          }
-        }
-        validBlocks.clear();
-        nodes.clear();
-        curSplitSize = 0;
-      
-      if(!(numNodes>0 && totalLength>0)) {
+
+      // Check if node-local assignments are complete.
+      if (completedNodes.size() == totalNodes || totalLength == 0) {
+        // All nodes have been walked over and marked as completed or all blocks
+        // have been assigned. The rest should be handled via rackLock assignment.
+        LOG.info("DEBUG: Terminated node allocation with : CompletedNodes: "
+            + completedNodes.size() + ", size left: " + totalLength);
-                HashMap<String, List<OneBlockInfo>> nodeToBlocks,
+                HashMap<String, Set<OneBlockInfo>> nodeToBlocks,
-                          HashMap<String, List<OneBlockInfo>> rackToBlocks,
-                          HashMap<OneBlockInfo, String[]> blockToNodes,
-                          HashMap<String, List<OneBlockInfo>> nodeToBlocks,
-                          HashMap<String, Set<String>> rackToNodes) {
+                          Map<String, List<OneBlockInfo>> rackToBlocks,
+                          Map<OneBlockInfo, String[]> blockToNodes,
+                          Map<String, Set<OneBlockInfo>> nodeToBlocks,
+                          Map<String, Set<String>> rackToNodes) {
-          List<OneBlockInfo> blklist = nodeToBlocks.get(node);
+          Set<OneBlockInfo> blklist = nodeToBlocks.get(node);
-            blklist = new ArrayList<OneBlockInfo>();
+            blklist = new LinkedHashSet<OneBlockInfo>();
-  private static void addHostToRack(HashMap<String, Set<String>> rackToNodes,
+  private static void addHostToRack(Map<String, Set<String>> rackToNodes,

INS26 INS26 INS26 INS26 INS26 INS26 INS26 INS40 INS40 INS40 INS40 INS40 INS40 INS40 INS23 MOV44 INS83 INS83 INS83 INS43 INS59 MOV60 MOV44 INS42 INS42 INS32 UPD74 UPD42 UPD74 UPD42 UPD74 UPD42 INS60 UPD74 INS42 INS42 INS57 UPD74 UPD43 MOV43 INS74 UPD43 INS43 INS5 UPD43 INS43 INS74 INS59 UPD74 UPD74 UPD42 UPD74 UPD42 UPD74 UPD42 UPD74 UPD43 INS43 MOV43 MOV43 UPD74 UPD42 INS43 MOV43 UPD42 INS42 INS43 INS85 UPD42 INS42 UPD42 INS43 INS43 INS42 INS32 UPD42 INS25 UPD74 UPD43 UPD43 INS43 INS5 UPD43 UPD43 INS74 UPD43 UPD42 INS42 UPD43 UPD74 INS42 MOV42 INS42 INS42 INS42 INS42 INS27 INS8 UPD43 UPD42 UPD42 INS42 INS43 INS85 UPD42 UPD42 INS43 INS43 UPD42 UPD42 MOV43 MOV43 UPD74 UPD74 INS60 INS25 INS60 INS61 INS25 INS27 INS27 INS21 MOV10 UPD42 MOV42 UPD42 MOV42 INS42 UPD43 UPD74 UPD74 INS43 INS59 INS32 INS8 UPD74 INS74 INS59 INS32 MOV8 INS27 INS8 INS8 INS32 UPD42 MOV42 UPD42 MOV42 MOV34 INS32 UPD42 UPD74 UPD74 INS42 INS42 MOV32 UPD42 MOV42 UPD42 MOV42 INS42 INS18 UPD43 UPD42 INS43 INS43 INS42 INS32 INS42 INS42 INS60 INS25 MOV21 MOV21 MOV21 MOV25 INS32 INS34 MOV25 MOV21 MOV21 MOV21 INS25 INS42 INS42 INS42 INS42 INS27 UPD74 UPD43 UPD43 UPD42 INS42 MOV42 INS42 INS42 INS43 INS59 INS38 INS8 MOV21 INS42 INS42 INS27 INS8 INS45 INS32 INS45 MOV42 UPD43 UPD42 UPD42 INS42 INS42 INS32 MOV32 INS21 INS18 INS21 MOV21 MOV10 MOV21 INS21 UPD42 UPD42 INS42 INS32 INS34 INS21 INS42 INS42 UPD42 INS42 INS42 INS32 MOV7 INS32 INS32 INS32 INS32 INS32 INS42 INS42 INS32 INS42 INS42 INS32 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS32 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 UPD74 INS42 INS42 UPD42 MOV42 INS42 INS42 UPD42 MOV42 UPD43 UPD42 DEL42 DEL43 DEL85 DEL5 DEL43 DEL42 DEL43 DEL42 DEL43 DEL74 DEL39 DEL42 DEL42 DEL34 DEL27 DEL42 DEL34 DEL27 DEL27 DEL39 DEL42 DEL42 DEL27 DEL36 DEL11 DEL36 DEL27 DEL40 DEL16 DEL59 DEL60 DEL39 DEL42 DEL27 DEL36 DEL42 DEL34 DEL16 DEL59 DEL60 DEL42 DEL34 DEL7 DEL21 DEL32 DEL21 DEL42 DEL37 DEL42 DEL37 DEL42 DEL42 DEL27 DEL8 DEL25 DEL8 DEL25 DEL42 DEL39 DEL42 DEL34 DEL59 DEL60 DEL43 DEL42 DEL44 DEL42 DEL70 DEL42 DEL34 DEL27 DEL34 DEL27 DEL27 DEL36 DEL38 DEL8 DEL25 DEL43 DEL42 DEL43 DEL42 DEL43 DEL74 DEL43 DEL85 DEL5