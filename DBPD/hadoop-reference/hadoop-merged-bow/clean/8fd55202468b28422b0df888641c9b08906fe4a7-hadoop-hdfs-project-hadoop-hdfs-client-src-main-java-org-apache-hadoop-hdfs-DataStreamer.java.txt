Merge remote-tracking branch 'apache/trunk' into HDFS-7285

Conflicts:
	hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
	hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
	hadoop-hdfs-project/hadoop-hdfs-client/src/main/proto/hdfs.proto
	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java
	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java
	hadoop-hdfs-project/hadoop-hdfs/src/main/proto/DatanodeProtocol.proto
	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockTokenWithDFS.java
	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsck.java

Change-Id: Ic7946c4ea35bed587fe879ce58b959b25ecc0823

-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileSystem;
-import org.apache.htrace.NullScope;
-import org.apache.htrace.Sampler;
-import org.apache.htrace.Span;
-import org.apache.htrace.Trace;
-import org.apache.htrace.TraceInfo;
-import org.apache.htrace.TraceScope;
+import org.apache.htrace.core.Sampler;
+import org.apache.htrace.core.Span;
+import org.apache.htrace.core.SpanId;
+import org.apache.htrace.core.TraceScope;
+import org.apache.htrace.core.Tracer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
-  static final Log LOG = LogFactory.getLog(DataStreamer.class);
+  static final Logger LOG = LoggerFactory.getLogger(DataStreamer.class);
-    TraceScope scope = NullScope.INSTANCE;
+    TraceScope scope = null;
-            long parents[] = one.getTraceParents();
+            SpanId[] parents = one.getTraceParents();
-              scope = Trace.startSpan("dataStreamer", new TraceInfo(0, parents[0]));
-              // TODO: use setParents API once it's available from HTrace 3.2
-              // scope = Trace.startSpan("dataStreamer", Sampler.ALWAYS);
-              // scope.getSpan().setParents(parents);
+              scope = dfsClient.getTracer().
+                  newScope("dataStreamer", parents[0]);
+              scope.getSpan().setParents(parents);
-        Span span = null;
+        SpanId spanId = SpanId.INVALID;
-            span = scope.detach();
-            one.setTraceSpan(span);
+            if (scope != null) {
+              spanId = scope.getSpanId();
+              scope.detach();
+              one.setTraceScope(scope);
+            }
+            scope = null;
-        TraceScope writeScope = Trace.startSpan("writeTo", span);
+        TraceScope writeScope = dfsClient.getTracer().
+            newScope("DataStreamer#writeTo", spanId);
-        scope.close();
+        if (scope != null) {
+          scope.close();
+          scope = null;
+        }
-    TraceScope scope = Trace.startSpan("waitForAckedSeqno", Sampler.NEVER);
+    TraceScope scope = dfsClient.getTracer().
+        newScope("waitForAckedSeqno");
-              Span span = Trace.currentSpan();
+              Span span = Tracer.getCurrentSpan();
-          Span span = Trace.currentSpan();
+          Span span = Tracer.getCurrentSpan();
-      TraceScope scope = NullScope.INSTANCE;
+      TraceScope scope = null;
-            scope = Trace.continueSpan(one.getTraceSpan());
-            one.setTraceSpan(null);
+            scope = one.getTraceScope();
+            if (scope != null) {
+              scope.reattach();
+              one.setTraceScope(null);
+            }
+          if (scope != null) {
+          }
+          scope = null;
-          Span span = endOfBlockPacket.getTraceSpan();
-          if (span != null) {
-            // Close any trace span associated with this Packet
-            TraceScope scope = Trace.continueSpan(span);
+          // Close any trace span associated with this Packet
+          TraceScope scope = endOfBlockPacket.getTraceScope();
+          if (scope != null) {
+            scope.reattach();
+            endOfBlockPacket.setTraceScope(null);
-    //get a new datanode
+    int tried = 0;
-    final LocatedBlock lb = dfsClient.namenode.getAdditionalDatanode(
-        src, stat.getFileId(), block, nodes, storageIDs,
-        failed.toArray(new DatanodeInfo[failed.size()]),
-        1, dfsClient.clientName);
-    setPipeline(lb);
+    final StorageType[] originalTypes = storageTypes;
+    final String[] originalIDs = storageIDs;
+    IOException caughtException = null;
+    ArrayList<DatanodeInfo> exclude = new ArrayList<DatanodeInfo>(failed);
+    while (tried < 3) {
+      LocatedBlock lb;
+      //get a new datanode
+      lb = dfsClient.namenode.getAdditionalDatanode(
+          src, stat.getFileId(), block, nodes, storageIDs,
+          exclude.toArray(new DatanodeInfo[exclude.size()]),
+          1, dfsClient.clientName);
+      // a new node was allocated by the namenode. Update nodes.
+      setPipeline(lb);
-    //find the new datanode
-    final int d = findNewDatanode(original);
+      //find the new datanode
+      final int d = findNewDatanode(original);
+      //transfer replica. pick a source from the original nodes
+      final DatanodeInfo src = original[tried % original.length];
+      final DatanodeInfo[] targets = {nodes[d]};
+      final StorageType[] targetStorageTypes = {storageTypes[d]};
-    //transfer replica
-    final DatanodeInfo src = d == 0? nodes[1]: nodes[d - 1];
-    final DatanodeInfo[] targets = {nodes[d]};
-    final StorageType[] targetStorageTypes = {storageTypes[d]};
-    transfer(src, targets, targetStorageTypes, lb.getBlockToken());
+      try {
+        transfer(src, targets, targetStorageTypes, lb.getBlockToken());
+      } catch (IOException ioe) {
+        DFSClient.LOG.warn("Error transferring data from " + src + " to " +
+            nodes[d] + ": " + ioe.getMessage());
+        caughtException = ioe;
+        // add the allocated node to the exclude list.
+        exclude.add(nodes[d]);
+        setPipeline(original, originalTypes, originalIDs);
+        tried++;
+        continue;
+      }
+      return; // finished successfully
+    }
+    // All retries failed
+    throw (caughtException != null) ? caughtException :
+        new IOException("Failed to add a node");
-      final long readTimeout = dfsClient.getDatanodeReadTimeout(2);
+
+      // transfer timeout multiplier based on the transfer size
+      // One per 200 packets = 12.8MB. Minimum is 2.
+      int multi = 2 + (int)(bytesSent/dfsClient.getConf().getWritePacketSize())/200;
+      final long readTimeout = dfsClient.getDatanodeReadTimeout(multi);
-          DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));
+          DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));
-            DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));
+            DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));
-      packet.addTraceParent(Trace.currentSpan());
+      packet.addTraceParent(Tracer.getCurrentSpanId());

MOV26 MOV26 MOV26 MOV26 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD40 UPD43 UPD42 INS60 INS60 INS60 INS60 INS61 INS53 UPD42 UPD42 INS39 INS59 INS5 MOV83 INS5 INS59 INS43 INS59 INS74 INS59 INS27 INS8 INS16 INS33 INS42 INS34 INS43 INS85 UPD42 INS42 INS43 INS85 INS42 UPD42 MOV42 INS42 INS42 INS33 INS43 MOV43 INS42 INS14 UPD42 MOV42 UPD34 MOV34 INS60 INS21 MOV21 MOV60 INS60 MOV60 MOV60 INS54 INS41 INS36 INS42 INS14 INS60 INS8 INS32 UPD42 INS33 INS42 INS42 INS42 INS74 INS42 MOV43 INS59 INS7 INS83 INS43 INS59 INS8 INS12 INS27 INS43 INS45 INS39 INS59 INS25 INS42 UPD42 MOV42 INS8 INS43 INS43 INS42 INS42 MOV32 INS42 INS42 INS2 MOV21 INS44 INS8 INS42 INS33 INS42 INS42 INS27 UPD43 INS27 INS8 INS25 INS21 INS42 INS42 INS42 INS27 INS43 INS42 INS21 INS21 INS21 INS21 INS21 INS18 INS34 INS27 INS42 UPD42 UPD42 UPD42 UPD42 INS40 INS42 INS33 MOV21 INS21 INS27 MOV8 INS7 UPD42 INS42 INS40 INS42 INS32 INS7 INS32 INS32 INS37 INS11 INS34 INS32 UPD42 UPD45 UPD42 INS7 MOV43 INS25 INS42 INS33 INS42 INS33 MOV43 INS40 INS42 INS27 INS42 INS42 INS42 INS42 INS2 INS42 INS42 INS42 INS42 INS42 INS39 INS36 UPD42 INS25 INS21 INS42 UPD42 MOV42 INS42 INS33 INS27 INS8 UPD42 UPD42 INS21 INS21 UPD42 INS45 INS42 INS45 INS2 INS45 INS32 MOV42 INS42 INS27 INS5 INS27 INS8 INS7 UPD42 UPD42 INS32 INS42 INS33 INS21 MOV21 UPD42 INS32 INS32 MOV42 INS42 INS42 INS42 INS42 INS32 UPD42 INS43 INS85 INS21 INS42 INS33 INS21 MOV21 MOV21 INS42 INS33 MOV42 UPD42 MOV42 INS32 INS42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 INS33 INS32 INS42 INS42 INS32 INS7 MOV32 MOV43 INS42 INS42 UPD42 INS42 INS42 INS32 INS42 INS42 INS42 INS32 UPD42 UPD42 INS32 UPD42 MOV2 INS42 INS42 INS42 INS42 UPD42 UPD42 INS42 UPD42 MOV42 DEL40 DEL39 DEL85 DEL42 DEL43 DEL34 DEL14 DEL33 DEL42 DEL7 DEL8 DEL40 DEL40 DEL42 DEL42 DEL32 DEL32 DEL42 DEL43 DEL42 DEL32 DEL59 DEL60 DEL27 DEL34 DEL2 DEL42 DEL34 DEL27 DEL2 DEL16 DEL59 DEL60 DEL34