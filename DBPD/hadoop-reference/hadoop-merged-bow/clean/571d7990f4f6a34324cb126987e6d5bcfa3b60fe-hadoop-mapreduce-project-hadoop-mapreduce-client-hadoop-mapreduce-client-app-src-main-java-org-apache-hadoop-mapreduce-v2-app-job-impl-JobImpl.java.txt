Merge trunk into HA branch


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1196458 13f79535-47bb-0310-9956-ffa450edef68

-import org.apache.hadoop.mapred.FileOutputCommitter;
-import org.apache.hadoop.mapreduce.OutputFormat;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
+import org.apache.hadoop.mapreduce.v2.api.records.AMInfo;
-import org.apache.hadoop.util.ReflectionUtils;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.factories.RecordFactory;
-
-  private final RecordFactory recordFactory =
-      RecordFactoryProvider.getRecordFactory(null);
+  private final OutputCommitter committer;
+  private final List<AMInfo> amInfos;
+  private final boolean newApiCommitter;
+  private final long appSubmitTime;
-  private OutputCommitter committer;
-
-  private long submitTime;
-  public JobImpl(ApplicationAttemptId applicationAttemptId, Configuration conf,
-      EventHandler eventHandler, TaskAttemptListener taskAttemptListener,
+  public JobImpl(JobId jobId, ApplicationAttemptId applicationAttemptId,
+      Configuration conf, EventHandler eventHandler,
+      TaskAttemptListener taskAttemptListener,
-      Credentials fsTokenCredentials, Clock clock, 
+      Credentials fsTokenCredentials, Clock clock,
-      String userName) {
+      OutputCommitter committer, boolean newApiCommitter, String userName,
+      long appSubmitTime, List<AMInfo> amInfos) {
-    this.jobId = recordFactory.newRecordInstance(JobId.class);
+    this.jobId = jobId;
+    this.amInfos = amInfos;
-    ApplicationId applicationId = applicationAttemptId.getApplicationId();
-    jobId.setAppId(applicationId);
-    jobId.setId(applicationId.getId());
-    oldJobId = TypeConverter.fromYarn(jobId);
-    LOG.info("Job created" +
-    		" appId=" + applicationId + 
-    		" jobId=" + jobId + 
-    		" oldJobId=" + oldJobId);
-    
+    this.appSubmitTime = appSubmitTime;
+    this.oldJobId = TypeConverter.fromYarn(jobId);
+    this.newApiCommitter = newApiCommitter;
+
+    this.committer = committer;
-            startTime, finishTime, setupProgress, 0.0f,
-            0.0f, cleanupProgress, remoteJobConfFile.toString());
+            appSubmitTime, startTime, finishTime, setupProgress, 0.0f, 0.0f,
+            cleanupProgress, remoteJobConfFile.toString(), amInfos);
-          startTime, finishTime, setupProgress, computeProgress(mapTasks),
-          computeProgress(reduceTasks), cleanupProgress, remoteJobConfFile.toString());
+          appSubmitTime, startTime, finishTime, setupProgress,
+          computeProgress(mapTasks), computeProgress(reduceTasks),
+          cleanupProgress, remoteJobConfFile.toString(), amInfos);
+  /**
+   * Create the default file System for this job.
+   * @param conf the conf object
+   * @return the default filesystem for this job
+   * @throws IOException
+   */
+  protected FileSystem getFileSystem(Configuration conf) throws IOException {
+    return FileSystem.get(conf);
+  }
+  
-      
+  
+  @Override
+  public List<AMInfo> getAMInfos() {
+    return amInfos;
+  }
-      job.submitTime = job.clock.getTime();
-        job.fs = FileSystem.get(job.conf);
+        job.fs = job.getFileSystem(job.conf);
-            job.submitTime,
+            job.appSubmitTime,
-        
-        boolean newApiCommitter = false;
-        if ((job.numReduceTasks > 0 && 
-            job.conf.getBoolean("mapred.reducer.new-api", false)) ||
-              (job.numReduceTasks == 0 && 
-               job.conf.getBoolean("mapred.mapper.new-api", false)))  {
-          newApiCommitter = true;
-          LOG.info("Using mapred newApiCommitter.");
-        }
-        
-        LOG.info("OutputCommitter set in config " + job.conf.get("mapred.output.committer.class"));
-        
-        if (newApiCommitter) {
+        if (job.newApiCommitter) {
-          org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = RecordFactoryProvider
-              .getRecordFactory(null)
-              .newRecordInstance(
-                  org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId.class);
-          attemptID.setTaskId(RecordFactoryProvider.getRecordFactory(null)
-              .newRecordInstance(TaskId.class));
-          attemptID.getTaskId().setJobId(job.jobId);
-          attemptID.getTaskId().setTaskType(TaskType.MAP);
-          TaskAttemptContext taskContext = new TaskAttemptContextImpl(job.conf,
-              TypeConverter.fromYarn(attemptID));
-          try {
-            OutputFormat outputFormat = ReflectionUtils.newInstance(
-                taskContext.getOutputFormatClass(), job.conf);
-            job.committer = outputFormat.getOutputCommitter(taskContext);
-          } catch(Exception e) {
-            throw new IOException("Failed to assign outputcommitter", e);
-          }
-          job.committer = ReflectionUtils.newInstance(
-              job.conf.getClass("mapred.output.committer.class", FileOutputCommitter.class,
-              org.apache.hadoop.mapred.OutputCommitter.class), job.conf);
-        LOG.info("OutputCommitter is " + job.committer.getClass().getName());
-//FIXME:  need new memory criterion for uber-decision (oops, too late here; until AM-resizing supported, must depend on job client to pass fat-slot needs)
+        //FIXME:  need new memory criterion for uber-decision (oops, too late here; 
+        // until AM-resizing supported, must depend on job client to pass fat-slot needs)
-            job.conf.getLong("dfs.block.size", 64*1024*1024));  //FIXME: this is wrong; get FS from [File?]InputFormat and default block size from that
-        //long sysMemSizeForUberSlot = JobTracker.getMemSizeForReduceSlot(); // FIXME [could use default AM-container memory size...]
+            job.conf.getLong("dfs.block.size", 64*1024*1024));  //FIXME: this is 
+        // wrong; get FS from [File?]InputFormat and default block size from that
+        //long sysMemSizeForUberSlot = JobTracker.getMemSizeForReduceSlot(); 
+        // FIXME [could use default AM-container memory size...]
-//  FIXME   && (Math.max(memoryPerMap, memoryPerReduce) <= sysMemSizeForUberSlot
-//              || sysMemSizeForUberSlot == JobConf.DISABLED_MEMORY_LIMIT)
+        //  FIXME   && (Math.max(memoryPerMap, memoryPerReduce) <= sysMemSizeForUberSlot
+        //              || sysMemSizeForUberSlot == JobConf.DISABLED_MEMORY_LIMIT)
-//        canSpeculateMaps = canSpeculateReduces = false; // [TODO: in old version, ultimately was from conf.getMapSpeculativeExecution(), conf.getReduceSpeculativeExecution()]
+          //canSpeculateMaps = canSpeculateReduces = false; // [TODO: in old 
+          //version, ultimately was from conf.getMapSpeculativeExecution(), 
+          //conf.getReduceSpeculativeExecution()]
-
-      Path remoteJobTokenFile =
-          new Path(job.remoteJobSubmitDir,
-              MRJobConfig.APPLICATION_TOKENS_FILE);
-      tokenStorage.writeTokenStorageFile(remoteJobTokenFile, job.conf);
-      LOG.info("Writing back the job-token file on the remote file system:"
-          + remoteJobTokenFile.toString());
-          job.submitTime, job.startTime);
+          job.appSubmitTime, job.startTime);

MOV26 MOV23 MOV23 UPD40 INS23 INS23 INS31 INS31 INS83 MOV83 MOV83 INS74 INS59 INS83 INS83 INS39 INS59 INS83 INS44 INS44 INS44 INS44 INS44 MOV21 MOV21 INS29 INS83 INS43 INS42 INS44 MOV43 INS8 INS78 INS83 INS74 INS42 INS8 INS43 INS43 INS42 INS42 UPD42 MOV43 INS42 INS43 INS42 INS39 INS42 INS39 INS42 INS74 INS42 INS21 INS65 INS65 INS65 INS65 INS42 INS43 INS42 INS41 INS42 INS43 INS43 INS41 UPD42 MOV42 INS42 INS42 INS43 INS43 INS7 INS7 INS7 INS7 INS66 INS42 INS66 INS66 INS42 INS42 INS32 INS42 INS42 INS42 INS42 INS42 INS42 INS22 INS42 INS22 INS42 INS22 INS22 INS42 INS22 INS42 INS42 INS42 INS42 MOV43 INS52 INS42 INS52 INS42 INS52 INS42 INS52 INS42 INS52 INS42 INS42 INS42 INS40 INS8 MOV43 UPD40 MOV21 INS42 INS42 UPD42 UPD42 UPD40 DEL40 DEL26 DEL40 DEL26 DEL40 DEL26 DEL40 DEL26 DEL40 DEL26 DEL40 DEL26 DEL43 DEL42 DEL42 DEL42 DEL33 DEL32 DEL59 DEL23 DEL42 DEL42 DEL57 DEL32 DEL42 DEL43 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL42 DEL42 DEL42 DEL32 DEL42 DEL42 DEL42 DEL42 DEL42 DEL32 DEL32 DEL42 DEL42 DEL45 DEL45 DEL27 DEL42 DEL45 DEL42 DEL45 DEL42 DEL27 DEL32 DEL40 DEL40 DEL42 DEL32 DEL7 DEL21 DEL39 DEL42 DEL9 DEL59 DEL60 DEL40 DEL34 DEL27 DEL40 DEL42 DEL45 DEL9 DEL32 DEL27 DEL36 DEL40 DEL34 DEL27 DEL40 DEL42 DEL45 DEL9 DEL32 DEL27 DEL36 DEL27 DEL42 DEL9 DEL7 DEL21 DEL42 DEL42 DEL45 DEL32 DEL21 DEL8 DEL25 DEL42 DEL42 DEL45 DEL40 DEL42 DEL45 DEL32 DEL27 DEL32 DEL21 DEL42 DEL40 DEL43 DEL42 DEL42 DEL42 DEL33 DEL32 DEL42 DEL40 DEL43 DEL57 DEL32 DEL59 DEL60 DEL42 DEL42 DEL42 DEL42 DEL33 DEL32 DEL42 DEL42 DEL43 DEL57 DEL32 DEL32 DEL21 DEL42 DEL42 DEL32 DEL42 DEL40 DEL32 DEL21 DEL42 DEL42 DEL32 DEL42 DEL40 DEL32 DEL21 DEL42 DEL43 DEL42 DEL42 DEL43 DEL40 DEL42 DEL42 DEL42 DEL32 DEL14 DEL59 DEL60 DEL42 DEL43 DEL42 DEL42 DEL42 DEL42 DEL42 DEL32 DEL40 DEL32 DEL59 DEL60 DEL40 DEL42 DEL42 DEL42 DEL32 DEL7 DEL21 DEL8 DEL42 DEL43 DEL42 DEL44 DEL45 DEL42 DEL14 DEL53 DEL8 DEL12 DEL54 DEL8 DEL40 DEL42 DEL42 DEL40 DEL42 DEL45 DEL42 DEL43 DEL57 DEL40 DEL43 DEL57 DEL32 DEL40 DEL32 DEL7 DEL21 DEL42 DEL42 DEL45 DEL40 DEL42 DEL32 DEL42 DEL32 DEL27 DEL32 DEL21 DEL42 DEL43 DEL42 DEL42 DEL43 DEL40 DEL40 DEL14 DEL59 DEL60 DEL42 DEL42 DEL42 DEL40 DEL32 DEL21 DEL42 DEL42 DEL45 DEL42 DEL42 DEL32 DEL27 DEL32 DEL21