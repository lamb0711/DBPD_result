merge the rest of trunk to branch HDFS-4949

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532967 13f79535-47bb-0310-9956-ffa450edef68

+import java.nio.ByteBuffer;
+import java.nio.channels.ClosedChannelException;
-import java.util.SortedMap;
-import java.util.TreeMap;
-import java.util.concurrent.locks.ReentrantLock;
+import java.util.Map.Entry;
+import java.util.concurrent.ConcurrentNavigableMap;
+import java.util.concurrent.ConcurrentSkipListMap;
+import java.util.concurrent.atomic.AtomicLong;
+import org.apache.hadoop.nfs.nfs3.response.COMMIT3Response;
+import org.apache.hadoop.oncrpc.security.VerifierNone;
+import org.apache.hadoop.util.Daemon;
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.base.Preconditions;
+
-  /**
-   * Lock to synchronize OpenFileCtx changes. Thread should get this lock before
-   * any read/write operation to an OpenFileCtx object
-   */
-  private final ReentrantLock ctxLock;
+  // Pending writes water mark for dump, 1MB
+  private static long DUMP_WRITE_WATER_MARK = 1024 * 1024;
+  static enum COMMIT_STATUS {
+    COMMIT_FINISHED,
+    COMMIT_WAIT,
+    COMMIT_INACTIVE_CTX,
+    COMMIT_INACTIVE_WITH_PENDING_WRITE,
+    COMMIT_ERROR,
+    COMMIT_DO_SYNC;
+  }
+
+  private final DFSClient client;
+  private final IdUserGroup iug;
+  
-  private boolean activeState;
+  private volatile boolean activeState;
-  private boolean asyncStatus;
+  private volatile boolean asyncStatus;
+  /**
+   * The current offset of the file in HDFS. All the content before this offset
+   * has been written back to HDFS.
+   */
+  private AtomicLong nextOffset;
-  private final Nfs3FileAttributes latestAttr;
-  private long nextOffset;
+  
+  // It's updated after each sync to HDFS
+  private Nfs3FileAttributes latestAttr;
-  private final SortedMap<OffsetRange, WriteCtx> pendingWrites;
+  private final ConcurrentNavigableMap<OffsetRange, WriteCtx> pendingWrites;
+  
+  private final ConcurrentNavigableMap<Long, CommitCtx> pendingCommits;
+
+  static class CommitCtx {
+    private final long offset;
+    private final Channel channel;
+    private final int xid;
+    private final Nfs3FileAttributes preOpAttr;
+
+    // Remember time for debug purpose
+    private final long startTime;
+
+    long getOffset() {
+      return offset;
+    }
+
+    Channel getChannel() {
+      return channel;
+    }
+
+    int getXid() {
+      return xid;
+    }
+
+    Nfs3FileAttributes getPreOpAttr() {
+      return preOpAttr;
+    }
+
+    long getStartTime() {
+      return startTime;
+    }
+
+    CommitCtx(long offset, Channel channel, int xid,
+        Nfs3FileAttributes preOpAttr) {
+      this.offset = offset;
+      this.channel = channel;
+      this.xid = xid;
+      this.preOpAttr = preOpAttr;
+      this.startTime = System.currentTimeMillis();
+    }
+
+    @Override
+    public String toString() {
+      return String.format("offset: %d xid: %d startTime: %d", offset, xid,
+          startTime);
+    }
+  }
-  // Pending writes water mark for dump, 1MB
-  private static int DUMP_WRITE_WATER_MARK = 1024 * 1024; 
+  private volatile boolean enabledDump;
-  private long nonSequentialWriteInMemory;
-  private boolean enabledDump;
+  private AtomicLong nonSequentialWriteInMemory;
+  private Daemon dumpThread;
+  public long getNextOffset() {
+    return nextOffset.get();
+  }
+  
-    nonSequentialWriteInMemory += count;
+    long newValue = nonSequentialWriteInMemory.addAndGet(count);
-          + nonSequentialWriteInMemory);
+          + newValue);
-    if (nonSequentialWriteInMemory < 0) {
-      LOG.error("nonSequentialWriteInMemory is negative after update with count "
-          + count);
-      throw new IllegalArgumentException(
-          "nonSequentialWriteInMemory is negative after update with count "
-              + count);
-    }
-    return nonSequentialWriteInMemory;
+    Preconditions.checkState(newValue >= 0,
+        "nonSequentialWriteInMemory is negative after update with count "
+            + count);
+    return newValue;
-      String dumpFilePath) {
+      String dumpFilePath, DFSClient client, IdUserGroup iug) {
-    pendingWrites = new TreeMap<OffsetRange, WriteCtx>();
+    // We use the ReverseComparatorOnMin as the comparator of the map. In this
+    // way, we first dump the data with larger offset. In the meanwhile, we
+    // retrieve the last element to write back to HDFS.
+    pendingWrites = new ConcurrentSkipListMap<OffsetRange, WriteCtx>(
+        OffsetRange.ReverseComparatorOnMin);
+    
+    pendingCommits = new ConcurrentSkipListMap<Long, CommitCtx>();
+    
-    nonSequentialWriteInMemory = 0;
+    nonSequentialWriteInMemory = new AtomicLong(0);
+  
-    nextOffset = latestAttr.getSize();
-    assert(nextOffset == this.fos.getPos());
-
-    ctxLock = new ReentrantLock(true);
+    nextOffset = new AtomicLong();
+    nextOffset.set(latestAttr.getSize());
+    assert(nextOffset.get() == this.fos.getPos());
+    dumpThread = null;
+    this.client = client;
+    this.iug = iug;
-  private void lockCtx() {
-    if (LOG.isTraceEnabled()) {
-      StackTraceElement[] stacktrace = Thread.currentThread().getStackTrace();
-      StackTraceElement e = stacktrace[2];
-      String methodName = e.getMethodName();
-      LOG.trace("lock ctx, caller:" + methodName);
-    }
-    ctxLock.lock();
-  }
-
-  private void unlockCtx() {
-    ctxLock.unlock();
-    if (LOG.isTraceEnabled()) {
-      StackTraceElement[] stacktrace = Thread.currentThread().getStackTrace();
-      StackTraceElement e = stacktrace[2];
-      String methodName = e.getMethodName();
-      LOG.info("unlock ctx, caller:" + methodName);
-    }
-  }
-  
-  // Make a copy of the latestAttr
-  public Nfs3FileAttributes copyLatestAttr() {
-    Nfs3FileAttributes ret;
-    lockCtx();
-    try {
-      ret = new Nfs3FileAttributes(latestAttr);
-    } finally {
-      unlockCtx();
-    }
-    return ret;
-  }
-  
-  private long getNextOffsetUnprotected() {
-    assert(ctxLock.isLocked());
-    return nextOffset;
-  }
-
-  public long getNextOffset() {
-    long ret;
-    lockCtx();
-    try {
-      ret = getNextOffsetUnprotected();
-    } finally {
-      unlockCtx();
-    }
-    return ret;
+  public Nfs3FileAttributes getLatestAttr() {
+    return latestAttr;
-  private void checkDump(long count) {
-    assert (ctxLock.isLocked());
-
-    // Always update the in memory count
-    updateNonSequentialWriteInMemory(count);
-
+  private void checkDump() {
-    if (nonSequentialWriteInMemory < DUMP_WRITE_WATER_MARK) {
+    if (nonSequentialWriteInMemory.get() < DUMP_WRITE_WATER_MARK) {
-    // Create dump outputstream for the first time
-    if (dumpOut == null) {
-      LOG.info("Create dump file:" + dumpFilePath);
-      File dumpFile = new File(dumpFilePath);
-      try {
-        if (dumpFile.exists()) {
-          LOG.fatal("The dump file should not exist:" + dumpFilePath);
-          throw new RuntimeException("The dump file should not exist:"
-              + dumpFilePath);
+    // wake up the dumper thread to dump the data
+    synchronized (this) {
+      if (nonSequentialWriteInMemory.get() >= DUMP_WRITE_WATER_MARK) {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Asking dumper to dump...");
-        dumpOut = new FileOutputStream(dumpFile);
-      } catch (IOException e) {
-        LOG.error("Got failure when creating dump stream " + dumpFilePath
-            + " with error:" + e);
-        enabledDump = false;
-        IOUtils.cleanup(LOG, dumpOut);
-        return;
-      }
-    }
-    // Get raf for the first dump
-    if (raf == null) {
-      try {
-        raf = new RandomAccessFile(dumpFilePath, "r");
-      } catch (FileNotFoundException e) {
-        LOG.error("Can't get random access to file " + dumpFilePath);
-        // Disable dump
-        enabledDump = false;
-        return;
-      }
-    }
-    
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Start dump, current write number:" + pendingWrites.size());
-    }
-    Iterator<OffsetRange> it = pendingWrites.keySet().iterator();
-    while (it.hasNext()) {
-      OffsetRange key = it.next();
-      WriteCtx writeCtx = pendingWrites.get(key);
-      try {
-        long dumpedDataSize = writeCtx.dumpData(dumpOut, raf);
-        if (dumpedDataSize > 0) {
-          updateNonSequentialWriteInMemory(-dumpedDataSize);
+        if (dumpThread == null) {
+          dumpThread = new Daemon(new Dumper());
+          dumpThread.start();
+        } else {
+          this.notifyAll();          
-      } catch (IOException e) {
-        LOG.error("Dump data failed:" + writeCtx + " with error:" + e);
-        // Disable dump
-        enabledDump = false;
-        return;
-    if (nonSequentialWriteInMemory != 0) {
-      LOG.fatal("After dump, nonSequentialWriteInMemory is not zero: "
-          + nonSequentialWriteInMemory);
-      throw new RuntimeException(
-          "After dump, nonSequentialWriteInMemory is not zero: "
-              + nonSequentialWriteInMemory);
+  }
+
+  class Dumper implements Runnable {
+    /** Dump data into a file */
+    private void dump() {
+      // Create dump outputstream for the first time
+      if (dumpOut == null) {
+        LOG.info("Create dump file:" + dumpFilePath);
+        File dumpFile = new File(dumpFilePath);
+        try {
+          synchronized (this) {
+            // check if alive again
+            Preconditions.checkState(dumpFile.createNewFile(),
+                "The dump file should not exist: %s", dumpFilePath);
+            dumpOut = new FileOutputStream(dumpFile);
+          }
+        } catch (IOException e) {
+          LOG.error("Got failure when creating dump stream " + dumpFilePath, e);
+          enabledDump = false;
+          if (dumpOut != null) {
+            try {
+              dumpOut.close();
+            } catch (IOException e1) {
+              LOG.error("Can't close dump stream " + dumpFilePath, e);
+            }
+          }
+          return;
+        }
+      }
+
+      // Get raf for the first dump
+      if (raf == null) {
+        try {
+          raf = new RandomAccessFile(dumpFilePath, "r");
+        } catch (FileNotFoundException e) {
+          LOG.error("Can't get random access to file " + dumpFilePath);
+          // Disable dump
+          enabledDump = false;
+          return;
+        }
+      }
+
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Start dump. Before dump, nonSequentialWriteInMemory == "
+            + nonSequentialWriteInMemory.get());
+      }
+
+      Iterator<OffsetRange> it = pendingWrites.keySet().iterator();
+      while (activeState && it.hasNext()
+          && nonSequentialWriteInMemory.get() > 0) {
+        OffsetRange key = it.next();
+        WriteCtx writeCtx = pendingWrites.get(key);
+        if (writeCtx == null) {
+          // This write was just deleted
+          continue;
+        }
+        try {
+          long dumpedDataSize = writeCtx.dumpData(dumpOut, raf);
+          if (dumpedDataSize > 0) {
+            updateNonSequentialWriteInMemory(-dumpedDataSize);
+          }
+        } catch (IOException e) {
+          LOG.error("Dump data failed:" + writeCtx + " with error:" + e
+              + " OpenFileCtx state:" + activeState);
+          // Disable dump
+          enabledDump = false;
+          return;
+        }
+      }
+
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("After dump, nonSequentialWriteInMemory == "
+            + nonSequentialWriteInMemory.get());
+      }
+    }
+
+    @Override
+    public void run() {
+      while (activeState && enabledDump) {
+        try {
+          if (nonSequentialWriteInMemory.get() >= DUMP_WRITE_WATER_MARK) {
+            dump();
+          }
+          synchronized (OpenFileCtx.this) {
+            if (nonSequentialWriteInMemory.get() < DUMP_WRITE_WATER_MARK) {
+              try {
+                OpenFileCtx.this.wait();
+                if (LOG.isDebugEnabled()) {
+                  LOG.debug("Dumper woke up");
+                }
+              } catch (InterruptedException e) {
+                LOG.info("Dumper is interrupted, dumpFilePath= "
+                    + OpenFileCtx.this.dumpFilePath);
+              }
+            }
+          }
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("Dumper checking OpenFileCtx activeState: " + activeState
+                + " enabledDump: " + enabledDump);
+          }
+        } catch (Throwable t) {
+          LOG.info("Dumper get Throwable: " + t + ". dumpFilePath: "
+              + OpenFileCtx.this.dumpFilePath);
+        }
+      }
-
-    lockCtx();
-    try {
-      if (!activeState) {
-        LOG.info("OpenFileCtx is inactive, fileId:"
-            + request.getHandle().getFileId());
-        WccData fileWcc = new WccData(latestAttr.getWccAttr(), latestAttr);
-        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_IO,
-            fileWcc, 0, request.getStableHow(), Nfs3Constant.WRITE_COMMIT_VERF);
-        Nfs3Utils.writeChannel(channel, response.send(new XDR(), xid));
-      } else {
-        // Handle repeated write requests(same xid or not).
-        // If already replied, send reply again. If not replied, drop the
-        // repeated request.
-        WriteCtx existantWriteCtx = checkRepeatedWriteRequest(request, channel,
-            xid);
-        if (existantWriteCtx != null) {
-          if (!existantWriteCtx.getReplied()) {
-            if (LOG.isDebugEnabled()) {
-              LOG.debug("Repeated write request which hasn't be served: xid="
-                  + xid + ", drop it.");
-            }
-          } else {
-            if (LOG.isDebugEnabled()) {
-              LOG.debug("Repeated write request which is already served: xid="
-                  + xid + ", resend response.");
-            }
-            WccData fileWcc = new WccData(latestAttr.getWccAttr(), latestAttr);
-            WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK,
-                fileWcc, request.getCount(), request.getStableHow(),
-                Nfs3Constant.WRITE_COMMIT_VERF);
-            Nfs3Utils.writeChannel(channel, response.send(new XDR(), xid));
+    
+    if (!activeState) {
+      LOG.info("OpenFileCtx is inactive, fileId:"
+          + request.getHandle().getFileId());
+      WccData fileWcc = new WccData(latestAttr.getWccAttr(), latestAttr);
+      WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_IO,
+          fileWcc, 0, request.getStableHow(), Nfs3Constant.WRITE_COMMIT_VERF);
+      Nfs3Utils.writeChannel(channel,
+          response.writeHeaderAndResponse(new XDR(), xid, new VerifierNone()),
+          xid);
+    } else {
+      // Update the write time first
+      updateLastAccessTime();
+      
+      // Handle repeated write requests (same xid or not).
+      // If already replied, send reply again. If not replied, drop the
+      // repeated request.
+      WriteCtx existantWriteCtx = checkRepeatedWriteRequest(request, channel,
+          xid);
+      if (existantWriteCtx != null) {
+        if (!existantWriteCtx.getReplied()) {
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("Repeated write request which hasn't be served: xid="
+                + xid + ", drop it.");
-          updateLastAccessTime();
-          
-          receivedNewWriteInternal(dfsClient, request, channel, xid,
-              asyncDataService, iug);
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("Repeated write request which is already served: xid="
+                + xid + ", resend response.");
+          }
+          WccData fileWcc = new WccData(latestAttr.getWccAttr(), latestAttr);
+          WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK,
+              fileWcc, request.getCount(), request.getStableHow(),
+              Nfs3Constant.WRITE_COMMIT_VERF);
+          Nfs3Utils.writeChannel(channel, response.writeHeaderAndResponse(
+              new XDR(), xid, new VerifierNone()), xid);
+        }
+      } else {
+        // not a repeated write request
+        receivedNewWriteInternal(dfsClient, request, channel, xid,
+            asyncDataService, iug);
+      }
+    }
+  }
+
+  @VisibleForTesting
+  public static void alterWriteRequest(WRITE3Request request, long cachedOffset) {
+    long offset = request.getOffset();
+    int count = request.getCount();
+    long smallerCount = offset + count - cachedOffset;
+    if (LOG.isDebugEnabled()) {
+      LOG.debug(String.format("Got overwrite with appended data (%d-%d),"
+          + " current offset %d," + " drop the overlapped section (%d-%d)"
+          + " and append new data (%d-%d).", offset, (offset + count - 1),
+          cachedOffset, offset, (cachedOffset - 1), cachedOffset, (offset
+              + count - 1)));
+    }
+    
+    ByteBuffer data = request.getData();
+    Preconditions.checkState(data.position() == 0,
+        "The write request data has non-zero position");
+    data.position((int) (cachedOffset - offset));
+    Preconditions.checkState(data.limit() - data.position() == smallerCount,
+        "The write request buffer has wrong limit/position regarding count");
+    
+    request.setOffset(cachedOffset);
+    request.setCount((int) smallerCount);
+  }
+  
+  /**
+   * Creates and adds a WriteCtx into the pendingWrites map. This is a
+   * synchronized method to handle concurrent writes.
+   * 
+   * @return A non-null {@link WriteCtx} instance if the incoming write
+   *         request's offset >= nextOffset. Otherwise null.
+   */
+  private synchronized WriteCtx addWritesToCache(WRITE3Request request,
+      Channel channel, int xid) {
+    long offset = request.getOffset();
+    int count = request.getCount();
+    long cachedOffset = nextOffset.get();
+    int originalCount = WriteCtx.INVALID_ORIGINAL_COUNT;
+    
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("requesed offset=" + offset + " and current offset="
+          + cachedOffset);
+    }
+
+    // Handle a special case first
+    if ((offset < cachedOffset) && (offset + count > cachedOffset)) {
+      // One Linux client behavior: after a file is closed and reopened to
+      // write, the client sometimes combines previous written data(could still
+      // be in kernel buffer) with newly appended data in one write. This is
+      // usually the first write after file reopened. In this
+      // case, we log the event and drop the overlapped section.
+      LOG.warn(String.format("Got overwrite with appended data (%d-%d),"
+          + " current offset %d," + " drop the overlapped section (%d-%d)"
+          + " and append new data (%d-%d).", offset, (offset + count - 1),
+          cachedOffset, offset, (cachedOffset - 1), cachedOffset, (offset
+              + count - 1)));
+
+      if (!pendingWrites.isEmpty()) {
+        LOG.warn("There are other pending writes, fail this jumbo write");
+        return null;
+      }
+      
+      LOG.warn("Modify this write to write only the appended data");
+      alterWriteRequest(request, cachedOffset);
+
+      // Update local variable
+      originalCount = count;
+      offset = request.getOffset();
+      count = request.getCount();
+    }
+    
+    // Fail non-append call
+    if (offset < cachedOffset) {
+      LOG.warn("(offset,count,nextOffset):" + "(" + offset + "," + count + ","
+          + nextOffset + ")");
+      return null;
+    } else {
+      DataState dataState = offset == cachedOffset ? WriteCtx.DataState.NO_DUMP
+          : WriteCtx.DataState.ALLOW_DUMP;
+      WriteCtx writeCtx = new WriteCtx(request.getHandle(),
+          request.getOffset(), request.getCount(), originalCount,
+          request.getStableHow(), request.getData(), channel, xid, false,
+          dataState);
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Add new write to the list with nextOffset " + cachedOffset
+            + " and requesed offset=" + offset);
+      }
+      if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {
+        // update the memory size
+        updateNonSequentialWriteInMemory(count);
+      }
+      // check if there is a WriteCtx with the same range in pendingWrites
+      WriteCtx oldWriteCtx = checkRepeatedWriteRequest(request, channel, xid);
+      if (oldWriteCtx == null) {
+        addWrite(writeCtx);
+      } else {
+        LOG.warn("Got a repeated request, same range, with xid:"
+            + writeCtx.getXid());
+      }
+      return writeCtx;
+    }
+  }
+  
+  /** Process an overwrite write request */
+  private void processOverWrite(DFSClient dfsClient, WRITE3Request request,
+      Channel channel, int xid, IdUserGroup iug) {
+    WccData wccData = new WccData(latestAttr.getWccAttr(), null);
+    long offset = request.getOffset();
+    int count = request.getCount();
+    WriteStableHow stableHow = request.getStableHow();
+    WRITE3Response response;
+    long cachedOffset = nextOffset.get();
+    if (offset + count > cachedOffset) {
+      LOG.warn("Treat this jumbo write as a real random write, no support.");
+      response = new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,
+          WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);
+    } else {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Process perfectOverWrite");
+      }
+      // TODO: let executor handle perfect overwrite
+      response = processPerfectOverWrite(dfsClient, offset, count, stableHow,
+          request.getData().array(),
+          Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);
+    }
+    updateLastAccessTime();
+    Nfs3Utils.writeChannel(channel,
+        response.writeHeaderAndResponse(new XDR(), xid, new VerifierNone()),
+        xid);
+  }
+  
+  /**
+   * Check if we can start the write (back to HDFS) now. If there is no hole for
+   * writing, and there is no other threads writing (i.e., asyncStatus is
+   * false), start the writing and set asyncStatus to true.
+   * 
+   * @return True if the new write is sequencial and we can start writing
+   *         (including the case that there is already a thread writing).
+   */
+  private synchronized boolean checkAndStartWrite(
+      AsyncDataService asyncDataService, WriteCtx writeCtx) {
+    
+    if (writeCtx.getOffset() == nextOffset.get()) {
+      if (!asyncStatus) {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Trigger the write back task. Current nextOffset: "
+              + nextOffset.get());
+        }
+        asyncStatus = true;
+        asyncDataService.execute(new AsyncDataService.WriteBackTask(this));
+      } else {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("The write back thread is working.");
-
-    } finally {
-      unlockCtx();
+      return true;
+    } else {
+      return false;
-    long offset = request.getOffset();
-    int count = request.getCount();
-
-    // Get file length, fail non-append call
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("requesed offset=" + offset + " and current filesize="
-          + preOpAttr.getSize());
-    }
+    int count = request.getCount();
-    long nextOffset = getNextOffsetUnprotected();
-    if (offset == nextOffset) {
-      LOG.info("Add to the list, update nextOffset and notify the writer,"
-          + " nextOffset:" + nextOffset);
-      WriteCtx writeCtx = new WriteCtx(request.getHandle(),
-          request.getOffset(), request.getCount(), request.getStableHow(),
-          request.getData().array(), channel, xid, false, DataState.NO_DUMP);
-      addWrite(writeCtx);
-      
-      // Create an async task and change openFileCtx status to indicate async
-      // task pending
-      if (!asyncStatus) {
-        asyncStatus = true;
-        asyncDataService.execute(new AsyncDataService.WriteBackTask(this));
-      }
-      
-      // Update the write time first
-      updateLastAccessTime();
-      Nfs3FileAttributes postOpAttr = new Nfs3FileAttributes(latestAttr);
-
-      // Send response immediately for unstable write
-      if (request.getStableHow() == WriteStableHow.UNSTABLE) {
-        WccData fileWcc = new WccData(preOpAttr, postOpAttr);
-        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK,
-            fileWcc, count, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);
-        Nfs3Utils.writeChannel(channel, response.send(new XDR(), xid));
-        writeCtx.setReplied(true);
-      }
-
-    } else if (offset > nextOffset) {
-      LOG.info("Add new write to the list but not update nextOffset:"
-          + nextOffset);
-      WriteCtx writeCtx = new WriteCtx(request.getHandle(),
-          request.getOffset(), request.getCount(), request.getStableHow(),
-          request.getData().array(), channel, xid, false, DataState.ALLOW_DUMP);
-      addWrite(writeCtx);
-
-      // Check if need to dump some pending requests to file
-      checkDump(request.getCount());
-      updateLastAccessTime();
-      Nfs3FileAttributes postOpAttr = new Nfs3FileAttributes(latestAttr);
-      
-      // In test, noticed some Linux client sends a batch (e.g., 1MB)
-      // of reordered writes and won't send more writes until it gets
-      // responses of the previous batch. So here send response immediately for
-      // unstable non-sequential write
-      if (request.getStableHow() == WriteStableHow.UNSTABLE) {
-        WccData fileWcc = new WccData(preOpAttr, postOpAttr);
-        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK,
-            fileWcc, count, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);
-        Nfs3Utils.writeChannel(channel, response.send(new XDR(), xid));
-        writeCtx.setReplied(true);
-      }
-
-    } else {
+    WriteCtx writeCtx = addWritesToCache(request, channel, xid);
+    if (writeCtx == null) {
-      LOG.warn("(offset,count,nextOffset):" + "(" + offset + "," + count + ","
-          + nextOffset + ")");
-      WccData wccData = new WccData(preOpAttr, null);
-      WRITE3Response response;
-
-      if (offset + count > nextOffset) {
-        LOG.warn("Haven't noticed any partial overwrite out of a sequential file"
-            + "write requests, so treat it as a real random write, no support.");
-        response = new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,
-            WriteStableHow.UNSTABLE, 0);
-      } else {
-        if (LOG.isDebugEnabled()) {
-          LOG.debug("Process perfectOverWrite");
+      processOverWrite(dfsClient, request, channel, xid, iug);
+    } else {
+      // The writes is added to pendingWrites.
+      // Check and start writing back if necessary
+      boolean startWriting = checkAndStartWrite(asyncDataService, writeCtx);
+      if (!startWriting) {
+        // offset > nextOffset. check if we need to dump data
+        checkDump();
+        
+        // In test, noticed some Linux client sends a batch (e.g., 1MB)
+        // of reordered writes and won't send more writes until it gets
+        // responses of the previous batch. So here send response immediately
+        // for unstable non-sequential write
+        if (stableHow != WriteStableHow.UNSTABLE) {
+          LOG.info("Have to change stable write to unstable write:"
+              + request.getStableHow());
+          stableHow = WriteStableHow.UNSTABLE;
-        response = processPerfectOverWrite(dfsClient, offset, count, stableHow,
-            request.getData().array(),
-            Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);
+
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("UNSTABLE write request, send response for offset: "
+              + writeCtx.getOffset());
+        }
+        WccData fileWcc = new WccData(preOpAttr, latestAttr);
+        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK,
+            fileWcc, count, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);
+        Nfs3Utils
+            .writeChannel(channel, response.writeHeaderAndResponse(new XDR(),
+                xid, new VerifierNone()), xid);
+        writeCtx.setReplied(true);
-      
-      updateLastAccessTime();
-      Nfs3Utils.writeChannel(channel, response.send(new XDR(), xid));
-    assert (ctxLock.isLocked());
-      
+    } catch (ClosedChannelException closedException) {
+      LOG.info("The FSDataOutputStream has been closed. " +
+      		"Continue processing the perfect overwrite.");
+    } catch (IOException e) {
+      LOG.info("hsync failed when processing possible perfect overwrite, path="
+          + path + " error:" + e);
+      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,
+          Nfs3Constant.WRITE_COMMIT_VERF);
+    }
+    
+    try {
-        return response = new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0,
-            stableHow, Nfs3Constant.WRITE_COMMIT_VERF);
+        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,
+            Nfs3Constant.WRITE_COMMIT_VERF);
-
-      return response = new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0,
-          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);
+      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,
+          Nfs3Constant.WRITE_COMMIT_VERF);
-          stableHow, 0);
+          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);
-            0);
+            Nfs3Constant.WRITE_COMMIT_VERF);
-          stableHow, 0);
+          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);
-  
-  public final static int COMMIT_FINISHED = 0;
-  public final static int COMMIT_WAIT = 1;
-  public final static int COMMIT_INACTIVE_CTX = 2;
-  public final static int COMMIT_ERROR = 3;
+
+  public COMMIT_STATUS checkCommit(DFSClient dfsClient, long commitOffset,
+      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {
+    // Keep stream active
+    updateLastAccessTime();
+    Preconditions.checkState(commitOffset >= 0);
+
+    COMMIT_STATUS ret = checkCommitInternal(commitOffset, channel, xid,
+        preOpAttr);
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Got commit status: " + ret.name());
+    }
+    // Do the sync outside the lock
+    if (ret == COMMIT_STATUS.COMMIT_DO_SYNC
+        || ret == COMMIT_STATUS.COMMIT_FINISHED) {
+      try {
+        // Sync file data and length
+        fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));
+        // Nothing to do for metadata since attr related change is pass-through
+      } catch (ClosedChannelException cce) {
+        if (pendingWrites.isEmpty()) {
+          ret = COMMIT_STATUS.COMMIT_FINISHED;
+        } else {
+          ret = COMMIT_STATUS.COMMIT_ERROR;
+        }
+      } catch (IOException e) {
+        LOG.error("Got stream error during data sync:" + e);
+        // Do nothing. Stream will be closed eventually by StreamMonitor.
+        // status = Nfs3Status.NFS3ERR_IO;
+        ret = COMMIT_STATUS.COMMIT_ERROR;
+      }
+    }
+    return ret;
+  }
-   * COMMIT_INACTIVE_CTX, COMMIT_ERROR
+   * COMMIT_INACTIVE_CTX, COMMIT_INACTIVE_WITH_PENDING_WRITE, COMMIT_ERROR
-  public int checkCommit(long commitOffset) {
-    int ret = COMMIT_WAIT;
-
-    lockCtx();
-    try {
-      if (!activeState) {
-        ret = COMMIT_INACTIVE_CTX;
+  private synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,
+      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {
+    if (!activeState) {
+      if (pendingWrites.isEmpty()) {
+        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;
-        ret = checkCommitInternal(commitOffset);
+        // TODO: return success if already committed
+        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;
-    } finally {
-      unlockCtx();
-    }
-    return ret;
-  }
-  
-  private int checkCommitInternal(long commitOffset) {
-    if (commitOffset == 0) {
-      // Commit whole file
-      commitOffset = getNextOffsetUnprotected();
-    LOG.info("getFlushedOffset=" + flushed + " commitOffset=" + commitOffset);
-    if (flushed < commitOffset) {
-      // Keep stream active
-      updateLastAccessTime();
-      return COMMIT_WAIT;
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("getFlushedOffset=" + flushed + " commitOffset=" + commitOffset);
-    int ret = COMMIT_WAIT;
-    try {
-      // Sync file data and length
-      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));
-      // Nothing to do for metadata since attr related change is pass-through
-      ret = COMMIT_FINISHED;
-    } catch (IOException e) {
-      LOG.error("Got stream error during data sync:" + e);
-      // Do nothing. Stream will be closed eventually by StreamMonitor.
-      ret = COMMIT_ERROR;
+    if (commitOffset > 0) {
+      if (commitOffset > flushed) {
+        CommitCtx commitCtx = new CommitCtx(commitOffset, channel, xid,
+            preOpAttr);
+        pendingCommits.put(commitOffset, commitCtx);
+        return COMMIT_STATUS.COMMIT_WAIT;
+      } else {
+        return COMMIT_STATUS.COMMIT_DO_SYNC;
+      }
-    // Keep stream active
-    updateLastAccessTime();
-    return ret;
+    Entry<OffsetRange, WriteCtx> key = pendingWrites.firstEntry();
+
+    // Commit whole file, commitOffset == 0
+    if (pendingWrites.isEmpty()) {
+      // Note that, there is no guarantee data is synced. TODO: We could still
+      // do a sync here though the output stream might be closed.
+      return COMMIT_STATUS.COMMIT_FINISHED;
+    } else {
+      // Insert commit
+      long maxOffset = key.getKey().getMax() - 1;
+      Preconditions.checkState(maxOffset > 0);
+      CommitCtx commitCtx = new CommitCtx(maxOffset, channel, xid, preOpAttr);
+      pendingCommits.put(maxOffset, commitCtx);
+      return COMMIT_STATUS.COMMIT_WAIT;
+    }
-    assert (ctxLock.isLocked());
+    // For the offset range (min, max), min is inclusive, and max is exclusive
-  
-  public boolean streamCleanup(long fileId, long streamTimeout) {
+  public synchronized boolean streamCleanup(long fileId, long streamTimeout) {
-    if (!ctxLock.tryLock()) {
-      if (LOG.isTraceEnabled()) {
-        LOG.trace("Another thread is working on it" + ctxLock.toString());
+    // Check the stream timeout
+    if (checkStreamTimeout(streamTimeout)) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("closing stream for fileId:" + fileId);
-      return flag;
-    }
-    
-    try {
-      // Check the stream timeout
-      if (checkStreamTimeout(streamTimeout)) {
-        LOG.info("closing stream for fileId:" + fileId);
-        cleanup();
-        flag = true;
-      }
-    } finally {
-      unlockCtx();
+      cleanup();
+      flag = true;
-  // Invoked by AsynDataService to do the write back
-  public void executeWriteBack() {
-    long nextOffset;
-    OffsetRange key;
-    WriteCtx writeCtx;
-
+  /**
+   * Get (and remove) the next WriteCtx from {@link #pendingWrites} if possible.
+   * 
+   * @return Null if {@link #pendingWrites} is null, or the next WriteCtx's
+   *         offset is larger than nextOffSet.
+   */
+  private synchronized WriteCtx offerNextToWrite() {
+    if (pendingWrites.isEmpty()) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("The asyn write task has no pending writes, fileId: "
+            + latestAttr.getFileId());
+      }
+      // process pending commit again to handle this race: a commit is added
+      // to pendingCommits map just after the last doSingleWrite returns.
+      // There is no pending write and the commit should be handled by the
+      // last doSingleWrite. Due to the race, the commit is left along and
+      // can't be processed until cleanup. Therefore, we should do another
+      // processCommits to fix the race issue.
+      processCommits(nextOffset.get()); // nextOffset has same value as
+                                        // flushedOffset
+      this.asyncStatus = false;
+      return null;
+    } 
+    
+      Entry<OffsetRange, WriteCtx> lastEntry = pendingWrites.lastEntry();
+      OffsetRange range = lastEntry.getKey();
+      WriteCtx toWrite = lastEntry.getValue();
+      
+      if (LOG.isTraceEnabled()) {
+        LOG.trace("range.getMin()=" + range.getMin() + " nextOffset="
+            + nextOffset);
+      }
+      
+      long offset = nextOffset.get();
+      if (range.getMin() > offset) {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("The next sequencial write has not arrived yet");
+        }
+        processCommits(nextOffset.get()); // handle race
+        this.asyncStatus = false;
+      } else if (range.getMin() < offset && range.getMax() > offset) {
+        // shouldn't happen since we do sync for overlapped concurrent writers
+        LOG.warn("Got a overlapping write (" + range.getMin() + ","
+            + range.getMax() + "), nextOffset=" + offset
+            + ". Silently drop it now");
+        pendingWrites.remove(range);
+        processCommits(nextOffset.get()); // handle race
+      } else {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Remove write(" + range.getMin() + "-" + range.getMax()
+              + ") from the list");
+        }
+        // after writing, remove the WriteCtx from cache 
+        pendingWrites.remove(range);
+        // update nextOffset
+        nextOffset.addAndGet(toWrite.getCount());
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Change nextOffset to " + nextOffset.get());
+        }
+        return toWrite;
+      }
+    
+    return null;
+  }
+  
+  /** Invoked by AsynDataService to write back to HDFS */
+  void executeWriteBack() {
+    Preconditions.checkState(asyncStatus,
+        "The openFileCtx has false async status");
-      // Don't lock OpenFileCtx for all writes to reduce the timeout of other
-      // client request to the same file
-      while (true) {
-        lockCtx();
-        if (!asyncStatus) {
-          // This should never happen. There should be only one thread working
-          // on one OpenFileCtx anytime.
-          LOG.fatal("The openFileCtx has false async status");
-          throw new RuntimeException("The openFileCtx has false async status");
-        }
-        // Any single write failure can change activeState to false, so do the
-        // check each loop.
-        if (pendingWrites.isEmpty()) {
-          if (LOG.isDebugEnabled()) {
-            LOG.debug("The asyn write task has no pendding writes, fileId: "
-                + latestAttr.getFileId());
-          }
-          break;
-        }
-        if (!activeState) {
-          if (LOG.isDebugEnabled()) {
-            LOG.debug("The openFileCtx is not active anymore, fileId: "
-                + latestAttr.getFileId());
-          }
-          break;
-        }
-
-        // Get the next sequential write
-        nextOffset = getNextOffsetUnprotected();
-        key = pendingWrites.firstKey();
-        if (LOG.isTraceEnabled()) {
-          LOG.trace("key.getMin()=" + key.getMin() + " nextOffset="
-              + nextOffset);
-        }
-
-        if (key.getMin() > nextOffset) {
-          if (LOG.isDebugEnabled()) {
-            LOG.info("The next sequencial write has not arrived yet");
-          }
-          break;
-
-        } else if (key.getMin() < nextOffset && key.getMax() > nextOffset) {
-          // Can't handle overlapping write. Didn't see it in tests yet.
-          LOG.fatal("Got a overlapping write (" + key.getMin() + ","
-              + key.getMax() + "), nextOffset=" + nextOffset);
-          throw new RuntimeException("Got a overlapping write (" + key.getMin()
-              + "," + key.getMax() + "), nextOffset=" + nextOffset);
-
-        } else {
-          if (LOG.isTraceEnabled()) {
-            LOG.trace("Remove write(" + key.getMin() + "-" + key.getMax()
-                + ") from the list");
-          }
-          writeCtx = pendingWrites.remove(key);
+      while (activeState) {
+        WriteCtx toWrite = offerNextToWrite();
+        if (toWrite != null) {
-          doSingleWrite(writeCtx);
+          doSingleWrite(toWrite);
+        } else {
+          break;
-        
-        unlockCtx();
-
+      
+      if (!activeState && LOG.isDebugEnabled()) {
+        LOG.debug("The openFileCtx is not active anymore, fileId: "
+            + latestAttr.getFileId());
+      }
-      // Always reset the async status so another async task can be created
-      // for this file
+      // make sure we reset asyncStatus to false
-      if (ctxLock.isHeldByCurrentThread()) {
-        unlockCtx();
-      }
+  private void processCommits(long offset) {
+    Preconditions.checkState(offset > 0);
+    long flushedOffset = getFlushedOffset();
+    Entry<Long, CommitCtx> entry = pendingCommits.firstEntry();
+
+    if (entry == null || entry.getValue().offset > flushedOffset) {
+      return;
+    }
+
+    // Now do sync for the ready commits
+    int status = Nfs3Status.NFS3ERR_IO;
+    try {
+      // Sync file data and length
+      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));
+      status = Nfs3Status.NFS3_OK;
+    } catch (ClosedChannelException cce) {
+      if (!pendingWrites.isEmpty()) {
+        LOG.error("Can't sync for fileId: " + latestAttr.getFileId()
+            + ". Channel closed with writes pending");
+      }
+      status = Nfs3Status.NFS3ERR_IO;
+    } catch (IOException e) {
+      LOG.error("Got stream error during data sync:" + e);
+      // Do nothing. Stream will be closed eventually by StreamMonitor.
+      status = Nfs3Status.NFS3ERR_IO;
+    }
+
+    // Update latestAttr
+    try {
+      latestAttr = Nfs3Utils.getFileAttr(client,
+          Nfs3Utils.getFileIdPath(latestAttr.getFileId()), iug);
+    } catch (IOException e) {
+      LOG.error("Can't get new file attr for fileId: " + latestAttr.getFileId());
+      status = Nfs3Status.NFS3ERR_IO;
+    }
+
+    if (latestAttr.getSize() != offset) {
+      LOG.error("After sync, the expect file size: " + offset
+          + ", however actual file size is: " + latestAttr.getSize());
+      status = Nfs3Status.NFS3ERR_IO;
+    }
+    WccData wccData = new WccData(Nfs3Utils.getWccAttr(latestAttr), latestAttr);
+
+    // Send response for the ready commits
+    while (entry != null && entry.getValue().offset <= flushedOffset) {
+      pendingCommits.remove(entry.getKey());
+      CommitCtx commit = entry.getValue();
+
+      COMMIT3Response response = new COMMIT3Response(status, wccData,
+          Nfs3Constant.WRITE_COMMIT_VERF);
+      Nfs3Utils.writeChannelCommit(commit.getChannel(), response
+          .writeHeaderAndResponse(new XDR(), commit.getXid(),
+              new VerifierNone()), commit.getXid());
+      
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("FileId: " + latestAttr.getFileid() + " Service time:"
+            + (System.currentTimeMillis() - commit.getStartTime())
+            + "ms. Sent response for commit:" + commit);
+      }
+      entry = pendingCommits.firstEntry();
+    }
+  }
+  
-    assert(ctxLock.isLocked());
-    byte[] data = null;
-    try {
-      data = writeCtx.getData();
-    } catch (IOException e1) {
-      LOG.error("Failed to get request data offset:" + offset + " count:"
-          + count + " error:" + e1);
-      // Cleanup everything
-      cleanup();
-      return;
-    }
-    assert (data.length == count);
-
+    
-    LOG.info("do write, fileId: " + handle.getFileId() + " offset: " + offset
-        + " length:" + count + " stableHow:" + stableHow.getValue());
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("do write, fileId: " + handle.getFileId() + " offset: "
+          + offset + " length:" + count + " stableHow:" + stableHow.getValue());
+    }
-      fos.write(data, 0, count);
+      // The write is not protected by lock. asyncState is used to make sure
+      // there is one thread doing write back at any time    
+      writeCtx.writeData(fos);
-      nextOffset = flushedOffset;
+      
-      if (writeCtx.getDataState() == DataState.ALLOW_DUMP) {
-        updateNonSequentialWriteInMemory(-count);
+      if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {
+        synchronized (writeCtx) {
+          if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {
+            writeCtx.setDataState(WriteCtx.DataState.NO_DUMP);
+            updateNonSequentialWriteInMemory(-count);
+            if (LOG.isDebugEnabled()) {
+              LOG.debug("After writing " + handle.getFileId() + " at offset "
+                  + offset + ", updated the memory count, new value:"
+                  + nonSequentialWriteInMemory.get());
+            }
+          }
+        }
+        if (writeCtx.getOriginalCount() != WriteCtx.INVALID_ORIGINAL_COUNT) {
+          LOG.warn("Return original count:" + writeCtx.getOriginalCount()
+              + " instead of real data count:" + count);
+          count = writeCtx.getOriginalCount();
+        }
-        Nfs3Utils.writeChannel(channel, response.send(new XDR(), xid));
+        Nfs3Utils.writeChannel(channel, response.writeHeaderAndResponse(
+            new XDR(), xid, new VerifierNone()), xid);
-
+      
+      // Handle the waiting commits without holding any lock
+      processCommits(writeCtx.getOffset() + writeCtx.getCount());
+     
-          + offset + " and length " + data.length, e);
+          + offset + " and length " + count, e);
-        Nfs3Utils.writeChannel(channel, response.send(new XDR(), xid));
+        Nfs3Utils.writeChannel(channel, response.writeHeaderAndResponse(
+            new XDR(), xid, new VerifierNone()), xid);
-  private void cleanup() {
-    assert(ctxLock.isLocked());
+  private synchronized void cleanup() {
+    if (!activeState) {
+      LOG.info("Current OpenFileCtx is already inactive, no need to cleanup.");
+      return;
+    }
+
+    // stop the dump thread
+    if (dumpThread != null) {
+      dumpThread.interrupt();
+      try {
+        dumpThread.join(3000);
+      } catch (InterruptedException e) {
+      }
+    }
-          + "), nextOffset=" + getNextOffsetUnprotected());
+          + "), nextOffset=" + nextOffset.get());
-        Nfs3Utils.writeChannel(writeCtx.getChannel(),
-            response.send(new XDR(), writeCtx.getXid()));
+        Nfs3Utils.writeChannel(writeCtx.getChannel(), response
+            .writeHeaderAndResponse(new XDR(), writeCtx.getXid(),
+                new VerifierNone()), writeCtx.getXid());
-    if (dumpOut!=null){
+    if (dumpOut != null) {
+      File dumpFile = new File(dumpFilePath);
+      if (dumpFile.exists() && !dumpFile.delete()) {
+        LOG.error("Failed to delete dumpfile: " + dumpFile);
+      }
-    if (raf!=null) {
+    if (raf != null) {
-    File dumpFile = new File(dumpFilePath);
-    if (dumpFile.delete()) {
-      LOG.error("Failed to delete dumpfile: "+ dumpFile);
-    }
+  }
+  
+  @VisibleForTesting
+  ConcurrentNavigableMap<OffsetRange, WriteCtx> getPendingWritesForTest(){
+    return pendingWrites;
+  }
+  
+  @VisibleForTesting
+  ConcurrentNavigableMap<Long, CommitCtx> getPendingCommitsForTest(){
+    return pendingCommits;
+  }
+  
+  @VisibleForTesting
+  long getNextOffsetForTest() {
+    return nextOffset.get();
+  }
+  
+  @VisibleForTesting
+  void setNextOffsetForTest(long newValue) {
+    nextOffset.set(newValue);
+  }
+  
+  @VisibleForTesting
+  void setActiveStatusForTest(boolean activeState) {
+    this.activeState = activeState;

INS26 INS26 INS26 INS26 INS26 INS26 INS26 INS26 MOV23 MOV23 INS40 INS40 UPD40 UPD40 INS40 UPD40 INS40 INS40 INS40 INS40 INS40 INS71 INS23 INS23 INS23 INS55 INS23 INS23 INS31 INS31 INS31 INS31 INS55 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS31 UPD39 INS83 INS42 INS72 INS72 INS72 INS72 INS72 INS72 UPD43 INS83 INS83 INS43 INS59 INS83 INS83 INS29 INS83 INS43 MOV59 UPD74 INS83 INS83 INS74 INS59 INS83 INS42 MOV23 INS23 INS23 INS23 MOV23 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS83 INS83 INS43 MOV59 INS83 INS43 INS59 INS83 MOV39 INS42 INS8 INS83 INS39 MOV42 MOV44 INS8 INS44 INS44 MOV21 INS83 MOV43 INS42 INS8 INS83 INS39 INS42 INS8 INS42 INS43 MOV31 MOV31 MOV8 INS78 UPD83 MOV83 INS83 MOV39 UPD42 MOV42 INS44 INS44 INS8 INS29 MOV83 INS83 INS43 INS42 INS44 INS44 INS44 INS8 INS29 INS83 INS39 INS42 MOV44 MOV44 MOV44 MOV44 MOV44 MOV8 INS29 INS83 INS83 INS39 INS42 MOV44 INS44 INS8 INS83 INS39 INS42 INS44 INS44 INS44 INS44 INS44 INS44 INS8 INS83 INS43 INS42 INS44 MOV44 INS44 INS44 INS44 MOV8 INS29 MOV83 INS83 INS43 INS42 MOV44 INS44 INS44 INS44 INS8 INS83 MOV8 INS29 INS83 INS83 INS43 INS42 INS8 INS29 INS39 INS42 INS8 INS83 INS39 INS42 INS44 INS8 INS83 INS78 INS74 INS42 INS8 INS78 INS74 INS42 INS8 INS78 INS39 INS42 INS8 INS78 INS39 INS42 INS44 INS8 INS78 INS39 INS42 INS44 INS8 INS42 INS42 INS42 INS42 INS42 INS42 UPD42 UPD42 INS42 INS42 INS65 INS42 UPD43 INS43 INS43 INS43 INS42 INS83 INS59 INS83 INS83 INS43 INS59 INS83 INS83 INS39 INS59 INS83 INS83 MOV43 INS59 INS83 INS59 INS39 INS42 INS8 INS43 INS42 INS8 INS39 INS42 INS8 MOV43 INS42 INS8 INS39 INS42 INS8 INS42 INS44 INS44 INS44 INS44 INS8 INS78 INS83 MOV43 INS42 INS8 INS42 INS42 INS42 INS41 INS60 MOV25 MOV21 INS41 INS43 INS42 INS43 INS42 INS21 INS21 INS21 INS21 INS41 MOV25 INS25 INS51 INS42 INS29 UPD42 INS78 UPD83 UPD42 INS42 INS43 INS42 INS39 INS42 INS60 INS60 INS60 INS25 MOV60 INS21 MOV21 INS21 MOV21 MOV21 INS65 INS65 INS42 INS43 INS42 INS43 INS42 INS39 INS42 MOV60 INS60 INS60 INS60 MOV25 INS25 INS25 INS65 MOV60 MOV60 MOV60 INS60 INS65 INS65 INS43 INS42 INS25 INS43 INS42 INS43 INS42 INS43 INS42 INS39 INS42 INS43 INS42 INS43 INS42 INS60 MOV60 INS60 INS60 INS25 INS54 INS42 INS43 INS42 INS43 INS42 INS39 INS42 MOV43 INS42 MOV21 INS21 INS25 INS25 MOV41 INS65 INS42 INS43 INS42 INS39 INS42 MOV43 INS42 INS25 MOV60 INS25 INS25 INS60 INS25 MOV25 MOV60 MOV41 INS65 INS65 INS42 MOV25 INS60 INS60 INS60 INS25 INS60 MOV25 INS41 INS65 MOV21 INS54 INS39 INS42 INS21 INS60 INS60 INS25 INS60 INS54 INS54 INS25 INS60 INS61 INS25 INS25 INS25 INS42 INS43 INS43 INS43 INS41 INS42 INS43 INS43 INS43 INS41 INS42 INS41 INS42 INS39 INS42 INS21 INS42 INS39 INS42 INS21 INS66 INS66 UPD42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS41 INS42 INS41 INS41 INS41 INS41 INS39 INS42 INS43 INS42 INS39 INS42 MOV43 INS42 INS21 INS21 INS21 INS21 INS21 INS42 INS41 INS32 INS39 INS59 INS42 INS42 INS42 INS7 INS32 INS7 INS7 INS7 INS42 MOV27 MOV8 INS52 INS8 INS65 INS42 INS61 INS42 INS39 INS59 INS39 INS59 INS39 INS59 MOV32 INS8 UPD43 INS32 INS32 INS32 INS66 INS66 INS66 INS65 INS66 INS66 INS42 INS42 INS39 INS59 INS39 INS59 INS39 INS59 INS32 INS27 INS8 INS27 INS8 INS8 INS66 MOV43 INS39 INS59 INS27 INS66 INS66 INS66 INS66 INS66 UPD42 MOV42 MOV27 INS8 INS8 UPD42 MOV42 UPD42 MOV42 INS42 UPD42 MOV42 INS42 INS43 INS59 INS39 INS59 MOV43 INS59 INS27 INS8 INS8 INS8 INS12 INS12 INS42 INS42 INS32 INS43 INS32 INS8 INS27 INS8 INS66 INS66 INS42 MOV38 INS8 INS32 INS8 UPD27 MOV27 INS8 INS74 INS59 INS32 INS8 INS8 INS66 INS65 INS66 INS66 INS65 INS66 INS66 INS32 INS74 INS59 MOV43 INS59 MOV43 INS59 MOV32 MOV8 INS39 INS59 INS33 INS66 MOV8 MOV8 INS32 INS39 INS59 INS74 INS59 INS27 INS8 INS39 INS59 INS8 INS12 INS12 INS8 INS12 INS27 INS8 MOV43 INS59 INS27 INS8 INS32 INS8 MOV21 INS38 INS8 INS27 INS8 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS32 INS32 INS7 INS42 INS42 INS42 INS42 INS42 INS42 INS7 INS7 INS7 INS7 INS7 INS32 INS42 INS42 INS42 INS32 UPD42 UPD42 INS27 INS42 INS14 INS14 UPD42 INS42 INS42 MOV32 INS42 INS33 INS22 INS42 INS22 INS42 INS32 INS42 INS25 INS66 INS32 INS27 INS32 INS27 INS8 MOV21 INS42 INS32 INS42 INS32 INS42 INS27 UPD42 INS21 UPD42 UPD42 MOV32 INS42 INS42 INS27 INS45 UPD42 UPD42 INS11 INS42 INS42 INS27 INS45 INS42 INS42 INS42 UPD42 INS42 INS11 INS42 INS42 INS32 INS42 INS32 INS42 INS32 INS42 INS40 INS42 INS42 INS36 INS36 INS21 INS25 INS21 INS21 INS21 INS21 MOV21 INS42 INS42 MOV21 INS41 INS60 MOV60 INS25 MOV25 MOV60 INS25 INS41 INS42 INS32 INS27 INS42 INS42 INS32 MOV25 INS41 INS41 UPD42 MOV42 INS42 INS32 INS42 INS32 INS42 INS32 INS42 INS33 INS21 INS60 INS25 INS21 INS44 INS8 INS44 INS8 INS42 INS42 INS27 INS42 INS32 INS42 INS42 MOV21 INS27 INS27 MOV54 INS25 INS42 INS42 MOV21 INS25 INS43 INS43 INS43 INS42 INS32 INS42 INS42 INS41 MOV60 INS21 MOV60 MOV21 INS41 INS25 INS67 INS67 INS42 INS42 INS21 INS21 INS41 INS43 INS43 INS43 INS42 INS32 INS42 INS32 INS42 INS32 INS42 INS32 UPD42 INS21 INS21 UPD42 INS42 INS42 INS45 INS61 INS42 INS42 INS27 INS42 INS32 INS43 INS43 INS43 INS42 INS32 INS27 INS27 INS41 INS42 INS40 INS21 INS21 INS44 INS8 INS44 INS8 INS21 INS44 INS8 INS32 INS42 INS21 INS21 INS42 INS14 INS27 INS27 INS21 INS60 INS60 INS21 INS25 INS21 UPD42 MOV42 UPD42 MOV42 MOV21 INS21 INS25 INS42 INS21 INS41 INS42 INS33 INS21 INS54 MOV60 INS25 INS42 INS42 INS42 INS42 INS42 INS22 INS42 INS22 INS42 INS22 INS42 INS22 INS42 INS22 INS42 INS22 INS32 INS42 INS42 INS45 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS34 UPD74 INS40 INS74 INS43 INS34 UPD43 INS32 INS52 INS42 INS52 INS42 INS42 INS42 INS27 INS8 INS42 INS42 INS42 MOV32 INS27 INS25 INS42 INS42 INS42 INS42 INS54 UPD42 MOV42 UPD42 MOV42 INS42 INS42 INS27 INS42 INS32 INS32 INS34 INS39 INS36 INS27 INS42 INS39 INS42 INS42 INS42 INS42 INS42 UPD42 MOV42 INS42 INS27 INS27 INS32 INS38 INS8 INS32 INS32 INS7 INS7 INS7 INS33 INS43 INS59 MOV43 INS32 INS8 MOV43 INS27 INS8 INS8 INS42 MOV43 INS32 INS42 INS42 INS42 INS42 UPD42 INS14 INS42 INS42 UPD42 UPD42 INS8 INS9 INS9 INS42 INS42 INS42 UPD42 MOV42 INS42 INS42 MOV42 INS42 INS32 INS39 INS59 INS38 INS8 INS32 INS43 INS42 INS21 INS43 INS42 INS21 MOV41 INS41 INS42 INS34 INS42 INS42 INS42 INS42 INS42 INS32 INS42 INS40 INS42 INS40 INS8 INS12 MOV12 INS32 INS8 INS8 INS27 INS8 INS8 INS42 INS42 INS42 UPD42 MOV42 INS42 INS40 INS32 INS43 INS40 INS32 INS8 INS42 INS42 INS32 INS32 INS7 INS33 UPD42 MOV42 INS42 INS42 UPD42 MOV42 INS42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 INS42 INS42 UPD42 INS32 INS7 INS21 INS21 INS25 INS41 INS42 INS8 INS27 INS42 INS34 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS33 INS22 INS42 INS32 INS7 INS43 INS42 INS25 INS21 INS43 INS42 INS21 INS21 INS7 INS43 INS42 INS21 INS21 INS42 INS42 INS32 INS7 MOV43 INS32 INS42 INS42 INS33 INS22 INS42 INS32 INS43 INS59 INS43 INS59 INS32 INS32 INS8 INS7 INS32 INS27 INS8 INS32 INS32 INS32 INS8 INS12 INS27 MOV8 INS52 INS42 INS52 INS42 INS52 INS42 INS52 INS42 INS52 INS42 INS52 INS42 INS42 INS42 UPD43 INS43 INS43 INS43 INS42 UPD42 INS42 INS42 INS32 INS42 INS25 INS25 INS32 INS34 INS27 INS8 INS8 INS12 INS42 INS42 INS42 INS42 INS42 INS32 UPD42 MOV42 UPD42 MOV42 INS27 INS32 INS32 INS42 INS42 INS27 INS42 INS42 INS42 INS32 INS32 INS21 INS41 INS42 INS42 INS45 INS42 INS42 INS42 INS42 INS42 INS42 MOV32 INS42 MOV32 INS42 INS42 INS16 INS42 INS42 MOV21 UPD40 MOV21 UPD42 INS32 INS42 INS33 MOV21 MOV21 INS42 INS42 INS45 INS43 INS25 INS25 UPD42 MOV42 UPD42 MOV42 INS42 INS42 MOV42 INS42 INS42 INS32 INS42 INS21 INS25 INS25 MOV60 MOV60 MOV21 MOV21 INS42 INS42 MOV32 INS42 INS32 INS42 INS32 MOV14 INS14 INS42 INS42 INS27 MOV21 INS44 INS8 INS42 INS42 INS41 INS41 UPD42 INS42 INS42 INS60 MOV21 INS41 INS41 UPD42 INS27 INS42 INS42 INS27 INS42 UPD42 INS14 INS42 INS42 INS42 UPD42 INS42 INS42 MOV21 INS42 INS42 INS42 INS32 INS22 INS9 INS42 INS32 INS22 INS9 UPD42 UPD42 INS32 INS32 INS32 INS32 INS32 INS32 INS8 INS42 INS60 INS25 MOV38 INS32 INS32 INS42 INS42 INS42 INS32 INS42 INS40 INS42 INS38 INS8 INS7 INS42 INS32 INS7 INS42 INS32 INS42 INS32 INS7 INS42 INS42 INS27 INS42 INS40 INS42 INS42 INS42 INS32 INS42 INS42 INS42 INS32 INS42 INS42 INS32 INS42 INS42 INS14 INS42 INS42 INS32 INS32 INS32 INS42 INS42 INS21 INS42 INS32 UPD42 INS42 INS42 INS42 INS32 INS40 INS51 INS25 INS42 INS27 UPD42 MOV42 UPD42 MOV42 INS45 UPD42 MOV42 UPD42 MOV42 MOV21 INS44 INS8 MOV32 INS38 INS21 UPD42 UPD42 INS42 INS42 INS42 INS42 INS42 MOV32 INS8 INS27 MOV8 INS8 INS51 MOV44 INS42 INS42 INS42 INS33 INS18 MOV44 UPD42 INS25 INS51 INS25 INS44 INS8 UPD42 INS14 INS14 UPD42 MOV42 INS42 INS27 INS42 INS36 UPD42 MOV42 INS42 INS36 INS42 INS36 UPD42 MOV42 INS42 INS42 INS42 UPD42 MOV42 INS42 UPD45 INS42 INS42 INS42 INS42 INS42 INS27 INS42 INS36 UPD42 MOV42 INS42 INS36 INS42 INS36 INS42 INS42 INS32 INS33 INS27 INS40 INS40 MOV43 INS42 MOV32 INS42 UPD42 MOV42 MOV42 INS42 INS42 MOV43 INS40 INS42 INS32 INS8 MOV32 INS8 UPD42 MOV42 UPD42 MOV42 INS42 INS32 INS27 INS8 MOV32 INS8 INS43 INS42 INS42 UPD42 MOV42 UPD42 MOV42 INS27 INS42 INS42 INS27 MOV14 INS43 INS40 INS42 INS34 INS42 INS40 MOV43 INS40 MOV43 INS40 INS45 INS32 INS43 INS42 MOV25 INS40 INS40 INS43 INS59 INS40 INS40 INS32 INS34 INS42 INS34 INS43 INS42 INS42 INS42 INS42 INS42 INS42 INS52 INS42 UPD45 INS42 INS42 INS52 INS42 UPD42 UPD42 UPD42 INS42 UPD42 MOV42 UPD42 MOV42 INS42 INS32 INS42 INS42 MOV42 MOV42 UPD42 MOV42 INS42 INS42 INS32 INS42 INS42 MOV21 INS43 INS59 INS27 INS8 INS8 MOV42 MOV42 INS42 INS42 INS42 INS42 INS40 INS32 INS21 INS42 INS40 INS42 INS42 INS27 INS42 INS40 INS42 INS42 INS42 INS32 INS42 INS42 INS42 INS27 INS42 INS40 INS45 INS42 INS45 INS32 INS42 INS42 INS42 INS42 INS42 INS42 INS43 INS42 INS42 INS40 INS42 INS42 INS42 INS42 MOV14 INS32 INS14 INS42 INS42 INS32 INS42 INS42 INS42 INS42 INS42 INS8 INS27 INS8 INS32 INS32 INS43 INS42 MOV32 INS32 INS21 INS42 INS33 INS21 INS21 INS21 INS52 INS8 MOV21 INS25 UPD45 MOV21 UPD45 INS32 INS27 INS8 INS52 INS8 MOV32 INS8 INS43 INS42 MOV21 INS43 INS43 INS45 INS45 INS45 INS45 INS27 INS27 UPD27 MOV27 INS45 INS45 INS45 INS45 INS27 INS27 INS27 INS42 INS42 INS45 MOV42 UPD42 MOV42 UPD42 UPD42 INS42 INS27 MOV42 INS42 INS42 INS21 INS42 INS42 INS21 INS42 INS42 INS40 MOV21 INS21 INS21 INS42 INS42 INS45 INS45 INS45 INS42 INS45 INS42 INS42 UPD42 MOV42 INS42 INS42 MOV32 INS42 INS42 INS14 UPD42 UPD42 UPD42 MOV42 UPD42 MOV42 INS32 INS42 INS42 UPD42 UPD42 UPD42 UPD42 INS45 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 INS32 INS42 INS42 INS32 INS42 INS33 INS21 MOV21 MOV10 INS42 INS42 INS32 INS45 INS42 INS42 INS42 INS32 INS45 INS32 INS42 INS42 INS42 INS42 INS42 INS43 INS42 INS42 INS27 INS25 INS32 INS40 INS21 INS21 INS42 INS42 INS42 INS42 INS42 INS42 UPD42 UPD42 INS34 INS42 UPD42 INS42 INS32 INS42 INS42 INS42 INS42 MOV27 INS32 INS7 INS32 INS32 INS21 MOV21 INS27 INS8 UPD42 UPD42 INS42 INS42 INS32 INS42 INS21 INS42 INS25 MOV21 INS42 INS42 INS42 INS27 INS34 INS42 INS34 INS34 INS27 INS34 INS42 INS34 INS27 INS34 UPD45 INS42 UPD45 UPD42 INS45 INS32 INS32 INS32 INS7 INS32 INS43 UPD42 UPD42 MOV14 INS14 INS43 INS40 INS40 INS43 INS42 INS42 INS42 INS42 INS42 INS42 UPD45 UPD42 UPD42 UPD42 INS42 INS42 INS27 INS42 INS32 INS42 INS42 INS27 INS42 INS42 INS42 INS42 INS42 INS45 INS32 INS45 INS36 INS45 INS42 INS27 INS8 INS42 INS42 INS32 INS7 UPD42 INS14 INS14 INS32 MOV42 UPD42 MOV14 INS14 INS42 INS42 INS42 INS42 INS45 INS42 INS14 INS42 INS42 INS52 INS42 INS32 INS27 INS42 INS42 INS33 INS54 INS42 INS42 INS32 INS27 INS8 UPD42 UPD42 INS27 INS42 INS42 INS42 INS42 INS42 INS42 INS42 MOV42 INS42 INS42 INS42 INS27 INS42 INS42 INS45 INS42 INS40 INS42 INS42 INS27 INS42 INS43 INS42 INS42 INS45 INS32 INS42 INS42 INS45 INS32 INS45 INS42 INS42 INS27 INS32 INS40 INS21 MOV21 INS25 INS42 INS42 INS27 INS42 INS32 INS43 INS43 MOV42 INS42 MOV14 UPD42 MOV42 MOV14 INS43 UPD43 MOV43 INS14 UPD42 MOV42 UPD42 MOV42 INS32 INS45 INS42 MOV45 MOV42 INS8 INS12 INS45 INS42 INS42 INS32 INS42 INS54 UPD42 INS45 INS42 INS45 INS22 UPD42 MOV14 INS14 INS45 INS32 UPD45 MOV32 INS45 MOV32 INS42 INS40 INS40 UPD42 UPD42 UPD42 MOV42 INS42 INS42 INS42 INS32 INS32 INS42 INS42 INS32 INS32 INS8 INS45 INS32 INS45 INS42 INS42 INS42 INS42 INS42 UPD43 INS42 UPD42 INS43 INS42 INS42 INS21 MOV44 INS8 INS42 INS42 INS8 INS12 UPD45 UPD42 INS45 INS42 INS52 INS42 INS43 INS42 INS42 UPD42 UPD42 INS42 INS42 INS42 INS42 INS42 INS42 INS40 INS42 INS42 MOV21 INS42 INS42 UPD42 INS42 INS32 MOV21 INS21 INS25 INS44 INS8 INS42 INS42 INS42 INS42 INS32 INS32 INS8 INS43 INS42 INS21 UPD42 INS42 INS27 UPD42 UPD42 INS27 UPD42 INS52 INS42 UPD42 MOV42 UPD42 MOV42 INS21 UPD42 MOV42 INS32 INS45 INS32 INS45 INS42 INS45 INS32 INS45 INS42 INS42 INS32 INS42 INS42 INS27 UPD42 MOV42 UPD42 MOV42 INS42 UPD42 MOV42 UPD42 MOV42 INS42 INS45 INS45 INS22 INS52 INS42 INS42 DEL66 DEL66 DEL65 DEL29 DEL83 DEL83 DEL39 DEL42 DEL44 DEL42 DEL42 DEL7 DEL21 DEL25 DEL42 DEL41 DEL8 DEL31 DEL34 DEL9 DEL42 DEL7 DEL42 DEL32 DEL21 DEL42 DEL43 DEL85 DEL5 DEL42 DEL42 DEL32 DEL32 DEL59 DEL60 DEL42 DEL43 DEL42 DEL32 DEL59 DEL60 DEL8 DEL25 DEL8 DEL31 DEL83 DEL42 DEL42 DEL59 DEL60 DEL32 DEL21 DEL8 DEL8 DEL54 DEL42 DEL41 DEL8 DEL31 DEL39 DEL42 DEL42 DEL42 DEL32 DEL36 DEL6 DEL42 DEL41 DEL8 DEL31 DEL83 DEL39 DEL42 DEL42 DEL32 DEL21 DEL42 DEL42 DEL32 DEL7 DEL21 DEL8 DEL32 DEL21 DEL8 DEL54 DEL42 DEL41 DEL8 DEL31 DEL42 DEL34 DEL45 DEL42 DEL27 DEL14 DEL53 DEL42 DEL42 DEL32 DEL36 DEL6 DEL42 DEL42 DEL27 DEL25 DEL45 DEL42 DEL27 DEL32 DEL21 DEL42 DEL43 DEL45 DEL42 DEL27 DEL14 DEL53 DEL8 DEL25 DEL45 DEL42 DEL27 DEL42 DEL42 DEL34 DEL27 DEL42 DEL42 DEL43 DEL45 DEL42 DEL27 DEL14 DEL53 DEL42 DEL42 DEL32 DEL42 DEL43 DEL85 DEL5 DEL42 DEL32 DEL32 DEL59 DEL60 DEL43 DEL42 DEL42 DEL34 DEL2 DEL59 DEL60 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL8 DEL25 DEL42 DEL32 DEL21 DEL42 DEL32 DEL21 DEL8 DEL54 DEL8 DEL42 DEL42 DEL34 DEL2 DEL45 DEL27 DEL42 DEL42 DEL14 DEL7 DEL42 DEL42 DEL42 DEL32 DEL42 DEL32 DEL42 DEL32 DEL40 DEL42 DEL32 DEL42 DEL32 DEL42 DEL32 DEL42 DEL42 DEL9 DEL40 DEL14 DEL42 DEL45 DEL45 DEL27 DEL34 DEL40 DEL42 DEL83 DEL39 DEL42 DEL39 DEL42 DEL32 DEL59 DEL60 DEL27 DEL42 DEL32 DEL21 DEL42 DEL43 DEL42 DEL14 DEL59 DEL60 DEL43 DEL42 DEL43 DEL42 DEL42 DEL14 DEL59 DEL60 DEL43 DEL42 DEL43 DEL40 DEL42 DEL42 DEL42 DEL40 DEL14 DEL59 DEL60 DEL42 DEL32 DEL32 DEL21 DEL9 DEL32 DEL21 DEL8 DEL25 DEL8 DEL42 DEL42 DEL27 DEL42 DEL42 DEL14 DEL59 DEL60 DEL40 DEL27 DEL8 DEL25 DEL8 DEL25 DEL25 DEL8 DEL31 DEL32 DEL36 DEL6 DEL42 DEL7 DEL42 DEL7 DEL34 DEL34 DEL34 DEL39 DEL42 DEL32 DEL42 DEL42 DEL42 DEL32 DEL42 DEL8 DEL42 DEL32 DEL21 DEL8 DEL42 DEL41 DEL83 DEL83 DEL83 DEL39 DEL42 DEL34 DEL59 DEL23 DEL83 DEL83 DEL83 DEL39 DEL42 DEL34 DEL59 DEL23 DEL83 DEL83 DEL83 DEL39 DEL42 DEL34 DEL59 DEL23 DEL83 DEL83 DEL83 DEL39 DEL42 DEL34 DEL59 DEL23 DEL66 DEL66 DEL65 DEL29 DEL83 DEL39 DEL42 DEL31 DEL39 DEL42 DEL42 DEL42 DEL32 DEL7 DEL21 DEL8 DEL25 DEL42 DEL42 DEL27 DEL42 DEL41 DEL8 DEL25 DEL39 DEL42 DEL42 DEL59 DEL60 DEL42 DEL42 DEL7 DEL21 DEL8 DEL54 DEL32 DEL21 DEL8 DEL31 DEL42 DEL42 DEL32 DEL36 DEL6 DEL42 DEL42 DEL32 DEL38 DEL42 DEL42 DEL32 DEL42 DEL42 DEL45 DEL42 DEL42 DEL32 DEL27 DEL32 DEL21 DEL8 DEL25 DEL42 DEL41 DEL8 DEL25 DEL42 DEL32 DEL21 DEL8 DEL54 DEL8 DEL10 DEL10 DEL42 DEL43 DEL45 DEL32 DEL45 DEL32 DEL45 DEL42 DEL27 DEL14 DEL53 DEL42 DEL32 DEL7 DEL32 DEL32 DEL42 DEL42 DEL32 DEL42 DEL32 DEL21 DEL8 DEL25 DEL83 DEL39 DEL42 DEL42 DEL59 DEL60 DEL9 DEL42 DEL38 DEL42 DEL42 DEL45 DEL32 DEL21 DEL43 DEL45 DEL14 DEL53 DEL8 DEL25 DEL25 DEL42 DEL32 DEL7 DEL21 DEL42 DEL32 DEL7 DEL21 DEL32 DEL25 DEL8 DEL61 DEL8 DEL54 DEL8 DEL31 DEL32 DEL36 DEL6 DEL39 DEL85 DEL5 DEL42 DEL33 DEL59 DEL60 DEL42 DEL42 DEL42 DEL32 DEL7 DEL21 DEL8 DEL42 DEL42 DEL45 DEL42 DEL45 DEL42 DEL45 DEL42 DEL27 DEL32 DEL21 DEL42 DEL32 DEL21 DEL41 DEL8 DEL12 DEL54 DEL40 DEL42 DEL27 DEL36 DEL6 DEL34 DEL42 DEL42 DEL7 DEL40 DEL32 DEL32 DEL36 DEL6 DEL25