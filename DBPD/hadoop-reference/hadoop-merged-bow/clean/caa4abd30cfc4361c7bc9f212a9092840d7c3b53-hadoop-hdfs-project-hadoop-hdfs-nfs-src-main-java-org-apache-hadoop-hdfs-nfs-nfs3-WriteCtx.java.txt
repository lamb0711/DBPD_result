HDFS-5259. Support client which combines appended data with old data before sends it to NFS server. Contributed by Brandon Li

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529730 13f79535-47bb-0310-9956-ffa450edef68

+import java.nio.ByteBuffer;
+import org.apache.hadoop.hdfs.client.HdfsDataOutputStream;
+import com.google.common.annotations.VisibleForTesting;
+  
+  //Only needed for overlapped write, referring OpenFileCtx.addWritesToCache()  
+  private final int originalCount; 
+  public static final int INVALID_ORIGINAL_COUNT = -1;
+  
+  public int getOriginalCount() {
+    return originalCount;
+  }
+
-  private volatile byte[] data;
+  private volatile ByteBuffer data;
+
+    // Resized write should not allow dump
+    Preconditions.checkState(originalCount == INVALID_ORIGINAL_COUNT);
+
-    dumpOut.write(data, 0, count);
+    dumpOut.write(data.array(), 0, count);
-  byte[] getData() throws IOException {
+  @VisibleForTesting
+  ByteBuffer getData() throws IOException {
-    data = new byte[count];
+    byte[] rawData = new byte[count];
-    int size = raf.read(data, 0, count);
+    int size = raf.read(rawData, 0, count);
+    data = ByteBuffer.wrap(rawData);
+  public void writeData(HdfsDataOutputStream fos) throws IOException {
+    Preconditions.checkState(fos != null);
+
+    ByteBuffer dataBuffer = null;
+    try {
+      dataBuffer = getData();
+    } catch (Exception e1) {
+      LOG.error("Failed to get request data offset:" + offset + " count:"
+          + count + " error:" + e1);
+      throw new IOException("Can't get WriteCtx.data");
+    }
+
+    byte[] data = dataBuffer.array();
+    int position = dataBuffer.position();
+    int limit = dataBuffer.limit();
+    Preconditions.checkState(limit - position == count);
+    // Modified write has a valid original count
+    if (position != 0) {
+      if (limit != getOriginalCount()) {
+        throw new IOException("Modified write has differnt original size."
+            + "buff position:" + position + " buff limit:" + limit + ". "
+            + toString());
+      }
+    }
+    
+    // Now write data
+    fos.write(data, position, count);
+  }
+  
-  WriteCtx(FileHandle handle, long offset, int count, WriteStableHow stableHow,
-      byte[] data, Channel channel, int xid, boolean replied, DataState dataState) {
+  WriteCtx(FileHandle handle, long offset, int count, int originalCount,
+      WriteStableHow stableHow, ByteBuffer data, Channel channel, int xid,
+      boolean replied, DataState dataState) {
+    this.originalCount = originalCount;
-        + " stableHow:" + stableHow + " replied:" + replied + " dataState:"
-        + dataState + " xid:" + xid;
+        + " originalCount:" + originalCount + " stableHow:" + stableHow
+        + " replied:" + replied + " dataState:" + dataState + " xid:" + xid;

INS26 INS26 INS26 INS40 INS40 INS40 INS23 INS23 INS31 INS23 INS31 MOV83 INS83 INS39 INS59 INS83 INS83 INS83 INS39 INS59 INS83 INS39 INS42 INS8 INS83 MOV83 INS43 MOV59 INS78 INS43 INS83 INS39 INS42 INS44 INS43 INS8 INS44 INS44 INS42 INS42 INS38 INS41 INS42 INS21 INS42 INS42 INS60 INS21 INS43 INS42 INS42 INS21 INS60 INS54 INS60 INS60 INS60 INS21 INS25 INS21 INS39 INS42 INS43 MOV42 INS21 INS34 INS42 INS32 MOV5 INS59 INS7 INS42 INS32 INS43 INS59 INS8 INS12 MOV5 INS59 INS39 INS59 INS39 INS59 INS32 INS27 INS8 INS32 INS42 INS7 INS42 INS42 INS27 INS32 INS42 MOV3 INS42 INS32 INS42 INS42 INS27 INS42 INS42 INS33 INS21 INS44 INS8 INS42 INS32 INS42 INS32 INS42 INS32 INS42 INS42 INS27 INS42 INS34 INS25 INS42 INS42 INS42 INS42 INS42 INS22 INS42 INS45 INS42 INS42 INS42 MOV42 INS42 UPD42 INS42 INS42 INS42 INS42 INS33 INS7 INS43 INS42 INS21 INS53 INS42 INS42 INS42 INS42 INS42 INS42 INS27 INS42 INS27 INS8 INS52 INS42 INS42 INS32 INS42 INS32 INS14 INS42 INS42 INS42 INS32 INS53 INS42 INS42 INS42 INS27 INS43 INS45 INS42 INS14 INS45 INS42 INS45 INS42 INS45 INS42 INS42 INS43 INS27 INS42 INS27 INS42 INS45 INS42 INS45 INS32 INS45 INS45 INS42 DEL39 DEL85 DEL5 DEL23 DEL42 DEL7 DEL21 DEL44