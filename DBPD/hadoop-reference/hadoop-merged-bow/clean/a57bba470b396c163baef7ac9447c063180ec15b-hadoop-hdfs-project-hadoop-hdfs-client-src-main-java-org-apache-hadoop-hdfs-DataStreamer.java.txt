Merge branch 'trunk' into HADOOP-12756

+  private class RefetchEncryptionKeyPolicy {
+    private int fetchEncryptionKeyTimes = 0;
+    private InvalidEncryptionKeyException lastException;
+    private final DatanodeInfo src;
+
+    RefetchEncryptionKeyPolicy(DatanodeInfo src) {
+      this.src = src;
+    }
+    boolean continueRetryingOrThrow() throws InvalidEncryptionKeyException {
+      if (fetchEncryptionKeyTimes >= 2) {
+        // hit the same exception twice connecting to the node, so
+        // throw the exception and exclude the node.
+        throw lastException;
+      }
+      // Don't exclude this node just yet.
+      // Try again with a new encryption key.
+      LOG.info("Will fetch a new encryption key and retry, "
+          + "encryption key was invalid when connecting to "
+          + this.src + ": ", lastException);
+      // The encryption key used is invalid.
+      dfsClient.clearDataEncryptionKey();
+      return true;
+    }
+
+    /**
+     * Record a connection exception.
+     * @param e
+     * @throws InvalidEncryptionKeyException
+     */
+    void recordFailure(final InvalidEncryptionKeyException e)
+        throws InvalidEncryptionKeyException {
+      fetchEncryptionKeyTimes++;
+      lastException = e;
+    }
+  }
+
+  private class StreamerStreams implements java.io.Closeable {
+    private Socket sock = null;
+    private DataOutputStream out = null;
+    private DataInputStream in = null;
+
+    StreamerStreams(final DatanodeInfo src,
+        final long writeTimeout, final long readTimeout,
+        final Token<BlockTokenIdentifier> blockToken)
+        throws IOException {
+      sock = createSocketForPipeline(src, 2, dfsClient);
+
+      OutputStream unbufOut = NetUtils.getOutputStream(sock, writeTimeout);
+      InputStream unbufIn = NetUtils.getInputStream(sock, readTimeout);
+      IOStreamPair saslStreams = dfsClient.saslClient
+          .socketSend(sock, unbufOut, unbufIn, dfsClient, blockToken, src);
+      unbufOut = saslStreams.out;
+      unbufIn = saslStreams.in;
+      out = new DataOutputStream(new BufferedOutputStream(unbufOut,
+          DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));
+      in = new DataInputStream(unbufIn);
+    }
+
+    void sendTransferBlock(final DatanodeInfo[] targets,
+        final StorageType[] targetStorageTypes,
+        final Token<BlockTokenIdentifier> blockToken) throws IOException {
+      //send the TRANSFER_BLOCK request
+      new Sender(out)
+          .transferBlock(block, blockToken, dfsClient.clientName, targets,
+              targetStorageTypes);
+      out.flush();
+      //ack
+      BlockOpResponseProto transferResponse = BlockOpResponseProto
+          .parseFrom(PBHelperClient.vintPrefixed(in));
+      if (SUCCESS != transferResponse.getStatus()) {
+        throw new IOException("Failed to add a datanode. Response status: "
+            + transferResponse.getStatus());
+      }
+    }
+
+    @Override
+    public void close() throws IOException {
+      IOUtils.closeStream(in);
+      IOUtils.closeStream(out);
+      IOUtils.closeSocket(sock);
+    }
+  }
+
+  private long computeTransferWriteTimeout() {
+    return dfsClient.getDatanodeWriteTimeout(2);
+  }
+  private long computeTransferReadTimeout() {
+    // transfer timeout multiplier based on the transfer size
+    // One per 200 packets = 12.8MB. Minimum is 2.
+    int multi = 2
+        + (int) (bytesSent / dfsClient.getConf().getWritePacketSize()) / 200;
+    return dfsClient.getDatanodeReadTimeout(multi);
+  }
+
-    Socket sock = null;
-    DataOutputStream out = null;
-    DataInputStream in = null;
-    try {
-      sock = createSocketForPipeline(src, 2, dfsClient);
-      final long writeTimeout = dfsClient.getDatanodeWriteTimeout(2);
+    RefetchEncryptionKeyPolicy policy = new RefetchEncryptionKeyPolicy(src);
+    do {
+      StreamerStreams streams = null;
+      try {
+        final long writeTimeout = computeTransferWriteTimeout();
+        final long readTimeout = computeTransferReadTimeout();
-      // transfer timeout multiplier based on the transfer size
-      // One per 200 packets = 12.8MB. Minimum is 2.
-      int multi = 2 + (int)(bytesSent /dfsClient.getConf().getWritePacketSize())
-          / 200;
-      final long readTimeout = dfsClient.getDatanodeReadTimeout(multi);
-
-      OutputStream unbufOut = NetUtils.getOutputStream(sock, writeTimeout);
-      InputStream unbufIn = NetUtils.getInputStream(sock, readTimeout);
-      IOStreamPair saslStreams = dfsClient.saslClient.socketSend(sock,
-          unbufOut, unbufIn, dfsClient, blockToken, src);
-      unbufOut = saslStreams.out;
-      unbufIn = saslStreams.in;
-      out = new DataOutputStream(new BufferedOutputStream(unbufOut,
-          DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));
-      in = new DataInputStream(unbufIn);
-
-      //send the TRANSFER_BLOCK request
-      new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,
-          targets, targetStorageTypes);
-      out.flush();
-
-      //ack
-      BlockOpResponseProto response =
-          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));
-      if (SUCCESS != response.getStatus()) {
-        throw new IOException("Failed to add a datanode");
+        streams = new StreamerStreams(src, writeTimeout, readTimeout,
+            blockToken);
+        streams.sendTransferBlock(targets, targetStorageTypes, blockToken);
+        return;
+      } catch (InvalidEncryptionKeyException e) {
+        policy.recordFailure(e);
+      } finally {
+        IOUtils.closeStream(streams);
-    } finally {
-      IOUtils.closeStream(in);
-      IOUtils.closeStream(out);
-      IOUtils.closeSocket(sock);
-    }
+    } while (policy.continueRetryingOrThrow());

INS55 INS55 INS31 INS31 INS31 INS83 INS42 INS23 INS23 INS23 INS31 INS31 INS31 INS83 INS42 INS43 INS23 INS23 INS23 MOV31 INS31 INS31 INS83 INS39 INS42 INS8 INS83 INS39 INS42 INS8 INS83 INS39 INS42 INS44 INS44 INS44 INS44 INS43 INS8 INS83 INS39 INS59 INS83 INS43 INS59 INS83 INS83 INS43 INS59 INS42 INS44 INS8 INS39 INS42 INS43 INS8 INS29 INS39 INS42 INS44 INS43 INS8 INS40 INS83 MOV43 MOV59 INS83 MOV43 MOV59 INS83 MOV43 MOV59 INS42 INS44 INS44 MOV8 INS39 INS42 MOV44 MOV44 INS44 INS43 INS8 INS78 INS83 INS39 INS42 INS43 MOV8 INS41 MOV60 INS41 INS83 INS43 INS42 INS83 INS5 INS42 INS83 INS5 INS42 INS83 INS74 INS42 INS42 INS60 INS19 INS42 INS34 INS42 INS42 INS42 INS42 INS43 INS42 INS21 INS42 INS25 INS21 INS21 INS41 INS65 INS65 INS65 INS83 INS43 INS42 INS42 INS21 INS21 INS83 INS39 INS42 INS83 INS39 INS42 INS83 INS74 INS42 INS42 MOV21 MOV21 MOV60 INS25 INS42 INS42 MOV32 MOV32 INS42 INS43 INS85 INS43 INS85 INS43 INS43 INS43 INS59 INS8 INS32 INS42 INS7 INS27 INS8 INS32 INS32 INS9 INS66 INS42 INS42 INS42 INS37 INS7 INS43 INS43 INS27 INS8 INS42 INS42 INS42 INS42 INS42 INS42 INS14 INS60 INS54 INS42 INS42 INS22 INS42 INS42 INS34 INS53 INS42 INS42 INS27 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 UPD42 INS42 INS32 INS53 INS43 INS42 INS43 INS59 INS8 INS12 INS8 INS52 INS42 INS42 INS27 INS22 INS45 INS42 INS42 INS14 INS42 INS42 INS42 INS33 INS60 INS60 INS21 INS21 INS41 INS44 INS8 INS21 INS45 INS45 INS52 INS42 MOV43 INS27 INS83 INS39 INS59 INS83 INS39 INS59 INS7 INS32 INS43 INS42 INS21 INS32 INS45 INS32 INS42 INS32 INS42 INS32 INS42 INS14 INS42 INS42 INS42 INS42 INS42 INS42 INS32 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS43 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 DEL83 DEL39 DEL42 DEL83 DEL39 DEL42 DEL59 DEL60 DEL83 DEL39 DEL42 DEL59 DEL60 DEL42 DEL42 DEL42 DEL32 DEL27 DEL45 DEL14 DEL53 DEL8 DEL25 DEL60 DEL60 DEL60 DEL54 DEL8