HDFS-14258. Introduce Java Concurrent Package To DataXceiverServer Class. Contributed by BELUGA BEHR.

-import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import org.apache.commons.io.IOUtils;
-import org.apache.hadoop.io.IOUtils;
+import com.google.common.base.Preconditions;
- * Server used for receiving/sending a block of data.
- * This is created to listen for requests from clients or 
- * other DataNodes.  This small server does not use the 
- * Hadoop IPC mechanism.
+ * Server used for receiving/sending a block of data. This is created to listen
+ * for requests from clients or other DataNodes. This small server does not use
+ * the Hadoop IPC mechanism.
-  
+
+  /**
+   * Default time to wait (in seconds) for the number of running threads to drop
+   * below the newly requested maximum before giving up.
+   */
+  private static final int DEFAULT_RECONFIGURE_WAIT = 30;
+
-  private final HashMap<Peer, Thread> peers = new HashMap<Peer, Thread>();
-  private final HashMap<Peer, DataXceiver> peersXceiver = new HashMap<Peer, DataXceiver>();
+  private final HashMap<Peer, Thread> peers = new HashMap<>();
+  private final HashMap<Peer, DataXceiver> peersXceiver = new HashMap<>();
+  private final Lock lock = new ReentrantLock();
+  private final Condition noPeers = lock.newCondition();
-  
+  private int maxReconfigureWaitTime = DEFAULT_RECONFIGURE_WAIT;
+
-  /** A manager to make sure that cluster balancing does not
-   * take too much resources.
-   * 
-   * It limits the number of block moves for balancing and
-   * the total amount of bandwidth they can use.
+  /**
+   * A manager to make sure that cluster balancing does not take too much
+   * resources.
+   *
+   * It limits the number of block moves for balancing and the total amount of
+   * bandwidth they can use.
-   private int numThreads;
-   private final AtomicInteger maxThreads = new AtomicInteger(0);
+    private final Semaphore semaphore;
+    private int maxThreads;
-   /**Constructor
-    * 
-    * @param bandwidth Total amount of bandwidth can be used for balancing 
+   /**
+    * Constructor.
+    *
+    * @param bandwidth Total amount of bandwidth can be used for balancing
-      this.maxThreads.set(maxThreads);
+      this.semaphore = new Semaphore(maxThreads, true);
+      this.maxThreads = maxThreads;
-    private void setMaxConcurrentMovers(int movers) {
-      this.maxThreads.set(movers);
+    /**
+     * Update the number of threads which may be used concurrently for moving
+     * blocks. The number of threads available can be scaled up or down. If
+     * increasing the number of threads, the request will be serviced
+     * immediately. However, if decreasing the number of threads, this method
+     * will block any new request for moves, wait for any existing backlog of
+     * move requests to clear, and wait for enough threads to have finished such
+     * that the total number of threads actively running is less than or equal
+     * to the new cap. If this method has been unable to successfully set the
+     * new, lower, cap within 'duration' seconds, the attempt will be aborted
+     * and the original cap will remain.
+     *
+     * @param newMaxThreads The new maximum number of threads for block moving
+     * @param duration The number of seconds to wait if decreasing threads
+     * @return true if new maximum was successfully applied; false otherwise
+     */
+    private boolean setMaxConcurrentMovers(final int newMaxThreads,
+        final int duration) {
+      Preconditions.checkArgument(newMaxThreads > 0);
+      final int delta = newMaxThreads - this.maxThreads;
+      LOG.debug("Change concurrent thread count to {} from {}", newMaxThreads,
+          this.maxThreads);
+      if (delta == 0) {
+        return true;
+      }
+      if (delta > 0) {
+        LOG.debug("Adding thread capacity: {}", delta);
+        this.semaphore.release(delta);
+        this.maxThreads = newMaxThreads;
+        return true;
+      }
+      try {
+        LOG.debug("Removing thread capacity: {}. Max wait: {}", delta,
+            duration);
+        boolean acquired = this.semaphore.tryAcquire(Math.abs(delta), duration,
+            TimeUnit.SECONDS);
+        if (acquired) {
+          this.maxThreads = newMaxThreads;
+        } else {
+          LOG.warn("Could not lower thread count to {} from {}. Too busy.",
+              newMaxThreads, this.maxThreads);
+        }
+        return acquired;
+      } catch (InterruptedException e) {
+        LOG.warn("Interrupted before adjusting thread count: {}", delta);
+        return false;
+      }
-      return this.maxThreads.get();
+      return this.maxThreads;
-   /** Check if the block move can start. 
-    * 
-    * Return true if the thread quota is not exceeded and 
+   /**
+    * Check if the block move can start
+    *
+    * Return true if the thread quota is not exceeded and
-    synchronized boolean acquire() {
-      if (numThreads >= maxThreads.get()) {
-        return false;
-      }
-      numThreads++;
-      return true;
+    boolean acquire() {
+      return this.semaphore.tryAcquire();
-    /** Mark that the move is completed. The thread counter is decremented. */
-    synchronized void release() {
-      numThreads--;
+    /**
+     * Mark that the move is completed. The thread counter is decremented.
+     */
+    void release() {
+      this.semaphore.release();
-  
+
-   * We need an estimate for block size to check if the disk partition has
-   * enough space. Newer clients pass the expected block size to the DataNode.
-   * For older clients we just use the server-side default block size.
+   * Stores an estimate for block size to check if the disk partition has enough
+   * space. Newer clients pass the expected block size to the DataNode. For
+   * older clients, just use the server-side default block size.
-  
-  
+
-    
-    this.maxXceiverCount = 
+
+    this.maxXceiverCount =
-    
+
-    
+
-          LOG.warn(datanode.getDisplayName() + ":DataXceiverServer: ", ace);
+          LOG.warn("{}:DataXceiverServer", datanode.getDisplayName(), ace);
-        IOUtils.cleanup(null, peer);
-        LOG.warn(datanode.getDisplayName() + ":DataXceiverServer: ", ie);
+        IOUtils.closeQuietly(peer);
+        LOG.warn("{}:DataXceiverServer", datanode.getDisplayName(), ie);
-        IOUtils.cleanup(null, peer);
+        IOUtils.closeQuietly(peer);
-          Thread.sleep(30 * 1000);
+          Thread.sleep(TimeUnit.SECONDS.toMillis(30L));
-        LOG.error(datanode.getDisplayName()
-            + ":DataXceiverServer: Exiting due to: ", te);
+        LOG.error("{}:DataXceiverServer: Exiting.", datanode.getDisplayName(),
+            te);
-      LOG.warn(datanode.getDisplayName()
-          + " :DataXceiverServer: close exception", ie);
+      LOG.warn("{}:DataXceiverServer: close exception",
+          datanode.getDisplayName(), ie);
-      // Allow roughly up to 2 seconds.
-      for (int i = 0; getNumPeers() > 0 && i < 10; i++) {
-        try {
-          Thread.sleep(200);
-        } catch (InterruptedException e) {
-          // ignore
-        }
-      }
+
+      waitAllPeers(2L, TimeUnit.SECONDS);
-    // Close all peers.
+
-      LOG.warn(datanode.getDisplayName() + ":DataXceiverServer.kill(): ", ie);
+      LOG.warn("{}:DataXceiverServer.kill()", datanode.getDisplayName(), ie);
-  
-  synchronized void addPeer(Peer peer, Thread t, DataXceiver xceiver)
-      throws IOException {
-    if (closed) {
-      throw new IOException("Server closed.");
-    }
-    peers.put(peer, t);
-    peersXceiver.put(peer, xceiver);
-    datanode.metrics.incrDataNodeActiveXceiversCount();
-  }
-  synchronized void closePeer(Peer peer) {
-    peers.remove(peer);
-    peersXceiver.remove(peer);
-    datanode.metrics.decrDataNodeActiveXceiversCount();
-    IOUtils.cleanup(null, peer);
+  void addPeer(Peer peer, Thread t, DataXceiver xceiver)
+      throws IOException {
+    lock.lock();
+    try {
+      if (closed) {
+        throw new IOException("Server closed.");
+      }
+      peers.put(peer, t);
+      peersXceiver.put(peer, xceiver);
+      datanode.metrics.incrDataNodeActiveXceiversCount();
+    } finally {
+      lock.unlock();
+    }
+  }
+
+  void closePeer(Peer peer) {
+    lock.lock();
+    try {
+      peers.remove(peer);
+      peersXceiver.remove(peer);
+      datanode.metrics.decrDataNodeActiveXceiversCount();
+      IOUtils.closeQuietly(peer);
+      if (peers.isEmpty()) {
+        this.noPeers.signalAll();
+      }
+    } finally {
+      lock.unlock();
+    }
-  public synchronized void sendOOBToPeers() {
-    if (!datanode.shutdownForUpgrade) {
-      return;
-    }
-
-    for (Peer p : peers.keySet()) {
-      try {
-        peersXceiver.get(p).sendOOB();
-      } catch (IOException e) {
-        LOG.warn("Got error when sending OOB message.", e);
-      } catch (InterruptedException e) {
-        LOG.warn("Interrupted when sending OOB message.");
+  public void sendOOBToPeers() {
+    lock.lock();
+    try {
+      if (!datanode.shutdownForUpgrade) {
+        return;
+      for (Peer p : peers.keySet()) {
+        try {
+          peersXceiver.get(p).sendOOB();
+        } catch (IOException e) {
+          LOG.warn("Got error when sending OOB message.", e);
+        } catch (InterruptedException e) {
+          LOG.warn("Interrupted when sending OOB message.");
+        }
+      }
+    } finally {
+      lock.unlock();
-  public synchronized void stopWriters() {
-    for (Peer p : peers.keySet()) {
-      peersXceiver.get(p).stopWriter();
+  public void stopWriters() {
+    lock.lock();
+    try {
+      peers.keySet().forEach(p -> peersXceiver.get(p).stopWriter());
+    } finally {
+      lock.unlock();
-  
-  // Notify all peers of the shutdown and restart.
-  // datanode.shouldRun should still be true and datanode.restarting should
-  // be set true before calling this method.
-  synchronized void restartNotifyPeers() {
-    assert (datanode.shouldRun == true && datanode.shutdownForUpgrade);
-    for (Thread t : peers.values()) {
+
+  /**
+   * Notify all peers of the shutdown and restart. 'datanode.shouldRun' should
+   * still be true and 'datanode.restarting' should be set true before calling
+   * this method.
+   */
+  void restartNotifyPeers() {
+    assert (datanode.shouldRun && datanode.shutdownForUpgrade);
+    lock.lock();
+    try {
-      t.interrupt();
+      peers.values().forEach(t -> t.interrupt());
+    } finally {
+      lock.unlock();
-  // Close all peers and clear the map.
-  synchronized void closeAllPeers() {
+  /**
+   * Close all peers and clear the map.
+   */
+  void closeAllPeers() {
-    for (Peer p : peers.keySet()) {
-      IOUtils.cleanup(null, p);
+    lock.lock();
+    try {
+      peers.keySet().forEach(p -> IOUtils.closeQuietly(p));
+      peers.clear();
+      peersXceiver.clear();
+      datanode.metrics.setDataNodeActiveXceiversCount(0);
+      this.noPeers.signalAll();
+    } finally {
+      lock.unlock();
-    peers.clear();
-    peersXceiver.clear();
-    datanode.metrics.setDataNodeActiveXceiversCount(0);
-  // Return the number of peers.
-  synchronized int getNumPeers() {
-    return peers.size();
+  /**
+   * Causes a thread to block until all peers are removed, a certain amount of
+   * time has passed, or the thread is interrupted.
+   *
+   * @param timeout the maximum time to wait, in nanoseconds
+   * @param unit the unit of time to wait
+   * @return true if thread returned because all peers were removed; false
+   *         otherwise
+   */
+  private boolean waitAllPeers(long timeout, TimeUnit unit) {
+    long nanos = unit.toNanos(timeout);
+    lock.lock();
+    try {
+      while (!peers.isEmpty()) {
+        if (nanos <= 0L) {
+          return false;
+        }
+        nanos = noPeers.awaitNanos(nanos);
+      }
+    } catch (InterruptedException e) {
+      LOG.debug("Interrupted waiting for peers to close");
+      return false;
+    } finally {
+      lock.unlock();
+    }
+    return true;
-  // Return the number of peers and DataXceivers.
+  /**
+   * Return the number of peers.
+   *
+   * @return the number of active peers
+   */
+  int getNumPeers() {
+    lock.lock();
+    try {
+      return peers.size();
+    } finally {
+      lock.unlock();
+    }
+  }
+
+  /**
+   * Return the number of peers and DataXceivers.
+   *
+   * @return the number of peers and DataXceivers.
+   */
-  synchronized int getNumPeersXceiver() {
-    return peersXceiver.size();
+  int getNumPeersXceiver() {
+    lock.lock();
+    try {
+      return peersXceiver.size();
+    } finally {
+      lock.unlock();
+    }
-  synchronized void releasePeer(Peer peer) {
-    peers.remove(peer);
-    peersXceiver.remove(peer);
-    datanode.metrics.decrDataNodeActiveXceiversCount();
+  /**
+   * Release a peer.
+   *
+   * @param peer The peer to release
+   */
+  void releasePeer(Peer peer) {
+    lock.lock();
+    try {
+      peers.remove(peer);
+      peersXceiver.remove(peer);
+      datanode.metrics.decrDataNodeActiveXceiversCount();
+    } finally {
+      lock.unlock();
+    }
-  public void updateBalancerMaxConcurrentMovers(int movers) {
-    balanceThrottler.setMaxConcurrentMovers(movers);
+  /**
+   * Update the number of threads which may be used concurrently for moving
+   * blocks.
+   *
+   * @param movers The new maximum number of threads for block moving
+   * @return true if new maximum was successfully applied; false otherwise
+   */
+  public boolean updateBalancerMaxConcurrentMovers(final int movers) {
+    return balanceThrottler.setMaxConcurrentMovers(movers,
+        this.maxReconfigureWaitTime);
+  }
+
+  /**
+   * Update the maximum amount of time to wait for reconfiguration of the
+   * maximum number of block mover threads to complete.
+   *
+   * @param max The new maximum number of threads for block moving, in seconds
+   */
+  @VisibleForTesting
+  void setMaxReconfigureWaitTime(int max) {
+    this.maxReconfigureWaitTime = max;

MOV26 INS26 INS26 INS26 INS26 INS26 INS40 UPD40 INS40 INS40 INS40 UPD40 INS40 INS23 INS23 INS23 INS23 INS31 INS31 INS31 INS29 INS83 INS83 INS83 INS39 INS59 INS83 INS83 INS43 INS59 INS83 INS83 INS43 INS59 INS83 INS39 INS59 INS23 INS31 INS8 INS8 INS8 INS29 INS29 INS29 UPD83 MOV83 INS39 INS42 INS44 INS44 INS8 INS29 MOV39 MOV42 INS8 INS29 INS8 INS29 INS8 INS29 UPD39 INS29 INS78 INS39 INS42 INS44 INS8 UPD66 UPD66 UPD66 INS65 INS42 INS34 INS42 INS42 INS14 INS42 INS42 INS32 INS42 INS42 INS83 INS83 INS43 INS59 INS29 INS83 INS39 INS42 INS44 INS44 INS8 INS21 INS54 INS21 INS54 INS21 INS54 INS21 INS54 INS65 INS21 INS54 INS65 INS21 INS54 INS65 INS65 INS65 INS65 INS39 INS42 INS43 INS42 INS60 INS21 INS54 MOV41 INS65 INS65 INS21 INS54 INS65 INS65 INS21 INS54 INS65 INS65 INS21 INS54 INS65 INS65 INS65 INS83 INS41 INS65 INS65 INS42 INS39 INS42 INS21 INS66 INS66 UPD74 UPD74 INS43 INS42 INS42 UPD66 UPD66 UPD66 UPD66 INS42 INS42 UPD42 INS21 INS21 INS65 INS65 INS65 INS65 INS83 INS39 INS42 INS83 INS39 INS42 INS21 INS60 INS21 INS25 INS25 INS54 INS41 UPD66 UPD66 UPD66 INS8 INS32 MOV8 INS8 INS32 MOV8 INS8 INS32 MOV8 INS8 INS32 MOV8 INS8 INS66 INS66 INS66 INS32 INS8 INS8 INS66 INS32 INS8 INS8 INS66 INS66 INS42 INS66 INS42 INS66 INS66 INS66 INS42 INS39 INS59 INS32 INS8 INS12 INS8 INS66 INS66 INS32 MOV8 INS8 INS66 INS66 INS32 MOV8 INS8 INS66 INS42 INS66 INS32 MOV8 INS8 INS66 INS66 INS42 INS66 INS66 INS32 INS66 INS66 INS42 INS66 INS7 INS42 UPD66 UPD66 INS7 INS7 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS42 INS66 INS42 INS66 INS66 INS32 INS83 INS39 INS59 INS32 INS27 INS8 INS27 INS8 INS8 INS12 INS22 UPD66 UPD66 INS32 UPD66 INS32 MOV21 MOV21 MOV21 MOV44 INS42 INS42 INS21 INS42 INS42 INS25 INS21 INS42 INS42 INS21 INS42 INS42 INS21 INS42 INS42 INS21 MOV21 INS42 INS42 MOV21 MOV21 MOV21 MOV21 INS21 INS21 INS42 INS32 INS42 INS42 INS61 INS44 INS8 INS21 INS42 INS42 INS21 INS42 INS42 INS21 INS42 INS42 INS21 MOV42 MOV42 MOV42 INS22 INS22 INS42 INS22 INS14 MOV22 INS42 INS42 INS42 INS27 INS42 INS27 INS42 INS42 INS45 INS42 INS22 INS42 INS34 INS41 INS42 INS34 INS21 INS21 MOV21 INS41 INS21 INS60 INS25 INS41 INS44 INS8 MOV52 MOV42 INS22 UPD42 MOV42 INS22 INS42 INS32 INS32 INS8 INS32 INS32 INS32 INS32 MOV40 INS32 INS32 INS32 INS32 INS32 INS42 INS42 INS42 INS38 INS8 INS43 INS42 INS21 INS41 INS32 INS32 INS32 INS32 INS52 INS42 INS52 INS42 INS52 INS42 INS43 INS42 INS9 INS52 INS42 INS42 INS34 INS42 INS22 INS52 INS42 INS9 INS32 INS32 INS7 INS9 INS32 INS39 INS59 INS42 INS8 INS8 INS42 INS43 INS42 INS21 INS41 INS52 INS42 INS52 INS42 MOV44 UPD42 UPD34 INS40 INS42 INS42 UPD42 INS42 INS42 INS21 INS42 INS42 INS42 INS42 MOV32 INS42 INS86 INS42 INS42 MOV32 INS42 INS86 INS42 INS42 MOV32 INS42 INS86 INS22 INS42 INS42 INS42 INS32 INS25 INS21 INS42 INS32 INS9 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS52 INS42 INS42 INS42 INS45 INS42 INS22 INS42 INS42 MOV22 INS42 INS42 INS42 INS45 INS42 INS42 INS42 INS32 MOV21 INS21 INS42 INS32 INS9 INS45 MOV32 INS45 MOV32 INS32 INS59 MOV32 INS59 MOV32 INS59 INS32 INS52 INS42 INS42 INS42 INS27 MOV8 INS7 INS42 INS42 INS45 INS52 INS42 INS22 INS42 INS32 INS42 INS40 INS7 INS32 INS42 INS42 INS45 INS42 INS22 INS42 INS42 INS42 INS42 MOV42 UPD42 MOV42 MOV42 INS42 INS34 INS42 INS32 INS52 INS42 INS42 INS42 INS42 MOV22 INS42 INS42 INS42 INS45 INS42 INS22 UPD42 INS45 MOV32 UPD42 MOV8 INS45 MOV32 INS52 INS42 INS42 INS42 INS42 INS52 INS42 MOV43 MOV42 INS45 MOV32 INS32 MOV42 INS40 INS42 INS34 DEL66 DEL42 DEL43 DEL42 DEL43 DEL42 DEL43 DEL42 DEL43 DEL83 DEL83 DEL42 DEL43 DEL42 DEL42 DEL43 DEL34 DEL14 DEL59 DEL23 DEL42 DEL42 DEL32 DEL42 DEL42 DEL32 DEL83 DEL39 DEL42 DEL39 DEL42 DEL44 DEL8 DEL31 DEL42 DEL32 DEL83 DEL42 DEL42 DEL32 DEL27 DEL25 DEL42 DEL37 DEL21 DEL83 DEL42 DEL37 DEL45 DEL27 DEL33 DEL45 DEL27 DEL33 DEL34 DEL34 DEL27 DEL45 DEL27 DEL45 DEL27 DEL42 DEL39 DEL42 DEL34 DEL59 DEL58 DEL42 DEL32 DEL34 DEL27 DEL42 DEL34 DEL27 DEL27 DEL42 DEL37 DEL8 DEL54 DEL8 DEL24 DEL8 DEL45 DEL27 DEL83 DEL83 DEL33 DEL83 DEL83 DEL42 DEL43 DEL42 DEL44 DEL70 DEL83 DEL9 DEL27 DEL42 DEL43 DEL42 DEL44 DEL8 DEL70 DEL83 DEL33 DEL32 DEL42 DEL43 DEL42 DEL44 DEL8 DEL70 DEL31 DEL83 DEL83 DEL32 DEL21