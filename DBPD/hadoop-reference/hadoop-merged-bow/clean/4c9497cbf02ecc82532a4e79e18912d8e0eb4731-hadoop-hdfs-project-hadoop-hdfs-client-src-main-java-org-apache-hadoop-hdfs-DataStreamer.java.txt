HDFS-9106. Transfer failure during pipeline recovery causes permanent write failures. Contributed by Kihwal Lee.

-    //get a new datanode
+    int tried = 0;
-    final LocatedBlock lb = dfsClient.namenode.getAdditionalDatanode(
-        src, stat.getFileId(), block, nodes, storageIDs,
-        failed.toArray(new DatanodeInfo[failed.size()]),
-        1, dfsClient.clientName);
-    setPipeline(lb);
+    final StorageType[] originalTypes = storageTypes;
+    final String[] originalIDs = storageIDs;
+    IOException caughtException = null;
+    ArrayList<DatanodeInfo> exclude = new ArrayList<DatanodeInfo>(failed);
+    while (tried < 3) {
+      LocatedBlock lb;
+      //get a new datanode
+      lb = dfsClient.namenode.getAdditionalDatanode(
+          src, stat.getFileId(), block, nodes, storageIDs,
+          exclude.toArray(new DatanodeInfo[exclude.size()]),
+          1, dfsClient.clientName);
+      // a new node was allocated by the namenode. Update nodes.
+      setPipeline(lb);
-    //find the new datanode
-    final int d = findNewDatanode(original);
+      //find the new datanode
+      final int d = findNewDatanode(original);
+      //transfer replica. pick a source from the original nodes
+      final DatanodeInfo src = original[tried % original.length];
+      final DatanodeInfo[] targets = {nodes[d]};
+      final StorageType[] targetStorageTypes = {storageTypes[d]};
-    //transfer replica
-    final DatanodeInfo src = d == 0? nodes[1]: nodes[d - 1];
-    final DatanodeInfo[] targets = {nodes[d]};
-    final StorageType[] targetStorageTypes = {storageTypes[d]};
-    transfer(src, targets, targetStorageTypes, lb.getBlockToken());
+      try {
+        transfer(src, targets, targetStorageTypes, lb.getBlockToken());
+      } catch (IOException ioe) {
+        DFSClient.LOG.warn("Error transferring data from " + src + " to " +
+            nodes[d] + ": " + ioe.getMessage());
+        caughtException = ioe;
+        // add the allocated node to the exclude list.
+        exclude.add(nodes[d]);
+        setPipeline(original, originalTypes, originalIDs);
+        tried++;
+        continue;
+      }
+      return; // finished successfully
+    }
+    // All retries failed
+    throw (caughtException != null) ? caughtException :
+        new IOException("Failed to add a node");
-      final long readTimeout = dfsClient.getDatanodeReadTimeout(2);
+
+      // transfer timeout multiplier based on the transfer size
+      // One per 200 packets = 12.8MB. Minimum is 2.
+      int multi = 2 + (int)(bytesSent/dfsClient.getConf().getWritePacketSize())/200;
+      final long readTimeout = dfsClient.getDatanodeReadTimeout(multi);

INS60 INS60 INS60 INS60 INS61 INS53 INS39 INS59 INS5 MOV83 INS5 INS59 INS43 INS59 INS74 INS59 INS27 INS8 INS16 INS42 INS34 INS43 INS85 UPD42 INS42 INS43 INS85 INS42 UPD42 MOV42 INS42 INS42 INS33 INS43 INS43 INS42 INS14 UPD42 MOV42 UPD34 MOV34 INS60 INS21 MOV21 MOV60 INS60 MOV60 MOV60 INS54 INS41 INS36 INS42 INS14 INS60 INS42 INS42 INS42 INS42 INS74 INS42 MOV43 INS59 INS7 INS83 INS43 INS59 INS8 INS12 INS27 INS43 INS45 INS39 INS59 INS43 MOV43 INS42 INS42 MOV32 INS42 INS42 INS2 MOV21 INS44 INS8 INS42 INS33 INS42 INS42 INS27 INS42 INS42 INS27 INS43 INS42 INS21 INS21 INS21 INS21 INS21 INS18 INS34 INS27 INS42 UPD42 INS42 INS40 INS42 INS32 INS7 INS32 INS32 INS37 INS11 INS34 INS40 INS42 INS27 INS42 INS42 INS42 INS42 INS2 INS42 INS42 INS42 INS42 INS42 INS39 INS36 UPD42 INS45 INS42 INS45 INS2 INS45 INS32 MOV42 INS42 INS27 MOV42 INS42 INS42 INS42 INS42 INS32 INS32 INS42 INS42 INS42 DEL27 DEL34 DEL2 DEL42 DEL34 DEL27 DEL2 DEL16 DEL59 DEL60 DEL34