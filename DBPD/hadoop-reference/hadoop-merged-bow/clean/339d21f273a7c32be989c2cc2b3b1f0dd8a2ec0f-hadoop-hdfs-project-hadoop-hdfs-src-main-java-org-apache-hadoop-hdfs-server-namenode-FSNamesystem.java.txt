Merge trunk into HDFS-6581

Conflicts:
	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeLayoutVersion.java

+    BlocksMapUpdateInfo toRemoveBlocks = null;
-          startFileInternal(pc, src, permissions, holder, clientMachine, create,
-              overwrite, createParent, replication, blockSize, isLazyPersist,
-              suite, edek, logRetryCache);
+          toRemoveBlocks = startFileInternal(pc, src, permissions, holder, 
+              clientMachine, create, overwrite, createParent, replication, 
+              blockSize, isLazyPersist, suite, edek, logRetryCache);
+        if (toRemoveBlocks != null) {
+          removeBlocks(toRemoveBlocks);
+          toRemoveBlocks.clear();
+        }
-  private void startFileInternal(FSPermissionChecker pc, String src,
-      PermissionStatus permissions, String holder, String clientMachine,
-      boolean create, boolean overwrite, boolean createParent,
-      short replication, long blockSize, boolean isLazyPersist,
-      CipherSuite suite, EncryptedKeyVersion edek, boolean logRetryEntry)
+  private BlocksMapUpdateInfo startFileInternal(FSPermissionChecker pc, 
+      String src, PermissionStatus permissions, String holder, 
+      String clientMachine, boolean create, boolean overwrite, 
+      boolean createParent, short replication, long blockSize, 
+       boolean isLazyPersist, CipherSuite suite,
+       EncryptedKeyVersion edek, boolean logRetryEntry)
-      } else {
-        checkAncestorAccess(pc, src, FsAction.WRITE);
+      /*
+       * To overwrite existing file, need to check 'w' permission 
+       * of parent (equals to ancestor in this case)
+       */
+      checkAncestorAccess(pc, src, FsAction.WRITE);
+      BlocksMapUpdateInfo toRemoveBlocks = null;
-          try {
-            deleteInt(src, true, false); // File exists - delete if overwrite
-          } catch (AccessControlException e) {
-            logAuditEvent(false, "delete", src);
-            throw e;
+          toRemoveBlocks = new BlocksMapUpdateInfo();
+          List<INode> toRemoveINodes = new ChunkedArrayList<INode>();
+          long ret = dir.delete(src, toRemoveBlocks, toRemoveINodes, now());
+          if (ret >= 0) {
+            incrDeletedFileCount(ret);
+            removePathAndBlocks(src, null, toRemoveINodes, true);
-      getEditLog().logOpenFile(src, newNode, logRetryEntry);
+      getEditLog().logOpenFile(src, newNode, overwrite, logRetryEntry);
+      return toRemoveBlocks;
-      getEditLog().logOpenFile(src, cons, logRetryCache);
+      getEditLog().logOpenFile(src, cons, false, logRetryCache);

INS43 INS60 INS42 MOV25 INS43 INS59 INS42 INS42 INS42 INS33 INS25 INS60 INS41 INS8 MOV27 MOV8 INS43 INS59 INS42 MOV21 INS25 INS42 INS42 INS33 INS42 INS9 INS27 MOV8 INS42 INS33 INS21 INS21 MOV21 INS60 INS60 INS25 INS7 INS32 INS32 INS7 INS74 INS59 INS39 INS59 INS27 INS8 INS42 MOV32 INS42 INS42 INS42 INS42 INS42 INS14 INS43 INS43 INS42 INS14 INS42 INS32 INS42 INS34 MOV21 INS21 INS43 UPD42 MOV42 INS42 INS74 INS42 INS42 INS42 INS42 INS42 INS32 INS32 INS42 INS43 INS43 INS42 UPD42 UPD42 INS42 INS42 INS33 INS42 INS9 INS42 INS42 DEL39 DEL42 DEL8 DEL25 DEL42 DEL42 DEL9 DEL9 DEL32 DEL9 DEL45 DEL8 DEL43 DEL42 DEL44 DEL42 DEL53 DEL8 DEL12 DEL54