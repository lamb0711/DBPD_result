HADOOP-13560. S3ABlockOutputStream to support huge (many GB) file writes. Contributed by Steve Loughran

-/**
+/*
+import com.amazonaws.services.s3.model.AbortMultipartUploadRequest;
+import com.amazonaws.services.s3.model.CompleteMultipartUploadRequest;
+import com.amazonaws.services.s3.model.CompleteMultipartUploadResult;
+import com.amazonaws.services.s3.model.CopyObjectRequest;
+import com.amazonaws.services.s3.model.InitiateMultipartUploadRequest;
+import com.amazonaws.services.s3.model.PartETag;
-import com.amazonaws.services.s3.model.CopyObjectRequest;
+import com.amazonaws.services.s3.model.PutObjectResult;
+import com.google.common.base.Preconditions;
+import com.google.common.util.concurrent.ListeningExecutorService;
+import org.apache.hadoop.fs.LocalDirAllocator;
-  private ExecutorService threadPoolExecutor;
+  private ListeningExecutorService threadPoolExecutor;
+  private static final Logger PROGRESS =
+      LoggerFactory.getLogger("org.apache.hadoop.fs.s3a.S3AFileSystem.Progress");
+  private LocalDirAllocator directoryAllocator;
+  private boolean blockUploadEnabled;
+  private String blockOutputBuffer;
+  private S3ADataBlocks.BlockFactory blockFactory;
+  private int blockOutputActiveBlocks;
-      partSize = conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);
-      if (partSize < 5 * 1024 * 1024) {
-        LOG.error(MULTIPART_SIZE + " must be at least 5 MB");
-        partSize = 5 * 1024 * 1024;
-      }
+      partSize = getMultipartSizeProperty(conf,
+          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);
+      multiPartThreshold = getMultipartSizeProperty(conf,
+          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);
-      multiPartThreshold = conf.getLong(MIN_MULTIPART_THRESHOLD,
-          DEFAULT_MIN_MULTIPART_THRESHOLD);
-      if (multiPartThreshold < 5 * 1024 * 1024) {
-        LOG.error(MIN_MULTIPART_THRESHOLD + " must be at least 5 MB");
-        multiPartThreshold = 5 * 1024 * 1024;
-      }
-      int totalTasks = conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);
-      if (totalTasks < 1) {
-        LOG.warn(MAX_TOTAL_TASKS + "must be at least 1: forcing to 1.");
-        totalTasks = 1;
-      }
-      long keepAliveTime = conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);
-      threadPoolExecutor = new BlockingThreadPoolExecutorService(maxThreads,
-          maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,
+      int totalTasks = intOption(conf,
+          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);
+      long keepAliveTime = longOption(conf, KEEPALIVE_TIME,
+          DEFAULT_KEEPALIVE_TIME, 0);
+      threadPoolExecutor = BlockingThreadPoolExecutorService.newInstance(
+          maxThreads,
+          maxThreads + totalTasks,
+          keepAliveTime, TimeUnit.SECONDS,
+      LOG.debug("Using encryption {}", serverSideEncryptionAlgorithm);
+
+      blockUploadEnabled = conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);
+
+      if (blockUploadEnabled) {
+        blockOutputBuffer = conf.getTrimmed(FAST_UPLOAD_BUFFER,
+            DEFAULT_FAST_UPLOAD_BUFFER);
+        partSize = ensureOutputParameterInRange(MULTIPART_SIZE, partSize);
+        blockFactory = S3ADataBlocks.createFactory(this, blockOutputBuffer);
+        blockOutputActiveBlocks = intOption(conf,
+            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);
+        LOG.debug("Using S3ABlockOutputStream with buffer = {}; block={};" +
+                " queue limit={}",
+            blockOutputBuffer, partSize, blockOutputActiveBlocks);
+      } else {
+        LOG.debug("Using S3AOutputStream");
+      }
+   * Demand create the directory allocator, then create a temporary file.
+   * {@link LocalDirAllocator#createTmpFileForWrite(String, long, Configuration)}.
+   *  @param pathStr prefix for the temporary file
+   *  @param size the size of the file that is going to be written
+   *  @param conf the Configuration object
+   *  @return a unique temporary file
+   *  @throws IOException IO problems
+   */
+  synchronized File createTmpFileForWrite(String pathStr, long size,
+      Configuration conf) throws IOException {
+    if (directoryAllocator == null) {
+      String bufferDir = conf.get(BUFFER_DIR) != null
+          ? BUFFER_DIR : "hadoop.tmp.dir";
+      directoryAllocator = new LocalDirAllocator(bufferDir);
+    }
+    return directoryAllocator.createTmpFileForWrite(pathStr, size, conf);
+  }
+
+  /**
+   * Get the bucket of this filesystem.
+   * @return the bucket
+   */
+  public String getBucket() {
+    return bucket;
+  }
+
+  /**
+  @SuppressWarnings("IOResourceOpenedButNotSafelyClosed")
-    if (getConf().getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD)) {
-      return new FSDataOutputStream(
-          new S3AFastOutputStream(s3,
-              this,
-              bucket,
+    FSDataOutputStream output;
+    if (blockUploadEnabled) {
+      output = new FSDataOutputStream(
+          new S3ABlockOutputStream(this,
+              new SemaphoredDelegatingExecutor(threadPoolExecutor,
+                  blockOutputActiveBlocks, true),
-              cannedACL,
-              multiPartThreshold,
-              threadPoolExecutor),
-          statistics);
+              blockFactory,
+              instrumentation.newOutputStreamStatistics(),
+              new WriteOperationHelper(key)
+          ),
+          null);
+    } else {
+
+      // We pass null to FSDataOutputStream so it won't count writes that
+      // are being buffered to a file
+      output = new FSDataOutputStream(
+          new S3AOutputStream(getConf(),
+              this,
+              key,
+              progress
+          ),
+          null);
-    // We pass null to FSDataOutputStream so it won't count writes that
-    // are being buffered to a file
-    return new FSDataOutputStream(
-        new S3AOutputStream(getConf(),
-            this,
-            key,
-            progress
-        ),
-        null);
+    return output;
+   * Decrement a gauge by a specific value.
+   * @param statistic The operation to decrement
+   * @param count the count to decrement
+   */
+  protected void decrementGauge(Statistic statistic, long count) {
+    instrumentation.decrementGauge(statistic, count);
+  }
+
+  /**
+   * Increment a gauge by a specific value.
+   * @param statistic The operation to increment
+   * @param count the count to increment
+   */
+  protected void incrementGauge(Statistic statistic, long count) {
+    instrumentation.incrementGauge(statistic, count);
+  }
+
+  /**
+   * Get the storage statistics of this filesystem.
+   * @return the storage statistics
+   */
+  @Override
+  public S3AStorageStatistics getStorageStatistics() {
+    return storageStatistics;
+  }
+
+  /**
-    om.setContentLength(length);
+    if (length >= 0) {
+      om.setContentLength(length);
+    }
-    return transfers.upload(putObjectRequest);
+    try {
+      Upload upload = transfers.upload(putObjectRequest);
+      incrementPutCompletedStatistics(true, len);
+      return upload;
+    } catch (AmazonClientException e) {
+      incrementPutCompletedStatistics(false, len);
+      throw e;
+    }
+  }
+
+  /**
+   * PUT an object directly (i.e. not via the transfer manager).
+   * Byte length is calculated from the file length, or, if there is no
+   * file, from the content length of the header.
+   * @param putObjectRequest the request
+   * @return the upload initiated
+   * @throws AmazonClientException on problems
+   */
+  public PutObjectResult putObjectDirect(PutObjectRequest putObjectRequest)
+      throws AmazonClientException {
+    long len;
+    if (putObjectRequest.getFile() != null) {
+      len = putObjectRequest.getFile().length();
+    } else {
+      len = putObjectRequest.getMetadata().getContentLength();
+    }
+    incrementPutStartStatistics(len);
+    try {
+      PutObjectResult result = s3.putObject(putObjectRequest);
+      incrementPutCompletedStatistics(true, len);
+      return result;
+    } catch (AmazonClientException e) {
+      incrementPutCompletedStatistics(false, len);
+      throw e;
+    }
+   * @throws AmazonClientException on problems
-  public UploadPartResult uploadPart(UploadPartRequest request) {
-    incrementPutStartStatistics(request.getPartSize());
-    return s3.uploadPart(request);
+  public UploadPartResult uploadPart(UploadPartRequest request)
+      throws AmazonClientException {
+    long len = request.getPartSize();
+    incrementPutStartStatistics(len);
+    try {
+      UploadPartResult uploadPartResult = s3.uploadPart(request);
+      incrementPutCompletedStatistics(true, len);
+      return uploadPartResult;
+    } catch (AmazonClientException e) {
+      incrementPutCompletedStatistics(false, len);
+      throw e;
+    }
+    incrementGauge(OBJECT_PUT_REQUESTS_ACTIVE, 1);
+    if (bytes > 0) {
+      incrementGauge(OBJECT_PUT_BYTES_PENDING, bytes);
+    }
+  }
+
+  /**
+   * At the end of a put/multipart upload operation, update the
+   * relevant counters and gauges.
+   *
+   * @param success did the operation succeed?
+   * @param bytes bytes in the request.
+   */
+  public void incrementPutCompletedStatistics(boolean success, long bytes) {
+    LOG.debug("PUT completed success={}; {} bytes", success, bytes);
+    incrementWriteOperations();
+      decrementGauge(OBJECT_PUT_BYTES_PENDING, bytes);
+    incrementStatistic(OBJECT_PUT_REQUESTS_COMPLETED);
+    decrementGauge(OBJECT_PUT_REQUESTS_ACTIVE, 1);
-    LOG.debug("PUT {}: {} bytes", key, bytes);
+    PROGRESS.debug("PUT {}: {} bytes", key, bytes);
-    final ObjectMetadata om = newObjectMetadata();
+    final ObjectMetadata om = newObjectMetadata(srcfile.length());
+    if (blockFactory != null) {
+      sb.append(", blockFactory=").append(blockFactory);
+    }
+    sb.append(", executor=").append(threadPoolExecutor);
+
+  /**
+   * Helper for an ongoing write operation.
+   * <p>
+   * It hides direct access to the S3 API from the output stream,
+   * and is a location where the object upload process can be evolved/enhanced.
+   * <p>
+   * Features
+   * <ul>
+   *   <li>Methods to create and submit requests to S3, so avoiding
+   *   all direct interaction with the AWS APIs.</li>
+   *   <li>Some extra preflight checks of arguments, so failing fast on
+   *   errors.</li>
+   *   <li>Callbacks to let the FS know of events in the output stream
+   *   upload process.</li>
+   * </ul>
+   *
+   * Each instance of this state is unique to a single output stream.
+   */
+  final class WriteOperationHelper {
+    private final String key;
+
+    private WriteOperationHelper(String key) {
+      this.key = key;
+    }
+
+    /**
+     * Create a {@link PutObjectRequest} request.
+     * The metadata is assumed to have been configured with the size of the
+     * operation.
+     * @param inputStream source data.
+     * @param length size, if known. Use -1 for not known
+     * @return the request
+     */
+    PutObjectRequest newPutRequest(InputStream inputStream, long length) {
+      return newPutObjectRequest(key, newObjectMetadata(length), inputStream);
+    }
+
+    /**
+     * Callback on a successful write.
+     */
+    void writeSuccessful() {
+      finishedWrite(key);
+    }
+
+    /**
+     * Callback on a write failure.
+     * @param e Any exception raised which triggered the failure.
+     */
+    void writeFailed(Exception e) {
+      LOG.debug("Write to {} failed", this, e);
+    }
+
+    /**
+     * Create a new object metadata instance.
+     * Any standard metadata headers are added here, for example:
+     * encryption.
+     * @param length size, if known. Use -1 for not known
+     * @return a new metadata instance
+     */
+    public ObjectMetadata newObjectMetadata(long length) {
+      return S3AFileSystem.this.newObjectMetadata(length);
+    }
+
+    /**
+     * Start the multipart upload process.
+     * @return the upload result containing the ID
+     * @throws IOException IO problem
+     */
+    String initiateMultiPartUpload() throws IOException {
+      LOG.debug("Initiating Multipart upload");
+      final InitiateMultipartUploadRequest initiateMPURequest =
+          new InitiateMultipartUploadRequest(bucket,
+              key,
+              newObjectMetadata(-1));
+      initiateMPURequest.setCannedACL(cannedACL);
+      try {
+        return s3.initiateMultipartUpload(initiateMPURequest)
+            .getUploadId();
+      } catch (AmazonClientException ace) {
+        throw translateException("initiate MultiPartUpload", key, ace);
+      }
+    }
+
+    /**
+     * Complete a multipart upload operation.
+     * @param uploadId multipart operation Id
+     * @param partETags list of partial uploads
+     * @return the result
+     * @throws AmazonClientException on problems.
+     */
+    CompleteMultipartUploadResult completeMultipartUpload(String uploadId,
+        List<PartETag> partETags) throws AmazonClientException {
+      Preconditions.checkNotNull(uploadId);
+      Preconditions.checkNotNull(partETags);
+      Preconditions.checkArgument(!partETags.isEmpty(),
+          "No partitions have been uploaded");
+      return s3.completeMultipartUpload(
+          new CompleteMultipartUploadRequest(bucket,
+              key,
+              uploadId,
+              partETags));
+    }
+
+    /**
+     * Abort a multipart upload operation.
+     * @param uploadId multipart operation Id
+     * @return the result
+     * @throws AmazonClientException on problems.
+     */
+    void abortMultipartUpload(String uploadId) throws AmazonClientException {
+      s3.abortMultipartUpload(
+          new AbortMultipartUploadRequest(bucket, key, uploadId));
+    }
+
+    /**
+     * Create and initialize a part request of a multipart upload.
+     * @param uploadId ID of ongoing upload
+     * @param uploadStream source of data to upload
+     * @param partNumber current part number of the upload
+     * @param size amount of data
+     * @return the request.
+     */
+    UploadPartRequest newUploadPartRequest(String uploadId,
+        InputStream uploadStream,
+        int partNumber,
+        int size) {
+      Preconditions.checkNotNull(uploadId);
+      Preconditions.checkNotNull(uploadStream);
+      Preconditions.checkArgument(size > 0, "Invalid partition size %s", size);
+      Preconditions.checkArgument(partNumber> 0 && partNumber <=10000,
+          "partNumber must be between 1 and 10000 inclusive, but is %s",
+          partNumber);
+
+      LOG.debug("Creating part upload request for {} #{} size {}",
+          uploadId, partNumber, size);
+      return new UploadPartRequest()
+          .withBucketName(bucket)
+          .withKey(key)
+          .withUploadId(uploadId)
+          .withInputStream(uploadStream)
+          .withPartNumber(partNumber)
+          .withPartSize(size);
+    }
+
+    /**
+     * The toString method is intended to be used in logging/toString calls.
+     * @return a string description.
+     */
+    @Override
+    public String toString() {
+      final StringBuilder sb = new StringBuilder(
+          "{bucket=").append(bucket);
+      sb.append(", key='").append(key).append('\'');
+      sb.append('}');
+      return sb.toString();
+    }
+  }
+

MOV26 INS26 INS26 INS26 INS26 INS26 INS26 INS26 INS26 INS26 INS40 INS40 INS40 INS40 INS40 INS40 INS40 INS40 INS40 INS23 INS23 INS23 INS23 INS23 INS23 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS55 UPD43 INS83 INS83 INS83 INS43 INS59 INS83 INS43 INS59 INS83 INS39 INS59 INS83 INS43 INS59 INS83 INS43 INS59 INS83 INS39 INS59 INS29 INS83 INS43 INS42 INS44 INS44 INS44 INS43 INS8 INS29 INS83 INS43 INS42 INS8 INS79 INS29 INS83 INS39 INS42 INS44 INS44 INS8 INS29 INS83 INS39 INS42 INS44 INS44 INS8 INS29 INS78 INS83 INS43 INS42 INS8 INS29 INS83 INS43 INS42 INS44 INS43 INS8 INS43 INS29 INS83 INS39 INS42 INS44 INS44 INS8 INS29 INS83 INS42 INS23 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS31 INS31 UPD42 INS42 INS42 INS32 INS42 INS42 INS42 INS42 INS42 INS40 INS42 INS42 INS65 INS65 INS65 INS65 INS65 INS65 INS42 INS43 INS42 INS39 INS42 INS43 INS42 INS42 INS25 INS41 INS65 INS65 INS42 INS41 INS42 INS45 INS60 INS65 INS65 INS65 INS43 INS42 INS39 INS42 INS21 INS65 INS65 INS65 INS43 INS42 INS39 INS42 INS21 INS65 INS65 INS42 INS42 INS41 INS25 INS54 INS65 INS65 INS65 INS65 INS42 INS43 INS42 INS42 INS60 INS25 INS21 INS54 INS65 INS42 INS60 INS54 INS21 INS25 INS65 INS65 INS65 INS39 INS42 INS39 INS42 INS21 INS21 MOV25 INS21 INS21 INS25 INS21 INS65 INS83 INS83 INS43 INS59 INS83 INS42 INS44 INS8 INS29 INS43 INS42 INS44 INS44 INS8 INS29 INS39 INS42 INS8 INS29 INS39 INS42 INS44 INS8 INS29 INS83 INS43 INS42 INS44 INS8 INS29 INS43 INS42 INS43 INS8 INS29 INS43 INS42 INS44 INS44 INS43 INS8 INS29 INS39 INS42 INS44 INS43 INS8 INS29 INS43 INS42 INS44 INS44 INS44 INS44 INS8 INS29 INS78 INS83 INS43 INS42 INS8 INS42 INS42 INS45 MOV21 INS66 INS65 INS66 INS42 INS66 INS42 INS66 INS42 INS66 INS66 INS42 INS66 INS42 INS42 INS27 INS8 INS32 INS66 INS66 INS42 MOV43 INS59 INS42 INS8 INS8 INS42 INS66 INS42 INS66 INS42 INS66 INS42 INS32 INS66 INS42 INS66 INS42 INS66 INS42 INS32 INS66 INS66 INS42 INS27 INS8 INS8 INS12 INS66 INS66 INS66 INS42 INS66 INS66 INS42 INS66 INS42 INS39 INS59 INS27 INS8 INS8 INS32 INS8 INS12 INS42 INS66 INS39 INS59 INS8 INS12 INS32 INS27 INS8 INS66 INS66 INS42 INS66 INS42 INS66 INS32 INS32 INS32 INS32 INS27 INS8 INS32 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS66 INS42 INS42 INS43 INS42 INS21 INS65 INS65 INS65 INS65 INS42 INS43 INS42 INS39 INS42 INS41 INS65 INS21 INS65 INS65 INS43 INS42 INS21 INS65 INS65 INS65 INS42 INS39 INS42 INS41 INS65 INS65 INS65 INS42 INS42 INS21 INS60 INS21 INS54 INS65 INS65 INS65 INS65 INS65 INS42 INS43 INS42 INS74 INS42 INS42 INS21 INS21 INS21 INS41 INS65 INS65 INS65 INS65 INS43 INS42 INS42 INS21 INS65 INS65 INS65 INS65 INS65 INS65 INS42 INS43 INS42 INS43 INS42 INS39 INS42 INS39 INS42 INS21 INS21 INS21 INS21 INS21 INS41 INS65 INS65 INS42 INS42 INS60 INS21 INS21 INS41 INS21 INS60 INS60 MOV21 MOV21 INS25 INS68 INS42 INS33 INS60 INS21 INS42 INS42 INS42 INS42 INS42 INS42 INS21 INS21 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS34 MOV21 INS60 INS21 MOV41 INS44 INS8 INS42 INS32 INS33 INS21 INS21 INS42 INS42 INS60 INS21 INS41 INS44 INS8 INS42 MOV32 INS42 INS60 INS21 MOV41 INS44 INS8 INS42 INS42 INS34 INS42 INS34 INS21 INS42 INS42 INS45 INS42 INS42 INS42 INS21 INS42 INS42 INS42 INS42 INS34 UPD42 INS42 INS33 INS21 INS32 INS42 INS42 INS42 INS7 INS66 INS65 INS66 INS66 INS66 INS42 INS66 INS42 INS66 INS66 INS42 INS32 INS66 INS32 INS66 INS42 INS66 INS42 INS32 INS66 INS66 INS66 INS42 INS66 INS66 INS32 INS66 INS66 INS42 INS66 INS32 INS83 INS43 INS59 INS32 INS8 INS12 INS66 INS42 INS66 INS42 INS66 INS66 INS42 INS66 INS42 INS43 INS43 INS32 INS32 INS32 INS32 INS66 INS42 INS66 INS66 INS42 INS66 INS42 INS32 INS66 INS42 INS66 INS42 INS66 INS42 INS66 INS42 INS66 INS66 INS42 INS42 INS32 INS32 INS32 INS32 INS32 INS32 INS66 INS66 INS83 INS43 INS59 INS32 INS32 INS32 INS7 INS39 INS59 INS39 INS59 INS42 INS8 INS8 INS42 INS42 INS69 INS69 INS69 INS43 INS59 INS7 INS7 INS7 INS43 INS59 INS32 INS42 INS43 INS42 INS21 INS53 INS42 INS42 INS7 INS7 INS43 INS59 INS32 INS42 INS43 INS42 INS21 INS53 INS43 INS59 INS32 INS42 INS43 INS42 INS21 INS53 INS32 INS32 INS32 INS32 INS42 INS42 INS45 INS22 INS42 INS42 INS42 INS42 INS32 INS42 INS42 INS42 INS42 INS42 INS45 INS52 INS42 INS52 INS42 INS42 INS42 INS42 INS45 INS42 INS42 INS14 INS42 INS42 INS42 INS41 INS44 INS8 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS38 INS45 INS42 INS42 INS14 INS42 INS42 INS14 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS27 INS45 INS42 INS42 INS42 INS27 INS45 INS42 INS42 INS42 INS45 INS42 INS42 INS42 INS32 INS42 INS42 INS42 INS42 INS32 INS32 INS42 INS13 INS42 INS42 INS13 INS42 INS42 INS42 INS32 INS42 INS32 INS42 INS32 UPD42 UPD42 INS45 INS42 UPD42 INS32 INS21 INS21 MOV21 MOV21 INS21 MOV21 INS43 INS39 INS43 INS42 INS42 INS16 INS42 INS14 INS42 INS14 INS42 MOV14 INS42 INS42 MOV32 INS42 INS9 INS42 INS42 INS32 INS42 INS42 INS32 INS42 INS32 INS42 INS42 INS32 INS42 INS9 INS42 INS42 INS32 INS42 INS42 INS42 MOV32 INS42 INS9 INS42 INS42 INS32 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS32 INS42 INS42 INS52 INS42 INS42 INS42 INS42 INS43 INS42 INS42 INS32 INS32 INS43 INS42 INS53 INS32 INS43 INS42 INS42 INS42 INS42 INS43 INS42 INS42 INS42 INS42 INS34 INS27 INS27 INS32 INS42 INS42 INS14 INS42 INS42 INS32 INS42 INS42 INS42 UPD42 MOV42 UPD42 MOV42 INS42 INS42 INS42 INS42 INS42 INS42 INS34 INS42 INS42 INS42 INS42 INS34 UPD42 UPD42 UPD42 MOV27 UPD42 INS40 INS45 INS42 INS42 INS42 INS42 INS7 INS7 INS7 INS32 INS32 INS42 INS42 INS27 INS42 INS45 INS43 INS42 INS43 INS14 INS33 INS42 INS9 INS42 INS32 INS42 INS32 INS42 INS42 INS42 INS42 INS42 INS9 INS42 INS42 INS9 INS42 INS42 INS42 INS45 INS42 INS42 INS38 INS32 INS42 INS42 INS32 INS42 INS42 INS42 INS42 INS42 INS34 INS42 INS34 INS32 INS42 INS42 INS43 INS45 INS42 INS42 INS45 INS42 INS32 INS42 INS32 INS42 INS32 UPD42 INS32 UPD42 MOV42 UPD42 MOV42 INS27 UPD42 MOV42 UPD42 MOV42 INS42 INS42 INS42 INS45 INS32 INS33 INS42 INS42 UPD43 MOV43 MOV52 MOV42 INS14 UPD42 MOV42 MOV42 UPD42 MOV42 INS32 INS14 INS42 INS42 INS42 INS42 INS34 INS42 INS42 INS42 INS42 INS45 INS42 INS42 INS32 INS42 INS42 INS42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 INS42 INS42 INS42 UPD42 MOV42 UPD42 MOV42 INS52 INS42 INS42 INS42 INS42 INS42 INS34 INS45 INS45 INS42 INS42 INS42 UPD42 INS43 INS42 UPD42 MOV42 INS9 INS42 INS42 INS43 INS42 INS32 INS42 INS42 INS42 INS42 INS14 INS42 INS42 INS43 INS42 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL65 DEL29 DEL42 DEL42 DEL34 DEL34 DEL34 DEL27 DEL27 DEL42 DEL45 DEL27 DEL32 DEL21 DEL42 DEL34 DEL34 DEL34 DEL27 DEL7 DEL21 DEL8 DEL25 DEL42 DEL34 DEL34 DEL34 DEL27 DEL27 DEL8 DEL25 DEL39 DEL42 DEL32 DEL59 DEL60 DEL42 DEL34 DEL27 DEL8 DEL25 DEL39 DEL42 DEL32 DEL59 DEL60 DEL42 DEL45 DEL27 DEL34 DEL34 DEL34 DEL27 DEL42 DEL45 DEL27 DEL32 DEL34 DEL42 DEL42 DEL43 DEL42 DEL42 DEL40 DEL45 DEL14 DEL7 DEL42 DEL32 DEL42 DEL42 DEL42 DEL32 DEL42 DEL42 DEL42 DEL14 DEL42 DEL14 DEL41 DEL8