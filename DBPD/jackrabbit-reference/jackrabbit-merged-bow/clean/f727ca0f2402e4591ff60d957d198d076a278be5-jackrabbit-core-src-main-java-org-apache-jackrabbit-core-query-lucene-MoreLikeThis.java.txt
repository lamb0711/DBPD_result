JCR-97: Improve Checkstyle conformance
    - Use spaces instead of tabs for indentation

git-svn-id: https://svn.apache.org/repos/asf/jackrabbit/trunk@615669 13f79535-47bb-0310-9956-ffa450edef68

-	/**
-	 * Default maximum number of tokens to parse in each example doc field that is not stored with TermVector support.
-	 * @see #getMaxNumTokensParsed
-	 */
+    /**
+     * Default maximum number of tokens to parse in each example doc field that is not stored with TermVector support.
+     * @see #getMaxNumTokensParsed
+     */
-
-	/**
+    /**
-	 * @see #getAnalyzer
+     * @see #getAnalyzer
-	 * @see #getMinTermFreq
-	 * @see #setMinTermFreq
+     * @see #getMinTermFreq
+     * @see #setMinTermFreq
-	 * @see #getMinDocFreq
-	 * @see #setMinDocFreq
+     * @see #getMinDocFreq
+     * @see #setMinDocFreq
-	 * @see #isBoost
-	 * @see #setBoost
+     * @see #isBoost
+     * @see #setBoost
-	 * @see #getMinWordLen
-	 * @see #setMinWordLen
+     * @see #getMinWordLen
+     * @see #setMinWordLen
-	 * @see #getMaxWordLen
-	 * @see #setMaxWordLen
+     * @see #getMaxWordLen
+     * @see #setMaxWordLen
-	/**
-	 * Default set of stopwords.
-	 * If null means to allow stop words.
-	 *
-	 * @see #setStopWords
-	 * @see #getStopWords
-	 */
-	public static final Set DEFAULT_STOP_WORDS = null;
+    /**
+     * Default set of stopwords.
+     * If null means to allow stop words.
+     *
+     * @see #setStopWords
+     * @see #getStopWords
+     */
+    public static final Set DEFAULT_STOP_WORDS = null;
-	/**
-	 * Current set of stop words.
-	 */
-	private Set stopWords = DEFAULT_STOP_WORDS;
+    /**
+     * Current set of stop words.
+     */
+    private Set stopWords = DEFAULT_STOP_WORDS;
-	 * @see #getMaxQueryTerms
-	 * @see #setMaxQueryTerms
+     * @see #getMaxQueryTerms
+     * @see #setMaxQueryTerms
-	/**
-	 * The maximum number of tokens to parse in each example doc field that is not stored with TermVector support
-	 */
-	private int maxNumTokensParsed=DEFAULT_MAX_NUM_TOKENS_PARSED;
-
-
+    /**
+     * The maximum number of tokens to parse in each example doc field that is not stored with TermVector support
+     */
+    private int maxNumTokensParsed=DEFAULT_MAX_NUM_TOKENS_PARSED;
-	 * @see #DEFAULT_ANALYZER
+     * @see #DEFAULT_ANALYZER
-	 * @see #setBoost
+     * @see #setBoost
-	 * @see #isBoost
+     * @see #isBoost
-	/**
-	 * Set the set of stopwords.
-	 * Any word in this set is considered "uninteresting" and ignored.
-	 * Even if your Analyzer allows stopwords, you might want to tell the MoreLikeThis code to ignore them, as
-	 * for the purposes of document similarity it seems reasonable to assume that "a stop word is never interesting".
-	 *
-	 * @param stopWords set of stopwords, if null it means to allow stop words
-	 *
-	 * @see org.apache.lucene.analysis.StopFilter#makeStopSet StopFilter.makeStopSet()
-	 * @see #getStopWords
-	 */
-	public void setStopWords(Set stopWords) {
-		this.stopWords = stopWords;
-	}
+    /**
+     * Set the set of stopwords.
+     * Any word in this set is considered "uninteresting" and ignored.
+     * Even if your Analyzer allows stopwords, you might want to tell the MoreLikeThis code to ignore them, as
+     * for the purposes of document similarity it seems reasonable to assume that "a stop word is never interesting".
+     *
+     * @param stopWords set of stopwords, if null it means to allow stop words
+     *
+     * @see org.apache.lucene.analysis.StopFilter#makeStopSet StopFilter.makeStopSet()
+     * @see #getStopWords
+     */
+    public void setStopWords(Set stopWords) {
+        this.stopWords = stopWords;
+    }
-	/**
-	 * Get the current stop words being used.
-	 * @see #setStopWords
-	 */
-	public Set getStopWords() {
-		return stopWords;
-	}
-
+    /**
+     * Get the current stop words being used.
+     * @see #setStopWords
+     */
+    public Set getStopWords() {
+        return stopWords;
+    }
-	/**
-	 * @return The maximum number of tokens to parse in each example doc field that is not stored with TermVector support
-	 * @see #DEFAULT_MAX_NUM_TOKENS_PARSED
-	 */
-	public int getMaxNumTokensParsed()
-	{
-		return maxNumTokensParsed;
-	}
+    /**
+     * @return The maximum number of tokens to parse in each example doc field that is not stored with TermVector support
+     * @see #DEFAULT_MAX_NUM_TOKENS_PARSED
+     */
+    public int getMaxNumTokensParsed() {
+        return maxNumTokensParsed;
+    }
-	/**
-	 * @param i The maximum number of tokens to parse in each example doc field that is not stored with TermVector support
-	 */
-	public void setMaxNumTokensParsed(int i)
-	{
-		maxNumTokensParsed = i;
-	}
-
-
-
+    /**
+     * @param i The maximum number of tokens to parse in each example doc field that is not stored with TermVector support
+     */
+    public void setMaxNumTokensParsed(int i) {
+        maxNumTokensParsed = i;
+    }
-			String summary = d.get( "summary");
+            String summary = d.get( "summary");
-			if ( summary != null)
-				o.println("\tsummary: " + d.get("summary"));
+            if (summary != null) {
+                o.println("\tsummary: " + d.get("summary"));
+            }
-            	Document d=ir.document(docNum);
-            	String text[]=d.getValues(fieldName);
-            	if(text!=null)
-            	{
-                for (int j = 0; j < text.length; j++) {
-                  addTermFrequencies(new StringReader(text[j]), termFreqMap, fieldName);
+                Document d = ir.document(docNum);
+                String[] text = d.getValues(fieldName);
+                if (text != null) {
+                    for (int j = 0; j < text.length; j++) {
+                        addTermFrequencies(new StringReader(text[j]), termFreqMap, fieldName);
+                    }
-            	}
-				addTermFrequencies(termFreqMap, vector);
+                addTermFrequencies(termFreqMap, vector);
-	/**
-	 * Adds terms and frequencies found in vector into the Map termFreqMap
-	 * @param termFreqMap a Map of terms and their frequencies
-	 * @param vector List of terms and their frequencies for a doc/field
-	 */
-	private void addTermFrequencies(Map termFreqMap, TermFreqVector vector)
-	{
-		String[] terms = vector.getTerms();
-		int freqs[]=vector.getTermFrequencies();
-		for (int j = 0; j < terms.length; j++) {
-		    String term = terms[j];
+    /**
+     * Adds terms and frequencies found in vector into the Map termFreqMap
+     * @param termFreqMap a Map of terms and their frequencies
+     * @param vector List of terms and their frequencies for a doc/field
+     */
+    private void addTermFrequencies(Map termFreqMap, TermFreqVector vector) {
+        String[] terms = vector.getTerms();
+        int freqs[]=vector.getTermFrequencies();
+        for (int j = 0; j < terms.length; j++) {
+            String term = terms[j];
-			if(isNoiseWord(term)){
-				continue;
-			}
-		    // increment frequency
-		    Int cnt = (Int) termFreqMap.get(term);
-		    if (cnt == null) {
-		    	cnt=new Int();
-				termFreqMap.put(term, cnt);
-				cnt.x=freqs[j];
-		    }
-		    else {
-		        cnt.x+=freqs[j];
-		    }
-		}
-	}
-	/**
-	 * Adds term frequencies found by tokenizing text from reader into the Map words
-	 * @param r a source of text to be tokenized
-	 * @param termFreqMap a Map of terms and their frequencies
-	 * @param fieldName Used by analyzer for any special per-field analysis
-	 */
-	private void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)
-		throws IOException
-	{
-		   TokenStream ts = analyzer.tokenStream(fieldName, r);
-			org.apache.lucene.analysis.Token token;
-			int tokenCount=0;
-			while ((token = ts.next()) != null) { // for every token
-				String word = token.termText();
-				tokenCount++;
-				if(tokenCount>maxNumTokensParsed)
-				{
-					break;
-				}
-				if(isNoiseWord(word)){
-					continue;
-				}
+            if(isNoiseWord(term)){
+                continue;
+            }
+            // increment frequency
+            Int cnt = (Int) termFreqMap.get(term);
+            if (cnt == null) {
+                cnt=new Int();
+                termFreqMap.put(term, cnt);
+                cnt.x=freqs[j];
+            }
+            else {
+                cnt.x+=freqs[j];
+            }
+        }
+    }
-				// increment frequency
-				Int cnt = (Int) termFreqMap.get(word);
-				if (cnt == null) {
-					termFreqMap.put(word, new Int());
-				}
-				else {
-					cnt.x++;
-				}
-			}
-	}
+    /**
+     * Adds term frequencies found by tokenizing text from reader into the Map words
+     * @param r a source of text to be tokenized
+     * @param termFreqMap a Map of terms and their frequencies
+     * @param fieldName Used by analyzer for any special per-field analysis
+     */
+    private void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)
+            throws IOException {
+        TokenStream ts = analyzer.tokenStream(fieldName, r);
+        org.apache.lucene.analysis.Token token;
+        int tokenCount = 0;
+        while ((token = ts.next()) != null) { // for every token
+            String word = token.termText();
+            tokenCount++;
+            if (tokenCount > maxNumTokensParsed) {
+                break;
+            }
+            if (isNoiseWord(word)) {
+                continue;
+            }
+            // increment frequency
+            Int cnt = (Int) termFreqMap.get(word);
+            if (cnt == null) {
+                termFreqMap.put(word, new Int());
+            }
+            else {
+                cnt.x++;
+            }
+        }
+    }
-	/** determines if the passed term is likely to be of interest in "more like" comparisons
-	 *
-	 * @param term The word being considered
-	 * @return true if should be ignored, false if should be used in further analysis
-	 */
-	private boolean isNoiseWord(String term)
-	{
-		int len = term.length();
-		if (minWordLen > 0 && len < minWordLen) {
-			return true;
-		}
-		if (maxWordLen > 0 && len > maxWordLen) {
-			return true;
-		}
-		if (stopWords != null && stopWords.contains( term)) {
-			return true;
-		}
-		return false;
-	}
+    /** determines if the passed term is likely to be of interest in "more like" comparisons
+     *
+     * @param term The word being considered
+     * @return true if should be ignored, false if should be used in further analysis
+     */
+    private boolean isNoiseWord(String term) {
+        int len = term.length();
+        if (minWordLen > 0 && len < minWordLen) {
+            return true;
+        }
+        if (maxWordLen > 0 && len > maxWordLen) {
+            return true;
+        }
+        if (stopWords != null && stopWords.contains( term)) {
+            return true;
+        }
+        return false;
+    }
-	 * The result is a priority queue of arrays with one entry for <b>every word</b> in the document.
-	 * Each array has 6 elements.
-	 * The elements are:
-	 * <ol>
-	 * <li> The word (String)
-	 * <li> The top field that this word comes from (String)
-	 * <li> The score for this word (Float)
-	 * <li> The IDF value (Float)
-	 * <li> The frequency of this word in the index (Integer)
-	 * <li> The frequency of this word in the source document (Integer)
-	 * </ol>
-	 * This is a somewhat "advanced" routine, and in general only the 1st entry in the array is of interest.
-	 * This method is exposed so that you can identify the "interesting words" in a document.
-	 * For an easier method to call see {@link #retrieveInterestingTerms retrieveInterestingTerms()}.
+     * The result is a priority queue of arrays with one entry for <b>every word</b> in the document.
+     * Each array has 6 elements.
+     * The elements are:
+     * <ol>
+     * <li> The word (String)
+     * <li> The top field that this word comes from (String)
+     * <li> The score for this word (Float)
+     * <li> The IDF value (Float)
+     * <li> The frequency of this word in the index (Integer)
+     * <li> The frequency of this word in the source document (Integer)
+     * </ol>
+     * This is a somewhat "advanced" routine, and in general only the 1st entry in the array is of interest.
+     * This method is exposed so that you can identify the "interesting words" in a document.
+     * For an easier method to call see {@link #retrieveInterestingTerms retrieveInterestingTerms()}.
-	 * @return the most intresting words in the document ordered by score, with the highest scoring, or best entry, first
-	 *
-	 * @see #retrieveInterestingTerms
+     * @return the most intresting words in the document ordered by score, with the highest scoring, or best entry, first
+     *
+     * @see #retrieveInterestingTerms
-			addTermFrequencies(r, words, fieldName);
+            addTermFrequencies(r, words, fieldName);
-	/**
-	 * Convenience routine to make it easy to return the most interesting words in a document.
-	 * More advanced users will call {@link #retrieveTerms(java.io.Reader) retrieveTerms()} directly.
-	 * @param r the source document
-	 * @return the most interesting words in the document
-	 *
-	 * @see #retrieveTerms(java.io.Reader)
-	 * @see #setMaxQueryTerms
-	 */
-	public String[] retrieveInterestingTerms( Reader r) throws IOException {
-		ArrayList al = new ArrayList( maxQueryTerms);
-		PriorityQueue pq = retrieveTerms( r);
-		Object cur;
-		int lim = maxQueryTerms; // have to be careful, retrieveTerms returns all words but that's probably not useful to our caller...
-		// we just want to return the top words
-		while (((cur = pq.pop()) != null) && lim-- > 0) {
+    /**
+     * Convenience routine to make it easy to return the most interesting words in a document.
+     * More advanced users will call {@link #retrieveTerms(java.io.Reader) retrieveTerms()} directly.
+     * @param r the source document
+     * @return the most interesting words in the document
+     *
+     * @see #retrieveTerms(java.io.Reader)
+     * @see #setMaxQueryTerms
+     */
+    public String[] retrieveInterestingTerms( Reader r) throws IOException {
+        ArrayList al = new ArrayList( maxQueryTerms);
+        PriorityQueue pq = retrieveTerms( r);
+        int lim = maxQueryTerms;
+        // have to be careful, retrieveTerms returns all words
+        // but that's probably not useful to our caller...
+        // we just want to return the top words
+        for (Object cur = pq.pop(); cur != null && lim-- > 0; cur = pq.pop()) {
-			al.add( ar[ 0]); // the 1st entry is the interesting word
-		}
-		String[] res = new String[ al.size()];
-		return (String[]) al.toArray( res);
-	}
+            al.add(ar[0]); // the 1st entry is the interesting word
+        }
+        return (String[]) al.toArray(new String[al.size()]);
+    }

INS24 INS58 INS27 MOV7 MOV8 MOV43 INS59 INS27 MOV27 MOV5 INS8 MOV42 INS32 INS42 MOV33 MOV3 MOV21 INS42 INS42 MOV5 DEL43 DEL85 DEL59 DEL60 DEL36 DEL27 DEL36 DEL27 DEL61 DEL42 DEL59 DEL60 DEL42