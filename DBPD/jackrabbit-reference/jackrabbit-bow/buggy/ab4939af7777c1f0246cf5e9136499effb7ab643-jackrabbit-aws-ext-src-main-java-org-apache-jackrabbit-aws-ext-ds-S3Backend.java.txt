JCR-3729 - S3 Datastore optimizations


git-svn-id: https://svn.apache.org/repos/asf/jackrabbit/trunk@1577127 13f79535-47bb-0310-9956-ffa450edef68

+import java.util.Collections;
+import java.util.concurrent.ExecutorService;
-import java.util.concurrent.ThreadFactory;
+import java.util.concurrent.TimeUnit;
+import org.apache.jackrabbit.core.data.AsyncUploadCallback;
+import org.apache.jackrabbit.core.data.util.NamedThreadFactory;
+import com.amazonaws.services.s3.model.ProgressEvent;
+import com.amazonaws.services.s3.model.ProgressListener;
+    /**
+     * Logger instance.
+     */
+    private static final Logger LOG = LoggerFactory.getLogger(S3Backend.class);
+
-
-    /**
-     * Logger instance.
-     */
-    private static final Logger LOG = LoggerFactory.getLogger(S3Backend.class);
-
+    
+    private Properties prop;
+
+    private Date startTime;
+
+        ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
-            Properties prop = Utils.readConfig(config);
-            LOG.debug("init");
+            startTime = new Date();
+            Thread.currentThread().setContextClassLoader(
+                getClass().getClassLoader());
+            prop = Utils.readConfig(config);
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("init");
+            }
-            bucket = prop.getProperty(S3Constants.S3_BUCKET);
+            if (bucket == null || "".equals(bucket.trim())) {
+                bucket = prop.getProperty(S3Constants.S3_BUCKET);
+            }
+            String propEndPoint = prop.getProperty(S3Constants.S3_END_POINT);
+            if (propEndPoint != null & !"".equals(propEndPoint)) {
+                endpoint = propEndPoint;
+            }
-            tmx = new TransferManager(s3service, createDefaultExecutorService());
-            LOG.debug("  done");
+
+            int writeThreads = 10;
+            String writeThreadsStr = prop.getProperty(S3Constants.S3_WRITE_THREADS);
+            if (writeThreadsStr != null) {
+                writeThreads = Integer.parseInt(writeThreadsStr);
+            }
+            LOG.info("Using thread pool of [" + writeThreads
+                + "] threads in S3 transfer manager");
+            tmx = new TransferManager(s3service,
+                (ThreadPoolExecutor) Executors.newFixedThreadPool(writeThreads,
+                    new NamedThreadFactory("s3-transfer-manager-worker")));
+            String renameKeyProp = prop.getProperty(S3Constants.S3_RENAME_KEYS);
+            boolean renameKeyBool = (renameKeyProp == null || "".equals(renameKeyProp))
+                    ? true
+                    : Boolean.parseBoolean(renameKeyProp);
+            if (renameKeyBool) {
+                renameKeys();
+            }
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("S3 Backend initialized in ["
+                    + (System.currentTimeMillis() - startTime.getTime())
+                    + "] ms");
+            }
-            LOG.debug("  error ", e);
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("  error ", e);
+            }
+        } finally {
+            if (contextClassLoader != null) {
+                Thread.currentThread().setContextClassLoader(contextClassLoader);
+            }
+        this.write(identifier, file, false, null);
+
+    }
+
+    @Override
+    public void writeAsync(DataIdentifier identifier, File file,
+            AsyncUploadCallback callback) throws DataStoreException {
+        if (callback == null) {
+            throw new IllegalArgumentException(
+                "callback parameter cannot be null in asyncUpload");
+        }
+        Thread th = new Thread(new AsyncUploadJob(identifier, file, callback));
+        th.start();
+    }
+
+    /**
+     * Check if record identified by identifier exists in Amazon S3.
+     */
+    @Override
+    public boolean exists(DataIdentifier identifier) throws DataStoreException {
+        long start = System.currentTimeMillis();
+        String key = getKeyName(identifier);
+        ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
+        try {
+            Thread.currentThread().setContextClassLoader(
+                getClass().getClassLoader());
+            ObjectMetadata objectMetaData = s3service.getObjectMetadata(bucket,
+                key);
+            if (objectMetaData != null) {
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug("exists [" + identifier + "]: [true] took ["
+                        + (System.currentTimeMillis() - start) + "] ms");
+                }
+                return true;
+            }
+            return false;
+        } catch (AmazonServiceException e) {
+            if (e.getStatusCode() == 404) {
+                LOG.info("exists [" + identifier + "]: [false] took ["
+                    + (System.currentTimeMillis() - start) + "] ms");
+                return false;
+            }
+            throw new DataStoreException(
+                "Error occured to getObjectMetadata for key ["
+                    + identifier.toString() + "]", e);
+        } finally {
+            if (contextClassLoader != null) {
+                Thread.currentThread().setContextClassLoader(contextClassLoader);
+            }
+        }
+    }
+
+    @Override
+    public boolean exists(DataIdentifier identifier, boolean touch)
+            throws DataStoreException {
+        long start = System.currentTimeMillis();
+        String key = getKeyName(identifier);
+        ObjectMetadata objectMetaData = null;
+        boolean retVal = false;
+        ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
+        try {
+            Thread.currentThread().setContextClassLoader(
+                getClass().getClassLoader());
+            objectMetaData = s3service.getObjectMetadata(bucket, key);
+            if (objectMetaData != null) {
+                retVal = true;
+                if (touch) {
+                    CopyObjectRequest copReq = new CopyObjectRequest(bucket,
+                        key, bucket, key);
+                    copReq.setNewObjectMetadata(objectMetaData);
+                    s3service.copyObject(copReq);
+                    if (LOG.isDebugEnabled()) {
+                        LOG.debug("[ " + identifier.toString()
+                            + "] touched took ["
+                            + (System.currentTimeMillis() - start) + "] ms");
+                    }
+                }
+            } else {
+                retVal = false;
+            }
+
+        } catch (AmazonServiceException e) {
+            if (e.getStatusCode() == 404) {
+                retVal = false;
+            } else {
+                throw new DataStoreException(
+                    "Error occured to find exists for key ["
+                        + identifier.toString() + "]", e);
+            }
+        } catch (Exception e) {
+            throw new DataStoreException(
+                "Error occured to find exists for key  "
+                    + identifier.toString(), e);
+        } finally {
+            if (contextClassLoader != null) {
+                Thread.currentThread().setContextClassLoader(contextClassLoader);
+            }
+        }
+
+        if (LOG.isDebugEnabled()) {
+            LOG.debug("exists [" + identifier + "]: [" + retVal + "] took ["
+                + (System.currentTimeMillis() - start) + "] ms");
+        }
+        return retVal;
+    }
+
+    @Override
+    public InputStream read(DataIdentifier identifier)
+            throws DataStoreException {
+        long start = System.currentTimeMillis();
+        String key = getKeyName(identifier);
+        ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
+        try {
+            Thread.currentThread().setContextClassLoader(
+                getClass().getClassLoader());
+            S3Object object = s3service.getObject(bucket, key);
+            InputStream in = object.getObjectContent();
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("[ " + identifier.toString() + "] read took ["
+                    + (System.currentTimeMillis() - start) + "] ms");
+            }
+            return in;
+        } catch (AmazonServiceException e) {
+            throw new DataStoreException("Object not found: " + key, e);
+        } finally {
+            if (contextClassLoader != null) {
+                Thread.currentThread().setContextClassLoader(contextClassLoader);
+            }
+        }
+    }
+
+    @Override
+    public Iterator<DataIdentifier> getAllIdentifiers()
+            throws DataStoreException {
+        long start = System.currentTimeMillis();
+        ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
+        try {
+            Thread.currentThread().setContextClassLoader(
+                getClass().getClassLoader());
+            Set<DataIdentifier> ids = new HashSet<DataIdentifier>();
+            ObjectListing prevObjectListing = s3service.listObjects(bucket);
+            while (true) {
+                for (S3ObjectSummary s3ObjSumm : prevObjectListing.getObjectSummaries()) {
+                    String id = getIdentifierName(s3ObjSumm.getKey());
+                    if (id != null) {
+                        ids.add(new DataIdentifier(id));
+                    }
+                }
+                if (!prevObjectListing.isTruncated()) break;
+                prevObjectListing = s3service.listNextBatchOfObjects(prevObjectListing);
+            }
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("getAllIdentifiers returned size [ " + ids.size()
+                    + "] took [" + (System.currentTimeMillis() - start)
+                    + "] ms");
+            }
+            return ids.iterator();
+        } catch (AmazonServiceException e) {
+            throw new DataStoreException("Could not list objects", e);
+        } finally {
+            if (contextClassLoader != null) {
+                Thread.currentThread().setContextClassLoader(contextClassLoader);
+            }
+        }
+    }
+
+    @Override
+    public long getLastModified(DataIdentifier identifier)
+            throws DataStoreException {
+        long start = System.currentTimeMillis();
+        String key = getKeyName(identifier);
+        ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
+        try {
+            Thread.currentThread().setContextClassLoader(
+                getClass().getClassLoader());
+            ObjectMetadata object = s3service.getObjectMetadata(bucket, key);
+            long lastModified = object.getLastModified().getTime();
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("Identifier [" + identifier.toString()
+                    + "] 's lastModified = [" + lastModified + "] took ["
+                    + (System.currentTimeMillis() - start) + "] ms");
+            }
+            return lastModified;
+        } catch (AmazonServiceException e) {
+            if (e.getStatusCode() == 404) {
+                LOG.info("getLastModified:Identifier [" + identifier.toString()
+                    + "] not found. Took ["
+                    + (System.currentTimeMillis() - start) + "]ms");
+            }
+            throw new DataStoreException(e);
+        } finally {
+            if (contextClassLoader != null) {
+                Thread.currentThread().setContextClassLoader(contextClassLoader);
+            }
+        }
+    }
+
+    @Override
+    public long getLength(DataIdentifier identifier) throws DataStoreException {
+        long start = System.currentTimeMillis();
+        String key = getKeyName(identifier);
+        ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
+        try {
+            Thread.currentThread().setContextClassLoader(
+                getClass().getClassLoader());
+            ObjectMetadata object = s3service.getObjectMetadata(bucket, key);
+            long length = object.getContentLength();
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("Identifier [" + identifier.toString()
+                    + "] 's length = [" + length + "] took ["
+                    + (System.currentTimeMillis() - start) + "] ms");
+            }
+            return length;
+        } catch (AmazonServiceException e) {
+            throw new DataStoreException("Could not length of dataIdentifier "
+                + identifier, e);
+        } finally {
+            if (contextClassLoader != null) {
+                Thread.currentThread().setContextClassLoader(contextClassLoader);
+            }
+        }
+    }
+
+    @Override
+    public void deleteRecord(DataIdentifier identifier)
+            throws DataStoreException {
+        long start = System.currentTimeMillis();
+        String key = getKeyName(identifier);
+        ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
+        try {
+            Thread.currentThread().setContextClassLoader(
+                getClass().getClassLoader());
+            s3service.deleteObject(bucket, key);
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("Identifier [" + identifier.toString()
+                    + "] 's deleted. It took ["
+                    + (System.currentTimeMillis() - start) + "] ms");
+            }
+        } catch (AmazonServiceException e) {
+            throw new DataStoreException(
+                "Could not getLastModified of dataIdentifier " + identifier, e);
+        } finally {
+            if (contextClassLoader != null) {
+                Thread.currentThread().setContextClassLoader(contextClassLoader);
+            }
+        }
+    }
+
+    @Override
+    public Set<DataIdentifier> deleteAllOlderThan(long min)
+            throws DataStoreException {
+        long start = System.currentTimeMillis();
+        // S3 stores lastModified to lower boundary of timestamp in ms.
+        // and hence min is reduced by 1000ms.
+        min = min - 1000;
+        Set<DataIdentifier> deleteIdSet = new HashSet<DataIdentifier>(30);
+        ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
+        try {
+            Thread.currentThread().setContextClassLoader(
+                getClass().getClassLoader());
+            ObjectListing prevObjectListing = s3service.listObjects(bucket);
+            while (true) {
+                List<DeleteObjectsRequest.KeyVersion> deleteList = new ArrayList<DeleteObjectsRequest.KeyVersion>();
+                for (S3ObjectSummary s3ObjSumm : prevObjectListing.getObjectSummaries()) {
+                    DataIdentifier identifier = new DataIdentifier(
+                        getIdentifierName(s3ObjSumm.getKey()));
+                    long lastModified = s3ObjSumm.getLastModified().getTime();
+                    if (LOG.isDebugEnabled()) {
+                        LOG.debug("id [" + identifier + "], lastModified ["
+                            + lastModified + "]");
+                    }
+                    if (!store.isInUse(identifier) && lastModified < min) {
+                        if (LOG.isDebugEnabled()) {
+                            LOG.debug("add id :" + s3ObjSumm.getKey()
+                                + " to delete lists");
+                        }
+                        deleteList.add(new DeleteObjectsRequest.KeyVersion(
+                            s3ObjSumm.getKey()));
+                        deleteIdSet.add(identifier);
+                    }
+                }
+                if (deleteList.size() > 0) {
+                    DeleteObjectsRequest delObjsReq = new DeleteObjectsRequest(
+                        bucket);
+                    delObjsReq.setKeys(deleteList);
+                    DeleteObjectsResult dobjs = s3service.deleteObjects(delObjsReq);
+                    if (dobjs.getDeletedObjects().size() != deleteList.size()) {
+                        throw new DataStoreException(
+                            "Incomplete delete object request. only  "
+                                + dobjs.getDeletedObjects().size() + " out of "
+                                + deleteList.size() + " are deleted");
+                    } else {
+                        if (LOG.isDebugEnabled()) {
+                            LOG.debug(deleteList
+                                + " records deleted from datastore");
+                        }
+                    }
+                }
+                if (!prevObjectListing.isTruncated()) {
+                    break;
+                }
+                prevObjectListing = s3service.listNextBatchOfObjects(prevObjectListing);
+            }
+        } finally {
+            if (contextClassLoader != null) {
+                Thread.currentThread().setContextClassLoader(contextClassLoader);
+            }
+        }
+        LOG.info("deleteAllOlderThan: min=[" + min + "] exit. Deleted ["
+            + deleteIdSet + "] records. Number of records deleted ["
+            + deleteIdSet.size() + "] took ["
+            + (System.currentTimeMillis() - start) + "] ms");
+        return deleteIdSet;
+    }
+
+    @Override
+    public void close() {
+        // backend is closing. abort all mulitpart uploads from start.
+        tmx.abortMultipartUploads(bucket, startTime);
+        tmx.shutdownNow();
+        s3service.shutdown();
+        LOG.info("S3Backend closed.");
+    }
+
+    public String getBucket() {
+        return bucket;
+    }
+
+    public void setBucket(String bucket) {
+        this.bucket = bucket;
+    }
+
+    private void write(DataIdentifier identifier, File file,
+            boolean asyncUpload, AsyncUploadCallback callback)
+            throws DataStoreException {
-        LOG.debug("write {0} length {1}", identifier, file.length());
+        ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
+            Thread.currentThread().setContextClassLoader(
+                getClass().getClassLoader());
-                LOG.debug(key + "   exists");
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug(key + "   exists, lastmodified ="
+                        + objectMetaData.getLastModified().getTime());
+                }
-                LOG.debug("lastModified of " + identifier.toString()
-                    + " updated successfully");
-                LOG.debug("   updated");
-            }
-        } catch (AmazonServiceException e) {
-            LOG.debug("   does not exist", e);
-            // not found - create it
-        }
-        if (objectMetaData == null) {
-            LOG.debug("   creating");
-            try {
-                // start multipart parallel upload using amazon sdk
-                Upload up = tmx.upload(new PutObjectRequest(bucket, key, file));
-                // wait for upload to finish
-                up.waitForUploadResult();
-                LOG.debug("   done");
-            } catch (Exception e2) {
-                LOG.debug("   could not upload", e2);
-                throw new DataStoreException("Could not upload " + key, e2);
-            }
-        }
-        LOG.debug("    ms: {0}", System.currentTimeMillis() - start);
-
-    }
-
-    /**
-     * Check if record identified by identifier exists in Amazon S3.
-     */
-    @Override
-    public boolean exists(DataIdentifier identifier) throws DataStoreException {
-        String key = getKeyName(identifier);
-        try {
-            LOG.debug("exists {0}", identifier);
-            ObjectMetadata objectMetaData = s3service.getObjectMetadata(bucket,
-                key);
-            if (objectMetaData != null) {
-                LOG.debug("  true");
-                return true;
-            }
-            return false;
-        } catch (AmazonServiceException e) {
-            if (e.getStatusCode() == 404) {
-                LOG.info("key [" + identifier.toString() + "] not found.");
-                return false;
-            }
-            throw new DataStoreException(
-                "Error occured to getObjectMetadata for key ["
-                    + identifier.toString() + "]", e);
-        }
-    }
-
-    @Override
-    public void touch(DataIdentifier identifier, long minModifiedDate)
-            throws DataStoreException {
-        String key = getKeyName(identifier);
-        try {
-            if (minModifiedDate != 0) {
-                ObjectMetadata objectMetaData = s3service.getObjectMetadata(
-                    bucket, key);
-                if (objectMetaData.getLastModified().getTime() < minModifiedDate) {
-                    CopyObjectRequest copReq = new CopyObjectRequest(bucket,
-                        key, bucket, key);
-                    copReq.setNewObjectMetadata(objectMetaData);
-                    s3service.copyObject(copReq);
+                if (LOG.isDebugEnabled()) {
+                if (callback != null) {
+                    callback.call(identifier, file,
+                        AsyncUploadCallback.RESULT.SUCCESS);
+                }
-        } catch (Exception e) {
-            throw new DataStoreException(
-                "An Exception occurred while trying to set the last modified date of record "
-                    + identifier.toString(), e);
+
+            if (objectMetaData == null) {
+                try {
+                    // start multipart parallel upload using amazon sdk
+                    Upload up = tmx.upload(new PutObjectRequest(bucket, key,
+                        file));
+                    // wait for upload to finish
+                    if (asyncUpload) {
+                        up.addProgressListener(new S3UploadProgressListener(
+                            identifier, file, callback));
+                        if (LOG.isDebugEnabled()) {
+                            LOG.debug("added upload progress listener to identifier ["
+                                + identifier + "]");
+                        }
+                    } else {
+                        up.waitForUploadResult();
+                        if (LOG.isDebugEnabled()) {
+                            LOG.debug("synchronous upload to identifier ["
+                                + identifier + "] completed.");
+                        }
+                        if (callback != null) {
+                            callback.call(identifier, file,
+                                AsyncUploadCallback.RESULT.SUCCESS);
+                        }
+                    }
+                } catch (Exception e2) {
+                    if (!asyncUpload) {
+                        callback.call(identifier, file,
+                            AsyncUploadCallback.RESULT.ABORTED);
+                    }
+                    throw new DataStoreException("Could not upload " + key, e2);
+                }
+            }
+        } finally {
+            if (contextClassLoader != null) {
+                Thread.currentThread().setContextClassLoader(contextClassLoader);
+            }
+        }
+        if (LOG.isDebugEnabled()) {
+            LOG.debug("write [" + identifier + "] length [" + file.length()
+                + "], in async mode [" + asyncUpload + "] in ["
+                + (System.currentTimeMillis() - start) + "] ms.");
-    @Override
-    public InputStream read(DataIdentifier identifier)
-            throws DataStoreException {
-        String key = getKeyName(identifier);
+    /**
+     * This method rename object keys in S3 concurrently. The number of
+     * concurrent threads is defined by 'maxConnections' property in
+     * aws.properties. As S3 doesn't have "move" command, this method simulate
+     * move as copy object object to new key and then delete older key.
+     */
+    private void renameKeys() throws DataStoreException {
+        long startTime = System.currentTimeMillis();
+        ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
+        long count = 0;
-            LOG.debug("read {" + identifier + "}");
-            S3Object object = s3service.getObject(bucket, key);
-            InputStream in = object.getObjectContent();
-            LOG.debug("  return");
-            return in;
-        } catch (AmazonServiceException e) {
-            throw new DataStoreException("Object not found: " + key, e);
-        }
-    }
-
-    @Override
-    public Iterator<DataIdentifier> getAllIdentifiers()
-            throws DataStoreException {
-        try {
-            LOG.debug("getAllIdentifiers");
-            Set<DataIdentifier> ids = new HashSet<DataIdentifier>();
+            Thread.currentThread().setContextClassLoader(
+                getClass().getClassLoader());
+            List<DeleteObjectsRequest.KeyVersion> deleteList = new ArrayList<DeleteObjectsRequest.KeyVersion>();
+            int nThreads = Integer.parseInt(prop.getProperty("maxConnections"));
+            ExecutorService executor = Executors.newFixedThreadPool(nThreads,
+                new NamedThreadFactory("s3-object-rename-worker"));
+            boolean taskAdded = false;
-                    String id = getIdentifierName(s3ObjSumm.getKey());
-                    if (id != null) {
-                        ids.add(new DataIdentifier(id));
-                    }
-                }
-                if (!prevObjectListing.isTruncated()) {
-                    break;
-                }
-                prevObjectListing = s3service.listNextBatchOfObjects(prevObjectListing);
-            }
-            LOG.debug("  return");
-            return ids.iterator();
-        } catch (AmazonServiceException e) {
-            throw new DataStoreException("Could not list objects", e);
-        }
-    }
-
-    @Override
-    public long getLastModified(DataIdentifier identifier)
-            throws DataStoreException {
-        String key = getKeyName(identifier);
-        try {
-            ObjectMetadata object = s3service.getObjectMetadata(bucket, key);
-            return object.getLastModified().getTime();
-        } catch (AmazonServiceException e) {
-            throw new DataStoreException(
-                "Could not getLastModified of dataIdentifier " + identifier, e);
-        }
-    }
-
-    @Override
-    public long getLength(DataIdentifier identifier) throws DataStoreException {
-        String key = getKeyName(identifier);
-        try {
-            ObjectMetadata object = s3service.getObjectMetadata(bucket, key);
-            return object.getContentLength();
-        } catch (AmazonServiceException e) {
-            throw new DataStoreException("Could not length of dataIdentifier "
-                + identifier, e);
-        }
-    }
-
-    @Override
-    public void deleteRecord(DataIdentifier identifier)
-            throws DataStoreException {
-        String key = getKeyName(identifier);
-        try {
-            s3service.deleteObject(bucket, key);
-        } catch (AmazonServiceException e) {
-            throw new DataStoreException(
-                "Could not getLastModified of dataIdentifier " + identifier, e);
-        }
-    }
-
-    @Override
-    public List<DataIdentifier> deleteAllOlderThan(long min)
-            throws DataStoreException {
-        LOG.info("deleteAllOlderThan " + new Date(min));
-        List<DataIdentifier> diDeleteList = new ArrayList<DataIdentifier>(30);
-        ObjectListing prevObjectListing = s3service.listObjects(bucket);
-        while (true) {
-            List<DeleteObjectsRequest.KeyVersion> deleteList = new ArrayList<DeleteObjectsRequest.KeyVersion>();
-            for (S3ObjectSummary s3ObjSumm : prevObjectListing.getObjectSummaries()) {
-                DataIdentifier identifier = new DataIdentifier(
-                    getIdentifierName(s3ObjSumm.getKey()));
-                if (!store.isInUse(identifier)
-                    && s3ObjSumm.getLastModified().getTime() < min) {
-                    LOG.info("add id :" + s3ObjSumm.getKey()
-                        + " to delete lists");
+                    executor.execute(new KeyRenameThread(s3ObjSumm.getKey()));
+                    taskAdded = true;
+                    count++;
-                    diDeleteList.add(new DataIdentifier(
-                        getIdentifierName(s3ObjSumm.getKey())));
+                if (!prevObjectListing.isTruncated()) break;
+                prevObjectListing = s3service.listNextBatchOfObjects(prevObjectListing);
+            // This will make the executor accept no new threads
+            // and finish all existing threads in the queue
+            executor.shutdown();
+
+            try {
+                // Wait until all threads are finish
+                while (taskAdded
+                    && !executor.awaitTermination(10, TimeUnit.SECONDS)) {
+                    LOG.info("Rename S3 keys tasks timedout. Waiting again");
+                }
+            } catch (InterruptedException ie) {
+
+            }
+            LOG.info("Renamed [" + count + "] keys, time taken ["
+                + ((System.currentTimeMillis() - startTime) / 1000) + "] sec");
+            // Delete older keys.
-                delObjsReq.setKeys(deleteList);
-                DeleteObjectsResult dobjs = s3service.deleteObjects(delObjsReq);
-                if (dobjs.getDeletedObjects().size() != deleteList.size()) {
-                    throw new DataStoreException(
-                        "Incomplete delete object request. only  "
-                            + dobjs.getDeletedObjects().size() + " out of "
-                            + deleteList.size() + " are deleted");
+                int batchSize = 500, startIndex = 0, size = deleteList.size();
+                int endIndex = batchSize < size ? batchSize : size;
+                while (endIndex <= size) {
+                    delObjsReq.setKeys(Collections.unmodifiableList(deleteList.subList(
+                        startIndex, endIndex)));
+                    DeleteObjectsResult dobjs = s3service.deleteObjects(delObjsReq);
+                    LOG.info("Records[" + dobjs.getDeletedObjects().size()
+                        + "] deleted in datastore from index [" + startIndex
+                        + "] to [" + (endIndex - 1) + "]");
+                    if (endIndex == size) {
+                        break;
+                    } else {
+                        startIndex = endIndex;
+                        endIndex = (startIndex + batchSize) < size
+                                ? (startIndex + batchSize)
+                                : size;
+                    }
-                LOG.info(deleteList.size() + " records deleted from datastore");
-            if (!prevObjectListing.isTruncated()) {
-                break;
+        } finally {
+            if (contextClassLoader != null) {
+                Thread.currentThread().setContextClassLoader(contextClassLoader);
-            prevObjectListing = s3service.listNextBatchOfObjects(prevObjectListing);
-        LOG.info("deleteAllOlderThan  exit");
-        return diDeleteList;
-    @Override
-    public void close() {
-        s3service.shutdown();
-        s3service = null;
-        tmx = null;
+    /**
+     * The method convert old key format to new format. For e.g. this method
+     * converts old key dataStore_004cb70c8f87d78f04da41e7547cb434094089ea to
+     * 004c-b70c8f87d78f04da41e7547cb434094089ea.
+     */
+    private static String convertKey(String oldKey)
+            throws IllegalArgumentException {
+        if (!oldKey.startsWith(KEY_PREFIX)) {
+            throw new IllegalArgumentException("[" + oldKey
+                + "] doesn't start with prefix [" + KEY_PREFIX + "]");
+        }
+        String key = oldKey.substring(KEY_PREFIX.length());
+        return key.substring(0, 4) + DASH + key.substring(4);
-        return KEY_PREFIX + identifier.toString();
+        String key = identifier.toString();
+        return key.substring(0, 4) + DASH + key.substring(4);
-        if (!key.startsWith(KEY_PREFIX)) {
+        if (!key.contains(DASH)) {
-        return key.substring(KEY_PREFIX.length());
+        return key.substring(0, 4) + key.substring(5);
-     * Returns a new thread pool configured with the default settings.
-     * 
-     * @return A new thread pool configured with the default settings.
+     * The class renames object key in S3 in a thread.
-    private ThreadPoolExecutor createDefaultExecutorService() {
-        ThreadFactory threadFactory = new ThreadFactory() {
-            private int threadCount = 1;
+    private class KeyRenameThread implements Runnable {
-            @Override
-            public Thread newThread(Runnable r) {
-                Thread thread = new Thread(r);
-                thread.setContextClassLoader(getClass().getClassLoader());
-                thread.setName("s3-transfer-manager-worker-" + threadCount++);
-                return thread;
+        private String oldKey;
+
+        public void run() {
+            ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
+            try {
+                Thread.currentThread().setContextClassLoader(
+                    getClass().getClassLoader());
+                String newS3Key = convertKey(oldKey);
+                CopyObjectRequest copReq = new CopyObjectRequest(bucket,
+                    oldKey, bucket, newS3Key);
+                s3service.copyObject(copReq);
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug(oldKey + " renamed to " + newS3Key);
+                }
+            } finally {
+                if (contextClassLoader != null) {
+                    Thread.currentThread().setContextClassLoader(
+                        contextClassLoader);
+                }
-        };
-        return (ThreadPoolExecutor) Executors.newFixedThreadPool(10,
-            threadFactory);
+        }
+
+        public KeyRenameThread(String oldKey) {
+            this.oldKey = oldKey;
+        }
+    }
+
+    /**
+     * Listener which receives callback on status of S3 upload.
+     */
+    private class S3UploadProgressListener implements ProgressListener {
+
+        private File file;
+
+        private DataIdentifier identifier;
+
+        private AsyncUploadCallback callback;
+
+        public S3UploadProgressListener(DataIdentifier identifier, File file,
+                AsyncUploadCallback callback) {
+            super();
+            this.identifier = identifier;
+            this.file = file;
+            this.callback = callback;
+        }
+
+        public void progressChanged(ProgressEvent progressEvent) {
+            switch (progressEvent.getEventCode()) {
+                case ProgressEvent.COMPLETED_EVENT_CODE:
+                    callback.call(identifier, file,
+                        AsyncUploadCallback.RESULT.SUCCESS);
+                    break;
+                case ProgressEvent.FAILED_EVENT_CODE:
+                    callback.call(identifier, file,
+                        AsyncUploadCallback.RESULT.FAILED);
+                    break;
+                default:
+                    break;
+            }
+        }
+    }
+
+    /**
+     * This class implements {@link Runnable} interface to upload {@link File}
+     * to S3 asynchronously.
+     */
+    private class AsyncUploadJob implements Runnable {
+
+        private DataIdentifier identifier;
+
+        private File file;
+
+        private AsyncUploadCallback callback;
+
+        public AsyncUploadJob(DataIdentifier identifier, File file,
+                AsyncUploadCallback callback) {
+            super();
+            this.identifier = identifier;
+            this.file = file;
+            this.callback = callback;
+        }
+
+        public void run() {
+            try {
+                write(identifier, file, true, callback);
+            } catch (DataStoreException e) {
+                LOG.error("Could not upload [" + identifier + "], file[" + file
+                    + "]", e);
+            }
+
+        }
