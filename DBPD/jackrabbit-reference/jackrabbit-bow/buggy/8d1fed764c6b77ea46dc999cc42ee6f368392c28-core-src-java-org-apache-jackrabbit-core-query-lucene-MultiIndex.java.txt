JCR-178: Allow concurrent index updates and queries

git-svn-id: https://svn.apache.org/repos/asf/incubator/jackrabbit/trunk@232801 13f79535-47bb-0310-9956-ffa450edef68

+import org.apache.commons.collections.iterators.EmptyIterator;
-import java.util.HashSet;
-import java.util.Set;
+import java.util.Arrays;
- * <p/>
- * This class is not thread-safe. Clients of this class must ensure
- * synchronization of multiple threads. The following conditions must hold true:
- * <ul>
- * <li>Only one thread may use {@link #addDocument(org.apache.lucene.document.Document)}
- * or {@link #removeDocument(org.apache.lucene.index.Term)} at a time.</li>
- * <li>While a thread uses the <code>IndexReader</code> returned by
- * {@link #getIndexReader()} other threads must not call {@link #addDocument(org.apache.lucene.document.Document)}
- * or {@link #removeDocument(org.apache.lucene.index.Term)}</li>
- * <li>Multiple threads may use the <code>IndexReader</code> returned by
- * {@link #getIndexReader()}</li>
- * </ul>
-    private final IndexInfos indexNames = new IndexInfos();
+    private final IndexInfos indexNames = new IndexInfos("indexes");
+
+    /**
+     * Names of index directories that can be deleted.
+     */
+    private final IndexInfos deletable = new IndexInfos("deletable");
+     * Flag indicating whether an update operation is in progress.
+     */
+    private boolean updateInProgress = false;
+
+    /**
-    private IndexReader multiReader;
+    private CachingMultiReader multiReader;
+
+    /**
+     * Monitor to use to synchronize access to {@link #multiReader} and
+     * {@link #updateInProgress}.
+     */
+    private final Object updateMonitor = new Object();
-        migrationCheck();
-        if (fs.exists("indexes")) {
+        if (fs.exists(indexNames.getFileName())) {
+        if (fs.exists(deletable.getFileName())) {
+            deletable.read(fs);
+        }
+        // try to remove deletable files if there are any
+        attemptDelete();
+                        } catch (Exception e) {
+                            log.warn("Unable to add node to index: ", e);
+     * Update the index by removing some documents and adding others.
+     *
+     * @param remove Iterator of <code>Term</code>s that identify documents to
+     *               remove
+     * @param add    Iterator of <code>Document</code>s to add. Calls to
+     *               <code>next()</code> on this iterator may return
+     *               <code>null</code>, to indicate that a node could not be
+     *               indexed successfully.
+     */
+    synchronized void update(Iterator remove, Iterator add) throws IOException {
+        synchronized (updateMonitor) {
+            updateInProgress = true;
+        }
+        boolean hasAdditions = add.hasNext();
+        try {
+            // todo block with remove & add is not atomic
+            while (remove.hasNext()) {
+                internalRemoveDocument((Term) remove.next());
+            }
+            while (add.hasNext()) {
+                Document doc = (Document) add.next();
+                if (doc != null) {
+                    internalAddDocument(doc);
+                }
+            }
+        } finally {
+            synchronized (updateMonitor) {
+                if (hasAdditions) {
+                    lastModificationTime = System.currentTimeMillis();
+                }
+                updateInProgress = false;
+                updateMonitor.notifyAll();
+                if (multiReader != null) {
+                    multiReader.close();
+                    multiReader = null;
+                }
+            }
+        }
+    }
+
+    /**
-    synchronized void addDocument(Document doc) throws IOException {
-        lastModificationTime = System.currentTimeMillis();
-        multiReader = null;
-        volatileIndex.addDocument(doc);
-        if (volatileIndex.getRedoLog().getSize() >= handler.getMinMergeDocs()) {
-            log.info("Committing in-memory index");
-            commit();
-        }
+    void addDocument(Document doc) throws IOException {
+        List add = Arrays.asList(new Document[]{doc});
+        update(EmptyIterator.INSTANCE, add.iterator());
-     * @return the number of deleted documents.
-    synchronized int removeDocument(Term idTerm) throws IOException {
-        lastModificationTime = System.currentTimeMillis();
-        // flush multi reader if it does not have deletions yet
-        if (multiReader != null && !multiReader.hasDeletions()) {
-            multiReader = null;
-        }
-        // if the document cannot be deleted from the volatile index
-        // delete it from one of the persistent indexes.
-        int num = volatileIndex.removeDocument(idTerm);
-        if (num == 0) {
-            for (int i = indexes.size() - 1; i >= 0; i--) {
-                PersistentIndex index = (PersistentIndex) indexes.get(i);
-                num = index.removeDocument(idTerm);
-                if (num > 0) {
-                    return num;
-                }
-            }
-        } else {
-            return num;
-        }
-        return 0;
+    void removeDocument(Term idTerm) throws IOException {
+        List remove = Arrays.asList(new Term[]{idTerm});
+        update(remove.iterator(), EmptyIterator.INSTANCE);
-        lastModificationTime = System.currentTimeMillis();
-        // flush multi reader if it does not have deletions yet
-        if (multiReader != null && !multiReader.hasDeletions()) {
-            multiReader = null;
+        synchronized (updateMonitor) {
+            updateInProgress = true;
-        int num = volatileIndex.removeDocument(idTerm);
-        for (int i = 0; i < indexes.size(); i++) {
-            PersistentIndex index = (PersistentIndex) indexes.get(i);
-            num += index.removeDocument(idTerm);
-            index.commit();
+        int num;
+        try {
+            num = volatileIndex.removeDocument(idTerm);
+            for (int i = 0; i < indexes.size(); i++) {
+                PersistentIndex index = (PersistentIndex) indexes.get(i);
+                num += index.removeDocument(idTerm);
+                index.commit();
+            }
+        } finally {
+            synchronized (updateMonitor) {
+                updateInProgress = false;
+                updateMonitor.notifyAll();
+                if (multiReader != null) {
+                    multiReader.close();
+                    multiReader = null;
+                }
+            }
-     * Returns an <code>IndexReader</code> that spans alls indexes of this
+     * Returns an read-only <code>IndexReader</code> that spans alls indexes of this
-    synchronized IndexReader getIndexReader() throws IOException {
-        if (multiReader == null) {
-            IndexReader[] readers = new IndexReader[indexes.size() + 1];
-            for (int i = 0; i < indexes.size(); i++) {
-                readers[i] = ((PersistentIndex) indexes.get(i)).getIndexReader();
+    IndexReader getIndexReader() throws IOException {
+        synchronized (updateMonitor) {
+            if (multiReader != null) {
+                multiReader.incrementRefCount();
+                return multiReader;
-            readers[readers.length - 1] = volatileIndex.getIndexReader();
-            multiReader = new CachingMultiReader(readers);
+            // no reader available
+            // wait until no update is in progress
+            while (updateInProgress) {
+                try {
+                    updateMonitor.wait();
+                } catch (InterruptedException e) {
+                    throw new IOException("Interrupted while waiting to aquire reader");
+                }
+            }
+            // some other read thread might have created the reader in the
+            // meantime -> check again
+            if (multiReader == null) {
+                IndexReader[] readers = new IndexReader[indexes.size() + 1];
+                for (int i = 0; i < indexes.size(); i++) {
+                    readers[i] = ((PersistentIndex) indexes.get(i)).getReadOnlyIndexReader();
+                }
+                readers[readers.length - 1] = volatileIndex.getReadOnlyIndexReader();
+                multiReader = new CachingMultiReader(readers);
+            }
+            multiReader.incrementRefCount();
+            return multiReader;
-        return multiReader;
-        multiReader = null;
+        if (multiReader != null) {
+            try {
+                multiReader.close();
+            } catch (IOException e) {
+                log.error("Exception while closing search index.", e);
+            }
+            multiReader = null;
+        }
+     * Unsynchronized implementation to remove a document from the index. Note:
+     * this method will at most remove 1 (one) document from the index. This
+     * method assumes <code>idTerm</code> is unique.
+     *
+     * @param idTerm term that identifies the document to remove.
+     * @return number of documents to remove.
+     * @throws IOException if an error occurs while updating the index.
+     */
+    private int internalRemoveDocument(Term idTerm) throws IOException {
+        // if the document cannot be deleted from the volatile index
+        // delete it from one of the persistent indexes.
+        int num = volatileIndex.removeDocument(idTerm);
+        if (num == 0) {
+            for (int i = indexes.size() - 1; i >= 0; i--) {
+                PersistentIndex index = (PersistentIndex) indexes.get(i);
+                num = index.removeDocument(idTerm);
+                if (num > 0) {
+                    return num;
+                }
+            }
+        } else {
+            return num;
+        }
+        return 0;
+    }
+
+    /**
+     * Unsynchronized implementation to add a document to the index.
+     *
+     * @param doc the document to add.
+     * @throws IOException if an error occurs while adding the document to the
+     *                     index.
+     */
+    private void internalAddDocument(Document doc) throws IOException {
+        volatileIndex.addDocument(doc);
+        if (volatileIndex.getRedoLog().getSize() >= handler.getMinMergeDocs()) {
+            log.info("Committing in-memory index");
+            commit();
+        }
+    }
+
+    /**
-        // create new index folder
-        String name = indexNames.newName();
-        FileSystem sub = new BasedFileSystem(fs, name);
-        PersistentIndex index;
-        try {
-            sub.init();
-            index = new PersistentIndex(name, sub, true, handler.getAnalyzer());
-            index.setMaxMergeDocs(handler.getMaxMergeDocs());
-            index.setMergeFactor(handler.getMergeFactor());
-            index.setMinMergeDocs(handler.getMinMergeDocs());
-            index.setUseCompoundFile(handler.getUseCompoundFile());
-            indexes.add(index);
-            indexNames.addName(name);
-            indexNames.write(fs);
-        } catch (FileSystemException e) {
-            throw new IOException(e.getMessage());
+
+        // check if volatile index contains documents at all
+        if (volatileIndex.getIndexReader().numDocs() > 0) {
+            // create new index folder
+            String name = indexNames.newName();
+            FileSystem sub = new BasedFileSystem(fs, name);
+            PersistentIndex index;
+            try {
+                sub.init();
+                index = new PersistentIndex(name, sub, true, handler.getAnalyzer());
+                index.setMaxMergeDocs(handler.getMaxMergeDocs());
+                index.setMergeFactor(handler.getMergeFactor());
+                index.setMinMergeDocs(handler.getMinMergeDocs());
+                index.setUseCompoundFile(handler.getUseCompoundFile());
+            } catch (FileSystemException e) {
+                throw new IOException(e.getMessage());
+            }
+            index.mergeIndex(volatileIndex);
+
+            // if merge has been successful add index
+            try {
+                indexes.add(index);
+                indexNames.addName(name);
+                indexNames.write(fs);
+            } catch (FileSystemException e) {
+                throw new IOException(e.getMessage());
+            }
+
+            // check if obsolete indexes can be deleted
+            // todo move to other place?
+            attemptDelete();
-        index.mergeIndex(volatileIndex);
-                    log.warn("Unable to delete obsolete index: " + index.getName());
-                    log.error(e.toString());
+                    // try again later
+                    deletable.addName(index.getName());
+                    try {
+                        deletable.write(fs);
+                    } catch (FileSystemException e1) {
+                        throw new IOException(e.getMessage());
+                    }
-                log.warn("Unable to delete obsolete index: " + name);
-                log.error(e.toString());
+                // try again later
+                deletable.addName(pi.getName());
+                try {
+                    deletable.write(fs);
+                } catch (FileSystemException e1) {
+                    throw new IOException(e.getMessage());
+                }
+     * Attempts to delete all files recorded in {@link #deletable}.
+     */
+    private void attemptDelete() {
+        for (int i = deletable.size() - 1; i >= 0; i--) {
+            String indexName = deletable.getName(i);
+            try {
+                fs.deleteFolder(indexName);
+                deletable.removeName(i);
+            } catch (FileSystemException e) {
+                log.info("Unable to delete obsolete index: " + indexName);
+            }
+        }
+        try {
+            deletable.write(fs);
+        } catch (Exception e) {
+            // catches IOException and FileSystemException
+            log.warn("Exception while writing deletable indexes: " + e);
+        }
+    }
+
+    /**
-                    commit();
+                    synchronized (updateMonitor) {
+                        updateInProgress = true;
+                    }
+                    try {
+                        commit();
+                    } finally {
+                        synchronized (updateMonitor) {
+                            lastModificationTime = System.currentTimeMillis();
+                            updateMonitor.notifyAll();
+                            if (multiReader != null) {
+                                multiReader.close();
+                                multiReader = null;
+                            }
+                        }
+                    }
-
-    /**
-     * <b>todo: This check will be removed when Jackrabbit 1.0 is final.</b>
-     * <p/>
-     * Checks if an old index format is present and moves it to the new
-     * subindex structure.
-     * @throws FileSystemException if an error occurs.
-     * @throws IOException if an error occurs.
-     */
-    private void migrationCheck() throws FileSystemException, IOException {
-        if (fs.exists("segments")) {
-            // move to a sub folder
-            String name = indexNames.newName();
-            fs.createFolder(name);
-            // move all files except: redo-log and ns-mappings
-            Set exclude = new HashSet();
-            exclude.add(REDO_LOG);
-            exclude.add(NS_MAPPING_FILE);
-            String[] files = fs.listFiles("/");
-            for (int i = 0; i < files.length; i++) {
-                if (exclude.contains(files[i])) {
-                    continue;
-                }
-                fs.move(files[i], name + FileSystem.SEPARATOR + files[i]);
-            }
-            indexNames.addName(name);
-            indexNames.write(fs);
-        }
-    }
