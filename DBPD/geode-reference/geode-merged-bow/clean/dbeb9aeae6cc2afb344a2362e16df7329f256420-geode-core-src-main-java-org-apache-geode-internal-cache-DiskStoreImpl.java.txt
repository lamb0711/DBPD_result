Merge branch 'release/1.1.0'

- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license
+ * agreements. See the NOTICE file distributed with this work for additional information regarding
+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License. You may obtain a
+ * copy of the License at
- *      http://www.apache.org/licenses/LICENSE-2.0
+ * http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
+ * Unless required by applicable law or agreed to in writing, software distributed under the License
+ * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+ * or implied. See the License for the specific language governing permissions and limitations under
+ * the License.
- * Represents a (disk-based) persistent store for region data. Used for both
- * persistent recoverable regions and overflow-only regions.
+ * Represents a (disk-based) persistent store for region data. Used for both persistent recoverable
+ * regions and overflow-only regions.
-  
-  public static final int MAX_OPEN_INACTIVE_OPLOGS = Integer.getInteger(
-      DistributionConfig.GEMFIRE_PREFIX + "MAX_OPEN_INACTIVE_OPLOGS", 7).intValue();
-  /* 
-   * If less than 20MB (default - configurable through this property) of the
-   * available space is left for logging and other misc stuff then it 
-   * is better to bail out.
+  public static final int MAX_OPEN_INACTIVE_OPLOGS = Integer
+      .getInteger(DistributionConfig.GEMFIRE_PREFIX + "MAX_OPEN_INACTIVE_OPLOGS", 7).intValue();
+
+  /*
+   * If less than 20MB (default - configurable through this property) of the available space is left
+   * for logging and other misc stuff then it is better to bail out.
-  public static final int MIN_DISK_SPACE_FOR_LOGS = Integer.getInteger(
-      DistributionConfig.GEMFIRE_PREFIX + "MIN_DISK_SPACE_FOR_LOGS", 20).intValue();
-  
+  public static final int MIN_DISK_SPACE_FOR_LOGS = Integer
+      .getInteger(DistributionConfig.GEMFIRE_PREFIX + "MIN_DISK_SPACE_FOR_LOGS", 20).intValue();
+
-  
+
-  
+
-   * The static field delays the joining of the close/clear/destroy & forceFlush
-   * operation, with the compactor thread. This joining occurs after the
-   * compactor thread is notified to exit. This was added to reproduce deadlock
-   * caused by concurrent destroy & clear operation where clear operation is
-   * restarting the compactor thread ( a new thread object different from the
-   * one for which destroy operation issued notification for release). The delay
-   * occurs iff the flag used for enabling callbacks to CacheObserver is enabled
-   * true
+   * The static field delays the joining of the close/clear/destroy & forceFlush operation, with the
+   * compactor thread. This joining occurs after the compactor thread is notified to exit. This was
+   * added to reproduce deadlock caused by concurrent destroy & clear operation where clear
+   * operation is restarting the compactor thread ( a new thread object different from the one for
+   * which destroy operation issued notification for release). The delay occurs iff the flag used
+   * for enabling callbacks to CacheObserver is enabled true
-  
+
-  private final static boolean ENABLE_NOTIFY_TO_ROLL = Boolean
-      .getBoolean(DistributionConfig.GEMFIRE_PREFIX + "ENABLE_NOTIFY_TO_ROLL");
+  private final static boolean ENABLE_NOTIFY_TO_ROLL =
+      Boolean.getBoolean(DistributionConfig.GEMFIRE_PREFIX + "ENABLE_NOTIFY_TO_ROLL");
-  public static final String RECOVER_VALUE_PROPERTY_NAME = DistributionConfig.GEMFIRE_PREFIX + "disk.recoverValues";
-  public static final String RECOVER_VALUES_SYNC_PROPERTY_NAME = DistributionConfig.GEMFIRE_PREFIX + "disk.recoverValuesSync";
-  boolean RECOVER_VALUES = getBoolean(
-      DiskStoreImpl.RECOVER_VALUE_PROPERTY_NAME, true);
-  boolean RECOVER_VALUES_SYNC = getBoolean(
-      DiskStoreImpl.RECOVER_VALUES_SYNC_PROPERTY_NAME, false);
-  boolean FORCE_KRF_RECOVERY = getBoolean(
-      DistributionConfig.GEMFIRE_PREFIX + "disk.FORCE_KRF_RECOVERY", false);
-  
+  public static final String RECOVER_VALUE_PROPERTY_NAME =
+      DistributionConfig.GEMFIRE_PREFIX + "disk.recoverValues";
+  public static final String RECOVER_VALUES_SYNC_PROPERTY_NAME =
+      DistributionConfig.GEMFIRE_PREFIX + "disk.recoverValuesSync";
+  boolean RECOVER_VALUES = getBoolean(DiskStoreImpl.RECOVER_VALUE_PROPERTY_NAME, true);
+  boolean RECOVER_VALUES_SYNC = getBoolean(DiskStoreImpl.RECOVER_VALUES_SYNC_PROPERTY_NAME, false);
+  boolean FORCE_KRF_RECOVERY =
+      getBoolean(DistributionConfig.GEMFIRE_PREFIX + "disk.FORCE_KRF_RECOVERY", false);
+
-    return Boolean.valueOf(System.getProperty(sysProp, Boolean.valueOf(def)
-        .toString()));
+    return Boolean.valueOf(System.getProperty(sysProp, Boolean.valueOf(def).toString()));
-  
+
-  
+
-   * Maximum number of oplogs to compact per compaction operations. Defaults to
-   * 1 to allows oplogs to be deleted quickly, to reduce amount of memory used
-   * during a compaction and to be fair to other regions waiting for a compactor
-   * thread from the pool. Ignored if set to <= 0. Made non static so tests can
-   * set it.
+   * Maximum number of oplogs to compact per compaction operations. Defaults to 1 to allows oplogs
+   * to be deleted quickly, to reduce amount of memory used during a compaction and to be fair to
+   * other regions waiting for a compactor thread from the pool. Ignored if set to <= 0. Made non
+   * static so tests can set it.
-  private final int MAX_OPLOGS_PER_COMPACTION = Integer.getInteger(
-      DistributionConfig.GEMFIRE_PREFIX + "MAX_OPLOGS_PER_COMPACTION",
-      Integer.getInteger(DistributionConfig.GEMFIRE_PREFIX + "MAX_OPLOGS_PER_ROLL", 1).intValue())
+  private final int MAX_OPLOGS_PER_COMPACTION = Integer
+      .getInteger(DistributionConfig.GEMFIRE_PREFIX + "MAX_OPLOGS_PER_COMPACTION", Integer
+          .getInteger(DistributionConfig.GEMFIRE_PREFIX + "MAX_OPLOGS_PER_ROLL", 1).intValue())
-  public static final int MAX_CONCURRENT_COMPACTIONS = Integer.getInteger(
-      DistributionConfig.GEMFIRE_PREFIX + "MAX_CONCURRENT_COMPACTIONS",
-      Integer.getInteger(DistributionConfig.GEMFIRE_PREFIX + "MAX_CONCURRENT_ROLLS", 1).intValue())
+  public static final int MAX_CONCURRENT_COMPACTIONS = Integer
+      .getInteger(DistributionConfig.GEMFIRE_PREFIX + "MAX_CONCURRENT_COMPACTIONS", Integer
+          .getInteger(DistributionConfig.GEMFIRE_PREFIX + "MAX_CONCURRENT_ROLLS", 1).intValue())
-  
-  /**
-   * This system property indicates that maximum number of delayed write
-   * tasks that can be pending before submitting the tasks start blocking. 
-   * These tasks are things like unpreblow oplogs, delete oplogs, etc. 
-   */
-  public static final int MAX_PENDING_TASKS = Integer.getInteger(DistributionConfig.GEMFIRE_PREFIX + "disk.MAX_PENDING_TASKS", 6);
-  /**
-   * This system property indicates that IF should also be preallocated. This property 
-   * will be used in conjunction with the PREALLOCATE_OPLOGS property. If PREALLOCATE_OPLOGS
-   * is ON the below will by default be ON but in order to switch it off you need to explicitly
-   */
-  static final boolean PREALLOCATE_IF = !System.getProperty(
-      DistributionConfig.GEMFIRE_PREFIX + "preAllocateIF", "true").equalsIgnoreCase("false");
-  /**
-   * This system property indicates that Oplogs should be preallocated till the
-   * maxOplogSize as specified for the disk store.
-   */
-  static final boolean PREALLOCATE_OPLOGS = !System.getProperty(
-      DistributionConfig.GEMFIRE_PREFIX + "preAllocateDisk", "true").equalsIgnoreCase("false");
-   * For some testing purposes we would not consider top property if this flag
-   * is set to true
+   * This system property indicates that maximum number of delayed write tasks that can be pending
+   * before submitting the tasks start blocking. These tasks are things like unpreblow oplogs,
+   * delete oplogs, etc.
+   */
+  public static final int MAX_PENDING_TASKS =
+      Integer.getInteger(DistributionConfig.GEMFIRE_PREFIX + "disk.MAX_PENDING_TASKS", 6);
+  /**
+   * This system property indicates that IF should also be preallocated. This property will be used
+   * in conjunction with the PREALLOCATE_OPLOGS property. If PREALLOCATE_OPLOGS is ON the below will
+   * by default be ON but in order to switch it off you need to explicitly
+   */
+  static final boolean PREALLOCATE_IF =
+      !System.getProperty(DistributionConfig.GEMFIRE_PREFIX + "preAllocateIF", "true")
+          .equalsIgnoreCase("false");
+  /**
+   * This system property indicates that Oplogs should be preallocated till the maxOplogSize as
+   * specified for the disk store.
+   */
+  static final boolean PREALLOCATE_OPLOGS =
+      !System.getProperty(DistributionConfig.GEMFIRE_PREFIX + "preAllocateDisk", "true")
+          .equalsIgnoreCase("false");
+
+  /**
+   * For some testing purposes we would not consider top property if this flag is set to true
-  static final boolean SYNC_IF_WRITES = Boolean
-      .getBoolean(DistributionConfig.GEMFIRE_PREFIX + "syncMetaDataWrites");
+  static final boolean SYNC_IF_WRITES =
+      Boolean.getBoolean(DistributionConfig.GEMFIRE_PREFIX + "syncMetaDataWrites");
-  
+
-   * Asif:Added as stop gap arrangement to fix bug 39380. It is not a clean fix
-   * as keeping track of the threads acquiring read lock, etc is not a good idea
-   * to solve the issue
+   * Asif:Added as stop gap arrangement to fix bug 39380. It is not a clean fix as keeping track of
+   * the threads acquiring read lock, etc is not a good idea to solve the issue
-   * The limit of how many items can be in the async queue before async starts
-   * blocking and a flush is forced. If this value is 0 then no limit.
+   * The limit of how many items can be in the async queue before async starts blocking and a flush
+   * is forced. If this value is 0 then no limit.
-   * Set if we have encountered a disk exception causing us to shutdown this
-   * disk store. This is currently used only to prevent trying to shutdown the
-   * disk store from multiple threads, but I think at some point we should use
-   * this to prevent any other ops from completing during the close operation.
+   * Set if we have encountered a disk exception causing us to shutdown this disk store. This is
+   * currently used only to prevent trying to shutdown the disk store from multiple threads, but I
+   * think at some point we should use this to prevent any other ops from completing during the
+   * close operation.
-  private final AtomicReference<DiskAccessException> diskException = new AtomicReference<DiskAccessException>();
+  private final AtomicReference<DiskAccessException> diskException =
+      new AtomicReference<DiskAccessException>();
-   * Only contains backup DiskRegions. The Value could be a RecoveredDiskRegion
-   * or a DiskRegion
+   * Only contains backup DiskRegions. The Value could be a RecoveredDiskRegion or a DiskRegion
-   * Contains all of the disk recovery stores for which we are recovering values
-   * asnynchronously.
+   * Contains all of the disk recovery stores for which we are recovering values asnynchronously.
-  private final Map<Long, DiskRecoveryStore> currentAsyncValueRecoveryMap = new HashMap<Long, DiskRecoveryStore>();
+  private final Map<Long, DiskRecoveryStore> currentAsyncValueRecoveryMap =
+      new HashMap<Long, DiskRecoveryStore>();
-   * Either set during recovery of an existing disk store when the
-   * IFREC_DISKSTORE_ID record is read or when a new init file is created.
+   * Either set during recovery of an existing disk store when the IFREC_DISKSTORE_ID record is read
+   * or when a new init file is created.
-  
+
-  
+
-  
+
-      if (System.getProperty(DistributionConfig.GEMFIRE_PREFIX + "OVERFLOW_ROLL_PERCENTAGE") != null) {
-        ct = (int) (Double.parseDouble(System.getProperty(
-            DistributionConfig.GEMFIRE_PREFIX + "OVERFLOW_ROLL_PERCENTAGE", "0.50")) * 100.0);
+      if (System
+          .getProperty(DistributionConfig.GEMFIRE_PREFIX + "OVERFLOW_ROLL_PERCENTAGE") != null) {
+        ct = (int) (Double.parseDouble(System
+            .getProperty(DistributionConfig.GEMFIRE_PREFIX + "OVERFLOW_ROLL_PERCENTAGE", "0.50"))
+            * 100.0);
-   * Creates a new <code>DiskRegion</code> that access disk on behalf of the
-   * given region.
+   * Creates a new <code>DiskRegion</code> that access disk on behalf of the given region.
-    this(cache, props.getName(), props, ownedByRegion, internalRegionArgs,
-        false, false/* upgradeVersionOnly */, false, false, true, false/*offlineModify*/);
+    this(cache, props.getName(), props, ownedByRegion, internalRegionArgs, false,
+        false/* upgradeVersionOnly */, false, false, true, false/* offlineModify */);
-  DiskStoreImpl(Cache cache, String name, DiskStoreAttributes props,
-      boolean ownedByRegion, InternalRegionArguments internalRegionArgs,
-      boolean offline, boolean upgradeVersionOnly, boolean offlineValidating,
-      boolean offlineCompacting, boolean needsOplogs, boolean offlineModify) {
+  DiskStoreImpl(Cache cache, String name, DiskStoreAttributes props, boolean ownedByRegion,
+      InternalRegionArguments internalRegionArgs, boolean offline, boolean upgradeVersionOnly,
+      boolean offlineValidating, boolean offlineCompacting, boolean needsOplogs,
+      boolean offlineModify) {
-    this.compactionThreshold = calcCompactionThreshold(props
-        .getCompactionThreshold());
+    this.compactionThreshold = calcCompactionThreshold(props.getCompactionThreshold());
-    
+
-    this.isCompactionPossible = isOfflineCompacting()
-        || (!isOffline() && (getAutoCompact() || getAllowForceCompaction() || ENABLE_NOTIFY_TO_ROLL));
+    this.isCompactionPossible = isOfflineCompacting() || (!isOffline()
+        && (getAutoCompact() || getAllowForceCompaction() || ENABLE_NOTIFY_TO_ROLL));
-      this.asyncQueue = new ForceableLinkedBlockingQueue<Object>(
-          this.maxAsyncItems); // fix for bug 41310
+      this.asyncQueue = new ForceableLinkedBlockingQueue<Object>(this.maxAsyncItems); // fix for bug
+                                                                                      // 41310
-      directories[i] = new DirectoryHolder(getName() + "_DIR#" + i, factory,
-          dirs[i], dirSizes[i], i);
+      directories[i] =
+          new DirectoryHolder(getName() + "_DIR#" + i, factory, dirs[i], dirSizes[i], i);
-     * The infoFileDir contains the lock file and the init file. It will be
-     * directories[0] on a brand new disk store. On an existing disk store it
-     * will be the directory the init file is found in.
+     * The infoFileDir contains the lock file and the init file. It will be directories[0] on a
+     * brand new disk store. On an existing disk store it will be the directory the init file is
+     * found in.
-    
+
-    
-    int MAXT = DiskStoreImpl.MAX_CONCURRENT_COMPACTIONS;
-    final ThreadGroup compactThreadGroup = LoggingThreadGroup.createThreadGroup("Oplog Compactor Thread Group", this.logger);
-    final ThreadFactory compactThreadFactory = GemfireCacheHelper.CreateThreadFactory(compactThreadGroup, "Idle OplogCompactor");
-    this.diskStoreTaskPool = new ThreadPoolExecutor(MAXT, MAXT, 10, TimeUnit.SECONDS,
-                                             new LinkedBlockingQueue(),
-                                             compactThreadFactory);
-    this.diskStoreTaskPool.allowCoreThreadTimeOut(true);
-    
-    
-    final ThreadGroup deleteThreadGroup = LoggingThreadGroup.createThreadGroup("Oplog Delete Thread Group", this.logger);
-    final ThreadFactory deleteThreadFactory = GemfireCacheHelper.CreateThreadFactory(deleteThreadGroup, "Oplog Delete Task");
+    int MAXT = DiskStoreImpl.MAX_CONCURRENT_COMPACTIONS;
+    final ThreadGroup compactThreadGroup =
+        LoggingThreadGroup.createThreadGroup("Oplog Compactor Thread Group", this.logger);
+    final ThreadFactory compactThreadFactory =
+        GemfireCacheHelper.CreateThreadFactory(compactThreadGroup, "Idle OplogCompactor");
+    this.diskStoreTaskPool = new ThreadPoolExecutor(MAXT, MAXT, 10, TimeUnit.SECONDS,
+        new LinkedBlockingQueue(), compactThreadFactory);
+    this.diskStoreTaskPool.allowCoreThreadTimeOut(true);
+
+
+    final ThreadGroup deleteThreadGroup =
+        LoggingThreadGroup.createThreadGroup("Oplog Delete Thread Group", this.logger);
+
+    final ThreadFactory deleteThreadFactory =
+        GemfireCacheHelper.CreateThreadFactory(deleteThreadGroup, "Oplog Delete Task");
-                 new LinkedBlockingQueue(MAX_PENDING_TASKS),
-                 deleteThreadFactory, new ThreadPoolExecutor.CallerRunsPolicy());
+        new LinkedBlockingQueue(MAX_PENDING_TASKS), deleteThreadFactory,
+        new ThreadPoolExecutor.CallerRunsPolicy());
-        logger.debug("allowForceCompaction {} != {}", getAllowForceCompaction(), props.getAllowForceCompaction());
+        logger.debug("allowForceCompaction {} != {}", getAllowForceCompaction(),
+            props.getAllowForceCompaction());
-        logger.debug("CompactionThreshold {} != {}", getCompactionThreshold(), props.getCompactionThreshold());
+        logger.debug("CompactionThreshold {} != {}", getCompactionThreshold(),
+            props.getCompactionThreshold());
-        logger.debug("MaxOplogSizeInBytes {} != {}", getMaxOplogSizeInBytes(), props.getMaxOplogSizeInBytes());
+        logger.debug("MaxOplogSizeInBytes {} != {}", getMaxOplogSizeInBytes(),
+            props.getMaxOplogSizeInBytes());
-        logger.debug("DiskDirs {} != {}", Arrays.toString(getDiskDirs()), Arrays.toString(props.getDiskDirs()));
+        logger.debug("DiskDirs {} != {}", Arrays.toString(getDiskDirs()),
+            Arrays.toString(props.getDiskDirs()));
-        logger.debug("DiskDirSizes {} != {}", Arrays.toString(getDiskDirSizes()), Arrays.toString(props.getDiskDirSizes()));
+        logger.debug("DiskDirSizes {} != {}", Arrays.toString(getDiskDirSizes()),
+            Arrays.toString(props.getDiskDirSizes()));
-        && getName().equals(props.getName())
-        && getQueueSize() == props.getQueueSize()
+        && getName().equals(props.getName()) && getQueueSize() == props.getQueueSize()
-   * Initializes the contents of any regions on this DiskStore that have been
-   * registered but are not yet initialized.
+   * Initializes the contents of any regions on this DiskStore that have been registered but are not
+   * yet initialized.
-    //We don't need to do recovery for overflow regions.
-    if(!lr.getDataPolicy().withPersistence() || !dr.isRecreated()) {
+    // We don't need to do recovery for overflow regions.
+    if (!lr.getDataPolicy().withPersistence() || !dr.isRecreated()) {
-    
+
-        
-        // acquire CompactorWriteLock only if the region attributes for the 
+
+        // acquire CompactorWriteLock only if the region attributes for the
-      throw new DiskAccessException(
-          "RuntimeException in initializing the disk store from the disk", re,
-          this);
+      throw new DiskAccessException("RuntimeException in initializing the disk store from the disk",
+          re, this);
-   * Stores a key/value pair from a region entry on disk. Updates all of the
-   * necessary {@linkplain DiskRegionStats statistics}and invokes
-   * {@link Oplog#create}or {@link Oplog#modify}.
+   * Stores a key/value pair from a region entry on disk. Updates all of the necessary
+   * {@linkplain DiskRegionStats statistics}and invokes {@link Oplog#create}or {@link Oplog#modify}.
-   * @param entry
-   *          The entry which is going to be written to disk
-   * @throws RegionClearedException
-   *           If a clear operation completed before the put operation completed
-   *           successfully, resulting in the put operation to abort.
-   * @throws IllegalArgumentException
-   *           If <code>id</code> is less than zero
+   * @param entry The entry which is going to be written to disk
+   * @throws RegionClearedException If a clear operation completed before the put operation
+   *         completed successfully, resulting in the put operation to abort.
+   * @throws IllegalArgumentException If <code>id</code> is less than zero
-  final void put(LocalRegion region, DiskEntry entry, ValueWrapper value,
-      boolean async) throws RegionClearedException {
+  final void put(LocalRegion region, DiskEntry entry, ValueWrapper value, boolean async)
+      throws RegionClearedException {
-          LocalizedStrings.DiskRegion_CANT_PUT_A_KEYVALUE_PAIR_WITH_ID_0
-              .toLocalizedString(id));
+          LocalizedStrings.DiskRegion_CANT_PUT_A_KEYVALUE_PAIR_WITH_ID_0.toLocalizedString(id));
-                  .toLocalizedString(), dr.getName());
+                  .toLocalizedString(),
+              dr.getName());
-                  .toLocalizedString(new Object[] {
-                      ((doingCreate) ? "creation" : "modification"), id }));
+                  .toLocalizedString(
+                      new Object[] {((doingCreate) ? "creation" : "modification"), id}));
-                .toLocalizedString(), dr.getName());
+                .toLocalizedString(),
+            dr.getName());
-      if (dr.getRegionVersionVector().contains(tag.getMemberID(),
-          tag.getRegionVersion())) {
+      if (dr.getRegionVersionVector().contains(tag.getMemberID(), tag.getRegionVersion())) {
-   * Returns the value of the key/value pair with the given diskId. Updates all
-   * of the necessary {@linkplain DiskRegionStats statistics}
+   * Returns the value of the key/value pair with the given diskId. Updates all of the necessary
+   * {@linkplain DiskRegionStats statistics}
-                    .toLocalizedString(), dr.getName());
+                    .toLocalizedString(),
+                dr.getName());
-                                                                              * Get
-                                                                              * only
-                                                                              * the
-                                                                              * userbit
+                                                                              * Get only the userbit
-            logger.debug("DiskRegion: Tried {}, getBytesAndBitsWithoutLock returns wrong byte array: {}",
+            logger.debug(
+                "DiskRegion: Tried {}, getBytesAndBitsWithoutLock returns wrong byte array: {}",
-        logger.debug("Retried 3 times, getting entry from DiskRegion still failed. It must be Oplog file corruption due to HA");
+        logger.debug(
+            "Retried 3 times, getting entry from DiskRegion still failed. It must be Oplog file corruption due to HA");
-   * This method was added to fix bug 40192. It is like getBytesAndBits except
-   * it will return Token.REMOVE_PHASE1 if the htreeReference has changed (which
-   * means a clear was done).
+   * This method was added to fix bug 40192. It is like getBytesAndBits except it will return
+   * Token.REMOVE_PHASE1 if the htreeReference has changed (which means a clear was done).
-              .toLocalizedString(), dr.getName());
+              .toLocalizedString(),
+          dr.getName());
-    BytesAndBits bb = dr.getDiskStore().getBytesAndBitsWithoutLock(dr, id,
-        true/* fault -in */, false /* Get only the userbit */);
+    BytesAndBits bb = dr.getDiskStore().getBytesAndBitsWithoutLock(dr, id, true/* fault -in */,
+        false /* Get only the userbit */);
-   * Given a BytesAndBits object convert it to the relevant Object (deserialize
-   * if necessary) and return the object
+   * Given a BytesAndBits object convert it to the relevant Object (deserialize if necessary) and
+   * return the object
-      value = DiskEntry.Helper
-                .readSerializedValue(bytes, bb.getVersion(), null, true);
+      value = DiskEntry.Helper.readSerializedValue(bytes, bb.getVersion(), null, true);
-      value = DiskEntry.Helper
-                .readSerializedValue(bytes, bb.getVersion(), null, false);
+      value = DiskEntry.Helper.readSerializedValue(bytes, bb.getVersion(), null, false);
-   * Gets the Object from the OpLog . It can be invoked from OpLog , if by the
-   * time a get operation reaches the OpLog, the entry gets compacted or if we
-   * allow concurrent put & get operations. It will also minimize the synch lock
-   * on DiskId
+   * Gets the Object from the OpLog . It can be invoked from OpLog , if by the time a get operation
+   * reaches the OpLog, the entry gets compacted or if we allow concurrent put & get operations. It
+   * will also minimize the synch lock on DiskId
-   * @param id
-   *          DiskId object for the entry
-   * @return value of the entry or CLEAR_BB if it is detected that the entry was
-   *         removed by a concurrent region clear.
+   * @param id DiskId object for the entry
+   * @return value of the entry or CLEAR_BB if it is detected that the entry was removed by a
+   *         concurrent region clear.
-  final BytesAndBits getBytesAndBitsWithoutLock(DiskRegionView dr, DiskId id,
-      boolean faultIn, boolean bitOnly) {
+  final BytesAndBits getBytesAndBitsWithoutLock(DiskRegionView dr, DiskId id, boolean faultIn,
+      boolean bitOnly) {
-              .toLocalizedString(id), dr.getName());
+              .toLocalizedString(id),
+          dr.getName());
-  final BytesAndBits getBytesAndBits(DiskRegion dr, DiskId id,
-      boolean faultingIn) {
+  final BytesAndBits getBytesAndBits(DiskRegion dr, DiskId id, boolean faultingIn) {
-                .toLocalizedString(), dr.getName());
+                .toLocalizedString(),
+            dr.getName());
-                .toLocalizedString(), dr.getName());
+                .toLocalizedString(),
+            dr.getName());
-                                                                              * Get
-                                                                              * only
-                                                                              * user
-                                                                              * bit
+                                                                              * Get only user bit
-                .toLocalizedString(), dr.getName());
+                .toLocalizedString(),
+            dr.getName());
-                .toLocalizedString(), dr.getName());
+                .toLocalizedString(),
+            dr.getName());
-                                                                       * Get
-                                                                       * only
-                                                                       * user
-                                                                       * bit
+                                                                       * Get only user bit
-   * Asif: THIS SHOULD ONLY BE USED FOR TESTING PURPOSES AS IT IS NOT THREAD
-   * SAFE
+   * Asif: THIS SHOULD ONLY BE USED FOR TESTING PURPOSES AS IT IS NOT THREAD SAFE
-   * Returns the object stored on disk with the given id. This method is used
-   * for testing purposes only. As such, it bypasses the buffer and goes
-   * directly to the disk. This is not a thread safe function , in the sense, it
-   * is possible that by the time the OpLog is queried , data might move HTree
-   * with the oplog being destroyed
+   * Returns the object stored on disk with the given id. This method is used for testing purposes
+   * only. As such, it bypasses the buffer and goes directly to the disk. This is not a thread safe
+   * function , in the sense, it is possible that by the time the OpLog is queried , data might move
+   * HTree with the oplog being destroyed
-   * @throws IllegalArgumentException
-   *           If <code>id</code> is less than zero, no action is taken.
+   * @throws IllegalArgumentException If <code>id</code> is less than zero, no action is taken.
-   * @param async
-   *          true if called by the async flusher thread
+   * @param async true if called by the async flusher thread
-   * @throws RegionClearedException
-   *           If a clear operation completed before the put operation completed
-   *           successfully, resulting in the put operation to abort.
-   * @throws IllegalArgumentException
-   *           If <code>id</code> is {@linkplain #INVALID_ID invalid}or is less
-   *           than zero, no action is taken.
+   * @throws RegionClearedException If a clear operation completed before the put operation
+   *         completed successfully, resulting in the put operation to abort.
+   * @throws IllegalArgumentException If <code>id</code> is {@linkplain #INVALID_ID invalid}or is
+   *         less than zero, no action is taken.
-  final void remove(LocalRegion region, DiskEntry entry, boolean async,
-      boolean isClear) throws RegionClearedException {
+  final void remove(LocalRegion region, DiskEntry entry, boolean async, boolean isClear)
+      throws RegionClearedException {
-                .toLocalizedString(), dr.getName());
+                .toLocalizedString(),
+            dr.getName());
-  
+
-   * This function is having a default visiblity as it is used in the
-   * OplogJUnitTest for a bug verification of Bug # 35012
+   * This function is having a default visiblity as it is used in the OplogJUnitTest for a bug
+   * verification of Bug # 35012
-   * All callers must have {@link #releaseWriteLock(DiskRegion)} in a matching
-   * finally block.
+   * All callers must have {@link #releaseWriteLock(DiskRegion)} in a matching finally block.
-   * Note that this is no longer implemented by getting a write lock but instead
-   * locks the same lock that acquireReadLock does.
+   * Note that this is no longer implemented by getting a write lock but instead locks the same lock
+   * that acquireReadLock does.
-   * This function is having a default visiblity as it is used in the
-   * OplogJUnitTest for a bug verification of Bug # 35012
+   * This function is having a default visiblity as it is used in the OplogJUnitTest for a bug
+   * verification of Bug # 35012
-   * All callers must have {@link #releaseReadLock(DiskRegion)} in a matching
-   * finally block. Note that this is no longer implemented by getting a read
-   * lock but instead locks the same lock that acquireWriteLock does.
+   * All callers must have {@link #releaseReadLock(DiskRegion)} in a matching finally block. Note
+   * that this is no longer implemented by getting a read lock but instead locks the same lock that
+   * acquireWriteLock does.
-        throw new RegionDestroyedException(
-            "The DiskRegion has been closed or destroyed", dr.getName());
+        throw new RegionDestroyedException("The DiskRegion has been closed or destroyed",
+            dr.getName());
-      String message = "Could not schedule asynchronous write because the flusher thread had been terminated.";
-      if(this.isClosing()) {
-     // for bug 41305
-        throw this.cache
-            .getCacheClosedException(message, null);
+      String message =
+          "Could not schedule asynchronous write because the flusher thread had been terminated.";
+      if (this.isClosing()) {
+        // for bug 41305
+        throw this.cache.getCacheClosedException(message, null);
-      
+
-  private void addAsyncItem(Object item, boolean forceAsync)
-      throws InterruptedException {
+  private void addAsyncItem(Object item, boolean forceAsync) throws InterruptedException {
-    
+
-   * To fix bug 41770 clear the list in a way that will not break a concurrent
-   * iterator that is not synced on drainSync. Only clear from it entries on the
-   * given region. Currently we do this by clearing the isPendingAsync bit on
-   * each entry in this list.
+   * To fix bug 41770 clear the list in a way that will not break a concurrent iterator that is not
+   * synced on drainSync. Only clear from it entries on the given region. Currently we do this by
+   * clearing the isPendingAsync bit on each entry in this list.
-  private boolean shouldClear(LocalRegion r, RegionVersionVector rvv,
-      AsyncDiskEntry ade) {
+  private boolean shouldClear(LocalRegion r, RegionVersionVector rvv, AsyncDiskEntry ade) {
-        .toLocalizedString(new Object[] { getName() });
-    this.flusherThread = new Thread(LoggingThreadGroup.createThreadGroup(
-        LocalizedStrings.DiskRegion_DISK_WRITERS.toLocalizedString(),logger),
+        .toLocalizedString(new Object[] {getName()});
+    this.flusherThread = new Thread(
+        LoggingThreadGroup.createThreadGroup(
+            LocalizedStrings.DiskRegion_DISK_WRITERS.toLocalizedString(), logger),
-    return this.stopFlusher || this.flusherThreadTerminated
-        || this.flusherThread == null || !this.flusherThread.isAlive();
+    return this.stopFlusher || this.flusherThreadTerminated || this.flusherThread == null
+        || !this.flusherThread.isAlive();
-                  if (o!=null && o instanceof LocalRegion) {
-                    LocalRegion lr = (LocalRegion)o;
+                  if (o != null && o instanceof LocalRegion) {
+                    LocalRegion lr = (LocalRegion) o;
-        throw new IllegalStateException(
-            "Async writer thread stopping due to unexpected interrupt");
+        throw new IllegalStateException("Async writer thread stopping due to unexpected interrupt");
-      } catch(Throwable t) {
-        logger.fatal(LocalizedMessage.create(LocalizedStrings.DiskStoreImpl_FATAL_ERROR_ON_FLUSH), t);
-        fatalDae = new DiskAccessException(LocalizedStrings.DiskStoreImpl_FATAL_ERROR_ON_FLUSH.toLocalizedString(), t, DiskStoreImpl.this);
+      } catch (Throwable t) {
+        logger.fatal(LocalizedMessage.create(LocalizedStrings.DiskStoreImpl_FATAL_ERROR_ON_FLUSH),
+            t);
+        fatalDae = new DiskAccessException(
+            LocalizedStrings.DiskStoreImpl_FATAL_ERROR_ON_FLUSH.toLocalizedString(), t,
+            DiskStoreImpl.this);
-    File f = new File(getInfoFileDir().getDir(), "DRLK_IF" + name
-        + LOCK_FILE_EXT);
+    File f = new File(getInfoFileDir().getDir(), "DRLK_IF" + name + LOCK_FILE_EXT);
-          throw new IOException(
-              LocalizedStrings.Oplog_THE_FILE_0_IS_BEING_USED_BY_ANOTHER_PROCESS
-                  .toLocalizedString(f));
+          throw new IOException(LocalizedStrings.Oplog_THE_FILE_0_IS_BEING_USED_BY_ANOTHER_PROCESS
+              .toLocalizedString(f));
-            LocalizedStrings.Oplog_COULD_NOT_LOCK_0.toLocalizedString(f
-                .getPath()), ex, this);
+            LocalizedStrings.Oplog_COULD_NOT_LOCK_0.toLocalizedString(f.getPath()), ex, this);
-            LocalizedStrings.Oplog_COULD_NOT_LOCK_0.toLocalizedString(f
-                .getPath()), ex2, this);
+            LocalizedStrings.Oplog_COULD_NOT_LOCK_0.toLocalizedString(f.getPath()), ex2, this);
-      logger.debug("Locked disk store {} for exclusive access in directory: {}", name, getInfoFileDir().getDir());
+      logger.debug("Locked disk store {} for exclusive access in directory: {}", name,
+          getInfoFileDir().getDir());
-   * Searches the given disk dirs for the files and creates the Oplog objects
-   * wrapping those files
+   * Searches the given disk dirs for the files and creates the Oplog objects wrapping those files
-                "Detected multiple disk store initialization files named \""
-                    + ifName
+                "Detected multiple disk store initialization files named \"" + ifName
-      Map<File, DirectoryHolder> persistentBackupFiles = persistentOplogs
-          .findFiles(partialFileName);
+      Map<File, DirectoryHolder> persistentBackupFiles =
+          persistentOplogs.findFiles(partialFileName);
-        this.initFile = new DiskInitFile(partialFileName, this, ifRequired,
-            persistentBackupFiles.keySet());
+        this.initFile =
+            new DiskInitFile(partialFileName, this, ifRequired, persistentBackupFiles.keySet());
-            throw new IllegalStateException("Recovered version = " + getRecoveredGFVersion() + ": " +
-                LocalizedStrings.DiskStoreAlreadyInVersion_0
+            throw new IllegalStateException("Recovered version = " + getRecoveredGFVersion() + ": "
+                + LocalizedStrings.DiskStoreAlreadyInVersion_0
-            throw new IllegalStateException("Recovered version = " + getRecoveredGFVersion() + ": " +
-                LocalizedStrings.DiskStoreStillAtVersion_0
+            throw new IllegalStateException("Recovered version = " + getRecoveredGFVersion() + ": "
+                + LocalizedStrings.DiskStoreStillAtVersion_0
-        FilenameFilter overflowFileFilter = new DiskStoreFilter(OplogType.OVERFLOW, true,
-            partialFileName);
+        FilenameFilter overflowFileFilter =
+            new DiskStoreFilter(OplogType.OVERFLOW, true, partialFileName);
-        logger.info(LocalizedMessage.create(
-            LocalizedStrings.DiskStoreImpl_RecoveredDiskStore_0_With_Id_1,
-            new Object[] { getName(), getDiskStoreID() }));
+        logger.info(
+            LocalizedMessage.create(LocalizedStrings.DiskStoreImpl_RecoveredDiskStore_0_With_Id_1,
+                new Object[] {getName(), getDiskStoreID()}));
-        logger.info(LocalizedMessage.create(
-            LocalizedStrings.DiskStoreImpl_CreatedDiskStore_0_With_Id_1,
-            new Object[] { getName(), getDiskStoreID() }));
+        logger.info(
+            LocalizedMessage.create(LocalizedStrings.DiskStoreImpl_CreatedDiskStore_0_With_Id_1,
+                new Object[] {getName(), getDiskStoreID()}));
-   * The diskStats are at PR level.Hence if the region is a bucket region, the
-   * stats should not be closed, but the figures of entriesInVM and
-   * overflowToDisk contributed by that bucket need to be removed from the stats
-   * .
+   * The diskStats are at PR level.Hence if the region is a bucket region, the stats should not be
+   * closed, but the figures of entriesInVM and overflowToDisk contributed by that bucket need to be
+   * removed from the stats .
-   * Reads the oplogs files and loads them into regions that are ready to be
-   * recovered.
+   * Reads the oplogs files and loads them into regions that are ready to be recovered.
-    ValueRecoveryTask task = new ValueRecoveryTask(oplogsNeedingValueRecovery,
-        recoveredStores);
+    ValueRecoveryTask task = new ValueRecoveryTask(oplogsNeedingValueRecovery, recoveredStores);
-   * @param rvv
-   *          if not null, clear the region using a version vector Clearing with
-   *          a version vector only removes entries less than the version
-   *          vector, which allows for a consistent clear across members.
+   * @param rvv if not null, clear the region using a version vector Clearing with a version vector
+   *        only removes entries less than the version vector, which allows for a consistent clear
+   *        across members.
-  private void basicClear(LocalRegion region, DiskRegion dr,
-      RegionVersionVector rvv) {
+  private void basicClear(LocalRegion region, DiskRegion dr, RegionVersionVector rvv) {
-  
+
-   * Obtained and held by clear/destroyRegion/close. Also obtained when adding
-   * to async queue.
+   * Obtained and held by clear/destroyRegion/close. Also obtained when adding to async queue.
-   * It invokes appropriate methods of super & current class to clear the
-   * Oplogs.
+   * It invokes appropriate methods of super & current class to clear the Oplogs.
-   * @param rvv
-   *          if not null, clear the region using the version vector
+   * @param rvv if not null, clear the region using the version vector
-                      .toLocalizedString(), dr.getName());
+                      .toLocalizedString(),
+                  dr.getName());
-  
+
-    
+
-        if(exception != null && rte != null) {
+        if (exception != null && rte != null) {
-        
+
-      
+
-    return diskException.get() == null && (this.oplogCompactor == null || this.oplogCompactor.keepCompactorRunning());
+    return diskException.get() == null
+        && (this.oplogCompactor == null || this.oplogCompactor.keepCompactorRunning());
-  
+
-          logger.warn(LocalizedMessage.create(
-                LocalizedStrings.DiskRegion_COMPLEXDISKREGION_CLOSE_EXCEPTION_IN_STOPPING_COMPACTOR), e);
+          logger.warn(
+              LocalizedMessage.create(
+                  LocalizedStrings.DiskRegion_COMPLEXDISKREGION_CLOSE_EXCEPTION_IN_STOPPING_COMPACTOR),
+              e);
-            // this.oplogCompactor.compactionCompletionRequired = orig;
+          // this.oplogCompactor.compactionCompletionRequired = orig;
-      if(!closeDataOnly) {
+      if (!closeDataOnly) {
-  
+
-      logger.debug("DiskRegion::close:Attempting to close DiskRegion. Region name ={}", dr.getName());
+      logger.debug("DiskRegion::close:Attempting to close DiskRegion. Region name ={}",
+          dr.getName());
-            if(!closeDataOnly) {
+            if (!closeDataOnly) {
-                if(!closeDataOnly) {
+                if (!closeDataOnly) {
-              logger.debug("DiskRegion::close:Before invoking basic Close. Region name ={}", dr.getName());
+              logger.debug("DiskRegion::close:Before invoking basic Close. Region name ={}",
+                  dr.getName());
-   * stops the compactor outside the write lock. Once stopped then it proceeds
-   * to destroy the current & old oplogs
+   * stops the compactor outside the write lock. Once stopped then it proceeds to destroy the
+   * current & old oplogs
-    
-    //Compact the oplogs
+
+    // Compact the oplogs
-    
+
-      FilenameFilter overflowFileFilter = new DiskStoreFilter(OplogType.OVERFLOW, true,
-          getName());
+      FilenameFilter overflowFileFilter = new DiskStoreFilter(OplogType.OVERFLOW, true, getName());
-      FilenameFilter backupFileFilter = new DiskStoreFilter(OplogType.BACKUP, true,
-          getName());
+      FilenameFilter backupFileFilter = new DiskStoreFilter(OplogType.BACKUP, true, getName());
-    for(AbstractDiskRegion dr : getDiskRegions()) {
+    for (AbstractDiskRegion dr : getDiskRegions()) {
-    for(AbstractDiskRegion dr : overflowMap) {
+    for (AbstractDiskRegion dr : overflowMap) {
-    if(!liveRegions.isEmpty()) {
-      throw new IllegalStateException("Disk store is currently in use by these regions " + liveRegions);
+    if (!liveRegions.isEmpty()) {
+      throw new IllegalStateException(
+          "Disk store is currently in use by these regions " + liveRegions);
-   * @return Oplog[] returns the array of oplogs to be compacted if present else
-   *         returns null
+   * @return Oplog[] returns the array of oplogs to be compacted if present else returns null
-      
+
-    if (!all && max > MAX_OPLOGS_PER_COMPACTION
-        && MAX_OPLOGS_PER_COMPACTION > 0) {
+    if (!all && max > MAX_OPLOGS_PER_COMPACTION && MAX_OPLOGS_PER_COMPACTION > 0) {
-    
-    if(l.isEmpty()) {
+
+    if (l.isEmpty()) {
-      
+
-   * Returns the dir name used to back up this DiskStore's directories under.
-   * The name is a concatenation of the disk store name and id.
+   * Returns the dir name used to back up this DiskStore's directories under. The name is a
+   * concatenation of the disk store name and id.
-    
-    if(name == null) {
+
+    if (name == null) {
-    
+
-  
+
-   * Filters and returns the current set of oplogs that aren't already in the
-   * baseline for incremental backup
+   * Filters and returns the current set of oplogs that aren't already in the baseline for
+   * incremental backup
-   * @param baselineInspector
-   *          the inspector for the previous backup.
-   * @param baselineCopyMap
-   *          this will be populated with baseline oplogs Files that will be
-   *          used in the restore script.
+   * @param baselineInspector the inspector for the previous backup.
+   * @param baselineCopyMap this will be populated with baseline oplogs Files that will be used in
+   *        the restore script.
-    File baselineDir = new File(baselineInspector.getBackupDir(),
-        BackupManager.DATA_STORES);
+    File baselineDir = new File(baselineInspector.getBackupDir(), BackupManager.DATA_STORES);
-    List<File> baselineOplogFiles = FileUtil.findAll(baselineDir,
-        ".*\\.[kdc]rf$");
+    List<File> baselineOplogFiles = FileUtil.findAll(baselineDir, ".*\\.[kdc]rf$");
-     * Loop through operation logs and see if they are already part of the
-     * baseline backup.
+     * Loop through operation logs and see if they are already part of the baseline backup.
-        Set<String> matchingOplogs = log
-            .gatherMatchingOplogFiles(baselineInspector
-                .getIncrementalOplogFileNames());
+        Set<String> matchingOplogs =
+            log.gatherMatchingOplogFiles(baselineInspector.getIncrementalOplogFileNames());
-            oplogMap.put(
-                new File(baselineInspector
-                    .getCopyFromForOplogFile(matchingOplog)), new File(
-                    baselineInspector.getCopyToForOplogFile(matchingOplog)));
+            oplogMap.put(new File(baselineInspector.getCopyFromForOplogFile(matchingOplog)),
+                new File(baselineInspector.getCopyToForOplogFile(matchingOplog)));
-         * These have been backed up before so lets just add their entries from
-         * the previous backup or restore script into the current one.
+         * These have been backed up before so lets just add their entries from the previous backup
+         * or restore script into the current one.
-  
+
-  //       hook that notifies them every time an oplog is created.
+  // hook that notifies them every time an oplog is created.
-      this.compactionCompletionRequired = Boolean
-          .getBoolean(COMPLETE_COMPACTION_BEFORE_TERMINATION_PROPERTY_NAME);
+      this.compactionCompletionRequired =
+          Boolean.getBoolean(COMPLETE_COMPACTION_BEFORE_TERMINATION_PROPERTY_NAME);
-     * Stops the thread from compaction and the compactor thread joins with the
-     * calling thread
+     * Stops the thread from compaction and the compactor thread joins with the calling thread
-      synchronized(this) {
+      synchronized (this) {
-     * A non-backup just needs values that are written to one of the oplogs
-     * being compacted that are still alive (have not been deleted or modified
-     * in a future oplog) to be copied forward to the current active oplog
+     * A non-backup just needs values that are written to one of the oplogs being compacted that are
+     * still alive (have not been deleted or modified in a future oplog) to be copied forward to the
+     * current active oplog
-                                                                     * @todo &&
-                                                                     * !owner.
-                                                                     * isDestroyed
+                                                                     * @todo && !owner. isDestroyed
-          new Object[] { totalCount, ((endTime - start) / 1000000) }));
+          new Object[] {totalCount, ((endTime - start) / 1000000)}));
-     * Just do compaction and then check to see if another needs to be done and
-     * if so schedule it. Asif:The compactor thread checks for an oplog in the
-     * LinkedHasMap in a synchronization on the oplogIdToOplog object. This will
-     * ensure that an addition of an Oplog to the Map does not get missed.
-     * Notifications need not be sent if the thread is already compaction
+     * Just do compaction and then check to see if another needs to be done and if so schedule it.
+     * Asif:The compactor thread checks for an oplog in the LinkedHasMap in a synchronization on the
+     * oplogIdToOplog object. This will ensure that an addition of an Oplog to the Map does not get
+     * missed. Notifications need not be sent if the thread is already compaction
-          String tName = "OplogCompactor " + getName() + " for oplog "
-              + oplogs[0].toString();
+          String tName = "OplogCompactor " + getName() + " for oplog " + oplogs[0].toString();
-              new Object[] { getName(), ids }));
+              new Object[] {getName(), ids}));
-                new Object[] { getName(), ids }));
+                new Object[] {getName(), ids}));
-      }
-      finally {
+      } finally {
-      //synchronized (DiskStoreImpl.this.oplogIdToOplog) {
-        if (this.compactorEnabled) {
-          if (isCompactionEnabled()) {
-            schedule(getOplogToBeCompacted());
-          }
+      // synchronized (DiskStoreImpl.this.oplogIdToOplog) {
+      if (this.compactorEnabled) {
+        if (isCompactionEnabled()) {
+          schedule(getOplogToBeCompacted());
-      //}
+      }
+      // }
-  public void memberOfflineAndEqual(DiskRegionView dr,
-      PersistentMemberID persistentID) {
+  public void memberOfflineAndEqual(DiskRegionView dr, PersistentMemberID persistentID) {
-                    new Object[] { Integer.valueOf(loopCount), dr.getName() }));
+                    new Object[] {Integer.valueOf(loopCount), dr.getName()}));
-    InternalDistributedMember memberId = ids.getDistributionManager()
-        .getDistributionManagerId();
-    
-    //NOTE - do NOT use DM.cacheTimeMillis here. See bug #49920
+    InternalDistributedMember memberId = ids.getDistributionManager().getDistributionManagerId();
+
+    // NOTE - do NOT use DM.cacheTimeMillis here. See bug #49920
-        firstDir.getAbsolutePath(), memberId.getName(),
-        timestamp, (short) 0);
+        firstDir.getAbsolutePath(), memberId.getName(), timestamp, (short) 0);
-    InetAddress host = cache.getDistributedSystem().getDistributedMember()
-        .getInetAddress();
+    InetAddress host = cache.getDistributedSystem().getDistributedMember().getInetAddress();
-   * Need a stopper that only triggers if this DiskRegion has been closed. If we
-   * use the LocalRegion's Stopper then our async writer will not be able to
-   * finish flushing on a cache close.
+   * Need a stopper that only triggers if this DiskRegion has been closed. If we use the
+   * LocalRegion's Stopper then our async writer will not be able to finish flushing on a cache
+   * close.
-   * Returns a set of the disk regions that are using this disk store. Note that
-   * this set is read only and live (its contents may change if the regions
-   * using this disk store changes).
+   * Returns a set of the disk regions that are using this disk store. Note that this set is read
+   * only and live (its contents may change if the regions using this disk store changes).
-   * This method is slow and should be optimized if used for anything important.
-   * At this time it was added to do some internal assertions that have since
-   * been removed.
+   * This method is slow and should be optimized if used for anything important. At this time it was
+   * added to do some internal assertions that have since been removed.
-      if(!isOffline()) {
+      if (!isOffline()) {
-      
+
-        throw new IllegalStateException("DiskRegion already exists with id "
-            + dr.getId() + " and name " + old.getName());
+        throw new IllegalStateException(
+            "DiskRegion already exists with id " + dr.getId() + " and name " + old.getName());
-    if(isClosed() && getOwnedByRegion()) {
-      //A region owned disk store will destroy
-      //itself when all buckets are removed, resulting
-      //in an exception when this method is called.
-      //Do nothing if the disk store is already
-      //closed
+    if (isClosed() && getOwnedByRegion()) {
+      // A region owned disk store will destroy
+      // itself when all buckets are removed, resulting
+      // in an exception when this method is called.
+      // Do nothing if the disk store is already
+      // closed
-    final StringId sid = LocalizedStrings.LocalRegion_A_DISKACCESSEXCEPTION_HAS_OCCURED_WHILE_WRITING_TO_THE_DISK_FOR_DISKSTORE_0_THE_CACHE_WILL_BE_CLOSED;
+    final StringId sid =
+        LocalizedStrings.LocalRegion_A_DISKACCESSEXCEPTION_HAS_OCCURED_WHILE_WRITING_TO_THE_DISK_FOR_DISKSTORE_0_THE_CACHE_WILL_BE_CLOSED;
-    final ThreadGroup exceptionHandlingGroup = LoggingThreadGroup.createThreadGroup(
-        "Disk Store Exception Handling Group", logger);
+    final ThreadGroup exceptionHandlingGroup =
+        LoggingThreadGroup.createThreadGroup("Disk Store Exception Handling Group", logger);
-    Thread thread = new Thread(exceptionHandlingGroup,
-        "Disk store exception handler") {
+    Thread thread = new Thread(exceptionHandlingGroup, "Disk store exception handler") {
-          
+
-          logger.error(LocalizedMessage.create(LocalizedStrings.LocalRegion_AN_EXCEPTION_OCCURED_WHILE_CLOSING_THE_CACHE), e);
+          logger.error(LocalizedMessage.create(
+              LocalizedStrings.LocalRegion_AN_EXCEPTION_OCCURED_WHILE_CLOSING_THE_CACHE), e);
-  
+
-    this.criticalPercent = criticalPercent; 
+    this.criticalPercent = criticalPercent;
-  
+
-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids
-   * in the unsigned int range.
+   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int
+   * range.
-    private final IntOpenHashSet ints = new IntOpenHashSet(
-        (int)INVALID_ID);
-    private final LongOpenHashSet longs = new LongOpenHashSet(
-        (int)INVALID_ID);
+    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);
+    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);
-   * Set to true if this diskStore is owned by a single region. This only
-   * happens in backwardsCompat mode.
+   * Set to true if this diskStore is owned by a single region. This only happens in backwardsCompat
+   * mode.
-   * Set to the region's {@link InternalRegionArguments} when the diskStore is
-   * owned by a single region in backwardsCompat mode ({@link #ownedByRegion}
-   * must be true).
+   * Set to the region's {@link InternalRegionArguments} when the diskStore is owned by a single
+   * region in backwardsCompat mode ({@link #ownedByRegion} must be true).
-    return this.upgradeVersionOnly
-        && Version.GFE_70.compareTo(this.getRecoveredGFVersion()) > 0;
+    return this.upgradeVersionOnly && Version.GFE_70.compareTo(this.getRecoveredGFVersion()) > 0;
-  
+
+
-   * @param regName
-   *          the name of the region to destroy
+   * @param regName the name of the region to destroy
-      if (drv == null &&  prConfig == null) {
-        throw new IllegalArgumentException("The disk store does not contain a region named: "
-                                           + regName);
+      if (drv == null && prConfig == null) {
+        throw new IllegalArgumentException(
+            "The disk store does not contain a region named: " + regName);
-  public String modifyRegion(String regName, String lruOption,
-      String lruActionOption, String lruLimitOption,
-      String concurrencyLevelOption, String initialCapacityOption,
-      String loadFactorOption, String compressorClassNameOption,
-      String statisticsEnabledOption, String offHeapOption, boolean printToConsole) {
+  public String modifyRegion(String regName, String lruOption, String lruActionOption,
+      String lruLimitOption, String concurrencyLevelOption, String initialCapacityOption,
+      String loadFactorOption, String compressorClassNameOption, String statisticsEnabledOption,
+      String offHeapOption, boolean printToConsole) {
-        return getDiskInitFile().modifyPRRegion(regName, lruOption,
-            lruActionOption, lruLimitOption, concurrencyLevelOption,
-            initialCapacityOption, loadFactorOption, compressorClassNameOption,
-            statisticsEnabledOption, offHeapOption, printToConsole);
+        return getDiskInitFile().modifyPRRegion(regName, lruOption, lruActionOption, lruLimitOption,
+            concurrencyLevelOption, initialCapacityOption, loadFactorOption,
+            compressorClassNameOption, statisticsEnabledOption, offHeapOption, printToConsole);
-      return getDiskInitFile().modifyRegion(drv, lruOption, lruActionOption,
-          lruLimitOption, concurrencyLevelOption, initialCapacityOption,
-          loadFactorOption, compressorClassNameOption,
-          statisticsEnabledOption, offHeapOption, printToConsole);
+      return getDiskInitFile().modifyRegion(drv, lruOption, lruActionOption, lruLimitOption,
+          concurrencyLevelOption, initialCapacityOption, loadFactorOption,
+          compressorClassNameOption, statisticsEnabledOption, offHeapOption, printToConsole);
+
-      for (Object i: getPdxTypesAndEnums()) {
+      for (Object i : getPdxTypesAndEnums()) {
-          enums.add((EnumInfo)i);
+          enums.add((EnumInfo) i);
-    
+
-      for (PdxType type: types) {
+      for (PdxType type : types) {
-      for (EnumInfo e: enums) {
+      for (EnumInfo e : enums) {
-  private Collection<Object/*PdxType or EnumInfo*/> pdxRename(String oldBase, String newBase) throws IOException {
+  private Collection<Object/* PdxType or EnumInfo */> pdxRename(String oldBase, String newBase)
+      throws IOException {
-    for (RegionEntry re: foundPdx.getRecoveredEntryMap().regionEntries()) {
+    for (RegionEntry re : foundPdx.getRecoveredEntryMap().regionEntries()) {
-  
+
+
-   * If existing matches pattern then return the string with the portion of it
-   * that matched the pattern changed to replacement.
-   * If it did not match return null.
+   * If existing matches pattern then return the string with the portion of it that matched the
+   * pattern changed to replacement. If it did not match return null.
-  public static String replacePdxRenamePattern(Pattern pattern, String existing, String replacement) {
+  public static String replacePdxRenamePattern(Pattern pattern, String existing,
+      String replacement) {
-  private Collection<PdxType> pdxDeleteField(String className, String fieldName) throws IOException {
+
+  private Collection<PdxType> pdxDeleteField(String className, String fieldName)
+      throws IOException {
-    for (RegionEntry re: foundPdx.getRecoveredEntryMap().regionEntries()) {
+    for (RegionEntry re : foundPdx.getRecoveredEntryMap().regionEntries()) {
-  
+
-      //throw new IllegalStateException("The disk store does not contain any PDX types.");
+      // throw new IllegalStateException("The disk store does not contain any PDX types.");
-    for (RegionEntry re: foundPdx.getRecoveredEntryMap().regionEntries()) {
+    for (RegionEntry re : foundPdx.getRecoveredEntryMap().regionEntries()) {
-  private Collection<Object /*PdxType or EnumInfo*/> getPdxTypesAndEnums() throws IOException {
+  private Collection<Object /* PdxType or EnumInfo */> getPdxTypesAndEnums() throws IOException {
-      //throw new IllegalStateException("The disk store does not contain any PDX types.");
+      // throw new IllegalStateException("The disk store does not contain any PDX types.");
-    for (RegionEntry re: foundPdx.getRecoveredEntryMap().regionEntries()) {
+    for (RegionEntry re : foundPdx.getRecoveredEntryMap().regionEntries()) {
-    
+
-        //Add a mapping from the bucket name to the writer for the PR
-        //if this is a bucket.
+        // Add a mapping from the bucket name to the writer for the PR
+        // if this is a bucket.
-      //Some writers are in the map multiple times because of multiple buckets
-      //get a the unique set of writers and close each writer once.
+      // Some writers are in the map multiple times because of multiple buckets
+      // get a the unique set of writers and close each writer once.
-      for(SnapshotWriter writer : uniqueWriters) {
+      for (SnapshotWriter writer : uniqueWriters) {
-      System.out.println("Disk store contains " + getDeadRecordCount()
-          + " compactable records.");
+      System.out.println("Disk store contains " + getDeadRecordCount() + " compactable records.");
-    System.out.println("Total number of region entries in this disk store is: "
-        + getLiveEntryCount());
+    System.out
+        .println("Total number of region entries in this disk store is: " + getLiveEntryCount());
-    
+
-    
-    //TODO soplogs - we need to do offline compaction for
-    //the soplog regions, but that is not currently implemented
+
+    // TODO soplogs - we need to do offline compaction for
+    // the soplog regions, but that is not currently implemented
-        System.out
-            .println("Offline compaction did not find anything to compact.");
+        System.out.println("Offline compaction did not find anything to compact.");
-        System.out.println("Offline compaction removed " + getDeadRecordCount()
-            + " records.");
+        System.out.println("Offline compaction removed " + getDeadRecordCount() + " records.");
-   * If we have recovered a bucket earlier for the given pr then we will have an
-   * LRUStatistics to return for it. Otherwise return null.
+   * If we have recovered a bucket earlier for the given pr then we will have an LRUStatistics to
+   * return for it. Otherwise return null.
-   * Lock the disk store to prevent updates. This is the first step of the
-   * backup process. Once all disk stores on all members are locked, we still
-   * move on to startBackup.
+   * Lock the disk store to prevent updates. This is the first step of the backup process. Once all
+   * disk stores on all members are locked, we still move on to startBackup.
-   * Release the lock that is preventing operations on this disk store during
-   * the backup process.
+   * Release the lock that is preventing operations on this disk store during the backup process.
-   * Start the backup process. This is the second step of the backup process. In
-   * this method, we define the data we're backing up by copying the init file
-   * and rolling to the next file. After this method returns operations can
-   * proceed as normal, except that we don't remove oplogs.
+   * Start the backup process. This is the second step of the backup process. In this method, we
+   * define the data we're backing up by copying the init file and rolling to the next file. After
+   * this method returns operations can proceed as normal, except that we don't remove oplogs.
-        
-        //Get an appropriate lock object for each set of oplogs.
+
+        // Get an appropriate lock object for each set of oplogs.
-        
+
-        
-        //This ensures that all writing to disk is blocked while we are
-        //creating the snapshot
+
+        // This ensures that all writing to disk is blocked while we are
+        // creating the snapshot
-          
+
-   * Copy the oplogs to the backup directory. This is the final step of the
-   * backup process. The oplogs we copy are defined in the startBackup method.
+   * Copy the oplogs to the backup directory. This is the final step of the backup process. The
+   * oplogs we copy are defined in the startBackup method.
-      //Wait for oplogs to be unpreblown before backing them up.
+      // Wait for oplogs to be unpreblown before backing them up.
-      
-      //Backup all of the oplogs
+
+      // Backup all of the oplogs
-        File backupDir = getBackupDir(this.diskStoreBackup.getTargetDir(),
-            index);
+        File backupDir = getBackupDir(this.diskStoreBackup.getTargetDir(), index);
-    for(DirectoryHolder holder : directories) {
-      if(holder.getDir().equals(searchDir)) {
+    for (DirectoryHolder holder : directories) {
+      if (holder.getDir().equals(searchDir)) {
-  
-  public DirectoryHolder[] getDirectoryHolders(){
+
+  public DirectoryHolder[] getDirectoryHolders() {
-  private static DiskStoreImpl createForOffline(String dsName, File[] dsDirs)
-      throws Exception {
-    return createForOffline(dsName, dsDirs, false, false,
-        false/* upgradeVersionOnly */, 0, true, false);
+  private static DiskStoreImpl createForOffline(String dsName, File[] dsDirs) throws Exception {
+    return createForOffline(dsName, dsDirs, false, false, false/* upgradeVersionOnly */, 0, true,
+        false);
-    return createForOffline(dsName, dsDirs, false, false, false, 0,
-        true/*needsOplogs*/, true/*offlineModify*/);
+    return createForOffline(dsName, dsDirs, false, false, false, 0, true/* needsOplogs */,
+        true/* offlineModify */);
-  private static DiskStoreImpl createForOffline(String dsName, File[] dsDirs,
-      boolean needsOplogs) throws Exception {
-    return createForOffline(dsName, dsDirs, false, false,
-        false/* upgradeVersionOnly */, 0, needsOplogs, false);
+  private static DiskStoreImpl createForOffline(String dsName, File[] dsDirs, boolean needsOplogs)
+      throws Exception {
+    return createForOffline(dsName, dsDirs, false, false, false/* upgradeVersionOnly */, 0,
+        needsOplogs, false);
-  private static DiskStoreImpl createForOfflineValidate(String dsName,
-      File[] dsDirs) throws Exception {
-    return createForOffline(dsName, dsDirs, false, true,
-        false/* upgradeVersionOnly */, 0, true, false);
+  private static DiskStoreImpl createForOfflineValidate(String dsName, File[] dsDirs)
+      throws Exception {
+    return createForOffline(dsName, dsDirs, false, true, false/* upgradeVersionOnly */, 0, true,
+        false);
-      boolean offlineCompacting, boolean offlineValidate,
-      boolean upgradeVersionOnly, long maxOplogSize, boolean needsOplogs, boolean offlineModify)
-      throws Exception {
+      boolean offlineCompacting, boolean offlineValidate, boolean upgradeVersionOnly,
+      long maxOplogSize, boolean needsOplogs, boolean offlineModify) throws Exception {
-      dsDirs = new File[] { new File("") };
+      dsDirs = new File[] {new File("")};
-    org.apache.geode.cache.DiskStoreFactory dsf = c
-        .createDiskStoreFactory();
+    org.apache.geode.cache.DiskStoreFactory dsf = c.createDiskStoreFactory();
-        ((DiskStoreFactoryImpl) dsf).getDiskStoreAttributes(), false, null,
-        true, upgradeVersionOnly, offlineValidate, offlineCompacting,
-        needsOplogs, offlineModify);
+        ((DiskStoreFactoryImpl) dsf).getDiskStoreAttributes(), false, null, true,
+        upgradeVersionOnly, offlineValidate, offlineCompacting, needsOplogs, offlineModify);
-   * @param dsName
-   *          the name of the disk store
-   * @param dsDirs
-   *          the directories that that the disk store wrote files to
-   * @param regName
-   *          the name of the region to destroy
+   * @param dsName the name of the disk store
+   * @param dsDirs the directories that that the disk store wrote files to
+   * @param regName the name of the region to destroy
-  public static void destroyRegion(String dsName, File[] dsDirs, String regName)
-      throws Exception {
+  public static void destroyRegion(String dsName, File[] dsDirs, String regName) throws Exception {
-  public static String modifyRegion(String dsName, File[] dsDirs,
-      String regName, String lruOption, String lruActionOption,
-      String lruLimitOption, String concurrencyLevelOption,
-      String initialCapacityOption, String loadFactorOption,
-      String compressorClassNameOption, String statisticsEnabledOption,
-      String offHeapOption,
-      boolean printToConsole) throws Exception {
+  public static String modifyRegion(String dsName, File[] dsDirs, String regName, String lruOption,
+      String lruActionOption, String lruLimitOption, String concurrencyLevelOption,
+      String initialCapacityOption, String loadFactorOption, String compressorClassNameOption,
+      String statisticsEnabledOption, String offHeapOption, boolean printToConsole)
+      throws Exception {
-      return dsi.modifyRegion(regName, lruOption, lruActionOption,
-          lruLimitOption, concurrencyLevelOption, initialCapacityOption,
-          loadFactorOption, compressorClassNameOption,
-          statisticsEnabledOption, offHeapOption, printToConsole);
+      return dsi.modifyRegion(regName, lruOption, lruActionOption, lruLimitOption,
+          concurrencyLevelOption, initialCapacityOption, loadFactorOption,
+          compressorClassNameOption, statisticsEnabledOption, offHeapOption, printToConsole);
-  public static void dumpInfo(PrintStream printStream, String dsName,
-      File[] dsDirs, String regName, Boolean listPdxTypes) throws Exception {  
+  public static void dumpInfo(PrintStream printStream, String dsName, File[] dsDirs, String regName,
+      Boolean listPdxTypes) throws Exception {
-  public static void dumpMetadata(String dsName, File[] dsDirs,
-      boolean showBuckets) throws Exception {
+  public static void dumpMetadata(String dsName, File[] dsDirs, boolean showBuckets)
+      throws Exception {
-  public static void exportOfflineSnapshot(String dsName, File[] dsDirs,
-      File out) throws Exception {
+  public static void exportOfflineSnapshot(String dsName, File[] dsDirs, File out)
+      throws Exception {
-  public static Collection<Object/*PdxType or EnumInfo*/> pdxRename(String dsName, File[] dsDirs, String oldRegEx, String newName) throws Exception {
+  public static Collection<Object/* PdxType or EnumInfo */> pdxRename(String dsName, File[] dsDirs,
+      String oldRegEx, String newName) throws Exception {
-  public static Collection<PdxType> pdxDeleteField(String dsName, File[] dsDirs, String className, String fieldName) throws Exception {
+  public static Collection<PdxType> pdxDeleteField(String dsName, File[] dsDirs, String className,
+      String fieldName) throws Exception {
-  public static DiskStoreImpl offlineCompact(String name, File[] dirs,
-      boolean upgradeVersionOnly, long maxOplogSize) throws Exception {
+  public static DiskStoreImpl offlineCompact(String name, File[] dirs, boolean upgradeVersionOnly,
+      long maxOplogSize) throws Exception {
-      DiskStoreImpl dsi = createForOffline(name, dirs, true, false,
-          upgradeVersionOnly, maxOplogSize, true, false);
+      DiskStoreImpl dsi =
+          createForOffline(name, dirs, true, false, upgradeVersionOnly, maxOplogSize, true, false);
-    public ValueRecoveryTask(Set<Oplog> oplogSet,
-        Map<Long, DiskRecoveryStore> recoveredStores) {
+    public ValueRecoveryTask(Set<Oplog> oplogSet, Map<Long, DiskRecoveryStore> recoveredStores) {
-      this.recoveredStores = new HashMap<Long, DiskRecoveryStore>(
-          recoveredStores);
+      this.recoveredStores = new HashMap<Long, DiskRecoveryStore>(recoveredStores);
-            DiskStoreImpl.this.currentAsyncValueRecoveryMap.keySet().removeAll(
-                recoveredStores.keySet());
+            DiskStoreImpl.this.currentAsyncValueRecoveryMap.keySet()
+                .removeAll(recoveredStores.keySet());
-      while (!isClosing()
-          && currentAsyncValueRecoveryMap.containsKey(diskRegion.getId())) {
+      while (!isClosing() && currentAsyncValueRecoveryMap.containsKey(diskRegion.getId())) {
-  
+
-  
+
-   * Execute a task which must be performed asnychronously, but has no requirement
-   * for timely execution. This task pool is used for compactions, creating KRFS, etc.
-   * So some of the queued tasks may take a while.
+   * Execute a task which must be performed asnychronously, but has no requirement for timely
+   * execution. This task pool is used for compactions, creating KRFS, etc. So some of the queued
+   * tasks may take a while.
-    return executeDiskStoreTask(runnable, this.diskStoreTaskPool) != null;
+    return executeDiskStoreAsyncTask(runnable, this.diskStoreTaskPool);
-  
-  /** 
-   * Execute a task asynchronously, or in the calling thread if the bound
-   * is reached. This pool is used for write operations which can be delayed,
-   * but we have a limit on how many write operations we delay so that
-   * we don't run out of disk space. Used for deletes, unpreblow, RAF close, etc.
+
+  /**
+   * Execute a task asynchronously, or in the calling thread if the bound is reached. This pool is
+   * used for write operations which can be delayed, but we have a limit on how many write
+   * operations we delay so that we don't run out of disk space. Used for deletes, unpreblow, RAF
+   * close, etc.
-  
+
-   * Wait for any current operations in the delayed write pool. Completion
-   * of this method ensures that the writes have completed or the pool was shutdown
+   * Wait for any current operations in the delayed write pool. Completion of this method ensures
+   * that the writes have completed or the pool was shutdown
-    if(lastWriteTask != null) {
+    if (lastWriteTask != null) {
-        //do nothing, an exception from the write task was already logged.
+        // do nothing, an exception from the write task was already logged.
- // schedule another thread to do it
+    // schedule another thread to do it
-          //getCache().getCachePerfStats().decDiskTasksWaiting();
+          // getCache().getCachePerfStats().decDiskTasksWaiting();
-    if(result == null) {
+    if (result == null) {
+  private boolean executeDiskStoreAsyncTask(final Runnable runnable, ThreadPoolExecutor executor) {
+    // schedule another thread to do it
+    incBackgroundTasks();
+    boolean isTaskAccepted = executeDiskStoreAsyncTask(new DiskStoreTask() {
+      public void run() {
+        try {
+          markBackgroundTaskThread(); // for bug 42775
+          // getCache().getCachePerfStats().decDiskTasksWaiting();
+          runnable.run();
+        } finally {
+          decBackgroundTasks();
+        }
+      }
+
+      public void taskCancelled() {
+        decBackgroundTasks();
+      }
+    }, executor);
+
+    if (!isTaskAccepted) {
+      decBackgroundTasks();
+    }
+
+    return isTaskAccepted;
+  }
+
+  private boolean executeDiskStoreAsyncTask(DiskStoreTask r, ThreadPoolExecutor executor) {
+    try {
+      executor.execute(r);
+      return true;
+    } catch (RejectedExecutionException ex) {
+      if (logger.isDebugEnabled()) {
+        logger.debug("Ignored compact schedule during shutdown", ex);
+      }
+    }
+    return false;
+  }
+
-    
-    //Allow the delayed writes to complete
+
+    // Allow the delayed writes to complete
-  
+
- // All the regions have already been closed
+    // All the regions have already been closed
-  
+
-                .toLocalizedString(), dr.getName());
+                .toLocalizedString(),
+            dr.getName());
-      for (VersionSource member : (Collection<VersionSource>) inMemoryRVV
-          .getMemberToGCVersion().keySet()) {
+      for (VersionSource member : (Collection<VersionSource>) inMemoryRVV.getMemberToGCVersion()
+          .keySet()) {
-                .toLocalizedString(), dr.getName());
+                .toLocalizedString(),
+            dr.getName());
-      RegionVersionVector inMemoryRVV = (region==null)?null:region.getVersionVector();
+      RegionVersionVector inMemoryRVV = (region == null) ? null : region.getVersionVector();
-   * Update the on disk GC version for the given member, only if the disk has
-   * actually recorded all of the updates including that member.
+   * Update the on disk GC version for the given member, only if the disk has actually recorded all
+   * of the updates including that member.
-   * @param diskRVV
-   *          the RVV for what has been persisted
-   * @param inMemoryRVV
-   *          the RVV of what is in memory
-   * @param member
-   *          The member we're trying to update
+   * @param diskRVV the RVV for what has been persisted
+   * @param inMemoryRVV the RVV of what is in memory
+   * @param member The member we're trying to update
-  private void updateDiskGCRVV(RegionVersionVector diskRVV,
-      RegionVersionVector inMemoryRVV, VersionSource member) {
+  private void updateDiskGCRVV(RegionVersionVector diskRVV, RegionVersionVector inMemoryRVV,
+      VersionSource member) {
-  
+
-  
+

INS31 INS31 INS83 INS39 INS42 INS44 INS44 INS8 INS83 INS39 INS42 INS44 INS44 INS8 UPD66 UPD66 INS83 INS43 INS42 INS43 INS42 INS21 INS60 INS25 INS41 INS43 INS42 INS43 INS42 INS54 INS41 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 MOV32 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 INS42 INS42 INS32 INS39 INS59 INS38 INS8 INS42 INS42 INS42 INS8 INS12 INS9 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD42 INS42 INS42 INS32 INS42 INS21 INS21 INS41 INS44 INS8 INS42 INS14 INS42 INS32 INS32 INS9 INS43 INS42 INS25 INS43 INS1 INS42 INS42 INS42 INS42 INS42 INS32 INS8 INS42 INS31 INS31 INS42 INS42 INS21 INS83 INS39 INS42 INS8 INS83 INS39 INS42 INS8 INS32 INS54 INS21 INS42 INS42 INS45 INS42 INS8 INS8 INS32 INS21 INS21 INS21 INS42 INS32 INS32 INS32 INS42 INS42 INS42 INS42 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL66 DEL33 DEL27