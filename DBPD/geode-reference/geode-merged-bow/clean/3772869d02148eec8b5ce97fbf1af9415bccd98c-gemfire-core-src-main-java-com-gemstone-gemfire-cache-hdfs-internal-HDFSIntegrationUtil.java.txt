GEODE-10: Refactor HdfsStore api to match spec

* Currently HdfsStore's configuration object is nested and a user needs to
  create multiple sub objects to manage the store instance. This is less usable
  and gets confusing at times. User also gets exposed to a lot of internal
  details. So replacing nested configuration with a flat structure will be
  better.
* Rename members

-
-import com.gemstone.gemfire.cache.hdfs.HDFSEventQueueAttributes;
-import com.gemstone.gemfire.cache.hdfs.HDFSEventQueueAttributesFactory;
-  
-  public static <K, V> AsyncEventQueue createDefaultAsyncQueueForHDFS(Cache cache, 
-     boolean writeOnly, String regionPath)
-  {
-    // Create default event attributes 
-    HDFSEventQueueAttributesFactory  hdfsqueueFactory = new HDFSEventQueueAttributesFactory();
-    return createAsyncQueueForHDFS(cache,
-        regionPath, writeOnly, hdfsqueueFactory.create());
-  }
-  
-  public static AsyncEventQueue createAsyncQueueForHDFS(Cache cache,
-      String regionPath, boolean writeOnly, HDFSEventQueueAttributes eventAttribs)
-   {
-     LogWriterI18n logger = cache.getLoggerI18n();
-     String defaultAsyncQueueName = HDFSStoreFactoryImpl.getEventQueueName(regionPath);
-     AsyncEventQueueFactory factory = cache.createAsyncEventQueueFactory();
-     factory.setBatchSize(eventAttribs.getBatchSizeMB());
-     factory.setPersistent(eventAttribs.isPersistent());
-     factory.setDiskStoreName(eventAttribs.getDiskStoreName());
-     factory.setMaximumQueueMemory(eventAttribs.getMaximumQueueMemory());
-     factory.setBatchTimeInterval(eventAttribs.getBatchTimeInterval());
-     factory.setDiskSynchronous(eventAttribs.isDiskSynchronous());
-     factory.setDiskSynchronous(eventAttribs.isDiskSynchronous());
-     factory.setDispatcherThreads(eventAttribs.getDispatcherThreads());
-     factory.setParallel(true);
-     factory.addGatewayEventFilter(new HDFSEventQueueFilter(logger));
-     ((AsyncEventQueueFactoryImpl)factory).setBucketSorted(!writeOnly);
-     ((AsyncEventQueueFactoryImpl)factory).setIsHDFSQueue(true);
-     
-     AsyncEventQueue asyncQ = null;
-     
-     if (!writeOnly)
-       asyncQ = factory.create(defaultAsyncQueueName, new HDFSEventListener(cache.getLoggerI18n()));
-     else
-       asyncQ = factory.create(defaultAsyncQueueName, new HDFSWriteOnlyStoreEventListener(cache.getLoggerI18n()));
-     
-     logger.fine("HDFS: async queue created for HDFS. Id: " + asyncQ.getId() + ". Disk store: " + asyncQ.getDiskStoreName() + 
-         ". Batch size: " + asyncQ.getBatchSize() + ". bucket sorted:  " + !writeOnly) ;
-     return asyncQ;
-     
-   }
-  
-  public static  void createAndAddAsyncQueue(String regionPath,
-      RegionAttributes regionAttributes, Cache cache) {
-    if(!regionAttributes.getDataPolicy().withHDFS()) {
-      return;
+  public static <K, V> AsyncEventQueue createDefaultAsyncQueueForHDFS(Cache cache, boolean writeOnly, String regionPath) {
+    return createAsyncQueueForHDFS(cache, regionPath, writeOnly, null);
+  }
+
+  private static AsyncEventQueue createAsyncQueueForHDFS(Cache cache, String regionPath, boolean writeOnly,
+      HDFSStore configView) {
+    LogWriterI18n logger = cache.getLoggerI18n();
+    String defaultAsyncQueueName = HDFSStoreFactoryImpl.getEventQueueName(regionPath);
+
+    if (configView == null) {
+      configView = new HDFSStoreFactoryImpl(cache).getConfigView();
+
+    AsyncEventQueueFactory factory = cache.createAsyncEventQueueFactory();
+    factory.setBatchSize(configView.getBatchSize());
+    factory.setPersistent(configView.getBufferPersistent());
+    factory.setDiskStoreName(configView.getDiskStoreName());
+    factory.setMaximumQueueMemory(configView.getMaxMemory());
+    factory.setBatchTimeInterval(configView.getBatchInterval());
+    factory.setDiskSynchronous(configView.getSynchronousDiskWrite());
+    factory.setDispatcherThreads(configView.getDispatcherThreads());
+    factory.setParallel(true);
+    factory.addGatewayEventFilter(new HDFSEventQueueFilter(logger));
+    ((AsyncEventQueueFactoryImpl) factory).setBucketSorted(!writeOnly);
+    ((AsyncEventQueueFactoryImpl) factory).setIsHDFSQueue(true);
+
+    AsyncEventQueue asyncQ = null;
+
+    if (!writeOnly)
+      asyncQ = factory.create(defaultAsyncQueueName, new HDFSEventListener(cache.getLoggerI18n()));
+    else
+      asyncQ = factory.create(defaultAsyncQueueName, new HDFSWriteOnlyStoreEventListener(cache.getLoggerI18n()));
+
+    logger.fine("HDFS: async queue created for HDFS. Id: " + asyncQ.getId() + ". Disk store: "
+        + asyncQ.getDiskStoreName() + ". Batch size: " + asyncQ.getBatchSize() + ". bucket sorted:  " + !writeOnly);
+    return asyncQ;
+
+  }
+
+  public static void createAndAddAsyncQueue(String regionPath, RegionAttributes regionAttributes, Cache cache) {
+    if (!regionAttributes.getDataPolicy().withHDFS()) {
+      return;
+    }
+
-    
+
-      if (regionAttributes.getHDFSStoreName() != null && regionAttributes.getPartitionAttributes() != null 
+      if (regionAttributes.getHDFSStoreName() != null && regionAttributes.getPartitionAttributes() != null
-        HDFSStore store = ((GemFireCacheImpl)cache).findHDFSStore(regionAttributes.getHDFSStoreName());
+        HDFSStore store = ((GemFireCacheImpl) cache).findHDFSStore(regionAttributes.getHDFSStoreName());
-              LocalizedStrings.HOPLOG_HDFS_STORE_NOT_FOUND
-                  .toLocalizedString(regionAttributes.getHDFSStoreName()));
+              LocalizedStrings.HOPLOG_HDFS_STORE_NOT_FOUND.toLocalizedString(regionAttributes.getHDFSStoreName()));
-        HDFSEventQueueAttributes queueAttrs = store.getHDFSEventQueueAttributes();
-        if(queueAttrs == null) {
-          // no async queue is specified for region with a HDFS store. Create a async queue with default 
-          // properties and set the bucketsorted=true.
-          HDFSIntegrationUtil.createDefaultAsyncQueueForHDFS(cache, regionAttributes.getHDFSWriteOnly(), leaderRegionPath);
-        }
-        else {
-          HDFSIntegrationUtil.createAsyncQueueForHDFS(cache, leaderRegionPath, regionAttributes.getHDFSWriteOnly(), queueAttrs);
-        }
+        HDFSIntegrationUtil
+            .createAsyncQueueForHDFS(cache, leaderRegionPath, regionAttributes.getHDFSWriteOnly(), store);
-  private static String getLeaderRegionPath(String regionPath,
-      RegionAttributes regionAttributes, Cache cache) {
+  private static String getLeaderRegionPath(String regionPath, RegionAttributes regionAttributes, Cache cache) {
-    while(regionAttributes.getPartitionAttributes() != null 
+    while (regionAttributes.getPartitionAttributes() != null
-      GemFireCacheImpl gfc = (GemFireCacheImpl)cache;
+      GemFireCacheImpl gfc = (GemFireCacheImpl) cache;
-      if(colocatedRegion == null) {
+      if (colocatedRegion == null) {

UPD83 UPD43 UPD42 INS25 UPD42 INS27 INS8 INS33 INS42 INS33 INS21 INS7 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 UPD42 INS42 INS32 MOV21 INS14 INS42 INS43 INS42 UPD42 INS42 DEL40 DEL26 DEL40 DEL26 DEL42 DEL43 DEL42 DEL42 DEL43 DEL14 DEL59 DEL60 DEL42 DEL42 DEL32 DEL42 DEL42 DEL42 DEL42 DEL32 DEL32 DEL21 DEL42 DEL43 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL42 DEL33 DEL27 DEL42 DEL42 DEL42 DEL42 DEL42 DEL32 DEL42 DEL32 DEL21 DEL8 DEL8 DEL25