Merge branch 'release/1.0.0-incubating.M3'

+import java.util.regex.Matcher;
-import com.gemstone.gemfire.cache.hdfs.internal.HDFSBucketRegionQueue;
-import com.gemstone.gemfire.cache.hdfs.internal.HDFSIntegrationUtil;
-import com.gemstone.gemfire.cache.hdfs.internal.HoplogListenerForRegion;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSRegionDirector;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSRegionDirector.HdfsRegionManager;
-import com.gemstone.gemfire.cache.query.internal.IndexUpdater;
+import com.gemstone.gemfire.distributed.internal.DistributionConfig;
+import com.gemstone.gemfire.i18n.StringId;
+import com.gemstone.gemfire.internal.admin.ClientHealthMonitoringRegion;
+import com.gemstone.gemfire.internal.offheap.annotations.Released;
-import com.gemstone.gemfire.i18n.StringId;
-public class LocalRegion extends AbstractRegion 
+public class LocalRegion extends AbstractRegion
-  
+  private static final Pattern NAME_PATTERN = Pattern.compile("[aA-zZ0-9-_.]+");
+
-   * @since 5.7
+   * @since GemFire 5.7
-
-  public static final String EXPIRY_MS_PROPERTY = "gemfire.EXPIRY_UNITS_MS";
+  public static final String EXPIRY_MS_PROPERTY = DistributionConfig.GEMFIRE_PREFIX + "EXPIRY_UNITS_MS";
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-  
-  protected HdfsRegionManager hdfsManager;
-  protected HoplogListenerForRegion hoplogListener;
-
-  /**
-   * Return true if the keys of this region implement
-   * {@link KeyWithRegionContext} that require region specific context
-   * initialization after deserialization or recovery from disk.
-   * 
-   * Currently used by SQLFabric for the optimized
-   * <code>CompactCompositeRegionKey</code> that points to the raw row bytes and
-   * so requires a handle to table schema for interpretation of those bytes.
-   */
-  public boolean keyRequiresRegionContext() {
-    return this.keyRequiresRegionContext;
-  }
-
-  /**
-   * Set the {@link #keyRequiresRegionContext} flag to given value.
-   */
-  public final void setKeyRequiresRegionContext(boolean v) {
-    this.keyRequiresRegionContext = v;
-  }
-
+  protected Map<String,CacheServiceProfile> cacheServiceProfiles;
+
-    // set the user-attribute object upfront for SQLFabric
-    setKeyRequiresRegionContext(internalRegionArgs.keyRequiresRegionContext());
-    this.hdfsManager = initHDFSManager();
-    
+    this.cacheServiceProfiles = internalRegionArgs.getCacheServiceProfiles() == null
+        ? null
+        : Collections.unmodifiableMap(internalRegionArgs.getCacheServiceProfiles());
+
-  private HdfsRegionManager initHDFSManager() {
-    HdfsRegionManager hdfsMgr = null;
-    if (this.getHDFSStoreName() != null) {
-      this.hoplogListener = new HoplogListenerForRegion();
-      HDFSRegionDirector.getInstance().setCache(cache);
-      hdfsMgr = HDFSRegionDirector.getInstance().manageRegion(this, 
-          this.getHDFSStoreName(), hoplogListener);
-    }
-    return hdfsMgr;
-  }
-
-	if ((internalRegionArgs.isReadWriteHDFSRegion()) && this.diskRegion != null) {
-      this.diskRegion.setEntriesMapIncompatible(true);
-    }
-  public IndexUpdater getIndexUpdater() {
-    return this.entries.getIndexUpdater();
-  }
-
-   * @since 5.7
+   * @since GemFire 5.7
-   * @since 5.0
+   * @since GemFire 5.0
-        validateRegionName(subregionName);
+        validateRegionName(subregionName, internalRegionArgs);
-            // create the async queue for HDFS if required. 
-            HDFSIntegrationUtil.createAndAddAsyncQueue(regionPath,
-                regionAttributes, this.cache);
-            regionAttributes = cache.setEvictionAttributesForLargeRegion(
-                regionAttributes);
-              internalRegionArgs.setIndexUpdater(pr.getIndexUpdater());
-              internalRegionArgs.setKeyRequiresRegionContext(pr
-                  .keyRequiresRegionContext());
-                if (!pr.isShadowPRForHDFS()) {
-                    newRegion = new BucketRegionQueue(subregionName, regionAttributes,
-                      this, this.cache, internalRegionArgs);
-                }
-                else {
-                   newRegion = new HDFSBucketRegionQueue(subregionName, regionAttributes,
-                      this, this.cache, internalRegionArgs);
-                }
-                
+                newRegion = new BucketRegionQueue(subregionName, regionAttributes,
+                  this, this.cache, internalRegionArgs);
-    EntryEventImpl event = newCreateEntryEvent(key, value, aCallbackArgument);
-    validatedCreate(event, startPut);
-    // TODO OFFHEAP: validatedCreate calls freeOffHeapResources
+    @Released EntryEventImpl event = newCreateEntryEvent(key, value, aCallbackArgument);
+    try {
+      validatedCreate(event, startPut);
+    } finally {
+      event.release();
+    }
-    try {
-      assert event.isFetchFromHDFS() : "validatedPut() should have been called";
-    } finally {
-
-      event.release();
-
-    }
-  // split into a separate newCreateEntryEvent since SQLFabric may need to
-  // manipulate event before doing the put (e.g. posDup flag)
+  @Retained
-    return !(isUsedForPartitionedRegionAdmin()
-        || isUsedForPartitionedRegionBucket() );
+    return !isUsedForPartitionedRegionAdmin();
-    EntryEventImpl event = newDestroyEntryEvent(key, aCallbackArgument);
-    return validatedDestroy(key, event);
-    // TODO OFFHEAP: validatedDestroy calls freeOffHeapResources
+    @Released EntryEventImpl event = newDestroyEntryEvent(key, aCallbackArgument);
+    try {
+      return validatedDestroy(key, event);
+    } finally {
+      event.release();
+    }
-    try {
-    } finally {
-      event.release();
-    }
-  // split into a separate newDestroyEntryEvent since SQLFabric may need to
-  // manipulate event before doing the put (e.g. posDup flag)
+  @Retained
-  public final Object getDeserializedValue(RegionEntry re, final KeyInfo keyInfo, final boolean updateStats, boolean disableCopyOnRead, 
-  boolean preferCD, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS, boolean retainResult) {
+  public final Object getDeserializedValue(RegionEntry re,
+                                           final KeyInfo keyInfo,
+                                           final boolean updateStats,
+                                           boolean disableCopyOnRead,
+                                           boolean preferCD,
+                                           EntryEventImpl clientEvent,
+                                           boolean returnTombstones,
+                                           boolean retainResult) {
-        if (allowReadFromHDFS) {
-          re = this.entries.getEntry(keyInfo.getKey());
-        } else {
-          re = this.entries.getOperationalEntryInVM(keyInfo.getKey());
-        }
+        re = this.entries.getEntry(keyInfo.getKey());
+    boolean disabledLRUCallback = this.entries.disableLruUpdateCallback();
+    } finally {
+      if(disabledLRUCallback) {
+        this.entries.enableLruUpdateCallback();
+        this.entries.lruUpdateCallback();
+      }
-    Object result = get(key, aCallbackArgument, generateCallbacks, false, false, null, clientEvent, false, true/*allowReadFromHDFS*/);
+    Object result = get(key, aCallbackArgument, generateCallbacks, false, false, null, clientEvent, false);
-  public Object get(Object key, Object aCallbackArgument,
-	      boolean generateCallbacks, boolean disableCopyOnRead, boolean preferCD,
-	      ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) throws TimeoutException, CacheLoaderException {
+  public Object get(Object key,
+                    Object aCallbackArgument,
+                    boolean generateCallbacks,
+                    boolean disableCopyOnRead,
+                    boolean preferCD,
+                    ClientProxyMembershipID requestingClient,
+                    EntryEventImpl clientEvent,
+                    boolean returnTombstones) throws TimeoutException, CacheLoaderException {
-		      generateCallbacks, disableCopyOnRead, preferCD,requestingClient, clientEvent, returnTombstones, false, allowReadFromHDFS, false);
+		      generateCallbacks, disableCopyOnRead, preferCD,requestingClient, clientEvent, returnTombstones, false, false);
-    // TODO OFFHEAP: the last parameter "retainResult" should be true for getRetained. Need to look into what it is being set to false.
-    return get(key, aCallbackArgument, generateCallbacks, disableCopyOnRead, true, requestingClient, clientEvent, returnTombstones, opScopeIsLocal, true, false);
+    return get(key, aCallbackArgument, generateCallbacks, disableCopyOnRead, true, requestingClient, clientEvent, returnTombstones, opScopeIsLocal,
+      false /* see GEODE-1291*/);
-      boolean generateCallbacks, boolean disableCopyOnRead, boolean preferCD,
-      ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones, 
-	  boolean opScopeIsLocal, boolean allowReadFromHDFS, boolean retainResult) throws TimeoutException, CacheLoaderException
+                    boolean generateCallbacks, boolean disableCopyOnRead, boolean preferCD,
+                    ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones,
+                    boolean opScopeIsLocal, boolean retainResult) throws TimeoutException, CacheLoaderException
-      Object value = getDataView().getDeserializedValue(keyInfo, this, true, disableCopyOnRead, preferCD, clientEvent, returnTombstones, allowReadFromHDFS, retainResult);
+      Object value = getDataView().getDeserializedValue(keyInfo, this, true, disableCopyOnRead, preferCD, clientEvent, returnTombstones,
+        retainResult);
-            && ((getScope().isDistributed() && !isHDFSRegion())
+            && ((getScope().isDistributed())
-          // TODO OFFHEAP OPTIMIZE: findObject can be enhanced to use the retainResult flag
-              preferCD, requestingClient, clientEvent, returnTombstones, false/*allowReadFromHDFS*/);
+              preferCD, requestingClient, clientEvent, returnTombstones);
-    if (re == null && !isTX() && !isHDFSRegion()) {
+    if (re == null && !isTX()) {
-   * @return true if this region has been configured for HDFS persistence
-   */
-  public boolean isHDFSRegion() {
-    return false;
-  }
-
-  /**
-   * @return true if this region is configured to read and write data from HDFS
-   */
-  public boolean isHDFSReadWriteRegion() {
-    return false;
-  }
-
-  /**
-   * @return true if this region is configured to only write to HDFS
-   */
-  protected boolean isHDFSWriteOnly() {
-    return false;
-  }
-
-  /**
-   * FOR TESTING ONLY
-   */
-  public HoplogListenerForRegion getHoplogListener() {
-    return hoplogListener;
-  }
-  
-  /**
-   * FOR TESTING ONLY
-   */
-  public HdfsRegionManager getHdfsRegionManager() {
-    return hdfsManager;
-  }
-  
-  /**
-   *
-   * @param keyInfo
+   *  @param keyInfo
-   *                the value retrieved from the region for this object.
+*                the value retrieved from the region for this object.
-  Object nonTxnFindObject(KeyInfo keyInfo, boolean p_isCreate,
-      boolean generateCallbacks, Object p_localValue, boolean disableCopyOnRead, boolean preferCD,
-      ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) 
+  Object nonTxnFindObject(KeyInfo keyInfo,
+                          boolean p_isCreate,
+                          boolean generateCallbacks,
+                          Object p_localValue,
+                          boolean disableCopyOnRead,
+                          boolean preferCD,
+                          ClientProxyMembershipID requestingClient,
+                          EntryEventImpl clientEvent,
+                          boolean returnTombstones)
-        localValue = getDeserializedValue(null, keyInfo, isCreate, disableCopyOnRead, preferCD, clientEvent, false, false/*allowReadFromHDFS*/, false);
+        localValue = getDeserializedValue(null, keyInfo, isCreate, disableCopyOnRead, preferCD, clientEvent, false,
+          false);
-            localValue, disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, false/*allowReadFromHDFS*/);
+            localValue, disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones);
-            localValue, disableCopyOnRead, preferCD, null, null, false, allowReadFromHDFS);
+            localValue, disableCopyOnRead, preferCD, null, null, false);
-   * @since 4.0
+   * @since GemFire 4.0
-   * @since 4.0
+   * @since GemFire 4.0
-    EntryEventImpl event = newUpdateEntryEvent(key, value, aCallbackArgument);
-     //Since Sqlfire directly calls validatedPut, the freeing is done in
-    // validatedPut
-     return validatedPut(event, startPut);
-     // TODO OFFHEAP: validatedPut calls freeOffHeapResources
-    
+    @Released EntryEventImpl event = newUpdateEntryEvent(key, value, aCallbackArgument);
+    try {
+      return validatedPut(event, startPut);
+    } finally {
+      event.release();
+    }
-    try {
-      // Sqlf changes begin
-      // see #40294.
-
-      // Rahul: this has to be an update.
-      // so executing it as an update.
-      boolean forceUpdateForDelta = event.hasDelta();
-      // Sqlf Changes end.
-          forceUpdateForDelta, // ifOld
+          false, // ifOld
-    } finally {
-      event.release();
-    }
-  // split into a separate newUpdateEntryEvent since SQLFabric may need to
-  // manipulate event before doing the put (e.g. posDup flag)
+  @Retained
-    final EntryEventImpl event = EntryEventImpl.create(
+    @Retained final EntryEventImpl event = EntryEventImpl.create(
-  /**
-   * Creates an EntryEventImpl that is optimized to not fetch data from HDFS.
-   * This is meant to be used by PUT dml from GemFireXD.
-   */
-  public final EntryEventImpl newPutEntryEvent(Object key, Object value,
-      Object aCallbackArgument) {
-    EntryEventImpl ev = newUpdateEntryEvent(key, value, aCallbackArgument);
-    ev.setFetchFromHDFS(false);
-    ev.setPutDML(true);
-    return ev;
-  }
-  protected boolean includeHDFSResults() {
-    return isUsedForPartitionedRegionBucket() 
-        && isHDFSReadWriteRegion() 
-        && getPartitionedRegion().includeHDFSResults();
-  }
-  
-
-      if (isHDFSReadWriteRegion() && this.initialized) {
-        // this size is not used by HDFS region iterators
-        // fixes bug 49239
-        return 0;
-      }
-   * @since 5.5
+   * @since GemFire 5.5
-    return new EntriesSet(this, false, IteratorType.KEYS,
-        false /* dontRememberReads */, false /* skipTxCheckInIteration */,
-        false /* allowTombstones */);
+    return new EntriesSet(this, false, IteratorType.KEYS, false /* allowTombstones */);
-	if (includeHDFSResults()) {
-      return result;
-    }
-   * @since 3.2
+   * @since GemFire 3.2
-   * @since 3.5.1
+   * @since GemFire 3.5.1
-    EntryEventImpl event = EntryEventImpl.create(
+    @Released EntryEventImpl event = EntryEventImpl.create(
-    EntryEventImpl event = EntryEventImpl.create(
+    @Released EntryEventImpl event = EntryEventImpl.create(
-    EntryEventImpl event = EntryEventImpl.create(
+    @Released EntryEventImpl event = EntryEventImpl.create(
-        } catch (Exception ex) {
-          logger.error("Failed to create index {} on region {} with exception: {}", icd.getIndexName(), this.getFullPath(), ex);
+        }
+        catch (Exception ex) {
+          logger.info("Failed to create index {} on region {} with exception: {}", icd.getIndexName(), this.getFullPath(), ex);
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-   * @see DistributedRegion#findObjectInSystem(KeyInfo, boolean, TXStateInterface, boolean, Object, boolean, boolean, ClientProxyMembershipID, EntryEventImpl, boolean, boolean )
+   * @see LocalRegion#findObjectInSystem(KeyInfo, boolean, TXStateInterface, boolean, Object, boolean, boolean, ClientProxyMembershipID, EntryEventImpl, boolean)
-  protected Object findObjectInSystem(KeyInfo keyInfo, boolean isCreate,
-      TXStateInterface tx, boolean generateCallbacks, Object localValue, boolean disableCopyOnRead, boolean preferCD, ClientProxyMembershipID requestingClient,
-      EntryEventImpl clientEvent, boolean returnTombstones,  boolean allowReadFromHDFS)
+  protected Object findObjectInSystem(KeyInfo keyInfo,
+                                      boolean isCreate,
+                                      TXStateInterface tx,
+                                      boolean generateCallbacks,
+                                      Object localValue,
+                                      boolean disableCopyOnRead,
+                                      boolean preferCD,
+                                      ClientProxyMembershipID requestingClient,
+                                      EntryEventImpl clientEvent,
+                                      boolean returnTombstones)
-    EntryEventImpl holder = null;
+    VersionTagHolder holder = null;
-        holder = EntryEventImpl.createVersionTagHolder();
-        try {
-          value = mySRP.get(key, aCallbackArgument, holder);
-          fromServer = value != null;
-        } finally {
-          holder.release();
-        }
+        holder = new VersionTagHolder();
+        value = mySRP.get(key, aCallbackArgument, holder);
+        fromServer = value != null;
-      EntryEventImpl event
+      @Released EntryEventImpl event
-   * @since 5.7
+   * @since GemFire 5.7
-   * @since 5.7
+   * @since GemFire 5.7
-   * @since 5.7
+   * @since GemFire 5.7
-   * @since 5.7
+   * @since GemFire 5.7
-   * @since 5.7
+   * @since GemFire 5.7
-        // TODO OFFHEAP: verify that the above assertion is true
-   * @since 5.7
+   * @since GemFire 5.7
-   * @since 5.0.2
+   * @since GemFire 5.0.2
-  private final boolean DO_EXPENSIVE_VALIDATIONS = Boolean.getBoolean("gemfire.DO_EXPENSIVE_VALIDATIONS");
+  private final boolean DO_EXPENSIVE_VALIDATIONS = Boolean.getBoolean(DistributionConfig.GEMFIRE_PREFIX + "DO_EXPENSIVE_VALIDATIONS");
-    if(getDiskRegion() != null) {
-      getDiskRegion().incNumEntriesInVM(-delta);
-    }
-    DiskEntry.Helper.incrementBucketStats(this, -delta/*InVM*/, 0/*OnDisk*/, 0);
-  
+    scheduleTombstone(entry, destroyedVersion, false);
+  }
+  
+  public void scheduleTombstone(RegionEntry entry, VersionTag destroyedVersion, boolean reschedule) {
+    if (!reschedule) {
+    }
-        logger.trace(LogMarker.TOMBSTONE_COUNT, "scheduling tombstone for {} version={} count is {} entryMap size is {}",
+        logger.trace(LogMarker.TOMBSTONE_COUNT, "{} tombstone for {} version={} count is {} entryMap size is {}", reschedule ? "rescheduling" : "scheduling",
-    Object sync = TombstoneService.DEBUG_TOMBSTONE_COUNT? TombstoneService.debugSync : new Object();
-    synchronized(sync) {
-      unscheduleTombstone(entry, false); // count is off by one, so don't allow validation to take place
-      scheduleTombstone(entry, version);
-    }
-
+    scheduleTombstone(entry, version, true);
-    getRegionMap().unscheduleTombstone(entry);
-      keys = this.cache.getTombstoneService().gcTombstones(this, regionGCVersions);
+      keys = this.cache.getTombstoneService().gcTombstones(this, regionGCVersions, needsTombstoneGCKeysForClients(eventID, clientRouting));
+  protected boolean needsTombstoneGCKeysForClients(EventID eventID, FilterInfo clientRouting) {
+    return false;
+  }
-   * @since 3.2
+   * @since GemFire 3.2
-   * @since 5.5
+   * @since GemFire 5.5
-     * @since 5.5
+     * @since GemFire 5.5
-   * DO NOT use in product code else it will break SQLFabric that has cases
-   * where routing object is not part of only the key.
-   * @since 3.2
+   * @since GemFire 3.2
-   * only. DO NOT use in product code else it will break SQLFabric that has
-   * cases where routing object is not part of only the key.
+   * only. 
-   * @since gemfire5.7_hotfix
+   * @since GemFire 5.7
-   * @since 5.1
+   * @since GemFire 5.1
-   * 
-   * Originally implemented in WAN gateway code and moved here in the sqlfire
-   * "cheetah" branch.
-    if (Boolean.getBoolean("gemfire.testing.slow-interest-recovery")) {
+    if (Boolean.getBoolean(DistributionConfig.GEMFIRE_PREFIX + "testing.slow-interest-recovery")) {
-    EntryEventImpl event = EntryEventImpl.create(this, Operation.LOCAL_DESTROY,
+    @Released EntryEventImpl event = EntryEventImpl.create(this, Operation.LOCAL_DESTROY,
-   * @since 5.0
+   * @since GemFire 5.0
-   * 
-   * TODO there has been some work on this on the sqlfire branch that should
-   * be picked up here.
-  if (this.hdfsStoreName != null) {
-    notifyGatewaySender(eventType, event);
-    }
-    //Asif: Modified the call to this constructor by passing the new value obtained from remote site
-    //instead of null .
-    //The need for this arose, because creation of EntryEvent, makes call to PartitionResolver,
-    //to get Hash. If the partitioning column is different from primary key, 
-    //the resolver for Sqlfabric is not able to obtain the hash object used for creation of KeyInfo  
-     
-    final EntryEventImpl event = EntryEventImpl.create(this, Operation.CREATE, key,
+    @Released final EntryEventImpl event = EntryEventImpl.create(this, Operation.CREATE, key,
-    /// For SqlFabric, if the new value happens to be an serialized object, then 
-    //it needs to be converted into VMCachedDeserializable , or serializable delta 
-    // as the case may be
-      boolean fromClient, EntryEventImpl clientEvent, boolean isSqlFabricSystem)
+      boolean fromClient, EntryEventImpl clientEvent)
-    final EntryEventImpl event = EntryEventImpl.create(this, Operation.UPDATE, key,
+    @Released final EntryEventImpl event = EntryEventImpl.create(this, Operation.UPDATE, key,
-      //Asif: If the system is SqlFabric, then the value byte[] corresponds to
-      //Delta , so we need to deserialize it & appropriately set the EntryEventImpl's.delta
-      //field to this value
-      if (isSqlFabricSystem) {
-        event.setNewValue(EntryEventImpl.deserialize((byte[])value));  
-      } else  {
-        event.setSerializedNewValue((byte[])value);
-      }
+      event.setSerializedNewValue((byte[])value);
-    //Asif: If the system is SqlFabric, then update will always have value of type
-    //SerializableDelta (i.e Delta) which requires that the old value should be present    
-    boolean ifOld = isSqlFabricSystem ; //false; // can create a new key
+    boolean ifOld = false; // can create a new key
-      EntryEventImpl event =
+      @Released EntryEventImpl event =
-      EntryEventImpl event =
+      @Released EntryEventImpl event =
-    final EntryEventImpl event = EntryEventImpl.create(this, Operation.DESTROY, key,
+    @Released final EntryEventImpl event = EntryEventImpl.create(this, Operation.DESTROY, key,
-    final EntryEventImpl event = EntryEventImpl.create(this, Operation.INVALIDATE, key,
+    @Released final EntryEventImpl event = EntryEventImpl.create(this, Operation.INVALIDATE, key,
-    EntryEventImpl event = EntryEventImpl.create(this, Operation.UPDATE_VERSION_STAMP, key,
+    @Released EntryEventImpl event = EntryEventImpl.create(this, Operation.UPDATE_VERSION_STAMP, key,
-      //if (this.isUsedForPartitionedRegionBucket) {
-      //  if (this.sqlfIndexManager != null) {
-      //    this.sqlfIndexManager.onEvent(this, event, entry);
-      //  }
-      //}
-
-    EntryEventImpl updateTimeStampEvent = EntryEventImpl.createVersionTagHolder(event.getVersionTag());
-    try {
+    VersionTagHolder updateTimeStampEvent = new VersionTagHolder(event.getVersionTag());
-    } finally {
-      updateTimeStampEvent.release();
-    }
-       // don't clobber existing posDup flag e.g. set from SQLFabric client
-   * @since 5.7
+   * @since GemFire 5.7
-    if (event.isConcurrencyConflict()) { // usually concurrent cache modification problem
+    if (isPdxTypesRegion() || event.isConcurrencyConflict() /* usually concurrent cache modification problem */) { 
-    
+
-    if (!event.getOperation().isLocal()) {
-      Set<String> allGatewaySenderIds = null;
-      checkSameSenderIdsAvailableOnAllNodes();
-      if (event.getOperation() == Operation.UPDATE_VERSION_STAMP) {
-        allGatewaySenderIds = getGatewaySenderIds();
-      } else {
-        allGatewaySenderIds = getAllGatewaySenderIds();
-      }
-      
-      List<Integer> allRemoteDSIds = getRemoteDsIds(allGatewaySenderIds);
+    
+    Set<String> allGatewaySenderIds = null;
+    checkSameSenderIdsAvailableOnAllNodes();
+    if (event.getOperation() == Operation.UPDATE_VERSION_STAMP) {
+      allGatewaySenderIds = getGatewaySenderIds();
+    } else {
+      allGatewaySenderIds = getAllGatewaySenderIds();
+    }
-      if (allRemoteDSIds != null) {
-        for (GatewaySender sender : getCache().getAllGatewaySenders()) {
-          if (!isPdxTypesRegion()) {
-            if (allGatewaySenderIds.contains(sender.getId())) {
-              //TODO: This is a BUG. Why return and not continue?
-              if((!this.getDataPolicy().withStorage()) && sender.isParallel()){
-                return;
-              }
-              if(logger.isDebugEnabled()) {
-                logger.debug("Notifying the GatewaySender : {}", sender.getId());
-              }
-              ((AbstractGatewaySender)sender).distribute(operation, event,
-                  allRemoteDSIds);
-            }
+    List<Integer> allRemoteDSIds = getRemoteDsIds(allGatewaySenderIds);
+    if (allRemoteDSIds != null) {
+      for (GatewaySender sender : getCache().getAllGatewaySenders()) {
+        if (allGatewaySenderIds.contains(sender.getId())) {
+          //TODO: This is a BUG. Why return and not continue?
+          if((!this.getDataPolicy().withStorage()) && sender.isParallel()){
+            return;
+          if(logger.isDebugEnabled()) {
+            logger.debug("Notifying the GatewaySender : {}", sender.getId());
+          }
+          ((AbstractGatewaySender)sender).distribute(operation, event,
+              allRemoteDSIds);
+
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since tx
+   * @since GemFire tx
-   * @since tx 
+   * @since GemFire tx
+  @Retained
-    EntryEventImpl event = EntryEventImpl.create(
+    @Retained EntryEventImpl event = EntryEventImpl.create(
-    event.setFetchFromHDFS(false);
-    protected EntryEventImpl generateCustomEvictDestroyEvent(final Object key) {
-    EntryEventImpl event =  EntryEventImpl.create(
-        this, Operation.CUSTOM_EVICT_DESTROY, key, null/* newValue */,
-        null, false, getMyId());
-    
-    // Fix for bug#36963
-    if (generateEventID()) {
-      event.setNewEventId(cache.getDistributedSystem());
-    }
-    event.setFetchFromHDFS(false);
-    return event;
-  }
-  
-    final EntryEventImpl event = 
+    @Released final EntryEventImpl event = 
-   * @since 5.1
+   * @since GemFire 5.1
-        if (!GemFireCacheImpl.ASYNC_EVENT_LISTENERS) {
+        if (this.cache.getEventThreadPool() == null) {
-            logger.warn(LocalizedMessage.create(LocalizedStrings.LocalRegion_0_EVENT_NOT_DISPATCHED_DUE_TO_REJECTED_EXECUTION), rex);
+            dispatchEvent(this, event, op);
-    // mark as destroyed
-    // TODO OFFHEAP MERGE: to fix 49905 asif commented out isDestroyed being set.
-    // But in xd it was set after closeEntries was called.
-    // Here it is set before and it fixed 49555.
+    // mark as destroyed fixes 49555.
-   * @since 5.7
+   * @since GemFire 5.7
-  void cleanUpOnIncompleteOp(EntryEventImpl event,   RegionEntry re, 
-      boolean eventRecorded, boolean updateStats, boolean isReplace) {
-    //TODO:Asif: This is incorrect implementation for replicated region in case of
-    //sql fabric, as sqlf index would already be  updated, if eventRecorded 
-    //flag is true.So if entry is being removed , 
-    //then the sqlfindex also needs to be corrected
-    IndexUpdater iu = this.getIndexUpdater(); // sqlf system
-    if(!eventRecorded || iu ==null || isReplace) {
-    //Ok to remove entry whether sqlfabric or gfe as index has not been modified yet by the operation
-      this.entries.removeEntry(event.getKey(), re, updateStats) ;      
-    }else {
-      // a sqlf system, with event recorded as true. we need to update index.
-      //Use the current event to indicate destroy.should be ok
-      Operation oldOp = event.getOperation();
-      event.setOperation(Operation.DESTROY);
-      this.entries.removeEntry(event.getKey(), re, updateStats, event, this, iu);
-      event.setOperation(oldOp);
-    } 
-    
+  void cleanUpOnIncompleteOp(EntryEventImpl event, RegionEntry re) {
+    //Ok to remove entry as index has not been modified yet by the operation
+    this.entries.removeEntry(event.getKey(), re, false) ;      
-  static void validateRegionName(String name)
+  static void validateRegionName(String name, InternalRegionArguments internalRegionArgs)
-    if (name.length() == 0) {
+    if (name.isEmpty()) {
-    if (name.indexOf(SEPARATOR) >= 0) {
+    if (name.contains(SEPARATOR)) {
+
+    // Validate the name of the region only if it isn't an internal region
+    if (internalRegionArgs.isInternalRegion()){
+      return;
+    }
+    if (internalRegionArgs.isUsedForMetaRegion()) {
+      return;
+    }
+    if (internalRegionArgs.isUsedForPartitionedRegionAdmin()) {
+      return;
+    }
+    if (internalRegionArgs.isUsedForPartitionedRegionBucket()) {
+      return;
+    }
+
+    if (name.startsWith("__")) {
+      throw new IllegalArgumentException("Region names may not begin with a double-underscore: " + name);
+    }
+
+    // Ensure the region only contains valid characters
+    Matcher matcher = NAME_PATTERN.matcher(name);
+    if (!matcher.matches()) {
+      throw new IllegalArgumentException("Region names may only be alphanumeric and may contain hyphens or underscores: " + name);
+    }
-   * @since prPersistSprint2
+   * @since GemFire prPersistSprint2
-   * @since prPersistSprint2
+   * @since GemFire prPersistSprint2
-        EntryEventImpl event = EntryEventImpl.create(
+        @Released EntryEventImpl event = EntryEventImpl.create(
-   * @since 3.2
+   * @since GemFire 3.2
-   * @since 6.1.2.9
+   * @since GemFire 6.1.2.9
+    /**
+     * released by the release method
+     */
+    @Retained
-     * @since 5.0
+     * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-    clearHDFSData();
-    
-  /**Clear HDFS data, if present */
-  protected void clearHDFSData() {
-    //do nothing, clear is implemented for subclasses like BucketRegion.
-  }
-
-          EntryEventImpl event = EntryEventImpl.create(
+          @Released EntryEventImpl event = EntryEventImpl.create(
-    final EntryEventImpl event = EntryEventImpl.create(this, Operation.PUTALL_CREATE, null,
+    @Released final EntryEventImpl event = EntryEventImpl.create(this, Operation.PUTALL_CREATE, null,
-    final EntryEventImpl event = EntryEventImpl.create(this, Operation.REMOVEALL_DESTROY, null,
+    @Released final EntryEventImpl event = EntryEventImpl.create(this, Operation.REMOVEALL_DESTROY, null,
-    EntryEventImpl event = EntryEventImpl.create(this, Operation.PUTALL_CREATE,
+    @Released EntryEventImpl event = EntryEventImpl.create(this, Operation.PUTALL_CREATE,
-          EntryEventImpl tagHolder = EntryEventImpl.createVersionTagHolder();
+          VersionTagHolder tagHolder = new VersionTagHolder();
-                try {
-                  basicEntryPutAll(key, value, dpao, offset, tagHolder);
-                } finally {
-                  tagHolder.release();
-                }
+                basicEntryPutAll(key, value, dpao, offset, tagHolder);
-          EntryEventImpl tagHolder = EntryEventImpl.createVersionTagHolder();
+          VersionTagHolder tagHolder = new VersionTagHolder();
-  // split into a separate newPutAllOperation since SQLFabric may need to
-  // manipulate event before doing the put (e.g. posDup flag)
+    // No need for release since disallowOffHeapValues called.
-    public final DistributedPutAllOperation newPutAllForPUTDmlOperation(Map<?, ?> map, Object callbackArg) {
-    DistributedPutAllOperation dpao = newPutAllOperation(map, callbackArg);
-    dpao.getEvent().setFetchFromHDFS(false);
-    dpao.getEvent().setPutDML(true);
-    return dpao;
-  }
-
+    // No need for release since disallowOffHeapValues called.
-    EntryEventImpl event = EntryEventImpl.createPutAllEvent(
+    @Released EntryEventImpl event = EntryEventImpl.createPutAllEvent(
-	event.setFetchFromHDFS(putallOp.getEvent().isFetchFromHDFS());
-    event.setPutDML(putallOp.getEvent().isPutDML());
-    
-    EntryEventImpl event = EntryEventImpl.createRemoveAllEvent(op, this, key);
+    @Released EntryEventImpl event = EntryEventImpl.createRemoveAllEvent(op, this, key);
-      EntryEventImpl event = (EntryEventImpl)it.next();
+      @Unretained EntryEventImpl event = (EntryEventImpl)it.next();
-      EntryEventImpl event = (EntryEventImpl)it.next();
+      @Unretained EntryEventImpl event = (EntryEventImpl)it.next();
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.1
+   * @since GemFire 5.1
-   * @since prPersistSprint1
+   * @since GemFire prPersistSprint1
+  public CacheServiceProfile getCacheServiceProfile(String id) {
+    return this.cacheServiceProfiles.get(id);
+  }
+
-      if (prof.regionInitialized // fix for bug 41102
-          && !prof.memberUnInitialized) {
+      if (prof.regionInitialized) { // fix for bug 41102
-      // if region in cache is in recovery, or member not initialized exclude
-      if (!prof.inRecovery && !prof.memberUnInitialized) {
+      // if region in cache is in recovery
+      if (!prof.inRecovery) {
-   * @since 5.7
+   * @since GemFire 5.7
-   * @since 5.8Beta
+   * @since GemFire 5.8Beta
-   * @since 6.0
+   * @since GemFire 6.0
-    EntryEventImpl event = EntryEventImpl.create(
-        this,
+    @Released EntryEventImpl event = EntryEventImpl.create(this,
-  /**
-   * @since SqlFabric
-   *
-   */
-  void distributeUpdatedProfileOnHubCreation()
-  {
-    // No op
-  }  
-  
-     * @since 3.5
+     * @since GemFire 3.5
-     * @since 3.5
+     * @since GemFire 3.5
-    Object sync = TombstoneService.DEBUG_TOMBSTONE_COUNT? TombstoneService.debugSync : new Object();
-      synchronized(sync) {
-      }
-     EntryEventImpl event = EntryEventImpl.create(
+     @Released EntryEventImpl event = EntryEventImpl.create(
-    EntryEventImpl event = EntryEventImpl.create(this,
+    @Released EntryEventImpl event = EntryEventImpl.create(this,
-    EntryEventImpl event = EntryEventImpl.create(this,
+    @Released EntryEventImpl event = EntryEventImpl.create(this,
-    EntryEventImpl event = EntryEventImpl.create(this,
+    @Released EntryEventImpl event = EntryEventImpl.create(this,
-    final EntryEventImpl event = EntryEventImpl.create(this, Operation.PUT_IF_ABSENT, key,
+    @Released final EntryEventImpl event = EntryEventImpl.create(this, Operation.PUT_IF_ABSENT, key,
-    final EntryEventImpl event = EntryEventImpl.create(this, Operation.REPLACE, key,
+    @Released final EntryEventImpl event = EntryEventImpl.create(this, Operation.REPLACE, key,
-    final EntryEventImpl event = EntryEventImpl.create(this, Operation.REPLACE, key,
+    @Released final EntryEventImpl event = EntryEventImpl.create(this, Operation.REPLACE, key,
-    final EntryEventImpl event = EntryEventImpl.create(this, Operation.REMOVE, key,
+    @Released final EntryEventImpl event = EntryEventImpl.create(this, Operation.REMOVE, key,
-  /// End of Variables and methods for test Hook for HDFS ///////
-  public void forceHDFSCompaction(boolean isMajor, Integer maxWaitTime) {
-    throw new UnsupportedOperationException(
-        LocalizedStrings.HOPLOG_DOES_NOT_USE_HDFSSTORE
-            .toLocalizedString(getName()));
-  }
-
-  public void flushHDFSQueue(int maxWaitTime) {
-    throw new UnsupportedOperationException(
-        LocalizedStrings.HOPLOG_DOES_NOT_USE_HDFSSTORE
-            .toLocalizedString(getName()));
-  }
-  
-  public long lastMajorHDFSCompaction() {
-    throw new UnsupportedOperationException();
-  }

MOV26 MOV26 MOV26 MOV26 MOV26 MOV31 MOV31 MOV31 MOV31 MOV31 UPD40 UPD40 UPD40 UPD40 INS23 INS23 INS23 UPD83 MOV83 MOV83 MOV83 UPD43 MOV43 INS59 INS83 INS83 INS83 MOV43 INS59 INS83 INS74 INS59 MOV8 INS78 MOV8 INS78 MOV8 INS78 UPD42 INS44 INS44 INS8 INS44 UPD83 INS39 UPD42 MOV44 MOV44 INS83 INS39 INS42 INS44 INS44 MOV8 MOV8 MOV8 INS78 UPD42 MOV44 MOV44 MOV8 INS44 UPD43 UPD42 MOV8 UPD42 UPD42 MOV42 INS32 INS42 INS27 MOV43 INS43 INS43 UPD42 MOV42 INS21 INS54 INS42 INS54 INS42 INS60 INS54 INS42 MOV8 INS43 INS42 INS43 INS42 MOV21 INS39 INS42 INS25 INS43 INS42 INS43 INS42 INS60 MOV25 MOV25 MOV60 MOV21 MOV25 MOV60 INS42 INS43 INS42 INS25 INS25 INS25 INS25 INS25 INS60 INS25 INS29 INS78 UPD42 UPD43 UPD42 INS41 INS60 MOV60 INS42 INS42 INS45 UPD66 INS40 INS45 UPD66 UPD66 UPD42 MOV42 UPD42 MOV42 INS7 UPD66 UPD66 INS78 INS8 MOV8 INS78 INS8 MOV8 INS39 INS59 INS8 MOV43 MOV27 UPD66 UPD66 INS78 INS8 MOV8 INS78 UPD66 UPD66 UPD66 INS78 INS78 INS78 UPD66 UPD66 UPD43 MOV60 INS25 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 INS27 UPD42 MOV42 UPD42 MOV42 INS38 INS8 UPD42 MOV42 UPD42 MOV42 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 UPD66 INS78 UPD66 INS78 INS78 INS78 INS78 INS78 INS43 INS59 UPD66 INS27 MOV27 INS8 UPD66 UPD66 UPD66 INS78 INS78 UPD66 UPD66 INS42 INS32 INS32 INS32 INS8 INS32 INS8 INS32 INS8 INS32 INS8 INS32 INS8 INS43 INS59 INS38 INS8 UPD66 UPD66 UPD66 UPD66 INS65 INS42 INS78 INS78 INS78 INS78 INS78 UPD66 UPD66 UPD66 UPD66 UPD66 UPD42 INS32 UPD66 UPD66 UPD66 INS78 MOV22 INS78 INS78 INS78 INS78 INS78 INS78 INS83 MOV43 MOV59 INS78 MOV43 INS78 INS22 INS16 INS42 MOV21 MOV32 INS42 MOV41 MOV25 INS42 MOV32 INS25 INS42 MOV41 INS9 INS42 INS42 INS42 INS42 UPD42 UPD42 MOV27 MOV8 INS40 INS45 UPD42 INS42 INS42 MOV21 INS42 UPD42 UPD42 UPD9 INS27 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS14 MOV32 MOV32 INS70 INS42 INS42 INS9 MOV42 UPD42 MOV42 MOV42 UPD42 MOV42 MOV42 INS42 INS42 INS41 INS42 INS42 INS41 INS42 INS42 INS41 INS42 INS42 INS41 INS42 INS42 INS45 INS53 INS42 INS42 INS32 INS32 INS53 INS66 UPD66 INS42 INS42 INS42 INS42 INS42 INS60 MOV60 INS22 UPD42 MOV42 UPD42 MOV42 INS42 UPD66 UPD66 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS42 INS52 INS42 INS27 INS33 INS32 MOV27 UPD42 MOV42 INS42 MOV8 MOV43 INS21 INS78 INS40 INS45 MOV8 INS78 INS78 INS43 MOV32 MOV44 MOV32 MOV8 INS14 INS42 INS42 INS42 INS42 INS42 INS14 INS78 MOV43 MOV59 INS78 INS52 INS42 INS32 INS33 INS42 INS42 INS32 INS21 INS21 INS7 INS42 UPD45 INS16 INS9 INS42 INS42 INS42 INS43 INS27 INS43 INS27 INS42 INS42 MOV42 UPD42 MOV42 UPD42 MOV42 INS42 MOV32 INS32 MOV32 INS42 INS14 INS42 INS45 INS45 INS32 INS27 INS42 INS45 INS42 INS42 INS45 INS42 INS78 INS40 MOV38 INS42 UPD42 MOV22 INS42 INS43 INS42 INS42 INS42 INS32 INS33 INS42 INS42 MOV22 INS42 INS78 INS42 MOV32 MOV32 UPD42 UPD43 UPD43 UPD42 INS14 UPD42 INS14 MOV8 UPD42 INS52 UPD42 UPD42 MOV42 INS43 INS43 INS42 INS42 MOV8 DEL40 DEL26 DEL40 DEL26 DEL42 DEL45 DEL59 DEL23 DEL83 DEL59 DEL23 DEL83 DEL42 DEL43 DEL42 DEL59 DEL23 DEL66 DEL42 DEL65 DEL66 DEL66 DEL66 DEL66 DEL66 DEL65 DEL29 DEL83 DEL39 DEL42 DEL52 DEL42 DEL22 DEL41 DEL8 DEL31 DEL66 DEL42 DEL67 DEL65 DEL66 DEL65 DEL29 DEL83 DEL83 DEL39 DEL42 DEL39 DEL42 DEL44 DEL52 DEL42 DEL22 DEL42 DEL7 DEL21 DEL8 DEL31 DEL42 DEL32 DEL32 DEL21 DEL52 DEL42 DEL22 DEL32 DEL7 DEL21 DEL83 DEL43 DEL42 DEL43 DEL33 DEL59 DEL60 DEL52 DEL42 DEL32 DEL33 DEL27 DEL52 DEL42 DEL22 DEL42 DEL43 DEL14 DEL7 DEL21 DEL42 DEL42 DEL32 DEL42 DEL42 DEL32 DEL21 DEL42 DEL42 DEL42 DEL32 DEL42 DEL52 DEL52 DEL42 DEL32 DEL42 DEL32 DEL7 DEL21 DEL8 DEL25 DEL42 DEL41 DEL8 DEL31 DEL42 DEL42 DEL32 DEL36 DEL52 DEL42 DEL22 DEL33 DEL27 DEL27 DEL52 DEL42 DEL22 DEL42 DEL9 DEL32 DEL21 DEL8 DEL25 DEL83 DEL42 DEL43 DEL42 DEL31 DEL42 DEL42 DEL42 DEL42 DEL52 DEL42 DEL22 DEL32 DEL21 DEL42 DEL42 DEL42 DEL42 DEL32 DEL7 DEL21 DEL42 DEL42 DEL42 DEL42 DEL32 DEL32 DEL21 DEL42 DEL42 DEL42 DEL42 DEL32 DEL32 DEL21 DEL42 DEL42 DEL32 DEL38 DEL42 DEL42 DEL43 DEL42 DEL42 DEL52 DEL42 DEL14 DEL7 DEL21 DEL8 DEL25 DEL8 DEL42 DEL42 DEL32 DEL45 DEL6 DEL54 DEL8 DEL42 DEL32 DEL27 DEL36 DEL54 DEL8 DEL39 DEL42 DEL44 DEL42 DEL42 DEL7 DEL21 DEL8 DEL8 DEL25 DEL42 DEL42 DEL32 DEL41 DEL9 DEL39 DEL42 DEL44 DEL42 DEL9 DEL39 DEL42 DEL44 DEL42 DEL42 DEL32 DEL38 DEL27 DEL9 DEL42 DEL32 DEL38 DEL27 DEL66 DEL65 DEL29 DEL83 DEL39 DEL42 DEL9 DEL41 DEL8 DEL31 DEL66 DEL65 DEL29 DEL83 DEL39 DEL42 DEL9 DEL41 DEL8 DEL31 DEL66 DEL65 DEL29 DEL83 DEL39 DEL42 DEL31 DEL66 DEL65 DEL29 DEL83 DEL42 DEL43 DEL42 DEL42 DEL41 DEL8 DEL31 DEL66 DEL65 DEL29 DEL83 DEL42 DEL43 DEL42 DEL42 DEL41 DEL8 DEL31 DEL39 DEL42 DEL44 DEL9 DEL9 DEL42 DEL39 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL42 DEL54 DEL8 DEL66 DEL66 DEL65 DEL29 DEL83 DEL83 DEL42 DEL42 DEL43 DEL42 DEL44 DEL42 DEL43 DEL42 DEL44 DEL42 DEL43 DEL42 DEL44 DEL42 DEL43 DEL42 DEL42 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL42 DEL42 DEL9 DEL32 DEL21 DEL42 DEL42 DEL9 DEL32 DEL21 DEL42 DEL41 DEL8 DEL31 DEL83 DEL39 DEL42 DEL42 DEL32 DEL42 DEL32 DEL42 DEL32 DEL42 DEL32 DEL27 DEL41 DEL8 DEL31 DEL42 DEL32 DEL52 DEL42 DEL22 DEL27 DEL34 DEL41 DEL8 DEL25 DEL9 DEL9 DEL42 DEL32 DEL42 DEL41 DEL8 DEL25 DEL39 DEL69 DEL39 DEL42 DEL44 DEL42 DEL42 DEL42 DEL32 DEL7 DEL21 DEL42 DEL42 DEL32 DEL21 DEL8 DEL54 DEL25 DEL8 DEL45 DEL42 DEL32 DEL33 DEL27 DEL42 DEL32 DEL42 DEL42 DEL38 DEL32 DEL21 DEL8 DEL25 DEL40 DEL42 DEL52 DEL42 DEL38 DEL34 DEL34 DEL32 DEL21 DEL43 DEL42 DEL40 DEL40 DEL43 DEL14 DEL16 DEL59 DEL60 DEL42 DEL42 DEL42 DEL42 DEL32 DEL21 DEL8 DEL51 DEL8 DEL83 DEL42 DEL43 DEL42 DEL44 DEL42 DEL43 DEL42 DEL42 DEL42 DEL52 DEL40 DEL42 DEL33 DEL33 DEL9 DEL42 DEL32 DEL32 DEL59 DEL60 DEL42 DEL32 DEL42 DEL42 DEL42 DEL42 DEL32 DEL32 DEL21 DEL8 DEL25 DEL42 DEL41 DEL42 DEL32 DEL42 DEL42 DEL32 DEL21 DEL39 DEL42 DEL39 DEL42 DEL44 DEL39 DEL42 DEL44 DEL39 DEL42 DEL44 DEL43 DEL42 DEL52 DEL42 DEL32 DEL59 DEL60 DEL42 DEL38 DEL42 DEL33 DEL27 DEL27 DEL42 DEL27 DEL43 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL42 DEL42 DEL40 DEL32 DEL21 DEL42 DEL42 DEL42 DEL32 DEL42 DEL42 DEL42 DEL52 DEL42 DEL32 DEL21 DEL42 DEL42 DEL42 DEL32 DEL21 DEL8 DEL25 DEL8 DEL66 DEL66 DEL66 DEL66 DEL66 DEL45 DEL66 DEL66 DEL52 DEL42 DEL22 DEL33 DEL27 DEL42 DEL42 DEL42 DEL32 DEL21 DEL8 DEL25 DEL39 DEL42 DEL44 DEL42 DEL42 DEL42 DEL42 DEL42 DEL39 DEL85 DEL5 DEL42 DEL11 DEL32 DEL32 DEL21 DEL8 DEL25 DEL8 DEL42 DEL42 DEL43 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL42 DEL42 DEL32 DEL21 DEL8 DEL54 DEL8 DEL38 DEL42 DEL42 DEL32 DEL42 DEL32 DEL38 DEL70 DEL8 DEL25 DEL8 DEL25 DEL8 DEL42 DEL42 DEL9 DEL32 DEL21 DEL40 DEL38 DEL42 DEL40 DEL32 DEL42 DEL66 DEL65 DEL29 DEL83 DEL42 DEL8 DEL32 DEL34 DEL27 DEL32 DEL34 DEL27 DEL42 DEL32 DEL21 DEL42 DEL42 DEL32 DEL42 DEL42 DEL32 DEL21 DEL8 DEL54 DEL8 DEL42 DEL42 DEL32 DEL42 DEL42 DEL42 DEL42 DEL32 DEL42 DEL32 DEL32 DEL21 DEL42 DEL42 DEL42 DEL42 DEL32 DEL42 DEL32 DEL32 DEL21 DEL42 DEL43 DEL60 DEL83 DEL76 DEL76 DEL74 DEL42 DEL44 DEL42 DEL43 DEL42 DEL42 DEL42 DEL42 DEL32 DEL59 DEL60 DEL42 DEL32 DEL9 DEL32 DEL21 DEL42 DEL42 DEL32 DEL42 DEL9 DEL32 DEL21 DEL42 DEL41 DEL40 DEL40 DEL38 DEL27 DEL40 DEL38 DEL27 DEL66 DEL65 DEL29 DEL39 DEL42 DEL8 DEL31 DEL42 DEL42 DEL43 DEL42 DEL40 DEL40 DEL42 DEL43 DEL14 DEL16 DEL59 DEL60 DEL51 DEL8 DEL83 DEL60 DEL83 DEL39 DEL42 DEL39 DEL42 DEL44 DEL42 DEL43 DEL42 DEL44 DEL42 DEL43 DEL40 DEL42 DEL42 DEL32 DEL32 DEL14 DEL53 DEL8 DEL31 DEL83 DEL39 DEL42 DEL39 DEL42 DEL44 DEL42 DEL43 DEL40 DEL42 DEL42 DEL32 DEL32 DEL14 DEL53 DEL8 DEL31 DEL83 DEL39 DEL42 DEL42 DEL43 DEL14 DEL53 DEL8 DEL31