GEODE-77 fixes for network failure handling & miscellaneous unit test failures

Network failure handling was not properly shutting down TCPConduit, leaving threads hanging trying to send messages.  The shutdown code was calling Services.emergencyClose too soon, and the recursion back into GMSMembershipManager shutdown code caused some problems, too.

GMSHealthMonitor was continually switching between two members to watch even though it had already sent suspect messages about them and had received no response.  I added a collection of IDs that are in this state and modified setNextNeighbor to avoid reusing them.

GMSHealthMonitor was sending removeMember messages to the locators and a random member, but for some reason this wasn't resolving a network partition fast enough.  I've disabled that behavior for now, sending the messages to all members.  This needs to be revisited because sending the message to all members is not scalable.

GMSHealthMonitor had some issues with initiating removals when it was in the process of shutting down.  I added some isStopping checks to fix this.

MembershipJUnitTest and StatRecorderJUnitTest were failing in gradle runs but not under Eclipse because my Eclipse launch configuration wasn't set to enable assertions.  After fixing that I found a number of problems with these tests and fixed them.

Multicast tests are now implemented in GMSMembershipManager and JGroupsMessenger.  This leverages the ping/pong messaging added for the quorum checker.

GMSJoinLeave was too slow in sending out new views when there were process failures.  I added code to inform the reply processor if there are queued leave/remove requests so it wouldn't wait for these, and also added similar checks in the removeHealthyMembers method (which performs checks on members using the HealthMonitor).

When there is a network partition GMSJoinLeave will now send a NetworkPartitionMessage to other members to prod them along in figuring out that they should shut down.

During a forced-disconnect there can be a lot of warning/fatail log messages.  If there are alert listeners in the system this can create a lot of network traffic and extra work figuring out whether the receiver is even there or not.  GMSMembershipManager now throws away outbound alerts when a forced-disconnect is in process.

Some of the forced-disconnect shutdown processing has been moved out of the membershp manager's DisconnectThread that was introduced with the quorum checker in order to set the shutdown cause, etc, as quickly as possible.

I noticed a lot of TXState log messages at debug level with a Throwable stack trace.  There was no comment saying why this was being done so I commented it out.

JGroups logging level is now set to FATAL by default.  The default log level was a problem during network partitions because each message send was causing a dire warning to be logged.

I observed a number of threads being left behind when a locator failed to start during auto-reconnect testing.  I added a unit test to LocatorJUnitTest for this and fixed the leaks.

+  public static boolean THROW_EXCEPTION_ON_START_HOOK;
+
+  
+  private volatile long pingsReceived;
+  
+  private volatile long pongsReceived;
+    if (THROW_EXCEPTION_ON_START_HOOK) {
+      THROW_EXCEPTION_ON_START_HOOK = false;
+      throw new SystemConnectException("failing for test");
+    }
+    
-    logger.info("JGroups channel created (took {}ms)", System.currentTimeMillis()-start);
+    logger.debug("JGroups channel created (took {}ms)", System.currentTimeMillis()-start);
+  
+  @Override
+  public boolean testMulticast(long timeout) throws InterruptedException {
+    long pongsSnapshot = pongsReceived;
+    JGAddress dest = null;
+    try {
+      pingPonger.sendPingMessage(myChannel, jgAddress, dest);
+    } catch (Exception e) {
+      logger.warn("unable to send multicast message: {}", (jgAddress==null? "multicast recipients":jgAddress),
+          e.getMessage());
+      return false;
+    }
+    long giveupTime = System.currentTimeMillis() + timeout;
+    while (pongsReceived == pongsSnapshot && System.currentTimeMillis() < giveupTime) {
+      Thread.sleep(100);
+    }
+    return pongsReceived > pongsSnapshot;
+  }
-      Object contents = jgmsg.getBuffer();
-      if (contents instanceof byte[]) {
-          byte[] msgBytes = (byte[]) contents;
-  	    if (pingPonger.isPingMessage(msgBytes)) {
-  	    	try {
-  	    	  pingPonger.sendPongMessage(myChannel, jgAddress, jgmsg.getSrc());
-            }
-            catch (Exception e) {
-              logger.info("Failed sending Pong message to " + jgmsg.getSrc());
-            }
-  	        return;
-  	    }
+      byte[] contents = jgmsg.getBuffer();
+      if (pingPonger.isPingMessage(contents)) {
+        pingsReceived++;
+        try {
+          pingPonger.sendPongMessage(myChannel, jgAddress, jgmsg.getSrc());
+        }
+        catch (Exception e) {
+          logger.info("Failed sending Pong message to " + jgmsg.getSrc());
+        }
+        return;
+      } else if (pingPonger.isPongMessage(contents)) {
+        pongsReceived++;
+        return;

INS23 INS23 INS23 INS31 INS83 INS83 INS39 INS59 INS83 INS83 INS39 INS59 INS83 INS83 INS39 INS59 INS78 INS83 INS39 INS42 INS44 INS43 INS8 INS42 INS42 INS42 INS25 INS42 INS39 INS42 INS42 INS60 INS60 INS54 INS60 INS61 INS41 INS42 INS8 INS39 INS59 INS43 INS59 INS8 INS12 INS39 INS59 INS27 INS8 INS27 MOV25 INS21 INS53 UPD42 INS42 INS42 INS42 INS42 INS33 INS21 INS44 INS8 INS42 INS27 INS27 INS27 INS21 INS42 INS42 MOV5 INS25 INS7 INS14 INS32 INS43 INS42 INS21 INS41 INS32 INS42 INS42 INS42 INS32 INS42 INS32 UPD42 INS21 INS32 INS8 INS42 INS9 INS43 INS45 INS42 INS42 INS42 INS42 INS42 INS42 INS32 INS9 INS42 INS42 INS42 INS42 INS42 INS42 INS34 INS37 INS42 INS42 INS42 INS21 INS41 INS42 INS42 INS42 INS45 INS36 INS32 INS42 INS37 INS16 INS42 INS42 INS42 INS27 INS45 INS42 INS42 INS33 DEL42 DEL43 DEL42 DEL39 DEL85 DEL5 DEL62 DEL39 DEL85 DEL5 DEL42 DEL42 DEL11 DEL59 DEL60 DEL8 DEL25