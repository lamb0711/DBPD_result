GEODE-1714: fix disk stats

A number of places were found that did not correctly
change the DiskRegionStats: entriesInVM and entriesOnlyOnDisk.
Also found places that did not correctly change
the entry count and size of a PR bucket region.
EntriesInVM is supposed to only count entries that have a value
stored in the JVM. But it was also counting invalid entries.
Since invalid entries do not have a value this stat no longer counts them.

+import com.gemstone.gemfire.internal.cache.Token.Tombstone;
-      if (newValue == null || Token.isRemovedFromDisk(newValue)) {
-        // it is not in vm and it is not on disk
-        DiskId did = entry.getDiskId();
-        if (did != null) {
-          did.setKeyId(DiskRegion.INVALID_ID);
-        }
-      }
-      else if (newValue instanceof RecoveredEntry) {
+      if (newValue instanceof RecoveredEntry) {
-          drv.incNumOverflowOnDisk(1L);
-          drv.incNumOverflowBytesOnDisk(did.getValueLength());
-          incrementBucketStats(r, 0/*InVM*/, 1/*OnDisk*/, did.getValueLength());
+          updateStats(drv, r, 0/*InVM*/, 1/*OnDisk*/, did.getValueLength());
-          drv.incNumEntriesInVM(1L);
-          incrementBucketStats(r, 1/*InVM*/, 0/*OnDisk*/, 0);
+          if (!Tombstone.isInvalidOrRemoved(re.getValue())) {
+            updateStats(drv, r, 1/*InVM*/, 0/*OnDisk*/, 0);
+          }
-        drv.incNumEntriesInVM(1L);
-        incrementBucketStats(r, 1/*InVM*/, 0/*OnDisk*/, 0);
+        if (newValue != null && !Token.isInvalidOrRemoved(newValue)) {
+          updateStats(drv, r, 1/*InVM*/, 0/*OnDisk*/, 0);
+        }
-      DiskRegion dr = region.getDiskRegion();
-      //If we have concurrency checks enabled for a persistent region, we need
-      //to add an entry to the async queue for every update to maintain the RVV
-      boolean maintainRVV = region.concurrencyChecksEnabled && dr.isBackup();
-      
-      Token oldValue = null;
-      int oldValueLength = 0;
-      boolean scheduleAsync = false;
-      boolean callRemoveFromDisk = false;
+      AsyncDiskEntry asyncDiskEntry = null;
+      DiskRegion dr = region.getDiskRegion();
-      VersionTag tag = null;
-      synchronized (syncObj) {
-        oldValue = entry.getValueAsToken();
-        if (Token.isRemovedFromDisk(newValue)) {
-          if (dr.isBackup()) {
-            dr.testIsRecoveredAndClear(did); // fixes bug 41409
-          }
-          RuntimeException rte = null;
-          try {
-            if (!Token.isRemovedFromDisk(oldValue)) {
-              // removeFromDisk takes care of oldValueLength
-              if (dr.isSync()) {
-                removeFromDisk(entry, region, false);
-              } else {
-                callRemoveFromDisk = true; // do it outside the sync
-              }
-            }
-          } catch (RuntimeException e) {
-            rte = e;
-            throw e;
-          }
-          finally {
-            if (rte != null && (rte instanceof CacheClosedException)) {
-             // 47616: not to set the value to be removedFromDisk since it failed to persist
-            } else {
-              // Asif Ensure that the value is rightly set despite clear so
-              // that it can be distributed correctly
-              entry.setValueWithContext(region, newValue); // OFFHEAP newValue was already preparedForCache
-            }
-          }
+        synchronized (syncObj) {
+          asyncDiskEntry = basicUpdate(entry, region, newValue, event);
-        else if (newValue instanceof RecoveredEntry) {
-          // Now that oplog creates are immediately put in cache
-          // a later oplog modify will get us here
-          RecoveredEntry re = (RecoveredEntry)newValue;
-          long oldKeyId = did.getKeyId();
-          long oldOplogId = did.getOplogId();
-          long newOplogId = re.getOplogId();
-          if (newOplogId != oldOplogId) {
-            did.setOplogId(newOplogId);
-            re.setOplogId(oldOplogId); // so caller knows oldoplog id
-          }
-          did.setOffsetInOplog(re.getOffsetInOplog());
-          // id already set
-          did.setUserBits(re.getUserBits());
-          oldValueLength = did.getValueLength();
-          did.setValueLength(re.getValueLength());
-          
-          if (re.getRecoveredKeyId() < 0) {
-            if (!entry.isValueNull()) {
-              entry.handleValueOverflow(region);
-              entry.setValueWithContext(region, null); // fixes bug 41119
-            }
-          } else {
-            entry.setValueWithContext(region, entry.prepareValueForCache(region, re.getValue(), false));
-          }
-          
-          if (re.getRecoveredKeyId() < 0) {
-            if(oldKeyId >= 0) {
-              dr.incNumEntriesInVM(-1L);
-              dr.incNumOverflowOnDisk(1L);
-              dr.incNumOverflowBytesOnDisk(did.getValueLength());
-              incrementBucketStats(region, -1/*InVM*/, 1/*OnDisk*/,
-                                   did.getValueLength());
-            }
-          } else {
-            if(oldKeyId < 0) {
-              dr.incNumEntriesInVM(1L);
-              dr.incNumOverflowOnDisk(-1L);
-              dr.incNumOverflowBytesOnDisk(-oldValueLength);
-              incrementBucketStats(region, 1/*InVM*/, -1/*OnDisk*/, -oldValueLength);
-            }
-          }
-        }
-        else {
-          //The new value in the entry needs to be set after the disk writing 
-          // has succeeded.
-          
-          //entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
-          
-          if(did != null && did.isPendingAsync()) {
-            //if the entry was not yet written to disk, we didn't update
-            //the bytes on disk.
-            oldValueLength = 0;
-          } else {
-            oldValueLength = getValueLength(did);
-          }
-          
-          if (dr.isBackup()) {
-            dr.testIsRecoveredAndClear(did); // fixes bug 41409
-            if (dr.isSync()) {
-              //In case of compression the value is being set first
-              if (AbstractRegionEntry.isCompressible(dr, newValue)) {
-                entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
-                
-                // newValue is prepared and compressed. We can't write compressed values to disk.
-                writeToDisk(entry, region, false, event);
-              } else {
-                writeBytesToDisk(entry, region, false, createValueWrapper(newValue, event));
-                entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
-              }
-              
-            } else if (did.isPendingAsync() && !maintainRVV) {
-              entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
-              
-              // nothing needs to be done except
-              // fixing up LRU stats
-              // @todo fixup LRU stats if needed
-              // I'm not sure anything needs to be done here.
-              // If we have overflow and it decided to evict this entry
-              // how do we handle that case when we are async?
-              // Seems like the eviction code needs to leave the value
-              // in memory until the pendingAsync is done.
-            } else {
-              //if the entry is not async, we need to schedule it
-              //for regions with concurrency checks enabled, we add an entry
-              //to the queue for every entry.
-              scheduleAsync = true;
-              did.setPendingAsync(true);
-              VersionStamp stamp = entry.getVersionStamp();
-              if(stamp != null) {
-                tag = stamp.asVersionTag();
-              }
-              entry.setValueWithContext(region, newValue); 
-            }
-          } else if (did != null) {
-            entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
-            
-            // Mark the id as needing to be written
-            // The disk remove that this section used to do caused bug 30961
-            // @todo this seems wrong. How does leaving it on disk fix the bug?
-            did.markForWriting();
-            //did.setValueSerializedSize(0);
-          }else {
-            entry.setValueWithContext(region, newValue);
-          }
-          
-          if (Token.isRemovedFromDisk(oldValue)) {
-            // Note we now initialize entries removed and then set their
-            // value once we find no existing entry.
-            // So this is the normal path for a brand new entry.
-            dr.incNumEntriesInVM(1L);
-            incrementBucketStats(region, 1/*InVM*/, 0/*OnDisk*/, 0);
-          }
-          
-          if(newValue == Token.TOMBSTONE) {
-            if (oldValue == null) {
-              dr.incNumOverflowOnDisk(-1L);
-              dr.incNumOverflowBytesOnDisk(-oldValueLength);
-              incrementBucketStats(region, 0/*InVM*/, -1/*OnDisk*/, -oldValueLength);
-            } else {
-              dr.incNumEntriesInVM(-1L);
-              incrementBucketStats(region, -1/*InVM*/, 0/*OnDisk*/, 0);
-            }
-          } else {
-            if (oldValue == null) {
-              dr.incNumEntriesInVM(1L);
-              dr.incNumOverflowOnDisk(-1L);
-              dr.incNumOverflowBytesOnDisk(-oldValueLength);
-              incrementBucketStats(region, 1/*InVM*/, -1/*OnDisk*/, -oldValueLength);
-            } else if(oldValue == Token.TOMBSTONE) {
-              dr.incNumEntriesInVM(1L);
-              incrementBucketStats(region, 1/*InVM*/, 0/*OnDisk*/, 0/*overflowBytesOnDisk*/);
-            }
-          }
-        }
-        if (entry instanceof LRUEntry) {
-          LRUEntry le = (LRUEntry)entry;
-          le.unsetEvicted();          
-        }
-
-      }
-      if (callRemoveFromDisk) {
-        removeFromDisk(entry, region, false, oldValue == null, false);
-      } else if (scheduleAsync && did.isPendingAsync()) {
+      if (asyncDiskEntry != null && did.isPendingAsync()) {
-        scheduleAsyncWrite(new AsyncDiskEntry(region, entry, tag));
+        scheduleAsyncWrite(asyncDiskEntry);
+    private static AsyncDiskEntry basicUpdate(DiskEntry entry, LocalRegion region, Object newValue, EntryEventImpl event) throws RegionClearedException {
+      AsyncDiskEntry result = null;
+      DiskRegion dr = region.getDiskRegion();
+      DiskId did = entry.getDiskId();
+      Token oldValue;
+      int oldValueLength;
+      oldValue = entry.getValueAsToken();
+      if (Token.isRemovedFromDisk(newValue)) {
+        if (dr.isBackup()) {
+          dr.testIsRecoveredAndClear(did); // fixes bug 41409
+        }
+        boolean caughtCacheClosed = false;
+        try {
+          if (!Token.isRemovedFromDisk(oldValue)) {
+            result = basicRemoveFromDisk(entry, region, false);
+          }
+        } catch (CacheClosedException e) {
+          caughtCacheClosed = true;
+          throw e;
+        } finally {
+          if (caughtCacheClosed) {
+           // 47616: not to set the value to be removedFromDisk since it failed to persist
+          } else {
+            // Asif Ensure that the value is rightly set despite clear so
+            // that it can be distributed correctly
+            entry.setValueWithContext(region, newValue); // OFFHEAP newValue was already preparedForCache
+          }
+        }
+      }
+      else if (newValue instanceof RecoveredEntry) {
+        // Now that oplog creates are immediately put in cache
+        // a later oplog modify will get us here
+        RecoveredEntry re = (RecoveredEntry)newValue;
+        long oldKeyId = did.getKeyId();
+        Object oldValueAsToken = entry.getValueAsToken();
+        long oldOplogId = did.getOplogId();
+        long newOplogId = re.getOplogId();
+        if (newOplogId != oldOplogId) {
+          did.setOplogId(newOplogId);
+          re.setOplogId(oldOplogId); // so caller knows oldoplog id
+        }
+        did.setOffsetInOplog(re.getOffsetInOplog());
+        // id already set
+        did.setUserBits(re.getUserBits());
+        oldValueLength = did.getValueLength();
+        did.setValueLength(re.getValueLength());
+        
+        if (re.getRecoveredKeyId() < 0) {
+          if (!entry.isValueNull()) {
+            entry.handleValueOverflow(region);
+            entry.setValueWithContext(region, null); // fixes bug 41119
+          }
+        } else {
+          entry.setValueWithContext(region, entry.prepareValueForCache(region, re.getValue(), false));
+        }
+        
+        if (re.getRecoveredKeyId() < 0) { // recovering an entry whose new value is on disk
+          if (oldKeyId >= 0) { // the entry's old value is in vm
+            // TODO: oldKeyId == 0 is the ILLEGAL id; what does that indicate?
+            int inVM = -1;
+            if (Token.isInvalidOrRemoved(oldValueAsToken)) { // but tokens are never in vm
+              inVM = 0;
+            }
+            updateStats(dr, region, inVM, 1/*OnDisk*/, did.getValueLength());
+          } else { // the entry's old value is also on disk
+            int valueLenDelta = -oldValueLength; // but it is no longer
+            valueLenDelta += did.getValueLength(); // new one is now on disk
+            updateStats(dr, region, 0, 0, valueLenDelta);
+          }
+        } else { // recovering an entry whose new value is in vm
+          int inVM = 1;
+          if (Token.isInvalidOrRemoved(re.getValue())) { // but tokens never in vm
+            inVM = 0;
+          }
+          if(oldKeyId < 0) { // the entry's old value is on disk
+            updateStats(dr, region, inVM, -1/*OnDisk*/, -oldValueLength);
+          } else { // the entry's old value was in the vm
+            if (inVM == 1 && Token.isInvalidOrRemoved(oldValueAsToken)) {
+              // the old state was not in vm and not on disk. But now we are in vm.
+              updateStats(dr, region, 1, 0, 0);
+            } else if (inVM == 0 && !Token.isInvalidOrRemoved(oldValueAsToken)) {
+              // the old state was in vm and not on disk. But now we are not in vm.
+              updateStats(dr, region, -1, 0, 0);
+            }
+          }
+        }
+      }
+      else {
+        //The new value in the entry needs to be set after the disk writing 
+        // has succeeded.
+        
+        //entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
+        
+        if(did != null && did.isPendingAsync()) {
+          //if the entry was not yet written to disk, we didn't update
+          //the bytes on disk.
+          oldValueLength = 0;
+        } else {
+          oldValueLength = getValueLength(did);
+        }
+        
+        if (dr.isBackup()) {
+          dr.testIsRecoveredAndClear(did); // fixes bug 41409
+          if (dr.isSync()) {
+            if (AbstractRegionEntry.isCompressible(dr, newValue)) {
+              //In case of compression the value is being set first
+              //so that writeToDisk can get it back from the entry
+              //decompressed if it does not have it already in the event.
+              // TODO: this may have introduced a bug with clear since
+              //       writeToDisk can throw RegionClearedException which
+              //       was supposed to stop us from changing entry.
+              entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
+              // newValue is prepared and compressed. We can't write compressed values to disk.
+              writeToDisk(entry, region, false, event);
+            } else {
+              writeBytesToDisk(entry, region, false, createValueWrapper(newValue, event));
+              entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
+            }
+            
+          } else {
+            //If we have concurrency checks enabled for a persistent region, we need
+            //to add an entry to the async queue for every update to maintain the RVV
+            boolean maintainRVV = region.concurrencyChecksEnabled;
+            
+            if (!did.isPendingAsync() || maintainRVV) {
+              //if the entry is not async, we need to schedule it
+              //for regions with concurrency checks enabled, we add an entry
+              //to the queue for every entry.
+              did.setPendingAsync(true);
+              VersionTag tag = null;
+              VersionStamp stamp = entry.getVersionStamp();
+              if(stamp != null) {
+                tag = stamp.asVersionTag();
+              }
+              result = new AsyncDiskEntry(region, entry, tag);
+            }
+            entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
+          }
+        } else if (did != null) {
+          entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
+          
+          // Mark the id as needing to be written
+          // The disk remove that this section used to do caused bug 30961
+          // @todo this seems wrong. How does leaving it on disk fix the bug?
+          did.markForWriting();
+          //did.setValueSerializedSize(0);
+        }else {
+          entry.setValueWithContext(region, newValue);
+        }
+        
+        if (Token.isInvalidOrRemoved(newValue)) {
+          if (oldValue == null) {
+            updateStats(dr, region, 0/*InVM*/, -1/*OnDisk*/, -oldValueLength);
+          } else if (!Token.isInvalidOrRemoved(oldValue)){
+            updateStats(dr, region, -1/*InVM*/, 0/*OnDisk*/, 0);
+          } else {
+            // oldValue was also a token which is neither in vm or on disk.
+          }
+        } else { // we have a value to put in the vm
+          if (oldValue == null) {
+            updateStats(dr, region, 1/*InVM*/, -1/*OnDisk*/, -oldValueLength);
+          } else if (Token.isInvalidOrRemoved(oldValue)) {
+            updateStats(dr, region, 1/*InVM*/, 0/*OnDisk*/, 0/*overflowBytesOnDisk*/);
+          } else {
+            // old value was also in vm so no change
+          }
+        }
+      }
+      if (entry instanceof LRUEntry) {
+        LRUEntry le = (LRUEntry)entry;
+        le.unsetEvicted();          
+      }
+      return result;
+    }
+
-        boolean oldValueWasNull = entry.isValueNull();
+        Object oldValueAsToken = entry.getValueAsToken();
+        boolean oldValueWasNull = oldValueAsToken == null;
+          int inVM = 1;
+          if (Token.isInvalidOrRemoved(newValue.getValue())) { // but tokens never in vm
+            inVM = 0;
+          }
+          if(oldValueWasNull) { // the entry's old value is on disk
+            updateStats(drv, null, inVM, -1/*OnDisk*/, -oldValueLength);
+          } else { // the entry's old value was in the vm
+            if (inVM == 1 && Token.isInvalidOrRemoved(oldValueAsToken)) {
+              // the old state was not in vm and not on disk. But now we are in vm.
+              updateStats(drv, null, 1, 0, 0);
+            } else if (inVM == 0 && !Token.isInvalidOrRemoved(oldValueAsToken)) {
+              // the old state was in vm and not on disk. But now we are not in vm.
+              updateStats(drv, null, -1, 0, 0);
+            }
+          }
-        }
-        if (entry instanceof LRUEntry) {
-          LRUEntry le = (LRUEntry)entry;
-          assert !le.testEvicted();
-          // we don't allow eviction during recovery
-          if (oldValueWasNull) {
-            // Note we do not append this entry because that will be
-            // done by lruEntryUpdate
-            drv.incNumEntriesInVM(1L);
-            drv.incNumOverflowOnDisk(-1L);
-            drv.incNumOverflowBytesOnDisk(-oldValueLength);
-            //No need to call incrementBucketStats here because we don't have
-            //a real bucket region, this is during recovery from disk.
+          if (!oldValueWasNull) { // the entry's old value is in vm
+            int inVM = -1;
+            if (Token.isInvalidOrRemoved(oldValueAsToken)) { // but tokens are never in vm
+              inVM = 0;
+            }
+            updateStats(drv, null, inVM, 1/*OnDisk*/, did.getValueLength());
+          } else { // the entry's old value is also on disk
+            int valueLenDelta = -oldValueLength; // but it is no longer
+            valueLenDelta += did.getValueLength(); // new one is now on disk
+            updateStats(drv, null, 0, 0, valueLenDelta);
-      dr.incNumEntriesInVM(1L);
-      dr.incNumOverflowOnDisk(-1L);
-      dr.incNumOverflowBytesOnDisk(-bytesOnDisk);
-      incrementBucketStats(region, 1/*InVM*/, -1/*OnDisk*/, -bytesOnDisk);
+      if (!Token.isInvalidOrRemoved(value)) {
+        updateStats(dr, region, 1/*InVM*/, -1/*OnDisk*/, -bytesOnDisk);
+      }
-    public static void incrementBucketStats(Object owner,
-                                             int entriesInVmDelta,
-                                             int overflowOnDiskDelta,
-                                             int overflowBytesOnDiskDelta) {
-      if (owner instanceof BucketRegion) {
-        ((BucketRegion)owner).incNumEntriesInVM(entriesInVmDelta);
-        ((BucketRegion)owner).incNumOverflowOnDisk(overflowOnDiskDelta);
-        ((BucketRegion)owner).incNumOverflowBytesOnDisk(overflowBytesOnDiskDelta);
-      } else if (owner instanceof DiskRegionView) {
-        ((DiskRegionView)owner).incNumOverflowBytesOnDisk(overflowBytesOnDiskDelta);
+    public static void updateStats(DiskRegionView drv, Object owner,
+        int entriesInVmDelta,
+        int overflowOnDiskDelta,
+        int overflowBytesOnDiskDelta) {
+      if (entriesInVmDelta != 0) {
+        drv.incNumEntriesInVM(entriesInVmDelta);
+      if (overflowOnDiskDelta != 0) {
+        drv.incNumOverflowOnDisk(overflowOnDiskDelta);
+      }
+      if (overflowBytesOnDiskDelta != 0) {
+        drv.incNumOverflowBytesOnDisk(overflowBytesOnDiskDelta);
+      }
+      if (owner instanceof BucketRegion) {
+        BucketRegion br = (BucketRegion) owner;
+        br.incNumEntriesInVM(entriesInVmDelta);
+        br.incNumOverflowOnDisk(overflowOnDiskDelta);
+        br.incNumOverflowBytesOnDisk(overflowBytesOnDiskDelta);
+      }
+      // Note: we used to call owner.incNumOverflowBytesOnDisk()
+      // if owner was a DiskRegionView.
+      // But since we also call drv.incNumOverflowBytesOnDisk()
+      // and since drv is == owner when owner is not a LocalRegion
+      // (see PlaceHolderDiskRegion.getDiskRegionView())
+      // this resulted in incNumOverflowBytesOnDisk being called twice.
-      {
-        Token entryVal = entry.getValueAsToken();
-        if (entryVal == null || Token.isRemovedFromDisk(entryVal)) {
-          // Note it could be removed token now because
-          // freeAllEntriesOnDisk is not able to sync on entry
-          return 0;
-        }
-      }
-        // check for a concurrent freeAllEntriesOnDisk
+        // check for a concurrent freeAllEntriesOnDisk which syncs on DiskId but not on the entry
-        boolean movedValueToDisk = false; // added for bug 41849
-          movedValueToDisk = true;
-        }
-        int valueLength = 0;
-        if (movedValueToDisk) {
-          valueLength = getValueLength(did);
-        }
-        if(dr.isSync() || movedValueToDisk) {
-          dr.incNumEntriesInVM(-1L);
-          dr.incNumOverflowOnDisk(1L);
-          dr.incNumOverflowBytesOnDisk(valueLength);
-          incrementBucketStats(region, -1/*InVM*/, 1/*OnDisk*/, valueLength);
+          // the caller checked to make sure we had something to overflow
+          // so dec inVM and inc onDisk
+          updateStats(dr, region, -1/*InVM*/, 1/*OnDisk*/, getValueLength(did));
-                // onDisk was already deced so just do the valueLength here
-                dr.incNumOverflowBytesOnDisk(-did.getValueLength());
-                incrementBucketStats(region, 0/*InVM*/, 0/*OnDisk*/,
-                                     -did.getValueLength());
-                  dr.incNumEntriesInVM(-1);
-                  dr.incNumOverflowOnDisk(1L);
-                  dr.incNumOverflowBytesOnDisk(did.getValueLength());
-                  incrementBucketStats(region, -1/*InVM*/, 1/*OnDisk*/,
-                                       did.getValueLength());
+                  updateStats(dr, region, -1/*InVM*/, 1/*OnDisk*/, did.getValueLength());
-      removeFromDisk(entry, region, true, false, isClear);
-    }
-    private static void removeFromDisk(DiskEntry entry, LocalRegion region,
-                                      boolean checkValue, boolean valueWasNull, boolean isClear) throws RegionClearedException {
-      
-      //If we have concurrency checks enabled for a persistent region, we need
-      //to add an entry to the async queue for every update to maintain the RVV
-      boolean maintainRVV = region.concurrencyChecksEnabled && dr.isBackup();
-      
-      VersionTag tag = null;
-      boolean scheduledAsyncHere = false;
+      AsyncDiskEntry asyncDiskEntry = null;
-      synchronized (syncObj) { 
-
-        if (did == null || (dr.isBackup() && did.getKeyId()== DiskRegion.INVALID_ID)) {
-          // Not on disk yet
-          dr.incNumEntriesInVM(-1L);
-          incrementBucketStats(region, -1/*InVM*/, 0/*OnDisk*/, 0);
-          dr.unscheduleAsyncWrite(did);
-          return;
-        } 
-        //Asif: This will convert the -ve OplogKeyId to positive as part of fixing
-        //Bug # 39989
-        did.unmarkForWriting();
-
-        //System.out.println("DEBUG: removeFromDisk doing remove(" + id + ")");
-        int oldValueLength = 0;
-        if (dr.isSync() || isClear) {
-          oldValueLength = did.getValueLength();
-          dr.remove(region, entry, false, isClear);
-          if (dr.isBackup()) {
-            did.setKeyId(DiskRegion.INVALID_ID); // fix for bug 41340
-          }
-          //If this is a clear, we should unschedule the async write for this
-          //entry
-          did.setPendingAsync(false);
-        } else {
-          if (!did.isPendingAsync() || maintainRVV) {
-            scheduledAsyncHere = true;
-            did.setPendingAsync(true);
-            VersionStamp stamp = entry.getVersionStamp();
-            if(stamp != null) {
-              tag = stamp.asVersionTag();
-            }
-          }
+        synchronized (syncObj) {
+          asyncDiskEntry = basicRemoveFromDisk(entry, region, isClear);
-        if (checkValue) {
-          valueWasNull = entry.isValueNull();
-          entry._removePhase1();
-        }
-        if (valueWasNull) {
-          dr.incNumOverflowOnDisk(-1L);
-          dr.incNumOverflowBytesOnDisk(-oldValueLength);
-          incrementBucketStats(region, 0/*InVM*/, -1/*OnDisk*/, -oldValueLength);
-        }
-        else {
-          dr.incNumEntriesInVM(-1L);
-          incrementBucketStats(region, -1/*InVM*/, 0/*OnDisk*/, 0);
-          if (!dr.isSync()) {
-            // we are going to do an async remove of an entry that is not currently
-            // overflowed to disk so we don't want to count its value length as being
-            // on disk when we finally do the async op. So we clear it here.
-            did.setValueLength(0);
-          }
-        }
-      }
-      if (scheduledAsyncHere && did.isPendingAsync()) {
+      if (asyncDiskEntry != null && did.isPendingAsync()) {
-        scheduleAsyncWrite(new AsyncDiskEntry(region, entry, tag));
+        scheduleAsyncWrite(asyncDiskEntry);
+    private static AsyncDiskEntry basicRemoveFromDisk(DiskEntry entry, LocalRegion region, boolean isClear) throws RegionClearedException {
+      final DiskRegion dr = region.getDiskRegion();
+      final DiskId did = entry.getDiskId();
+      final Object curValAsToken = entry.getValueAsToken();
+      if (did == null || (dr.isBackup() && did.getKeyId()== DiskRegion.INVALID_ID)) {
+        // Not on disk yet
+        if (!Token.isInvalidOrRemoved(curValAsToken)) {
+          updateStats(dr, region, -1/*InVM*/, 0/*OnDisk*/, 0);
+        }
+        dr.unscheduleAsyncWrite(did);
+        return null;
+      }
+      AsyncDiskEntry result = null;
+      
+      //Asif: This will convert the -ve OplogKeyId to positive as part of fixing
+      //Bug # 39989
+      did.unmarkForWriting();
+
+      //System.out.println("DEBUG: removeFromDisk doing remove(" + id + ")");
+      int oldValueLength = did.getValueLength();
+      if (dr.isSync() || isClear) {
+        dr.remove(region, entry, false, isClear);
+        if (dr.isBackup()) {
+          did.setKeyId(DiskRegion.INVALID_ID); // fix for bug 41340
+        }
+        //If this is a clear, we should unschedule the async write for this
+        //entry
+        did.setPendingAsync(false);
+      } else {
+        //If we have concurrency checks enabled for a persistent region, we need
+        //to add an entry to the async queue for every update to maintain the RVV
+        boolean maintainRVV = region.concurrencyChecksEnabled && dr.isBackup();
+        if (!did.isPendingAsync() || maintainRVV) {
+          did.setPendingAsync(true);
+          VersionTag tag = null;
+          VersionStamp stamp = entry.getVersionStamp();
+          if(stamp != null) {
+            tag = stamp.asVersionTag();
+          }
+          result = new AsyncDiskEntry(region, entry, tag);
+        }
+      }
+      if (curValAsToken == null) {
+        updateStats(dr, region, 0/*InVM*/, -1/*OnDisk*/, -oldValueLength);
+      } else if (!Token.isInvalidOrRemoved(curValAsToken)) {
+        updateStats(dr, region, -1/*InVM*/, 0/*OnDisk*/, 0);
+      }
+      return result;
+    }
+

INS26 INS40 INS31 MOV29 INS83 INS83 INS39 INS42 INS44 INS44 INS44 INS44 INS43 INS8 UPD83 INS43 INS42 MOV8 UPD42 INS44 INS8 INS43 INS42 MOV8 MOV25 INS43 INS42 INS43 INS42 INS43 INS42 INS43 INS42 INS42 MOV25 INS60 INS60 MOV60 MOV60 MOV25 MOV25 INS54 INS25 MOV42 INS60 MOV60 MOV60 INS60 INS60 INS41 INS25 MOV43 INS42 INS25 INS25 INS25 INS25 MOV60 MOV60 MOV60 MOV25 INS60 MOV25 INS54 INS25 MOV42 INS60 INS60 MOV60 INS60 INS60 INS25 INS41 INS42 INS42 INS42 INS42 INS43 INS59 INS43 INS59 INS8 MOV8 MOV27 INS8 INS43 INS59 MOV43 INS59 INS39 INS59 INS42 INS38 INS8 INS27 INS8 INS27 INS8 INS27 INS8 MOV62 INS8 INS43 INS59 INS8 MOV8 MOV27 INS8 INS83 INS43 INS59 INS83 INS43 INS59 INS83 UPD43 INS43 INS59 INS39 INS59 INS27 INS8 INS25 INS42 INS25 INS42 INS42 INS33 INS42 INS42 INS32 INS51 INS27 INS21 INS42 INS42 INS33 INS42 INS42 INS60 INS25 INS32 MOV21 INS42 INS34 MOV21 INS42 INS34 MOV21 INS42 INS34 MOV21 INS60 INS21 INS21 MOV21 INS42 INS42 INS33 INS51 INS27 INS21 INS42 INS42 INS32 INS42 INS42 INS32 UPD42 UPD42 INS25 INS42 INS42 INS33 INS42 INS32 MOV60 INS42 INS33 MOV21 MOV38 MOV8 INS27 INS8 INS42 INS42 INS42 INS8 UPD42 MOV42 INS33 INS32 INS39 INS8 INS60 INS25 INS43 INS39 INS59 MOV27 INS8 INS8 UPD42 MOV42 UPD42 MOV42 INS42 INS43 INS59 INS32 INS32 INS42 INS8 UPD42 MOV42 INS33 INS32 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 UPD42 MOV42 INS38 INS8 INS33 UPD42 MOV42 UPD42 MOV42 MOV21 MOV38 INS25 INS27 INS38 MOV21 INS21 INS42 INS42 UPD42 INS9 MOV25 INS43 INS59 MOV32 INS8 MOV8 INS42 UPD42 INS42 INS27 MOV21 INS60 MOV25 INS25 MOV25 MOV25 INS42 UPD42 INS34 MOV38 MOV38 INS42 INS42 INS42 UPD42 MOV42 INS42 MOV11 INS42 INS42 INS42 INS42 INS42 INS42 INS42 MOV21 INS42 INS42 INS32 MOV21 MOV60 UPD42 INS42 MOV38 UPD42 UPD42 INS42 INS38 INS8 INS42 INS33 INS32 INS7 MOV38 MOV43 INS42 INS42 INS42 INS32 INS60 INS25 UPD42 UPD42 INS25 UPD42 INS42 INS33 INS39 INS59 INS32 INS42 INS8 INS8 INS38 INS8 INS8 UPD34 MOV34 MOV21 INS7 UPD42 MOV42 UPD42 MOV42 INS42 INS42 UPD42 UPD42 MOV38 INS34 UPD42 INS42 INS32 MOV21 UPD42 MOV42 UPD42 MOV42 INS42 UPD42 INS42 INS42 INS32 INS42 INS42 INS8 INS8 INS39 INS59 INS32 INS8 INS8 INS8 INS8 MOV27 MOV8 INS25 MOV27 INS42 INS34 INS42 INS42 INS32 INS21 INS21 INS25 INS42 INS60 INS25 MOV21 INS60 MOV21 MOV21 INS42 INS32 UPD42 INS42 INS42 MOV14 UPD42 MOV42 UPD42 MOV42 INS32 INS42 INS42 INS42 INS42 INS42 INS7 UPD42 INS9 INS60 INS25 MOV21 INS60 MOV21 MOV21 INS42 INS34 INS42 INS42 INS32 INS21 MOV21 INS25 INS60 MOV25 MOV21 INS38 MOV8 INS8 INS32 INS8 INS8 INS42 INS42 INS7 INS32 INS27 INS8 INS25 INS39 INS59 INS32 INS8 INS39 INS59 INS7 INS32 UPD42 MOV42 MOV42 MOV42 MOV42 INS42 INS42 UPD42 INS42 INS42 INS32 INS39 INS59 INS32 INS8 INS39 INS59 INS7 INS42 INS42 INS7 INS27 INS8 INS25 INS39 INS59 UPD27 MOV21 INS32 UPD42 MOV42 UPD42 MOV42 INS42 MOV21 INS42 INS34 INS42 INS42 INS33 INS42 MOV38 MOV38 INS27 INS32 INS21 INS27 INS8 INS42 MOV38 INS42 INS42 INS42 INS21 INS42 INS33 UPD42 INS34 INS42 MOV38 INS42 MOV32 INS42 INS33 INS34 INS34 UPD42 INS42 INS42 INS42 MOV38 INS34 MOV32 INS42 UPD42 MOV42 MOV42 MOV42 MOV9 INS42 MOV38 INS42 INS42 INS42 INS21 INS42 INS42 UPD42 UPD34 MOV32 INS42 MOV38 INS42 MOV32 UPD42 INS42 UPD34 INS34 INS42 INS42 INS34 INS42 INS42 UPD42 INS38 MOV38 INS27 INS32 MOV21 INS27 INS8 INS42 INS40 INS38 INS42 MOV60 UPD42 INS42 INS38 MOV38 UPD42 MOV42 UPD42 MOV42 INS42 INS42 UPD42 INS34 MOV38 INS42 INS34 INS42 INS42 INS42 INS32 INS27 INS38 INS21 INS7 UPD42 MOV42 INS7 INS34 INS42 INS34 INS42 INS42 INS42 INS27 INS38 MOV21 INS32 INS34 UPD42 INS42 MOV38 UPD34 UPD42 INS42 INS42 INS42 INS33 INS34 INS34 INS34 INS42 INS34 MOV32 INS32 INS42 INS34 INS42 INS34 INS42 UPD42 INS34 INS34 INS34 INS42 INS34 INS32 MOV34 MOV42 MOV42 INS42 MOV14 UPD42 UPD42 INS42 INS42 INS42 INS33 MOV38 INS34 INS34 UPD42 MOV42 UPD42 MOV42 INS42 UPD42 INS42 UPD34 INS34 INS42 UPD42 INS42 DEL42 DEL42 DEL34 DEL32 DEL21 DEL34 DEL32 DEL21 DEL34 DEL32 DEL21 DEL42 DEL33 DEL27 DEL42 DEL42 DEL42 DEL32 DEL27 DEL42 DEL33 DEL27 DEL42 DEL42 DEL40 DEL32 DEL21 DEL8 DEL25 DEL8 DEL25 DEL39 DEL42 DEL42 DEL43 DEL33 DEL42 DEL42 DEL32 DEL32 DEL42 DEL9 DEL7 DEL21 DEL8 DEL8 DEL25 DEL8 DEL42 DEL43 DEL42 DEL42 DEL33 DEL27 DEL42 DEL62 DEL36 DEL27 DEL42 DEL42 DEL32 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL8 DEL34 DEL34 DEL38 DEL32 DEL21 DEL8 DEL32 DEL42 DEL38 DEL42 DEL42 DEL42 DEL42 DEL32 DEL21 DEL8 DEL42 DEL9 DEL42 DEL42 DEL34 DEL32 DEL21 DEL34 DEL34 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL32 DEL21 DEL42 DEL42 DEL32 DEL21 DEL42 DEL42 DEL34 DEL32 DEL21 DEL42 DEL40 DEL27 DEL34 DEL32 DEL21 DEL8 DEL25 DEL42 DEL40 DEL27 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL42 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL32 DEL21 DEL8 DEL25 DEL8 DEL25 DEL39 DEL42 DEL40 DEL42 DEL42 DEL32 DEL27 DEL59 DEL60 DEL42 DEL33 DEL59 DEL60 DEL39 DEL42 DEL34 DEL59 DEL60 DEL39 DEL42 DEL9 DEL59 DEL60 DEL39 DEL42 DEL9 DEL59 DEL60 DEL42 DEL51 DEL8 DEL54 DEL42 DEL42 DEL42 DEL42 DEL9 DEL42 DEL33 DEL27 DEL9 DEL32 DEL21 DEL8 DEL42 DEL32 DEL21 DEL8 DEL25 DEL25 DEL8 DEL39 DEL8 DEL34 DEL38 DEL32 DEL42 DEL42 DEL42 DEL34 DEL32 DEL21 DEL8 DEL42 DEL42 DEL43 DEL62 DEL42 DEL43 DEL42 DEL42 DEL43 DEL42 DEL11 DEL59 DEL60 DEL38 DEL6 DEL8 DEL25 DEL34 DEL32 DEL21 DEL42 DEL42 DEL42 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL32 DEL21 DEL42 DEL43 DEL42 DEL11 DEL36 DEL42 DEL43 DEL42 DEL11 DEL36 DEL36 DEL42 DEL11 DEL36 DEL8 DEL42 DEL43 DEL62 DEL8 DEL25 DEL25 DEL42 DEL33 DEL27 DEL42 DEL42 DEL42 DEL32 DEL27 DEL34 DEL41 DEL8 DEL25 DEL8 DEL39 DEL42 DEL9 DEL59 DEL60 DEL42 DEL9 DEL7 DEL39 DEL42 DEL34 DEL59 DEL60 DEL42 DEL42 DEL7 DEL21 DEL8 DEL25 DEL42 DEL42 DEL32 DEL42 DEL27 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL32 DEL21 DEL42 DEL42 DEL42 DEL32 DEL21 DEL42 DEL42 DEL34 DEL42 DEL32 DEL21 DEL8 DEL25 DEL42 DEL42 DEL42 DEL42 DEL32 DEL38 DEL32 DEL21 DEL42 DEL42 DEL34 DEL34 DEL42 DEL42 DEL32 DEL38 DEL32 DEL21 DEL42 DEL42 DEL32 DEL21 DEL42 DEL42 DEL34 DEL32 DEL21 DEL42 DEL42 DEL32 DEL21 DEL9 DEL9 DEL32 DEL8 DEL39 DEL42 DEL39 DEL42 DEL44 DEL39 DEL42 DEL44 DEL34 DEL38 DEL32 DEL21 DEL39 DEL42 DEL34 DEL59 DEL60 DEL42 DEL42 DEL42 DEL32 DEL7 DEL21 DEL42 DEL9 DEL34 DEL42 DEL42 DEL32 DEL7 DEL21 DEL42 DEL42 DEL32 DEL21 DEL8 DEL25 DEL42 DEL34 DEL38 DEL32 DEL21 DEL42 DEL42 DEL42 DEL38 DEL32 DEL21 DEL34 DEL32 DEL21 DEL8 DEL42 DEL42 DEL34 DEL38 DEL32 DEL21 DEL25 DEL8 DEL25 DEL39 DEL42 DEL9 DEL59 DEL60 DEL42 DEL51 DEL8 DEL54 DEL42 DEL32 DEL21 DEL8 DEL25 DEL8