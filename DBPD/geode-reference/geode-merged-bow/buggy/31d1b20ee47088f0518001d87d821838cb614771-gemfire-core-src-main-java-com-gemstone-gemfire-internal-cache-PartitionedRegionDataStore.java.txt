Initial import of geode-1.0.0.0-SNAPSHOT-2.
All the new sub-project directories (like jvsd) were not imported.
A diff was done to confirm that this commit is exactly the same as
the open directory the snapshot was made from.

+import com.gemstone.gemfire.cache.hdfs.HDFSIOException;
+                if (getPartitionedRegion().isShadowPR()) {
+                  getPartitionedRegion().getColocatedWithRegion()
+                  .getRegionAdvisor()
+                  .getBucketAdvisor(possiblyFreeBucketId)
+                  .setShadowBucketDestroyed(false);
+                }
+            for (GatewaySenderEventImpl event : tempQueue) {
+              event.release();
+            }
+    factory.setOffHeap(this.partitionedRegion.getOffHeap());
-    return putLocally(bucketId, event, ifNew, ifOld, expectedOldValue,
-        requireOldValue, lastModified, br);
+    return putLocally(br, event, ifNew, ifOld, expectedOldValue,
+        requireOldValue, lastModified);
-  public boolean putLocally(final Integer bucketId,
+  public boolean putLocally(final BucketRegion bucketRegion,
-                            final long lastModified,
-                            final BucketRegion bucketRegion)
+                            final long lastModified)
-			  clientEvent, returnTombstones, false);
+			  clientEvent, returnTombstones, false, false);
-      boolean returnTombstones, boolean opScopeIsLocal) throws PrimaryBucketException,
+      boolean returnTombstones, boolean opScopeIsLocal, boolean allowReadFromHDFS) throws PrimaryBucketException,
-      logger.debug("getLocally:  key {}) bucketId={}{}{} region {} returnTombstones {}", key,
-          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, bucketRegion.getName(), returnTombstones);
+      logger.debug("getLocally:  key {}) bucketId={}{}{} region {} returnTombstones {} allowReadFromHDFS {}", key,
+          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, bucketRegion.getName(), returnTombstones, allowReadFromHDFS);
-      ret = bucketRegion.get(key, aCallbackArgument, true, disableCopyOnRead , preferCD, requestingClient, clientEvent, returnTombstones, opScopeIsLocal);
+      ret = bucketRegion.get(key, aCallbackArgument, true, disableCopyOnRead , preferCD, requestingClient, clientEvent, returnTombstones, opScopeIsLocal, allowReadFromHDFS, false);
-  public RawValue getSerializedLocally(KeyInfo keyInfo, boolean doNotLockEntry, EntryEventImpl clientEvent, boolean returnTombstones) throws PrimaryBucketException,
+  public RawValue getSerializedLocally(KeyInfo keyInfo, boolean doNotLockEntry, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) throws PrimaryBucketException,
-      RawValue result = bucketRegion.getSerialized(keyInfo, true, doNotLockEntry, clientEvent, returnTombstones);
+      RawValue result = bucketRegion.getSerialized(keyInfo, true, doNotLockEntry, clientEvent, returnTombstones, allowReadFromHDFS);
-      boolean access, boolean allowTombstones)
+      boolean access, boolean allowTombstones, boolean allowReadFromHDFS)
-      ent = bucketRegion.entries.getEntry(key);
+      if (allowReadFromHDFS) {
+        ent = bucketRegion.entries.getEntry(key);
+      }
+      else {
+        ent = bucketRegion.entries.getOperationalEntryInVM(key);
+      }
+        Set keys = r.keySet(allowTombstones);
+        if (getPartitionedRegion().isHDFSReadWriteRegion()) {
+          // hdfs regions can't copy all keys into memory
+          ret = keys;
+
+        } else  { 
+		}
-   * @param bucketId
-   *          the bucket id of the key
+   * @param bucketRegion
+   *          the bucket to do the create in
-  public boolean createLocally(Integer bucketId,
+  public boolean createLocally(final BucketRegion bucketRegion,
-    final BucketRegion bucketRegion = getInitializedBucketForId(event.getKey(), bucketId);
-
+  public Map<Integer, SizeEntry> getSizeEstimateForLocalPrimaryBuckets() {
+    return getSizeEstimateLocallyForBuckets(getAllLocalPrimaryBucketIds());
+  }
+
+    return getSizeLocallyForPrimary(bucketIds, false);
+  }
+
+  public Map<Integer, SizeEntry> getSizeEstimateLocallyForBuckets(Collection<Integer> bucketIds) {
+    return getSizeLocallyForPrimary(bucketIds, true);
+  }
+
+  private Map<Integer, SizeEntry> getSizeLocallyForPrimary(Collection<Integer> bucketIds, boolean estimate) {
-    BucketRegion r;
+    BucketRegion r = null;
-        mySizeMap.put(bucketId, new SizeEntry(r.size(), r.getBucketAdvisor().isPrimary()));
+        mySizeMap.put(bucketId, new SizeEntry(estimate ? r.sizeEstimate() : r.size(), r.getBucketAdvisor().isPrimary()));
+//        if (getLogWriter().fineEnabled() && r.getBucketAdvisor().isPrimary()) {
+//          r.verifyTombstoneCount();
+//        }
+      } catch (PrimaryBucketException skip) {
+        // sizeEstimate() will throw this exception as it will not try to read from HDFS on a secondary bucket,
+        // this bucket will be retried in PartitionedRegion.getSizeForHDFS() fixes bug 49033
+        continue;
+  /** a fast estimate of total bucket size */
+  public long getEstimatedLocalBucketSize(boolean primaryOnly) {
+    long size = 0;
+    for (BucketRegion br : localBucket2RegionMap.values()) {
+      if (!primaryOnly || br.getBucketAdvisor().isPrimary()) {
+        size += br.getEstimatedLocalSize();
+      }
+    }
+    return size;
+  }
+
+  /** a fast estimate of total bucket size */
+  public long getEstimatedLocalBucketSize(Set<Integer> bucketIds) {
+    long size = 0;
+    for (Integer bid : bucketIds) {
+      BucketRegion br = localBucket2RegionMap.get(bid);
+      if (br != null) {
+        size += br.getEstimatedLocalSize();
+      }
+    }
+    return size;
+  }
+

INS26 INS40 MOV44 INS31 INS31 INS31 INS31 INS31 INS44 INS44 INS44 INS83 INS74 INS42 INS8 MOV29 INS83 INS74 INS42 INS44 INS8 INS83 INS74 INS42 INS44 INS8 UPD83 UPD42 INS44 INS29 INS83 INS39 INS42 INS44 INS8 INS29 INS83 INS39 INS42 INS44 INS8 INS21 INS39 INS42 INS39 INS42 INS39 INS42 INS83 UPD43 UPD42 INS43 INS43 INS43 INS41 INS43 INS43 INS43 INS74 INS42 INS41 INS43 INS43 INS43 INS74 INS42 INS41 INS39 INS42 INS65 INS39 INS42 INS60 INS70 INS41 INS65 INS74 INS42 INS60 INS70 INS41 INS32 UPD42 UPD66 UPD42 INS42 INS42 INS42 INS32 INS42 INS42 INS42 INS43 INS43 INS32 INS42 INS42 INS42 INS43 INS43 INS32 INS66 INS39 INS59 INS44 INS32 INS8 INS42 INS66 INS43 INS43 INS39 INS59 INS44 INS42 INS8 INS42 INS42 INS42 INS32 UPD42 INS9 INS25 INS42 INS32 INS42 INS42 INS42 INS42 INS9 INS42 INS42 INS42 INS42 INS9 INS33 INS42 INS34 MOV43 INS42 INS42 INS42 INS25 INS42 INS42 INS42 INS34 INS43 INS42 INS60 INS25 INS22 INS42 INS42 INS8 INS8 INS8 INS42 INS12 INS27 INS8 INS42 INS43 INS59 INS27 INS8 INS52 INS42 UPD45 INS42 MOV21 INS21 INS60 INS25 MOV21 INS44 INS8 INS38 INS32 INS21 INS42 INS42 INS32 INS42 INS33 INS21 INS42 INS9 INS42 INS7 INS43 INS59 INS32 INS8 MOV8 INS43 INS42 INS18 INS42 INS32 INS42 INS7 INS42 INS42 INS42 INS7 INS42 INS32 INS42 INS42 INS32 INS32 INS42 INS21 INS42 INS42 INS42 INS42 INS32 INS42 INS32 INS8 INS40 INS42 INS42 INS42 INS42 INS42 INS42 INS7 INS16 INS42 INS42 INS42 INS42 INS70 MOV21 INS42 INS42 INS42 INS32 MOV32 INS44 INS42 INS8 INS42 INS42 INS43 INS42 INS21 INS42 INS32 INS25 INS42 INS42 INS32 INS8 INS32 INS42 INS21 INS42 INS32 INS32 INS42 INS9 INS32 INS42 INS42 INS32 INS42 INS32 INS42 INS42 DEL8 DEL42 DEL83 DEL42 DEL43 DEL42 DEL44 DEL83 DEL42 DEL42 DEL42 DEL42 DEL32 DEL42 DEL32 DEL59 DEL60