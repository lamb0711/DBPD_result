Initial import of geode-1.0.0.0-SNAPSHOT-2.
All the new sub-project directories (like jvsd) were not imported.
A diff was done to confirm that this commit is exactly the same as
the open directory the snapshot was made from.

+import com.gemstone.gemfire.LogWriter;
+import com.gemstone.gemfire.internal.cache.control.HeapMemoryMonitor;
+import com.gemstone.gemfire.internal.cache.control.InternalResourceManager.ResourceType;
+import com.gemstone.gemfire.internal.lang.ThreadUtils;
- * Triggers centralized eviction(asynchornously) when ResourceManager sends
- * EVICTION_UP event. This is registered with Resource Manager.
+ * Triggers centralized eviction(asynchronously) when the ResourceManager sends
+ * an eviction event for on-heap regions. This is registered with the ResourceManager.
- * @author Yogesh, Suranjan, Amardeep
+ * @author Yogesh, Suranjan, Amardeep, rholmes
+
+  // Add 1 for the management task that's putting more eviction tasks on the queue
-      "gemfire.HeapLRUCapacityController.MAX_EVICTOR_THREADS", Runtime.getRuntime().availableProcessors()*4);
+      "gemfire.HeapLRUCapacityController.MAX_EVICTOR_THREADS", (Runtime.getRuntime().availableProcessors()*4)) + 1;
-      "gemfire.HeapLRUCapacityController.higherEntryCountBucketCalculationInterval", 100).intValue();               
+      "gemfire.HeapLRUCapacityController.higherEntryCountBucketCalculationInterval",
+      100).intValue();
+  private static final String EVICTOR_THREAD_GROUP_NAME = "EvictorThreadGroup";
+  
+  private static final String EVICTOR_THREAD_NAME = "EvictorThread";
+
-    long maxTenuredBytes = InternalResourceManager.getTenuredPoolMaxMemory();
+    long maxTenuredBytes = HeapMemoryMonitor.getTenuredPoolMaxMemory();
-  private final Cache cache;  
+  protected final Cache cache;  
-  private ArrayList testTaskSetSizes = new  ArrayList();
+  private final ArrayList testTaskSetSizes = new  ArrayList();
+  public volatile int testAbortAfterLoopCount = Integer.MAX_VALUE;
+  protected boolean includePartitionedRegion(PartitionedRegion region) {
+    return (region.getEvictionAttributes().getAlgorithm().isLRUHeap() 
+        && (region.getDataStore() != null) 
+        && !region.getAttributes().getOffHeap());
+  }
+  
+  protected boolean includeLocalRegion(LocalRegion region) {
+    return (region.getEvictionAttributes().getAlgorithm().isLRUHeap() 
+        && !region.getAttributes().getOffHeap());
+  }
+  
-    for (ResourceListener<MemoryEvent> listener : irm.getMemoryEventListeners()) {
+    
+    for (ResourceListener<MemoryEvent> listener : irm.getResourceListeners(getResourceType())) {
-        if (pr.getEvictionAttributes().getAlgorithm().isLRUHeap()
-            && pr.getDataStore() != null) {
+        if (includePartitionedRegion(pr)) {
-        if (lr.getEvictionAttributes().getAlgorithm().isLRUHeap()) {
+        if (includeLocalRegion(lr)) {
-        "EvictorThreadGroup", logger);
+        getEvictorThreadGroupName(), logger);
-        Thread t = new Thread(evictorThreadGroup, command, "EvictorThread "
+        Thread t = new Thread(evictorThreadGroup, command, getEvictorThreadName()
-      long bytesToEvictPerTask = (long)(TOTAL_BYTES_TO_EVICT_FROM_HEAP * percentage);
+      long bytesToEvictPerTask = (long)(getTotalBytesToEvict() * percentage);
-    long bytesToEvictPerTask = TOTAL_BYTES_TO_EVICT_FROM_HEAP/threadsAvailable;
+    long bytesToEvictPerTask = getTotalBytesToEvict() / threadsAvailable;
+  
+  // Since the amount of memory used is to a large degree dependent upon when
+  // garbage collection is run, it's difficult to determine when to stop 
+  // evicting.  So, an initial calculation is done to determine the number of
+  // evictions that are likely needed in order to bring memory usage below the
+  // eviction threshold.  This number is stored in 'numFastLoops' and we
+  // quickly loop through this number performing evictions.  We then continue
+  // to evict, but at a progressively slower rate waiting either for an event
+  // which indicates we've dropped below the eviction threshold or another
+  // eviction event with an updated "number of bytes used".  If we get another
+  // eviction event with an updated "number of bytes used" then 'numFastLoops'
+  // is recalculated and we start over.
+  protected volatile int numEvictionLoopsCompleted = 0;
+  protected volatile int numFastLoops;
+  private long previousBytesUsed;
+  private final Object evictionLock = new Object();
+  @Override
+  public void onEvent(final MemoryEvent event) {
+    if (DISABLE_HEAP_EVICTIOR_THREAD_POOL) {
+      return;
+    }
+    
+    // Do we care about eviction events and did the eviction event originate
+    // in this VM ...
+    if(this.isRunning.get() && event.isLocal()) {
+      if (event.getState().isEviction()) {
+        final LogWriter logWriter = cache.getLogger();
+        
+        // Have we previously received an eviction event and already started eviction ...
+        if (this.mustEvict.get() == true) {
+          if (logWriter.fineEnabled()) {
+            logWriter.fine("Updating eviction in response to memory event: " + event + ". previousBytesUsed=" + previousBytesUsed);
+          }
-  public void onEvent(MemoryEvent event) {
-    if(isRunning.get()) {
-      if (event.isLocal()) {
-        if (event.getType().isEvictionUp() || event.getType().isEvictMore()) {
-          this.mustEvict.set(true);
-          if (!DISABLE_HEAP_EVICTIOR_THREAD_POOL) {
-            for (;;) {
+          // We lock here to make sure that the thread that was previously
+          // started and running eviction loops is in a state where it's okay
+          // to update the number of fast loops to perform.
+          synchronized (evictionLock) {
+            numEvictionLoopsCompleted = 0;
+            numFastLoops = (int) ((event.getBytesUsed() - event.getThresholds().getEvictionThresholdClearBytes()
+                + getTotalBytesToEvict()) / getTotalBytesToEvict());
+            evictionLock.notifyAll();
+          }
+          
+          // We've updated the number of fast loops to perform, and there's
+          // already a thread running the evictions, so we're done.
+          return;
+        }
+        
+        if (!this.mustEvict.compareAndSet(false, true)) {
+          // Another thread just started evicting.
+          return;
+        }
+        
+        numEvictionLoopsCompleted = 0;
+        numFastLoops = (int) ((event.getBytesUsed() - event.getThresholds().getEvictionThresholdClearBytes()
+            + getTotalBytesToEvict()) / getTotalBytesToEvict());
+        if (logWriter.fineEnabled()) {
+          logWriter.fine("Starting eviction in response to memory event: " + event);
+        }
+        
+        // The new thread which will run in a loop performing evictions
+        final Runnable evictionManagerTask = new Runnable() {
+          @Override
+          public void run() {
+            // Has the test hook been set which will cause eviction to abort early
+            if (numEvictionLoopsCompleted < testAbortAfterLoopCount) {
+                // Submit tasks into the queue to do the evictions
-                }
-                else {
+                } else {
-                RegionEvictorTask.setLastTaskCompletionTime(System.currentTimeMillis()); // bug 41938
-                break;
+                RegionEvictorTask.setLastTaskCompletionTime(System.currentTimeMillis());
+  
+                // Make sure that another thread isn't processing a new eviction event
+                // and changing the number of fast loops to perform.
+                synchronized (evictionLock) {
+                  int delayTime = getEvictionLoopDelayTime();
+                  if (logWriter.fineEnabled()) {
+                    logWriter.fine("Eviction loop delay time calculated to be " + delayTime + " milliseconds. Fast Loops="
+                        + numFastLoops + ", Loop #=" + numEvictionLoopsCompleted+1);
+                  }
+                  numEvictionLoopsCompleted++;
+                  try {
+                    // Wait and release the lock so that the number of fast loops
+                    // needed can be updated by another thread processing a new
+                    // eviction event.
+                    evictionLock.wait(delayTime);
+                  } catch (InterruptedException iex) {
+                    // Loop and try again
+                  }
+                }
+                
+                // Do we think we're still above the eviction threshold ...
+                if (HeapEvictor.this.mustEvict.get()) {
+                  // Submit this runnable back into the thread pool and execute
+                  // another pass at eviction.
+                  HeapEvictor.this.evictorThreadPool.submit(this);
+                }
+                if (HeapEvictor.this.mustEvict.get()) {
+                  HeapEvictor.this.evictorThreadPool.submit(this);
+                }
-        }
-        else if (event.getType().isEvictionDown()
-            || event.getType().isEvictionDisabled()) {
-          this.mustEvict.set(false);
-        }
+        };
+        
+        // Submit the first pass at eviction into the pool
+        this.evictorThreadPool.submit(evictionManagerTask);
+          
+      } else {
+        this.mustEvict.set(false);
+  protected int getEvictionLoopDelayTime() {
+    int delayTime = 850; // The waiting period when running fast loops
+    if ((numEvictionLoopsCompleted - numFastLoops) > 2) {
+      delayTime = 3000;  // Way below the threshold
+    } else if (numEvictionLoopsCompleted >= numFastLoops) {
+      delayTime = (numEvictionLoopsCompleted - numFastLoops + 3) * 500; // Just below the threshold
+    }
+    
+    return delayTime;  
+  }
+  
+  
+  protected String getEvictorThreadGroupName() {
+    return HeapEvictor.EVICTOR_THREAD_GROUP_NAME;
+  }
+  
+  protected String getEvictorThreadName() {
+    return HeapEvictor.EVICTOR_THREAD_NAME;
+  }
+  
+  public long getTotalBytesToEvict() {
+    return TOTAL_BYTES_TO_EVICT_FROM_HEAP;
+  }
+  
+  protected ResourceType getResourceType() {
+    return ResourceType.HEAP_MEMORY;
+  }
