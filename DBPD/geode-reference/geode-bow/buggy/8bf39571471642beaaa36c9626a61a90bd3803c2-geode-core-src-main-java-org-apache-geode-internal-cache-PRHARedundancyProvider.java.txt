Added Spotless plugin to enforce formatting standards.
Added Google Java Style guide formatter templates, removed existing formatter templates.

Ran './gradlew clean build' for verification

This closes #268

- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license
+ * agreements. See the NOTICE file distributed with this work for additional information regarding
+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License. You may obtain a
+ * copy of the License at
- *      http://www.apache.org/licenses/LICENSE-2.0
+ * http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
+ * Unless required by applicable law or agreed to in writing, software distributed under the License
+ * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+ * or implied. See the License for the specific language governing permissions and limitations under
+ * the License.
- * This class provides the redundancy management for partitioned region. It will
- * provide the following to the PartitionedRegion: <br>
- * (1) Redundancy Management at the time of bucket creation.</br><br>
- * (2) Redundancy management at the new node arrival.</br><br>
- * (3) Redundancy management when the node leaves the partitioned region
- * distributed system gracefully. i.e. Cache.close()</br><br>
+ * This class provides the redundancy management for partitioned region. It will provide the
+ * following to the PartitionedRegion: <br>
+ * (1) Redundancy Management at the time of bucket creation.</br>
+ * <br>
+ * (2) Redundancy management at the new node arrival.</br>
+ * <br>
+ * (3) Redundancy management when the node leaves the partitioned region distributed system
+ * gracefully. i.e. Cache.close()</br>
+ * <br>
-public class PRHARedundancyProvider
-  {
+public class PRHARedundancyProvider {
-    private static final boolean DISABLE_CREATE_BUCKET_RANDOMNESS
-        = Boolean.getBoolean(DistributionConfig.GEMFIRE_PREFIX + "DISABLE_CREATE_BUCKET_RANDOMNESS");
+  private static final boolean DISABLE_CREATE_BUCKET_RANDOMNESS =
+      Boolean.getBoolean(DistributionConfig.GEMFIRE_PREFIX + "DISABLE_CREATE_BUCKET_RANDOMNESS");
+
+
-  public static final String  DATASTORE_DISCOVERY_TIMEOUT_PROPERTY_NAME =
+  public static final String DATASTORE_DISCOVERY_TIMEOUT_PROPERTY_NAME =
-    Long.getLong(DATASTORE_DISCOVERY_TIMEOUT_PROPERTY_NAME);
+      Long.getLong(DATASTORE_DISCOVERY_TIMEOUT_PROPERTY_NAME);
-  
+
-   * An executor to submit tasks for redundancy recovery too. It makes sure
-   * that there will only be one redundancy recovery task in the queue at a time.
+   * An executor to submit tasks for redundancy recovery too. It makes sure that there will only be
+   * one redundancy recovery task in the queue at a time.
-  
+
-  
+
-   * Used to consolidate logging for bucket regions waiting on other
-   * members to come online.
+   * Used to consolidate logging for bucket regions waiting on other members to come online.
-   * @param region
-   *          The PartitionedRegion for which the HA redundancy is required to
-   *          be managed.
+   * @param region The PartitionedRegion for which the HA redundancy is required to be managed.
-    final InternalResourceManager resourceManager = region.getGemFireCache()
-    .getResourceManager();
+    final InternalResourceManager resourceManager = region.getGemFireCache().getResourceManager();
-    });
+        });
+   * 
-   * @param allStores the list of available stores.  If null, unknown.
+   * @param allStores the list of available stores. If null, unknown.
-  public static String regionStatus(PartitionedRegion prRegion,
-      Set allStores, Collection alreadyUsed, boolean forLog) {
+  public static String regionStatus(PartitionedRegion prRegion, Set allStores,
+      Collection alreadyUsed, boolean forLog) {
-    final String spaces; 
+    final String spaces;
-      sb.append(newLine + spaces + "Redundancy level set to "
-          + prRegion.getRedundantCopies());
-      sb.append(newLine + ". Number of available data stores: " +
-          allStores.size());
-      sb.append(newLine + spaces + ". Number successfully allocated = " +
-          alreadyUsed.size());
-      sb.append(newLine + ". Data stores: "  
-          + PartitionedRegionHelper.printCollection(allStores));
+      sb.append(newLine + spaces + "Redundancy level set to " + prRegion.getRedundantCopies());
+      sb.append(newLine + ". Number of available data stores: " + allStores.size());
+      sb.append(newLine + spaces + ". Number successfully allocated = " + alreadyUsed.size());
+      sb.append(newLine + ". Data stores: " + PartitionedRegionHelper.printCollection(allStores));
-      sb.append(newLine + ". Equivalent members: "
-          + PartitionedRegionHelper.printCollection(prRegion.getDistributionManager().getMembersInThisZone()));
+      sb.append(newLine + ". Equivalent members: " + PartitionedRegionHelper
+          .printCollection(prRegion.getDistributionManager().getMembersInThisZone()));
-  
-  static public final StringId TIMEOUT_MSG 
-      = LocalizedStrings.PRHARedundancyProvider_IF_YOUR_SYSTEM_HAS_SUFFICIENT_SPACE_PERHAPS_IT_IS_UNDER_MEMBERSHIP_OR_REGION_CREATION_STRESS;
-  
+
+  static public final StringId TIMEOUT_MSG =
+      LocalizedStrings.PRHARedundancyProvider_IF_YOUR_SYSTEM_HAS_SUFFICIENT_SPACE_PERHAPS_IT_IS_UNDER_MEMBERSHIP_OR_REGION_CREATION_STRESS;
+
-   * @param allStores all feasible stores.  If null, we don't know.
+   * 
+   * @param allStores all feasible stores. If null, we don't know.
-  public static void timedOut(PartitionedRegion prRegion, Set allStores, 
-      Collection alreadyUsed, String opString, long timeOut) {
+  public static void timedOut(PartitionedRegion prRegion, Set allStores, Collection alreadyUsed,
+      String opString, long timeOut) {
-      LocalizedStrings.PRHARedundancyProvider_TIMED_OUT_ATTEMPTING_TO_0_IN_THE_PARTITIONED_REGION__1_WAITED_FOR_2_MS.toLocalizedString(new Object[] {opString, regionStatus(prRegion, allStores, alreadyUsed, true), Long.valueOf(timeOut)}) + TIMEOUT_MSG;
-    throw new PartitionedRegionStorageException(tooManyRetries);        
+        LocalizedStrings.PRHARedundancyProvider_TIMED_OUT_ATTEMPTING_TO_0_IN_THE_PARTITIONED_REGION__1_WAITED_FOR_2_MS
+            .toLocalizedString(new Object[] {opString,
+                regionStatus(prRegion, allStores, alreadyUsed, true), Long.valueOf(timeOut)})
+            + TIMEOUT_MSG;
+    throw new PartitionedRegionStorageException(tooManyRetries);
-  
+
-    if(partitionName != null){
-      
+    if (partitionName != null) {
+
-    final Set<InternalDistributedMember> allStores = this.prRegion.getRegionAdvisor().adviseDataStore(true);
+    final Set<InternalDistributedMember> allStores =
+        this.prRegion.getRegionAdvisor().adviseDataStore(true);
-  
-  /**
-   * This is for FPR, for given partition, we have to return the set of
-   * datastores on which the given partition is defined
-   * 
-   * @param partitionName
-   *          name of the partition for which datastores need to be found out
-   */
-  private Set<InternalDistributedMember> getFixedPartitionStores(
-      String partitionName) {
-    Set<InternalDistributedMember> members = this.prRegion.getRegionAdvisor()
-        .adviseFixedPartitionDataStores(partitionName);
-    List<FixedPartitionAttributesImpl> FPAs = this.prRegion
-        .getFixedPartitionAttributesImpl();
+  /**
+   * This is for FPR, for given partition, we have to return the set of datastores on which the
+   * given partition is defined
+   * 
+   * @param partitionName name of the partition for which datastores need to be found out
+   */
+  private Set<InternalDistributedMember> getFixedPartitionStores(String partitionName) {
+    Set<InternalDistributedMember> members =
+        this.prRegion.getRegionAdvisor().adviseFixedPartitionDataStores(partitionName);
+
+    List<FixedPartitionAttributesImpl> FPAs = this.prRegion.getFixedPartitionAttributesImpl();
-          members.add((InternalDistributedMember)this.prRegion.getMyId());
+          members.add((InternalDistributedMember) this.prRegion.getMyId());
-        
+
-   * Signature string indicating that not enough stores are
-   * available. 
+   * Signature string indicating that not enough stores are available.
-  static public final StringId INSUFFICIENT_STORES_MSG 
-      = LocalizedStrings.PRHARedundancyProvider_CONSIDER_STARTING_ANOTHER_MEMBER;
-  
+  static public final StringId INSUFFICIENT_STORES_MSG =
+      LocalizedStrings.PRHARedundancyProvider_CONSIDER_STARTING_ANOTHER_MEMBER;
+
-   * Signature string indicating that there are enough stores
-   * available.
+   * Signature string indicating that there are enough stores available.
-  static public final StringId SUFFICIENT_STORES_MSG 
-    = LocalizedStrings.PRHARRedundancyProvider_FOUND_A_MEMBER_TO_HOST_A_BUCKET;
+  static public final StringId SUFFICIENT_STORES_MSG =
+      LocalizedStrings.PRHARRedundancyProvider_FOUND_A_MEMBER_TO_HOST_A_BUCKET;
-  private static final StringId ALLOCATE_ENOUGH_MEMBERS_TO_HOST_BUCKET
-    = LocalizedStrings.PRHARRedundancyProvider_ALLOCATE_ENOUGH_MEMBERS_TO_HOST_BUCKET;
+  private static final StringId ALLOCATE_ENOUGH_MEMBERS_TO_HOST_BUCKET =
+      LocalizedStrings.PRHARRedundancyProvider_ALLOCATE_ENOUGH_MEMBERS_TO_HOST_BUCKET;
-  
+
-   * Indicate that we are unable to allocate sufficient stores and
-   * the timeout period has passed
+   * Indicate that we are unable to allocate sufficient stores and the timeout period has passed
+   * 
-  private void insufficientStores(Set allStores, Collection alreadyUsed, 
-      boolean onlyLog) {
-    final String regionStat = regionStatus(this.prRegion, allStores,
-        alreadyUsed, onlyLog);
+  private void insufficientStores(Set allStores, Collection alreadyUsed, boolean onlyLog) {
+    final String regionStat = regionStatus(this.prRegion, allStores, alreadyUsed, onlyLog);
-    if(alreadyUsed.isEmpty()) {
-      notEnoughValidNodes = LocalizedStrings.PRHARRedundancyProvider_UNABLE_TO_FIND_ANY_MEMBERS_TO_HOST_A_BUCKET_IN_THE_PARTITIONED_REGION_0;
+    if (alreadyUsed.isEmpty()) {
+      notEnoughValidNodes =
+          LocalizedStrings.PRHARRedundancyProvider_UNABLE_TO_FIND_ANY_MEMBERS_TO_HOST_A_BUCKET_IN_THE_PARTITIONED_REGION_0;
-      notEnoughValidNodes = LocalizedStrings.PRHARRedundancyProvider_CONFIGURED_REDUNDANCY_LEVEL_COULD_NOT_BE_SATISFIED_0;
+      notEnoughValidNodes =
+          LocalizedStrings.PRHARRedundancyProvider_CONFIGURED_REDUNDANCY_LEVEL_COULD_NOT_BE_SATISFIED_0;
-    final Object[] notEnoughValidNodesArgs = new Object[] {PRHARedundancyProvider.INSUFFICIENT_STORES_MSG, newLine + regionStat + newLine};
+    final Object[] notEnoughValidNodesArgs = new Object[] {
+        PRHARedundancyProvider.INSUFFICIENT_STORES_MSG, newLine + regionStat + newLine};
-    }
-    else {
-      throw new PartitionedRegionStorageException(notEnoughValidNodes.toLocalizedString(notEnoughValidNodesArgs));
+    } else {
+      throw new PartitionedRegionStorageException(
+          notEnoughValidNodes.toLocalizedString(notEnoughValidNodesArgs));
-   * Create a single copy of this bucket on one node.  The bucket must
-   * already be locked.
+   * Create a single copy of this bucket on one node. The bucket must already be locked.
-   * @param bucketId The bucket we are working on 
+   * @param bucketId The bucket we are working on
-   * @param excludedMembers 
+   * @param excludedMembers
-  private InternalDistributedMember createBucketInstance(int bucketId,
-      final int newBucketSize,
+  private InternalDistributedMember createBucketInstance(int bucketId, final int newBucketSize,
-    
+
-    HashSet<InternalDistributedMember> candidateMembers = new HashSet<InternalDistributedMember>(allStores);
+    HashSet<InternalDistributedMember> candidateMembers =
+        new HashSet<InternalDistributedMember>(allStores);
-      logger.debug("AllStores={} AlreadyUsed={} excluded={} failed={}", allStores, alreadyUsed, excludedMembers, failedMembers);
+      logger.debug("AllStores={} AlreadyUsed={} excluded={} failed={}", allStores, alreadyUsed,
+          excludedMembers, failedMembers);
-      this.prRegion.checkReadiness();  // fix for bug #37207
+      this.prRegion.checkReadiness(); // fix for bug #37207
-      // Run out of candidates.  Refetch?
+      // Run out of candidates. Refetch?
-    
+
-      }
-      else {
-        String prName = this.prRegion.getAttributes().getPartitionAttributes()
-          .getColocatedWith();
+      } else {
+        String prName = this.prRegion.getAttributes().getPartitionAttributes().getColocatedWith();
-        }
-        else {
+        } else {
-            new ArrayList<InternalDistributedMember>(candidateMembers);
+              new ArrayList<InternalDistributedMember>(candidateMembers);
-    if(!this.prRegion.isShadowPR() && !ColocationHelper.checkMembersColocation(this.prRegion, candidate)) {
+    if (!this.prRegion.isShadowPR()
+        && !ColocationHelper.checkMembersColocation(this.prRegion, candidate)) {
-        logger.debug("createBucketInstances - Member does not have all of the regions colocated with prRegion {}",  candidate);
+        logger.debug(
+            "createBucketInstances - Member does not have all of the regions colocated with prRegion {}",
+            candidate);
-    if (! (candidate.equals(this.prRegion.getMyId()))) { // myself
-      PartitionProfile pp = this.prRegion.getRegionAdvisor()
-      .getPartitionProfile(candidate);
+    if (!(candidate.equals(this.prRegion.getMyId()))) { // myself
+      PartitionProfile pp = this.prRegion.getRegionAdvisor().getPartitionProfile(candidate);
-          logger.debug("createBucketInstance: {}: no partition profile for {}", this.prRegion.getFullPath(), candidate);
+          logger.debug("createBucketInstance: {}: no partition profile for {}",
+              this.prRegion.getFullPath(), candidate);
-    final ManageBucketRsp response = createBucketOnMember(bucketId,
-        candidate, newBucketSize, failedMembers.wasCleared());
+    final ManageBucketRsp response =
+        createBucketOnMember(bucketId, candidate, newBucketSize, failedMembers.wasCleared());
-          this.prRegion.getFullPath(), candidate, this.prRegion.bucketStringForLogs(bucketId), response);
+          this.prRegion.getFullPath(), candidate, this.prRegion.bucketStringForLogs(bucketId),
+          response);
-    if(response.equals(ManageBucketRsp.CLOSED)) {
+    if (response.equals(ManageBucketRsp.CLOSED)) {
-  
-  public static final long INSUFFICIENT_LOGGING_THROTTLE_TIME =
-      TimeUnit.SECONDS.toNanos(Integer.getInteger(DistributionConfig.GEMFIRE_PREFIX + "InsufficientLoggingThrottleTime", 2).intValue());
+
+  public static final long INSUFFICIENT_LOGGING_THROTTLE_TIME = TimeUnit.SECONDS.toNanos(
+      Integer.getInteger(DistributionConfig.GEMFIRE_PREFIX + "InsufficientLoggingThrottleTime", 2)
+          .intValue());
-  //since 6.6, please use the distributed system property enforce-unique-host instead.
-  //  public static final boolean ENFORCE_UNIQUE_HOST_STORAGE_ALLOCATION = DistributionConfig.DEFAULT_ENFORCE_UNIQUE_HOST;
-    
-  public InternalDistributedMember createBucketOnDataStore(int bucketId,
-      int size, long startTime, RetryTimeKeeper snoozer) {
+  // since 6.6, please use the distributed system property enforce-unique-host instead.
+  // public static final boolean ENFORCE_UNIQUE_HOST_STORAGE_ALLOCATION =
+  // DistributionConfig.DEFAULT_ENFORCE_UNIQUE_HOST;
+
+  public InternalDistributedMember createBucketOnDataStore(int bucketId, int size, long startTime,
+      RetryTimeKeeper snoozer) {
-      primaryForFixedPartition = this.prRegion.getRegionAdvisor()
-          .adviseFixedPrimaryPartitionDataStore(bucketId);
+      primaryForFixedPartition =
+          this.prRegion.getRegionAdvisor().adviseFixedPrimaryPartitionDataStore(bucketId);
-    
+
-      Set<InternalDistributedMember> available = this.prRegion
-          .getRegionAdvisor().adviseInitializedDataStore();
+      Set<InternalDistributedMember> available =
+          this.prRegion.getRegionAdvisor().adviseInitializedDataStore();
-        }
-        else {
+        } else {
-     try {
-        if(isDebugEnabled) {
-          logger.debug("Attempting to get data store {} to create the bucket {} for us", target, this.prRegion.bucketStringForLogs(bucketId));
+      try {
+        if (isDebugEnabled) {
+          logger.debug("Attempting to get data store {} to create the bucket {} for us", target,
+              this.prRegion.bucketStringForLogs(bucketId));
-        CreateBucketMessage.NodeResponse response = CreateBucketMessage.send(target, this.prRegion, bucketId, size);
+        CreateBucketMessage.NodeResponse response =
+            CreateBucketMessage.send(target, this.prRegion, bucketId, size);
-        if(ret != null) {
+        if (ret != null) {
-      } catch(ForceReattemptException e) {
-        //do nothing, we will already check again for a primary.
+      } catch (ForceReattemptException e) {
+        // do nothing, we will already check again for a primary.
-    }
-    while((ret = this.prRegion.getNodeForBucketWrite(bucketId, snoozer)) == null);
+    } while ((ret = this.prRegion.getNodeForBucketWrite(bucketId, snoozer)) == null);
-   * Creates bucket atomically by creating all the copies to satisfy redundancy. In case all
-   * copies can not be created, a PartitionedRegionStorageException is thrown to
-   * the user and BucketBackupMessage is sent to the nodes to make copies of a bucket
-   * that was only partially created. Other VMs are informed 
-   * of bucket creation through updates through their {@link BucketAdvisor.BucketProfile}s.
+   * Creates bucket atomically by creating all the copies to satisfy redundancy. In case all copies
+   * can not be created, a PartitionedRegionStorageException is thrown to the user and
+   * BucketBackupMessage is sent to the nodes to make copies of a bucket that was only partially
+   * created. Other VMs are informed of bucket creation through updates through their
+   * {@link BucketAdvisor.BucketProfile}s.
-   * This method is synchronized to enforce a single threaded ordering, allowing 
-   * for a more accurate picture of bucket distribution in the face of concurrency.
-   * See bug 37275.
+   * This method is synchronized to enforce a single threaded ordering, allowing for a more accurate
+   * picture of bucket distribution in the face of concurrency. See bug 37275.
-   * This method is now slightly misnamed. Another member could be in the process
-   * of creating this same bucket at the same time.
+   * This method is now slightly misnamed. Another member could be in the process of creating this
+   * same bucket at the same time.
-   * @param bucketId
-   *          Id of the bucket to be created.
-   * @param newBucketSize
-   *          size of the first entry.
+   * @param bucketId Id of the bucket to be created.
+   * @param newBucketSize size of the first entry.
-   * @throws PartitionedRegionStorageException
-   *           if required # of buckets can not be created to satisfy
-   *           redundancy.
-   * @throws PartitionedRegionException
-   *           if d-lock can not be acquired to create bucket.
-   * @throws PartitionOfflineException
-   *           if persistent data recovery is not complete for a partitioned
-   *           region referred to in the query.
+   * @throws PartitionedRegionStorageException if required # of buckets can not be created to
+   *         satisfy redundancy.
+   * @throws PartitionedRegionException if d-lock can not be acquired to create bucket.
+   * @throws PartitionOfflineException if persistent data recovery is not complete for a partitioned
+   *         region referred to in the query.
-  public InternalDistributedMember
-    createBucketAtomically(final int bucketId,
-                           final int newBucketSize,
-                           final long startTime,
-                           final boolean finishIncompleteCreation, String partitionName) throws PartitionedRegionStorageException,
-                                    PartitionedRegionException, PartitionOfflineException
-  {
+  public InternalDistributedMember createBucketAtomically(final int bucketId,
+      final int newBucketSize, final long startTime, final boolean finishIncompleteCreation,
+      String partitionName) throws PartitionedRegionStorageException, PartitionedRegionException,
+      PartitionOfflineException {
-    
+
-    synchronized(this) {
+    synchronized (this) {
-    if (isDebugEnabled) {
-      logger.debug("Starting atomic creation of bucketId={}", this.prRegion.bucketStringForLogs(bucketId));
-    }
-    Collection<InternalDistributedMember> acceptedMembers = new ArrayList<InternalDistributedMember>(); // ArrayList<DataBucketStores>
-    Set <InternalDistributedMember> excludedMembers = new HashSet<InternalDistributedMember>();
-    ArrayListWithClearState<InternalDistributedMember> failedMembers = new ArrayListWithClearState<InternalDistributedMember>();
-    final long timeOut = System.currentTimeMillis() + computeTimeout();
-    BucketMembershipObserver observer = null;
-    boolean needToElectPrimary = true;
-    InternalDistributedMember bucketPrimary = null;
-    try {
-      this.prRegion.checkReadiness();
-
-      Bucket toCreate = this.prRegion.getRegionAdvisor().getBucket(bucketId);
-      
-      if(!finishIncompleteCreation) {
-        bucketPrimary = 
-          this.prRegion.getBucketPrimary(bucketId);
-        if (bucketPrimary != null) {
-          if(isDebugEnabled) {
-            logger.debug("during atomic creation, discovered that the primary already exists {} returning early", bucketPrimary);
-          }
-          needToElectPrimary = false;
-          return bucketPrimary;
-        }
+      if (isDebugEnabled) {
+        logger.debug("Starting atomic creation of bucketId={}",
+            this.prRegion.bucketStringForLogs(bucketId));
-      
-      observer = new BucketMembershipObserver(toCreate).beginMonitoring();
-      boolean loggedInsufficentStores = false; // track if insufficient data stores have been detected
-      for (;;) {
+      Collection<InternalDistributedMember> acceptedMembers =
+          new ArrayList<InternalDistributedMember>(); // ArrayList<DataBucketStores>
+      Set<InternalDistributedMember> excludedMembers = new HashSet<InternalDistributedMember>();
+      ArrayListWithClearState<InternalDistributedMember> failedMembers =
+          new ArrayListWithClearState<InternalDistributedMember>();
+      final long timeOut = System.currentTimeMillis() + computeTimeout();
+      BucketMembershipObserver observer = null;
+      boolean needToElectPrimary = true;
+      InternalDistributedMember bucketPrimary = null;
+      try {
-        if (this.prRegion.getCache().isCacheAtShutdownAll()) {
-          if (isDebugEnabled) {
-            logger.debug("Aborted createBucketAtomically due to ShutdownAll");
+
+        Bucket toCreate = this.prRegion.getRegionAdvisor().getBucket(bucketId);
+
+        if (!finishIncompleteCreation) {
+          bucketPrimary = this.prRegion.getBucketPrimary(bucketId);
+          if (bucketPrimary != null) {
+            if (isDebugEnabled) {
+              logger.debug(
+                  "during atomic creation, discovered that the primary already exists {} returning early",
+                  bucketPrimary);
+            }
+            needToElectPrimary = false;
+            return bucketPrimary;
-          throw new CacheClosedException("Cache is shutting down");
-//        this.prRegion.getCache().getLogger().config(
-//            "DEBUG createBucketAtomically: "
-//            + " bucketId=" + this.prRegion.getBucketName(bucketId) + 
-//            " accepted: " + acceptedMembers + 
-//            " failed: " + failedMembers);
-        
-        long timeLeft = timeOut - System.currentTimeMillis();
-        if (timeLeft < 0) {
+
+        observer = new BucketMembershipObserver(toCreate).beginMonitoring();
+        boolean loggedInsufficentStores = false; // track if insufficient data stores have been
+                                                 // detected
+        for (;;) {
+          this.prRegion.checkReadiness();
+          if (this.prRegion.getCache().isCacheAtShutdownAll()) {
+            if (isDebugEnabled) {
+              logger.debug("Aborted createBucketAtomically due to ShutdownAll");
+            }
+            throw new CacheClosedException("Cache is shutting down");
+          }
+          // this.prRegion.getCache().getLogger().config(
+          // "DEBUG createBucketAtomically: "
+          // + " bucketId=" + this.prRegion.getBucketName(bucketId) +
+          // " accepted: " + acceptedMembers +
+          // " failed: " + failedMembers);
+
+          long timeLeft = timeOut - System.currentTimeMillis();
+          if (timeLeft < 0) {
-            timedOut(this.prRegion, getAllStores(partitionName),
-                acceptedMembers, ALLOCATE_ENOUGH_MEMBERS_TO_HOST_BUCKET
-                    .toLocalizedString(), computeTimeout());
+            timedOut(this.prRegion, getAllStores(partitionName), acceptedMembers,
+                ALLOCATE_ENOUGH_MEMBERS_TO_HOST_BUCKET.toLocalizedString(), computeTimeout());
-        if (isDebugEnabled) {
-          logger.debug("createBucketAtomically: have {} ms left to finish this", timeLeft);
-        }
+          if (isDebugEnabled) {
+            logger.debug("createBucketAtomically: have {} ms left to finish this", timeLeft);
+          }
-        // Always go back to the advisor, see if any fresh data stores are
+          // Always go back to the advisor, see if any fresh data stores are
-        Set<InternalDistributedMember> allStores = getAllStores(partitionName);
+          Set<InternalDistributedMember> allStores = getAllStores(partitionName);
-        loggedInsufficentStores  = checkSufficientStores(allStores, 
-            loggedInsufficentStores);
+          loggedInsufficentStores = checkSufficientStores(allStores, loggedInsufficentStores);
-        InternalDistributedMember candidate = createBucketInstance(bucketId, 
-	   newBucketSize, excludedMembers, acceptedMembers, failedMembers, timeOut, allStores);
-        if (candidate != null) {
-          if (this.prRegion.getDistributionManager().enforceUniqueZone()) {
-        	//enforceUniqueZone property has no effect for a loner. Fix for defect #47181
-        	if (!(this.prRegion.getDistributionManager() instanceof LonerDistributionManager)) {
-        	  Set<InternalDistributedMember> exm = getBuddyMembersInZone(candidate, allStores);
-              exm.remove(candidate);
-              exm.removeAll(acceptedMembers);
-              excludedMembers.addAll(exm);
-        	} else {
-        	  //log a warning if Loner
-        	  logger.warn(LocalizedMessage.create(LocalizedStrings.GemFireCache_ENFORCE_UNIQUE_HOST_NOT_APPLICABLE_FOR_LONER));
-        	}
+          InternalDistributedMember candidate = createBucketInstance(bucketId, newBucketSize,
+              excludedMembers, acceptedMembers, failedMembers, timeOut, allStores);
+          if (candidate != null) {
+            if (this.prRegion.getDistributionManager().enforceUniqueZone()) {
+              // enforceUniqueZone property has no effect for a loner. Fix for defect #47181
+              if (!(this.prRegion.getDistributionManager() instanceof LonerDistributionManager)) {
+                Set<InternalDistributedMember> exm = getBuddyMembersInZone(candidate, allStores);
+                exm.remove(candidate);
+                exm.removeAll(acceptedMembers);
+                excludedMembers.addAll(exm);
+              } else {
+                // log a warning if Loner
+                logger.warn(LocalizedMessage.create(
+                    LocalizedStrings.GemFireCache_ENFORCE_UNIQUE_HOST_NOT_APPLICABLE_FOR_LONER));
+              }
+            }
-        }
-        // Get an updated list of bucket owners, which should include
-        // buckets created concurrently with this createBucketAtomically call 
-        acceptedMembers = prRegion.getRegionAdvisor().getBucketOwners(bucketId);
+          // Get an updated list of bucket owners, which should include
+          // buckets created concurrently with this createBucketAtomically call
+          acceptedMembers = prRegion.getRegionAdvisor().getBucketOwners(bucketId);
+          if (isDebugEnabled) {
+            logger.debug("Accepted members: {}", acceptedMembers);
+          }
+
+          // [sumedh] set the primary as the candidate in the first iteration if
+          // the candidate has accepted
+          if (bucketPrimary == null && acceptedMembers.contains(candidate)) {
+            bucketPrimary = candidate;
+          }
+
+          // prune out the stores that have left
+          verifyBucketNodes(excludedMembers, partitionName);
+
+          // Note - we used to wait for the created bucket to become primary here
+          // if this is a colocated region. We no longer need to do that, because
+          // the EndBucketMessage is sent out after bucket creation completes to
+          // select the primary.
+
+          // Have we exhausted all candidates?
+          final int potentialCandidateCount = (allStores.size()
+              - (excludedMembers.size() + acceptedMembers.size() + failedMembers.size()));
+          // Determining exhausted members competes with bucket balancing; it's
+          // important to re-visit all failed members since "failed" set may
+          // contain datastores which at the moment are imbalanced, but yet could
+          // be candidates. If the failed members list is empty, its expected
+          // that the next iteration clears the (already empty) list.
+          final boolean exhaustedPotentialCandidates =
+              failedMembers.wasCleared() && potentialCandidateCount <= 0;
+          final boolean redundancySatisfied =
+              acceptedMembers.size() > this.prRegion.getRedundantCopies();
+          final boolean bucketNotCreated = acceptedMembers.size() == 0;
+
+          if (isDebugEnabled) {
+            logger.debug(
+                "potentialCandidateCount={}, exhaustedPotentialCandidates={}, redundancySatisfied={}, bucketNotCreated={}",
+                potentialCandidateCount, exhaustedPotentialCandidates, redundancySatisfied,
+                bucketNotCreated);
+          }
+
+          if (bucketNotCreated) {
+            // if we haven't managed to create the bucket on any nodes, retry.
+            continue;
+          }
+
+          if (exhaustedPotentialCandidates && !redundancySatisfied) {
+            insufficientStores(allStores, acceptedMembers, true);
+          }
+
+          // Allow the thread to potentially finish bucket creation even if redundancy was not met.
+          // Fix for bug 39283
+          if (redundancySatisfied || exhaustedPotentialCandidates) {
+            // Tell one of the members to become primary.
+            // The rest of the members will be allowed to
+            // volunteer for primary.
+            endBucketCreation(bucketId, acceptedMembers, bucketPrimary, partitionName);
+
+            final int expectedRemoteHosts = acceptedMembers.size()
+                - (acceptedMembers.contains(this.prRegion.getMyId()) ? 1 : 0);
+            boolean interrupted = Thread.interrupted();
+            try {
+              BucketMembershipObserverResults results = observer
+                  .waitForOwnersGetPrimary(expectedRemoteHosts, acceptedMembers, partitionName);
+              if (results.problematicDeparture) {
+                // BZZZT! Member left. Start over.
+                continue;
+              }
+              bucketPrimary = results.primary;
+            } catch (InterruptedException e) {
+              interrupted = true;
+              this.prRegion.getCancelCriterion().checkCancelInProgress(e);
+            } finally {
+              if (interrupted) {
+                Thread.currentThread().interrupt();
+              }
+            }
+            needToElectPrimary = false;
+            return bucketPrimary;
+          } // almost done
+        } // for
+      } catch (CancelException e) {
+        // Fix for 43544 - We don't need to elect a primary
+        // if the cache was closed. The other members will
+        // take care of it. This ensures we don't compromise
+        // redundancy.
+        needToElectPrimary = false;
+        throw e;
+      } catch (RegionDestroyedException e) {
+        // Fix for 43544 - We don't need to elect a primary
+        // if the region was destroyed. The other members will
+        // take care of it. This ensures we don't compromise
+        // redundancy.
+        needToElectPrimary = false;
+        throw e;
+      } catch (PartitionOfflineException e) {
+        throw e;
+      } catch (RuntimeException e) {
-          logger.debug("Accepted members: {}", acceptedMembers);
+          logger.debug("Unable to create new bucket {}: {}", bucketId, e.getMessage(), e);
-        // [sumedh] set the primary as the candidate in the first iteration if
-        // the candidate has accepted
-        if (bucketPrimary == null && acceptedMembers.contains(candidate)) {
-          bucketPrimary = candidate;
+        // If we're finishing an incomplete bucket creation, don't blast out
+        // another message to peers to do so.
+        // TODO - should we ignore a PartitionRegionStorageException, rather
+        // than reattempting on other nodes?
+        if (!finishIncompleteCreation) {
+          cleanUpBucket(bucketId);
-
-        // prune out the stores that have left
-        verifyBucketNodes(excludedMembers, partitionName);
-
-        //Note - we used to wait for the created bucket to become primary here
-        //if this is a colocated region. We no longer need to do that, because
-        //the EndBucketMessage is sent out after bucket creation completes to
-        //select the primary.
-        
-        // Have we exhausted all candidates?
-        final int potentialCandidateCount = (allStores.size() - (excludedMembers
-            .size() + acceptedMembers.size() + failedMembers.size()));
-        // Determining exhausted members competes with bucket balancing;  it's
-        // important to re-visit all failed members since "failed" set may
-        // contain datastores which at the moment are imbalanced, but yet could
-        // be candidates.  If the failed members list is empty, its expected
-        // that the next iteration clears the (already empty) list.
-        final boolean exhaustedPotentialCandidates = failedMembers.wasCleared() && potentialCandidateCount <= 0;
-        final boolean redundancySatisfied = acceptedMembers.size() > this.prRegion.getRedundantCopies();
-        final boolean bucketNotCreated = acceptedMembers.size() == 0;
-
-        if (isDebugEnabled) {
-          logger.debug("potentialCandidateCount={}, exhaustedPotentialCandidates={}, redundancySatisfied={}, bucketNotCreated={}",
-              potentialCandidateCount, exhaustedPotentialCandidates, redundancySatisfied, bucketNotCreated);
+        throw e;
+      } finally {
+        if (observer != null) {
+          observer.stopMonitoring();
-
-        if (bucketNotCreated) {
-          // if we haven't managed to create the bucket on any nodes, retry.
-          continue;
-        }
-
-        if (exhaustedPotentialCandidates && ! redundancySatisfied) {
-          insufficientStores(allStores, acceptedMembers, true);
-        }
-
-        // Allow the thread to potentially finish bucket creation even if redundancy was not met.
-        // Fix for bug 39283
-        if (redundancySatisfied || exhaustedPotentialCandidates) {
-          //Tell one of the members to become primary.
-          //The rest of the members will be allowed to
-          //volunteer for primary.
-          endBucketCreation(bucketId, acceptedMembers, bucketPrimary, partitionName);
-
-          final int expectedRemoteHosts = acceptedMembers.size()
-          - (acceptedMembers.contains(this.prRegion.getMyId()) ? 1: 0);
-          boolean interrupted = Thread.interrupted();
+        // Try to make sure everyone that created the bucket can volunteer for primary
+        if (needToElectPrimary) {
-            BucketMembershipObserverResults results = 
-              observer.waitForOwnersGetPrimary(expectedRemoteHosts, 
-                  acceptedMembers, partitionName);
-            if (results.problematicDeparture) {
-              // BZZZT! Member left.  Start over.
-              continue;
+            endBucketCreation(bucketId, prRegion.getRegionAdvisor().getBucketOwners(bucketId),
+                bucketPrimary, partitionName);
+          } catch (Exception e) {
+            // if region is going down, then no warning level logs
+            if (e instanceof CancelException || e instanceof CacheClosedException
+                || (prRegion.getCancelCriterion().isCancelInProgress())) {
+              logger.debug("Exception trying choose a primary after bucket creation failure", e);
+            } else {
+              logger.warn("Exception trying choose a primary after bucket creation failure", e);
-            bucketPrimary = results.primary;
-          } 
-          catch (InterruptedException e) {
-            interrupted = true;
-            this.prRegion.getCancelCriterion().checkCancelInProgress(e);
-          }
-          finally {
-            if (interrupted) {
-              Thread.currentThread().interrupt();
-            }
-          }
-          needToElectPrimary = false;
-          return bucketPrimary;
-         } // almost done
-      } // for
-    }
-    catch (CancelException e) {
-      //Fix for 43544 - We don't need to elect a primary
-      //if the cache was closed. The other members will
-      //take care of it. This ensures we don't compromise
-      //redundancy.
-      needToElectPrimary = false;
-      throw e;
-    }
-    catch (RegionDestroyedException e) {
-      //Fix for 43544 - We don't need to elect a primary
-      //if the region was destroyed. The other members will
-      //take care of it. This ensures we don't compromise
-      //redundancy.
-      needToElectPrimary = false;
-      throw e;
-    }
-    catch (PartitionOfflineException e) {
-      throw e;
-    }
-    catch (RuntimeException e) {
-      if(isDebugEnabled) {
-        logger.debug("Unable to create new bucket {}: {}", bucketId, e.getMessage(), e);
-      }
-
-      //If we're finishing an incomplete bucket creation, don't blast out
-      //another message to peers to do so.
-      //TODO - should we ignore a PartitionRegionStorageException, rather
-      //than reattempting on other nodes?
-      if(!finishIncompleteCreation) {
-        cleanUpBucket(bucketId);
-      }
-      throw e;
-    } finally {
-      if (observer != null) {
-        observer.stopMonitoring();
-      }
-      //Try to make sure everyone that created the bucket can volunteer for primary
-      if(needToElectPrimary) {
-        try {
-            endBucketCreation(bucketId, prRegion.getRegionAdvisor()
-                .getBucketOwners(bucketId), bucketPrimary, partitionName);
-        } catch (Exception e) {
-          // if region is going down, then no warning level logs
-          if (e instanceof CancelException || e instanceof CacheClosedException
-              || (prRegion.getCancelCriterion().isCancelInProgress())) {
-            logger.debug("Exception trying choose a primary after bucket creation failure", e);
-          }
-          else {
-            logger.warn("Exception trying choose a primary after bucket creation failure", e);
-    }
-   * Figure out which member should be primary for a bucket
-   * among the members that have created the bucket, and tell
-   * that member to become the primary.
+   * Figure out which member should be primary for a bucket among the members that have created the
+   * bucket, and tell that member to become the primary.
+   * 
-    if(acceptedMembers.isEmpty()) {
+    if (acceptedMembers.isEmpty()) {
-    //TODO prpersist - we need to factor out a method that just chooses
-    //the primary. But this will do the trick for the moment.
-    
+    // TODO prpersist - we need to factor out a method that just chooses
+    // the primary. But this will do the trick for the moment.
+
-      }
-      else {
-        targetPrimary = this.prRegion.getRegionAdvisor()
-            .adviseFixedPrimaryPartitionDataStore(bucketId);
+      } else {
+        targetPrimary =
+            this.prRegion.getRegionAdvisor().adviseFixedPrimaryPartitionDataStore(bucketId);
-      //  the parent's in case of colocation) so it is now passed
-      //InternalDistributedMember targetPrimary = getPreferredDataStore(
-      //    acceptedMembers, Collections.<InternalDistributedMember> emptySet());
-      targetPrimary = getPreferredDataStore(acceptedMembers, Collections
-          .<InternalDistributedMember> emptySet());
+      // the parent's in case of colocation) so it is now passed
+      // InternalDistributedMember targetPrimary = getPreferredDataStore(
+      // acceptedMembers, Collections.<InternalDistributedMember> emptySet());
+      targetPrimary =
+          getPreferredDataStore(acceptedMembers, Collections.<InternalDistributedMember>emptySet());
-    boolean isHosting = acceptedMembers.remove(prRegion
-        .getDistributionManager().getId());
-    EndBucketCreationMessage.send(acceptedMembers, targetPrimary,
-        this.prRegion, bucketId);
+    boolean isHosting = acceptedMembers.remove(prRegion.getDistributionManager().getId());
+    EndBucketCreationMessage.send(acceptedMembers, targetPrimary, this.prRegion, bucketId);
-  
+
-    List<FixedPartitionAttributesImpl> FPAs = this.prRegion
-        .getFixedPartitionAttributesImpl();
+    List<FixedPartitionAttributesImpl> FPAs = this.prRegion.getFixedPartitionAttributesImpl();
-  public static void setTestEndBucketCreationObserver(
-      EndBucketCreationObserver observer) {
+  public static void setTestEndBucketCreationObserver(EndBucketCreationObserver observer) {
-    public void afterEndBucketCreationMessageSend(PartitionedRegion pr,
-        int bucketId);
+    public void afterEndBucketCreationMessageSend(PartitionedRegion pr, int bucketId);
-  public void endBucketCreationLocally(int bucketId,
-      InternalDistributedMember newPrimary) {
-    
-    //Don't elect ourselves as primary or tell others to persist our ID if this member
-    //has been destroyed.
-    if(prRegion.getCancelCriterion().isCancelInProgress() || prRegion.isDestroyed()) {
+  public void endBucketCreationLocally(int bucketId, InternalDistributedMember newPrimary) {
+
+    // Don't elect ourselves as primary or tell others to persist our ID if this member
+    // has been destroyed.
+    if (prRegion.getCancelCriterion().isCancelInProgress() || prRegion.isDestroyed()) {
-    
+
-    BucketAdvisor bucketAdvisor = 
-      prRegion.getRegionAdvisor().getBucketAdvisor(bucketId);
-    
+    BucketAdvisor bucketAdvisor = prRegion.getRegionAdvisor().getBucketAdvisor(bucketId);
+
-    
-    //prevent multiple threads from ending bucket creation at the same time.
-    //This fixes an issue with 41336, where multiple threads were calling endBucketCreation
-    //on the persistent advisor and marking a bucket as initialized twice.
-    synchronized(proxyBucketRegion) {
-      if(persistentAdvisor != null) {
+
+    // prevent multiple threads from ending bucket creation at the same time.
+    // This fixes an issue with 41336, where multiple threads were calling endBucketCreation
+    // on the persistent advisor and marking a bucket as initialized twice.
+    synchronized (proxyBucketRegion) {
+      if (persistentAdvisor != null) {
-        if(realBucket != null) {
+        if (realBucket != null) {
-      //We've received an endBucketCreationMessage, but the primary
-      //may not have. So now we wait for the chosen member to become
-      //primary.
+      // We've received an endBucketCreationMessage, but the primary
+      // may not have. So now we wait for the chosen member to become
+      // primary.
-      
-      if(prRegion.getGemFireCache().getMyId().equals(newPrimary)) {
-        //If we're the choosen primary, volunteer for primary now
+
+      if (prRegion.getGemFireCache().getMyId().equals(newPrimary)) {
+        // If we're the choosen primary, volunteer for primary now
-        //It's possible the chosen primary has already left. In 
-        //that case, volunteer for primary now.
-        if(!bucketAdvisor.adviseInitialized().contains(newPrimary)) {
+        // It's possible the chosen primary has already left. In
+        // that case, volunteer for primary now.
+        if (!bucketAdvisor.adviseInitialized().contains(newPrimary)) {
-        
-        //If the bucket has had a primary, that means the
-        //chosen bucket was primary for a while. Go ahead and
-        //clear the primary elector field.
-        if(bucketAdvisor.getHadPrimary()) {
+
+        // If the bucket has had a primary, that means the
+        // chosen bucket was primary for a while. Go ahead and
+        // clear the primary elector field.
+        if (bucketAdvisor.getHadPrimary()) {
+        }
-    }
-    //send out a profile update to indicate the persistence is initialized, if needed.
-    if(persistentAdvisor != null) {
+    // send out a profile update to indicate the persistence is initialized, if needed.
+    if (persistentAdvisor != null) {
-    
+
-    for(PartitionedRegion child : colocatedWithList) {
-      if(child.getRegionAdvisor().isBucketLocal(bucketId)) {
+    for (PartitionedRegion child : colocatedWithList) {
+      if (child.getRegionAdvisor().isBucketLocal(bucketId)) {
-  }
+      }
-  /** 
+  /**
+   * 
-  private Set <InternalDistributedMember> getBuddyMembersInZone(
-      final InternalDistributedMember acceptedMember, 
-      final Set<InternalDistributedMember> allStores) 
-  {
-    HashSet<InternalDistributedMember> allMembersOnSystem = new HashSet<InternalDistributedMember>();
+  private Set<InternalDistributedMember> getBuddyMembersInZone(
+      final InternalDistributedMember acceptedMember,
+      final Set<InternalDistributedMember> allStores) {
+    HashSet<InternalDistributedMember> allMembersOnSystem =
+        new HashSet<InternalDistributedMember>();
-    //TODO Dan - I'm not sure this retain all is necessary, but there may have been a reason we were 
-    //passing this set in before.
+    // TODO Dan - I'm not sure this retain all is necessary, but there may have been a reason we
+    // were
+    // passing this set in before.
-   * Early check for resources. This code may be executed for every put operation if
-   * there are no datastores present, limit excessive logging.
+   * Early check for resources. This code may be executed for every put operation if there are no
+   * datastores present, limit excessive logging.
+   * 
-    assert Assert.assertHoldsLock(this,false);
+    assert Assert.assertHoldsLock(this, false);
-   * Limit the frequency for logging the {@link #INSUFFICIENT_STORES_MSG} message
-   * to once per PR after which once every {@link #INSUFFICIENT_LOGGING_THROTTLE_TIME}
-   * second
+   * Limit the frequency for logging the {@link #INSUFFICIENT_STORES_MSG} message to once per PR
+   * after which once every {@link #INSUFFICIENT_LOGGING_THROTTLE_TIME} second
+   * 
-    if (this.firstInsufficentStoresLogged.compareAndSet(false, true) ||
-        delta >= INSUFFICIENT_LOGGING_THROTTLE_TIME) {
+    if (this.firstInsufficentStoresLogged.compareAndSet(false, true)
+        || delta >= INSUFFICIENT_LOGGING_THROTTLE_TIME) {
-   * Compute timeout for waiting for a bucket.  Prefer {@link #DATASTORE_DISCOVERY_TIMEOUT_MILLISECONDS}
-   * over {@link PartitionedRegion#getRetryTimeout()}
+   * Compute timeout for waiting for a bucket. Prefer
+   * {@link #DATASTORE_DISCOVERY_TIMEOUT_MILLISECONDS} over
+   * {@link PartitionedRegion#getRetryTimeout()}
+   * 
-      if (millis > 0) {  // only positive values allowed
+      if (millis > 0) { // only positive values allowed
-   * Check to determine that there are enough datastore VMs to start the bucket
-   * creation processes.  Log a warning or throw an exception indicating
-   * when there are not enough datastore VMs.
+   * Check to determine that there are enough datastore VMs to start the bucket creation processes.
+   * Log a warning or throw an exception indicating when there are not enough datastore VMs.
+   * 
-    if (! loggedInsufficentStores) {
+    if (!loggedInsufficentStores) {
-      } 
+      }
-        final StringId logStr = 
-          LocalizedStrings.PRHARRedundancyProvider_0_IN_THE_PARTITIONED_REGION_REGION_NAME_1;
-        final Object[] logArgs = new Object[] {SUFFICIENT_STORES_MSG.toLocalizedString(), prRegion.getFullPath()};
+        final StringId logStr =
+            LocalizedStrings.PRHARRedundancyProvider_0_IN_THE_PARTITIONED_REGION_REGION_NAME_1;
+        final Object[] logArgs =
+            new Object[] {SUFFICIENT_STORES_MSG.toLocalizedString(), prRegion.getFullPath()};
-   * Clean up locally created bucket and tell other VMs to 
-   * attempt recovering redundancy
+   * Clean up locally created bucket and tell other VMs to attempt recovering redundancy
+   * 
-  private void cleanUpBucket(int buck)
-  {
+  private void cleanUpBucket(int buck) {
-    BucketBackupMessage.send(dataStores, this.prRegion,  buck);
+    BucketBackupMessage.send(dataStores, this.prRegion, buck);
-  
+
-      FixedPartitionAttributesImpl fpa = PartitionedRegionHelper
-          .getFixedPartitionAttributesForBucket(this.prRegion, bucketId);
+      FixedPartitionAttributesImpl fpa =
+          PartitionedRegionHelper.getFixedPartitionAttributesForBucket(this.prRegion, bucketId);
-  
+
-   * Creates bucket with ID bucketId on targetNode. This method
-   * will also create the bucket for all of the child colocated PRs.
+   * Creates bucket with ID bucketId on targetNode. This method will also create the bucket for all
+   * of the child colocated PRs.
-   * @param replaceOfflineData 
+   * @param replaceOfflineData
-      boolean replaceOfflineData, InternalDistributedMember moveSource,
-      boolean forceCreation) {
-    
+      boolean replaceOfflineData, InternalDistributedMember moveSource, boolean forceCreation) {
+
-        logger.debug("createBackupBucketOnMember for bucketId={} member: {}", this.prRegion.bucketStringForLogs(bucketId), targetNMember);
+      logger.debug("createBackupBucketOnMember for bucketId={} member: {}",
+          this.prRegion.bucketStringForLogs(bucketId), targetNMember);
-    if (! (targetNMember.equals(this.prRegion.getMyId()))) {
-//      final StoppableReentrantReadWriteLock.StoppableReadLock isClosingReadLock;
-      PartitionProfile pp = this.prRegion.getRegionAdvisor()
-          .getPartitionProfile(targetNMember);
+    if (!(targetNMember.equals(this.prRegion.getMyId()))) {
+      // final StoppableReentrantReadWriteLock.StoppableReadLock isClosingReadLock;
+      PartitionProfile pp = this.prRegion.getRegionAdvisor().getPartitionProfile(targetNMember);
-//        isClosingReadLock = pp.getIsClosingReadLock(
-//            this.prRegion.getCancelCriterion());        
+        // isClosingReadLock = pp.getIsClosingReadLock(
+        // this.prRegion.getCancelCriterion());
-        ManageBackupBucketMessage.NodeResponse response = ManageBackupBucketMessage
-            .send(targetNMember, this.prRegion, bucketId, isRebalance,
+        ManageBackupBucketMessage.NodeResponse response =
+            ManageBackupBucketMessage.send(targetNMember, this.prRegion, bucketId, isRebalance,
-            logger.debug("createBackupBucketOnMember: Bucket creation succeed for bucketId={} on member = {}", 
+            logger.debug(
+                "createBackupBucketOnMember: Bucket creation succeed for bucketId={} on member = {}",
-        }
-        else {
+        } else {
-            logger.debug("createBackupBucketOnMember: Bucket creation failed for bucketId={} on member = {}", 
+            logger.debug(
+                "createBackupBucketOnMember: Bucket creation failed for bucketId={} on member = {}",
-            
+
-      }
-      catch (VirtualMachineError err) {
+      } catch (VirtualMachineError err) {
-        // If this ever returns, rethrow the error.  We're poisoned
+        // If this ever returns, rethrow the error. We're poisoned
-      }
-      catch (Throwable e) {
+      } catch (Throwable e) {
-        // catch VirtualMachineError (see above).  However, there is
+        // catch VirtualMachineError (see above). However, there is
-        }
-        else if (e instanceof CancelException
-                 || (e.getCause() != null
-                     && (e.getCause() instanceof CancelException))) {
+        } else if (e instanceof CancelException
+            || (e.getCause() != null && (e.getCause() instanceof CancelException))) {
-        }
-        else {
-          logger.warn(LocalizedMessage.create(LocalizedStrings.PRHARedundancyProvider_EXCEPTION_CREATING_PARTITION_ON__0, targetNMember), e);
+        } else {
+          logger.warn(LocalizedMessage.create(
+              LocalizedStrings.PRHARedundancyProvider_EXCEPTION_CREATING_PARTITION_ON__0,
+              targetNMember), e);
-      } 
-    }
-    else {
+      }
+    } else {
-      boolean bucketManaged = prDS!=null &&
-        prDS.grabBucket(bucketId, moveSource, forceCreation,
-              replaceOfflineData, isRebalance, null, false).equals(
-              CreateBucketResult.CREATED);
-      if (! bucketManaged) {
+      boolean bucketManaged = prDS != null && prDS.grabBucket(bucketId, moveSource, forceCreation,
+          replaceOfflineData, isRebalance, null, false).equals(CreateBucketResult.CREATED);
+      if (!bucketManaged) {
-          logger.debug("createBackupBucketOnMember: Local data store refused to accommodate the data for bucketId={} prDS={}", 
-              this.prRegion.bucketStringForLogs(bucketId), prDS); 
+          logger.debug(
+              "createBackupBucketOnMember: Local data store refused to accommodate the data for bucketId={} prDS={}",
+              this.prRegion.bucketStringForLogs(bucketId), prDS);
-    Boolean v = (Boolean)forceLocalPrimaries.get();
+    Boolean v = (Boolean) forceLocalPrimaries.get();
-  
+
-   * @param forceCreation inform the targetMember it must attempt host the bucket,
-   *  appropriately ignoring it's maximums 
+   * @param forceCreation inform the targetMember it must attempt host the bucket, appropriately
+   *        ignoring it's maximums
-  public ManageBucketRsp createBucketOnMember(final int bucketId, 
-      final InternalDistributedMember targetNMember,
-      final int newBucketSize, boolean forceCreation)
-  {
+  public ManageBucketRsp createBucketOnMember(final int bucketId,
+      final InternalDistributedMember targetNMember, final int newBucketSize,
+      boolean forceCreation) {
-        logger.debug("createBucketOnMember for bucketId={} member: {}{}", this.prRegion.bucketStringForLogs(bucketId),
-            targetNMember, (forceCreation ? " forced" : ""));
+      logger.debug("createBucketOnMember for bucketId={} member: {}{}",
+          this.prRegion.bucketStringForLogs(bucketId), targetNMember,
+          (forceCreation ? " forced" : ""));
-    if (! (targetNMember.equals(this.prRegion.getMyId()))) {
-//      final StoppableReentrantReadWriteLock.StoppableReadLock isClosingReadLock;
-      PartitionProfile pp = this.prRegion.getRegionAdvisor()
-          .getPartitionProfile(targetNMember);
+    if (!(targetNMember.equals(this.prRegion.getMyId()))) {
+      // final StoppableReentrantReadWriteLock.StoppableReadLock isClosingReadLock;
+      PartitionProfile pp = this.prRegion.getRegionAdvisor().getPartitionProfile(targetNMember);
-//        isClosingReadLock = pp.getIsClosingReadLock(
-//            this.prRegion.getCancelCriterion());        
+        // isClosingReadLock = pp.getIsClosingReadLock(
+        // this.prRegion.getCancelCriterion());
-//        isClosingReadLock.lock(); // Grab the read lock, preventing any region closures
+        // isClosingReadLock.lock(); // Grab the read lock, preventing any region closures
-        NodeResponse response = ManageBucketMessage.send(targetNMember, 
-            this.prRegion, bucketId, newBucketSize, forceCreation);
+        NodeResponse response = ManageBucketMessage.send(targetNMember, this.prRegion, bucketId,
+            newBucketSize, forceCreation);
-            logger.debug("createBucketOnMember: Bucket creation succeed for bucketId={} on member = {}", 
+            logger.debug(
+                "createBucketOnMember: Bucket creation succeed for bucketId={} on member = {}",
-//          lockList.add(isClosingReadLock);
+          // lockList.add(isClosingReadLock);
-        }
-        else {
+        } else {
-            logger.debug("createBucketOnMember: Bucket creation failed for bucketId={} on member = {}", 
+            logger.debug(
+                "createBucketOnMember: Bucket creation failed for bucketId={} on member = {}",
-//          isClosingReadLock.unlock();
-          return response.rejectedDueToInitialization()
-            ? ManageBucketRsp.NO_INITIALIZING : ManageBucketRsp.NO;
+          // isClosingReadLock.unlock();
+          return response.rejectedDueToInitialization() ? ManageBucketRsp.NO_INITIALIZING
+              : ManageBucketRsp.NO;
-      } catch(PartitionOfflineException e) {
+      } catch (PartitionOfflineException e) {
-      }
-      catch (VirtualMachineError err) {
+      } catch (VirtualMachineError err) {
-        // If this ever returns, rethrow the error.  We're poisoned
+        // If this ever returns, rethrow the error. We're poisoned
-      }
-      catch (Throwable e) {
+      } catch (Throwable e) {
-        // catch VirtualMachineError (see above).  However, there is
+        // catch VirtualMachineError (see above). However, there is
-                 || (e.getCause() != null
-                     && (e.getCause() instanceof CancelException))) {
+            || (e.getCause() != null && (e.getCause() instanceof CancelException))) {
+        } else {
+          logger.warn(LocalizedMessage.create(
+              LocalizedStrings.PRHARedundancyProvider_EXCEPTION_CREATING_PARTITION_ON__0,
+              targetNMember), e);
-        else {
-          logger.warn(LocalizedMessage.create(LocalizedStrings.PRHARedundancyProvider_EXCEPTION_CREATING_PARTITION_ON__0, targetNMember), e);
-        }
-//        isClosingReadLock.unlock();
+        // isClosingReadLock.unlock();
-      } 
-    }
-    else {
+      }
+    } else {
-      boolean bucketManaged = prDS!=null &&
-        prDS.handleManageBucketRequest(bucketId, newBucketSize, 
-            this.prRegion.getMyId(), forceCreation);
-      if (! bucketManaged) {
+      boolean bucketManaged = prDS != null && prDS.handleManageBucketRequest(bucketId,
+          newBucketSize, this.prRegion.getMyId(), forceCreation);
+      if (!bucketManaged) {
-          logger.debug("createBucketOnMember: Local data store not able to accommodate the data for bucketId={}", 
+          logger.debug(
+              "createBucketOnMember: Local data store not able to accommodate the data for bucketId={}",
-  
+
-   * Select the member with which is hosting the same bucketid for the PR it is
-   * colocated with In case of primary it returns the same node whereas in case
-   * of secondary it will return the least loaded datastore which is hosting the
-   * bucketid.
+   * Select the member with which is hosting the same bucketid for the PR it is colocated with In
+   * case of primary it returns the same node whereas in case of secondary it will return the least
+   * loaded datastore which is hosting the bucketid.
-      Collection<InternalDistributedMember> alreadyUsed, int bucketId,
-      String prName) {
+      Collection<InternalDistributedMember> alreadyUsed, int bucketId, String prName) {
-    Region prRoot = PartitionedRegionHelper.getPRRoot(prRegion
-        .getCache());
-    PartitionRegionConfig config = (PartitionRegionConfig)prRoot.get(prRegion
-        .getRegionIdentifier());
+    Region prRoot = PartitionedRegionHelper.getPRRoot(prRegion.getCache());
+    PartitionRegionConfig config =
+        (PartitionRegionConfig) prRoot.get(prRegion.getRegionIdentifier());
-      throw new IllegalStateException(
-          "Cannot create buckets, as colocated regions are not "
-              + "configured to be at the same nodes.");
+      throw new IllegalStateException("Cannot create buckets, as colocated regions are not "
+          + "configured to be at the same nodes.");
-    ArrayList members = new ArrayList(bucketOwnersSet);    
-    if(members.isEmpty()){
-      return null;    
+    ArrayList members = new ArrayList(bucketOwnersSet);
+    if (members.isEmpty()) {
+      return null;
-   * Select the member with the fewest buckets, among those with the fewest
-   * randomly select one.
+   * Select the member with the fewest buckets, among those with the fewest randomly select one.
-   * Under concurrent access, the data that this method uses, may be somewhat 
-   * volatile, note that createBucketAtomically synchronizes
-   * to enhance the consistency of the data used in this method.
+   * Under concurrent access, the data that this method uses, may be somewhat volatile, note that
+   * createBucketAtomically synchronizes to enhance the consistency of the data used in this method.
-   * @param candidates  ArrayList of InternalDistributedMember, potential datastores
+   * @param candidates ArrayList of InternalDistributedMember, potential datastores
-    
+
-    
+
-    
+
-    final HashSet<InternalDistributedMember> existingHosts = new HashSet<InternalDistributedMember>();
+    final HashSet<InternalDistributedMember> existingHosts =
+        new HashSet<InternalDistributedMember>();
-    
+
-      public int compare(DataStoreBuckets d1, DataStoreBuckets d2)
-      {
+      public int compare(DataStoreBuckets d1, DataStoreBuckets d2) {
-        
+
-        
-        // Six eggs, half a dozen.  Look for least loaded.
+
+        // Six eggs, half a dozen. Look for least loaded.
-        }
-        else {
+        } else {
-        if(result == 0) {
-          //if they have the same load, choose the member with the
-          //higher localMaxMemory
+        if (result == 0) {
+          // if they have the same load, choose the member with the
+          // higher localMaxMemory
-      } 
+      }
-    
+
-    
+
-    
+
-    for (int i = 1; i < stores.size(); i ++) {
+    for (int i = 1; i < stores.size(); i++) {
-      if (!allStoresInUse && 
-          alreadyUsed.contains(aDataStore.memberId)) {
+      if (!allStoresInUse && alreadyUsed.contains(aDataStore.memberId)) {
-      
+
-    if(DISABLE_CREATE_BUCKET_RANDOMNESS) {
+    if (DISABLE_CREATE_BUCKET_RANDOMNESS) {
-    }
-    else {
+    } else {
-    DataStoreBuckets aDataStore =  bestStores.get(chosen);
+    DataStoreBuckets aDataStore = bestStores.get(chosen);
-  
+
-   * Adds a membership listener to watch for member departures,
-   * and schedules a task to recover redundancy of existing buckets 
+   * Adds a membership listener to watch for member departures, and schedules a task to recover
+   * redundancy of existing buckets
+   * 
-   * After the member name, the +/- indicates whether or not this bucket is
-   * already hosted on the given member.  This is followed by the number of
-   * hosted primaries followed by the number of hosted non-primary buckets.
+   * 
+   * After the member name, the +/- indicates whether or not this bucket is already hosted on the
+   * given member. This is followed by the number of hosted primaries followed by the number of
+   * hosted non-primary buckets.
-  private String fancyFormatBucketAllocation(String prefix, List dataStores,
-      Set existingStores) {
+  private String fancyFormatBucketAllocation(String prefix, List dataStores, Set existingStores) {
-      logStr.append(prefix);  
+      logStr.append(prefix);
-    logStr.append("Bucket Allocation for prId=" + 
-        this.prRegion.getPRId() + ":\n");
-    for (Iterator i = dataStores.iterator(); i.hasNext(); ) {
-      DataStoreBuckets dsb = (DataStoreBuckets)i.next();
+    logStr.append("Bucket Allocation for prId=" + this.prRegion.getPRId() + ":\n");
+    for (Iterator i = dataStores.iterator(); i.hasNext();) {
+      DataStoreBuckets dsb = (DataStoreBuckets) i.next();
-      }
-      else {
+      } else {
-//      for (int j = 0; j < dsb.numPrimaries; j++) {
-//        logStr.append('#');
-//      }
-//      int nonPrimary = dsb.numBuckets - dsb.numPrimaries;
-//      for (int j = 0; j < nonPrimary; j++) {
-//        logStr.append('*');
-//      }
+      // for (int j = 0; j < dsb.numPrimaries; j++) {
+      // logStr.append('#');
+      // }
+      // int nonPrimary = dsb.numBuckets - dsb.numPrimaries;
+      // for (int j = 0; j < nonPrimary; j++) {
+      // logStr.append('*');
+      // }
-  public static class DataStoreBuckets  {
+  public static class DataStoreBuckets {
-    
-    public DataStoreBuckets(InternalDistributedMember mem, int buckets, 
-        int primaryBuckets, int localMaxMemory) {
+
+    public DataStoreBuckets(InternalDistributedMember mem, int buckets, int primaryBuckets,
+        int localMaxMemory) {
-    
+
-    public boolean equals(Object obj)
-    {
+    public boolean equals(Object obj) {
-      return this.numBuckets == other.numBuckets &&
-        this.memberId.equals(other.memberId);
+      return this.numBuckets == other.numBuckets && this.memberId.equals(other.memberId);
-    public int hashCode()
-    {
-      return this.memberId.hashCode(); 
+    public int hashCode() {
+      return this.memberId.hashCode();
-    public String toString()
-    {
-      return "DataStoreBuckets memberId=" + this.memberId
-        + "; numBuckets=" + this.numBuckets + "; numPrimaries=" + this.numPrimaries;
+    public String toString() {
+      return "DataStoreBuckets memberId=" + this.memberId + "; numBuckets=" + this.numBuckets
+          + "; numPrimaries=" + this.numPrimaries;
-  
+
-   * DistributedSystem or are no longer part of the PartitionedRegion 
-   * (close/localDestroy has been performed.) .
+   * DistributedSystem or are no longer part of the PartitionedRegion (close/localDestroy has been
+   * performed.) .
- 
+
-    
-//  boolean debugAnyRemoved = false;
+
+    // boolean debugAnyRemoved = false;
-      if ( ! availableMembers.contains(node)) {
+      if (!availableMembers.contains(node)) {
-//          debugAnyRemoved = true;
+          // debugAnyRemoved = true;
-        Assert.assertTrue(!members.contains(node), 
-            "return value does not contain " + node);
+        Assert.assertTrue(!members.contains(node), "return value does not contain " + node);
-  
+
-   * Schedule a task to perform redundancy recovery for a new node or for
-   * the node departed.
+   * Schedule a task to perform redundancy recovery for a new node or for the node departed.
-    
-    final boolean isStartup = failedMemId==null?true:false;
+
+    final boolean isStartup = failedMemId == null ? true : false;
-    final boolean movePrimaries; 
+    final boolean movePrimaries;
-      movePrimaries = !Boolean.getBoolean(DistributionConfig.GEMFIRE_PREFIX + "DISABLE_MOVE_PRIMARIES_ON_STARTUP");
+      movePrimaries = !Boolean
+          .getBoolean(DistributionConfig.GEMFIRE_PREFIX + "DISABLE_MOVE_PRIMARIES_ON_STARTUP");
-    
-    if(!requiresRedundancyRecovery) {
+
+    if (!requiresRedundancyRecovery) {
-      public void run2()
-      {
+      public void run2() {
-          final boolean isFixedPartitionedRegion 
-          = PRHARedundancyProvider.this.prRegion.isFixedPartitionedRegion();
-          
-          
-          //Fix for 43582 - always replace offline data for fixed partitioned
-          //regions - this guarantees we create the buckets we are supposed to
-          //create on this node.
+          final boolean isFixedPartitionedRegion =
+              PRHARedundancyProvider.this.prRegion.isFixedPartitionedRegion();
+
+
+          // Fix for 43582 - always replace offline data for fixed partitioned
+          // regions - this guarantees we create the buckets we are supposed to
+          // create on this node.
-          
+
-          if(isFixedPartitionedRegion) {
+          if (isFixedPartitionedRegion) {
-            director=  new CompositeDirector(true, true, false, 
-                    movePrimaries);
+            director = new CompositeDirector(true, true, false, movePrimaries);
-              PRHARedundancyProvider.this.prRegion, false, director, replaceOfflineData,false);
-          
-          long start = PRHARedundancyProvider.this.prRegion.getPrStats()
-              .startRecovery();
-          
+              PRHARedundancyProvider.this.prRegion, false, director, replaceOfflineData, false);
+
+          long start = PRHARedundancyProvider.this.prRegion.getPrStats().startRecovery();
+
-          
+
-        } catch(CancelException e) {
+        } catch (CancelException e) {
-        } catch(RegionDestroyedException e) {
+        } catch (RegionDestroyedException e) {
-        }  catch (Exception e) {
-          logger.error(LocalizedMessage.create(LocalizedStrings.PRHARedundancyProvider_UNEXPECTED_EXCEPTION_DURING_BUCKET_RECOVERY), e);
+        } catch (Exception e) {
+          logger.error(
+              LocalizedMessage.create(
+                  LocalizedStrings.PRHARedundancyProvider_UNEXPECTED_EXCEPTION_DURING_BUCKET_RECOVERY),
+              e);
-    
+
-          if(logger.isDebugEnabled()) {
+          if (logger.isDebugEnabled()) {
-              logger.debug("prRegion scheduling redundancy recovery after departure/crash/error in {} in {} ms", failedMemId, delay);
+              logger.debug(
+                  "prRegion scheduling redundancy recovery after departure/crash/error in {} in {} ms",
+                  failedMemId, delay);
-        } catch(RejectedExecutionException e) {
-          //ok, the executor is shutting down.
+        } catch (RejectedExecutionException e) {
+          // ok, the executor is shutting down.
-  
+
-    
-    for (int i =0; i < numBuckets; i++) {
+
+    for (int i = 0; i < numBuckets; i++) {
-      if (redundancy < targetRedundancy && redundancy != -1 
-          || redundancy > targetRedundancy) {
+      if (redundancy < targetRedundancy && redundancy != -1 || redundancy > targetRedundancy) {
-    
+
-  
+
-    
+
-     * To handle a case where ParallelGatewaySender is persistent but userPR is not
-     * First recover the GatewaySender buckets for ParallelGatewaySender
-     * irrespective of whether colocation is complete or not.
+     * To handle a case where ParallelGatewaySender is persistent but userPR is not First recover
+     * the GatewaySender buckets for ParallelGatewaySender irrespective of whether colocation is
+     * complete or not.
-    //Check if the leader region or some child shadow PR region is persistent
-    //and return the first persistent region found
+    // Check if the leader region or some child shadow PR region is persistent
+    // and return the first persistent region found
-    
-    //If there is no persistent region in the colocation chain, no need to recover.
-    if(persistentLeader == null) {
+
+    // If there is no persistent region in the colocation chain, no need to recover.
+    if (persistentLeader == null) {
-    
-    if (!ColocationHelper.checkMembersColocation(leaderRegion, 
+
+    if (!ColocationHelper.checkMembersColocation(leaderRegion,
-      if(logger.isDebugEnabled()) {
-        logger.debug("Skipping persistent recovery of {} because colocation is not complete for {}", prRegion, leaderRegion);
+      if (logger.isDebugEnabled()) {
+        logger.debug("Skipping persistent recovery of {} because colocation is not complete for {}",
+            prRegion, leaderRegion);
-//TODO prpersist - It would make sense to hold the lock here in some cases
-//to prevent confusing members that are trying to rebalance. BUT, these persistent regions
-//need to wait for other members to recover during initialization.
-//    RecoveryLock lock = leaderRegion.getRecoveryLock();
-//    lock.lock();
-//    try {
-      final ProxyBucketRegion[] proxyBucketArray = persistentLeader.getRegionAdvisor().getProxyBucketArray();
-      
-      for(ProxyBucketRegion proxyBucket : proxyBucketArray) {
-        proxyBucket.initializePersistenceAdvisor();
-      }
-      Set<InternalDistributedMember> peers = this.prRegion.getRegionAdvisor().adviseGeneric();
-      
-    //TODO prpersist - Ok, this is super lame. We need to make sure here that we don't run into this race condition
-      //1) We get a membership view from member A
-      //2) Member B removes itself, and distributes to us and A. We don't remove B
-      //3) We apply the membership view from A, which includes B.
-      //That will add B back into the set.
-      //This state flush will make sure that any membership changes
-      //That are in progress on the peers are finished.
-      MembershipFlushRequest.send(peers, this.prRegion.getDistributionManager(), this.prRegion.getFullPath());
-      
-    
-      ArrayList<ProxyBucketRegion> bucketsNotHostedLocally 
-        = new ArrayList<ProxyBucketRegion>(proxyBucketArray.length);
-      ArrayList<ProxyBucketRegion> bucketsHostedLocally 
-        = new ArrayList<ProxyBucketRegion>(proxyBucketArray.length);
+    // TODO prpersist - It would make sense to hold the lock here in some cases
+    // to prevent confusing members that are trying to rebalance. BUT, these persistent regions
+    // need to wait for other members to recover during initialization.
+    // RecoveryLock lock = leaderRegion.getRecoveryLock();
+    // lock.lock();
+    // try {
+    final ProxyBucketRegion[] proxyBucketArray =
+        persistentLeader.getRegionAdvisor().getProxyBucketArray();
+
+    for (ProxyBucketRegion proxyBucket : proxyBucketArray) {
+      proxyBucket.initializePersistenceAdvisor();
+    }
+    Set<InternalDistributedMember> peers = this.prRegion.getRegionAdvisor().adviseGeneric();
+
+    // TODO prpersist - Ok, this is super lame. We need to make sure here that we don't run into
+    // this race condition
+    // 1) We get a membership view from member A
+    // 2) Member B removes itself, and distributes to us and A. We don't remove B
+    // 3) We apply the membership view from A, which includes B.
+    // That will add B back into the set.
+    // This state flush will make sure that any membership changes
+    // That are in progress on the peers are finished.
+    MembershipFlushRequest.send(peers, this.prRegion.getDistributionManager(),
+        this.prRegion.getFullPath());
+
+
+    ArrayList<ProxyBucketRegion> bucketsNotHostedLocally =
+        new ArrayList<ProxyBucketRegion>(proxyBucketArray.length);
+    ArrayList<ProxyBucketRegion> bucketsHostedLocally =
+        new ArrayList<ProxyBucketRegion>(proxyBucketArray.length);
-      allBucketsRecoveredFromDisk = new CountDownLatch(proxyBucketArray.length);
-      try {
-        if(proxyBucketArray.length > 0) {
-          this.redundancyLogger = new RedundancyLogger(this);
-          Thread loggingThread = new Thread(this.redundancyLogger,"RedundancyLogger for region " + this.prRegion.getName());
-          loggingThread.start();
-        }    
-      } catch(RuntimeException e) {
-        allBucketsRecoveredFromDisk = null;
-        throw e;
+    allBucketsRecoveredFromDisk = new CountDownLatch(proxyBucketArray.length);
+    try {
+      if (proxyBucketArray.length > 0) {
+        this.redundancyLogger = new RedundancyLogger(this);
+        Thread loggingThread = new Thread(this.redundancyLogger,
+            "RedundancyLogger for region " + this.prRegion.getName());
+        loggingThread.start();
-    
-      /*
-       * Spawn a separate thread for bucket that we previously hosted
-       * to recover that bucket.
-       * 
-       * That thread will get to the point at which it has determined that
-       * at least one member (possibly the local member) has fully initialized 
-       * the bucket, at which it will count down the someMemberRecoveredLatch
-       * latch on the bucket.
-       * 
-       * Once at least one copy of each bucket has been created in the distributed
-       * system, the initPRInternals method will exit. Some of the threads
-       * spawned here will still be doing GII's in the background. This
-       * allows the system to become usable as fast as possible.
-       * 
-       * If we used a bounded thread pool here, we end up waiting for
-       * some buckets to finish there GII before returning from initPRInternals.
-       * In the future maybe we could let the create bucket return and pass
-       * the GII task to a separate thread pool.
-       * 
-       */
-      for(final ProxyBucketRegion proxyBucket : proxyBucketArray) {
-        if(proxyBucket.getPersistenceAdvisor().wasHosting()) {
-          final RecoveryRunnable recoveryRunnable = new RecoveryRunnable(this) {
-            
-            
-            @Override
-            public void run() {
-              //Fix for 44551 - make sure that we always count down
-              //this latch, even if the region was destroyed.
-              try {
-                super.run();
-              } finally {
-                allBucketsRecoveredFromDisk.countDown();
-              }
-            }
+    } catch (RuntimeException e) {
+      allBucketsRecoveredFromDisk = null;
+      throw e;
+    }
-            @Override
-            public void run2() {
-                proxyBucket.recoverFromDiskRecursively();
+    /*
+     * Spawn a separate thread for bucket that we previously hosted to recover that bucket.
+     * 
+     * That thread will get to the point at which it has determined that at least one member
+     * (possibly the local member) has fully initialized the bucket, at which it will count down the
+     * someMemberRecoveredLatch latch on the bucket.
+     * 
+     * Once at least one copy of each bucket has been created in the distributed system, the
+     * initPRInternals method will exit. Some of the threads spawned here will still be doing GII's
+     * in the background. This allows the system to become usable as fast as possible.
+     * 
+     * If we used a bounded thread pool here, we end up waiting for some buckets to finish there GII
+     * before returning from initPRInternals. In the future maybe we could let the create bucket
+     * return and pass the GII task to a separate thread pool.
+     * 
+     */
+    for (final ProxyBucketRegion proxyBucket : proxyBucketArray) {
+      if (proxyBucket.getPersistenceAdvisor().wasHosting()) {
+        final RecoveryRunnable recoveryRunnable = new RecoveryRunnable(this) {
+
+
+          @Override
+          public void run() {
+            // Fix for 44551 - make sure that we always count down
+            // this latch, even if the region was destroyed.
+            try {
+              super.run();
+            } finally {
+              allBucketsRecoveredFromDisk.countDown();
-          };
-          Thread recoveryThread = new Thread(recoveryRunnable, "Recovery thread for bucket " + proxyBucket.getName());
-          recoveryThread.start();
-          bucketsHostedLocally.add(proxyBucket);
-        } else {
-          bucketsNotHostedLocally.add(proxyBucket);
-        }
+          }
+
+          @Override
+          public void run2() {
+            proxyBucket.recoverFromDiskRecursively();
+          }
+        };
+        Thread recoveryThread =
+            new Thread(recoveryRunnable, "Recovery thread for bucket " + proxyBucket.getName());
+        recoveryThread.start();
+        bucketsHostedLocally.add(proxyBucket);
+      } else {
+        bucketsNotHostedLocally.add(proxyBucket);
-      
-      try {
-        //Partial fix for 44045, try to recover the local
-        //buckets before the proxy buckets. This will allow us
-        //to detect any ConflictingDataException before the proxy
-        //buckets update their membership view.
-        for(final ProxyBucketRegion proxyBucket : bucketsHostedLocally) {
-          proxyBucket.waitForPrimaryPersistentRecovery();
-        }
-        for(final ProxyBucketRegion proxyBucket : bucketsNotHostedLocally) {
-          proxyBucket.recoverFromDiskRecursively();
-        }
-      } finally {
-        for(final ProxyBucketRegion proxyBucket : bucketsNotHostedLocally) {
-          allBucketsRecoveredFromDisk.countDown();
-        }
+    }
+
+    try {
+      // Partial fix for 44045, try to recover the local
+      // buckets before the proxy buckets. This will allow us
+      // to detect any ConflictingDataException before the proxy
+      // buckets update their membership view.
+      for (final ProxyBucketRegion proxyBucket : bucketsHostedLocally) {
+        proxyBucket.waitForPrimaryPersistentRecovery();
-      
-      return true;      
-//    } finally {
-//      lock.unlock();
-//    }
+      for (final ProxyBucketRegion proxyBucket : bucketsNotHostedLocally) {
+        proxyBucket.recoverFromDiskRecursively();
+      }
+    } finally {
+      for (final ProxyBucketRegion proxyBucket : bucketsNotHostedLocally) {
+        allBucketsRecoveredFromDisk.countDown();
+      }
+    }
+
+    return true;
+    // } finally {
+    // lock.unlock();
+    // }
-   * Check to see if any colocated region of the current region is persistent.
-   * It's not enough to check just the leader region, because a child region might
-   * be a persistent parallel WAN queue, which is allowed.
+   * Check to see if any colocated region of the current region is persistent. It's not enough to
+   * check just the leader region, because a child region might be a persistent parallel WAN queue,
+   * which is allowed.
-   * @return the most senior region in the colocation chain (closest to the leader)
-   * that is persistent.
+   * @return the most senior region in the colocation chain (closest to the leader) that is
+   *         persistent.
-    
+
-  
-  private PartitionedRegion findPersistentRegionRecursively(
-      PartitionedRegion pr) {
-    if(pr.getDataPolicy().withPersistence()) {
+
+  private PartitionedRegion findPersistentRegionRecursively(PartitionedRegion pr) {
+    if (pr.getDataPolicy().withPersistence()) {
-    for(PartitionedRegion child : ColocationHelper.getColocatedChildRegions(pr)) {
+    for (PartitionedRegion child : ColocationHelper.getColocatedChildRegions(pr)) {
-      if(leader != null) {
+      if (leader != null) {
-        && ColocationHelper
-            .isColocationComplete(this.prRegion)) {
+        && ColocationHelper.isColocationComplete(this.prRegion)) {
-      final InternalResourceManager resourceManager = this.prRegion
-          .getGemFireCache().getResourceManager();
+      final InternalResourceManager resourceManager =
+          this.prRegion.getGemFireCache().getResourceManager();
-  
+
-        recoveryFuture.cancel(false/*mayInterruptIfRunning*/);
+        recoveryFuture.cancel(false/* mayInterruptIfRunning */);
-  public InternalPRInfo buildPartitionedRegionInfo(
-      final boolean internal, 
+  public InternalPRInfo buildPartitionedRegionInfo(final boolean internal,
-    
+
-    
+
-    
+
-    
+
-    
+
-    
-    Set<InternalDistributedMember> datastores = 
-      pr.getRegionAdvisor().adviseDataStore();
-    
-    //int size = datastores.size() + (ds == null ? 0 : 1);
-    
-    Set<InternalPartitionDetails> memberDetails = 
-        new TreeSet<InternalPartitionDetails>();
-    
+
+    Set<InternalDistributedMember> datastores = pr.getRegionAdvisor().adviseDataStore();
+
+    // int size = datastores.size() + (ds == null ? 0 : 1);
+
+    Set<InternalPartitionDetails> memberDetails = new TreeSet<InternalPartitionDetails>();
+
-      FetchPartitionDetailsResponse response = 
-          FetchPartitionDetailsMessage.send(datastores, pr, internal, fetchOfflineMembers, loadProbe);
+      FetchPartitionDetailsResponse response = FetchPartitionDetailsMessage.send(datastores, pr,
+          internal, fetchOfflineMembers, loadProbe);
-      if(fetchOfflineMembers) {
+      if (fetchOfflineMembers) {
-    
+
-    
-    
-    
-    InternalPRInfo details = new PartitionRegionInfoImpl(
-        pr.getFullPath(),
-        configuredBucketCount,
-        createdBucketCount,
-        lowRedundancyBucketCount,
-        configuredRedundantCopies,
-        actualRedundantCopies,
-        memberDetails,
-        colocatedWithPath,
-        offlineMembers);
-    
+
+
+
+    InternalPRInfo details = new PartitionRegionInfoImpl(pr.getFullPath(), configuredBucketCount,
+        createdBucketCount, lowRedundancyBucketCount, configuredRedundantCopies,
+        actualRedundantCopies, memberDetails, colocatedWithPath, offlineMembers);
+
-   * Retrieve the set of members which are currently offline
-   * for all buckets.
+   * Retrieve the set of members which are currently offline for all buckets.
-    Set<PersistentMemberID>[] offlineMembers = new Set[proxyBuckets.length]; 
-    for(int i =0; i < proxyBuckets.length; i++) {
+    Set<PersistentMemberID>[] offlineMembers = new Set[proxyBuckets.length];
+    for (int i = 0; i < proxyBuckets.length; i++) {
-      if(this.prRegion.getDataPolicy().withPersistence()) {
-        Set<PersistentMemberID> persistedMembers = proxy.getPersistenceAdvisor().getMissingMembers();
-        if(persistedMembers == null) {
+      if (this.prRegion.getDataPolicy().withPersistence()) {
+        Set<PersistentMemberID> persistedMembers =
+            proxy.getPersistenceAdvisor().getMissingMembers();
+        if (persistedMembers == null) {
-  public InternalPartitionDetails buildPartitionMemberDetails(
-      final boolean internal, 
+  public InternalPartitionDetails buildPartitionMemberDetails(final boolean internal,
-    
+
-    
+
-    
+
-    InternalDistributedMember localMember = (InternalDistributedMember) 
-        pr.getMyId();
-    
+    InternalDistributedMember localMember = (InternalDistributedMember) pr.getMyId();
+
-    Map<Integer,Integer> bucketSizeMap = ds.getSizeLocally();
-    for (Iterator<Map.Entry<Integer,Integer>> iter = 
-      bucketSizeMap.entrySet().iterator(); iter.hasNext();) {
-      Map.Entry<Integer,Integer> me = iter.next();
+    Map<Integer, Integer> bucketSizeMap = ds.getSizeLocally();
+    for (Iterator<Map.Entry<Integer, Integer>> iter = bucketSizeMap.entrySet().iterator(); iter
+        .hasNext();) {
+      Map.Entry<Integer, Integer> me = iter.next();
-    
+
-     
-      
+
+
-      localDetails = new PartitionMemberInfoImpl(
-          localMember,
-          pr.getLocalMaxMemory() * (1024L * 1024L),
-          size,
-          ds.getBucketsManaged(),
-          ds.getNumberOfPrimaryBucketsManaged(),
-          prLoad,
-          bucketSizes);
-    }
-    else {
-      localDetails = new PartitionMemberInfoImpl(
-          localMember,
-          pr.getLocalMaxMemory() * (1024L * 1024L),
-          size,
-          ds.getBucketsManaged(),
-          ds.getNumberOfPrimaryBucketsManaged());
+      localDetails =
+          new PartitionMemberInfoImpl(localMember, pr.getLocalMaxMemory() * (1024L * 1024L), size,
+              ds.getBucketsManaged(), ds.getNumberOfPrimaryBucketsManaged(), prLoad, bucketSizes);
+    } else {
+      localDetails =
+          new PartitionMemberInfoImpl(localMember, pr.getLocalMaxMemory() * (1024L * 1024L), size,
+              ds.getBucketsManaged(), ds.getNumberOfPrimaryBucketsManaged());
-  
+
-   * Wait for all persistent buckets to be recovered from disk,
-   * or for the region to be closed, whichever happens first.
+   * Wait for all persistent buckets to be recovered from disk, or for the region to be closed,
+   * whichever happens first.
-    if(recoveryLatch != null) {
-      boolean interrupted =  false;
+    if (recoveryLatch != null) {
+      boolean interrupted = false;
-              PartitionedRegionHelper.DEFAULT_WAIT_PER_RETRY_ITERATION,
-              TimeUnit.MILLISECONDS);
+              PartitionedRegionHelper.DEFAULT_WAIT_PER_RETRY_ITERATION, TimeUnit.MILLISECONDS);
-        } catch(InterruptedException e) {
+        } catch (InterruptedException e) {
-      if(interrupted) {
+      if (interrupted) {
-    
-    List<PartitionedRegion> colocatedRegions = ColocationHelper.getColocatedChildRegions(this.prRegion);
-    for(PartitionedRegion child : colocatedRegions) {
+
+    List<PartitionedRegion> colocatedRegions =
+        ColocationHelper.getColocatedChildRegions(this.prRegion);
+    for (PartitionedRegion child : colocatedRegions) {
-  
+
-   * Wait for all persistent buckets to be recovered from disk,
-   * regardless of whether the region is currently being closed.
+   * Wait for all persistent buckets to be recovered from disk, regardless of whether the region is
+   * currently being closed.
-    if(recoveryLatch != null) {
-      boolean interrupted =  false;
+    if (recoveryLatch != null) {
+      boolean interrupted = false;
-        } catch(InterruptedException e) {
+        } catch (InterruptedException e) {
-      if(interrupted) {
+      if (interrupted) {
-  
+
-    if(!ColocationHelper.checkMembersColocation(this.prRegion, this.prRegion.getMyId())) {
+    if (!ColocationHelper.checkMembersColocation(this.prRegion, this.prRegion.getMyId())) {
-    
-    if(allBucketsRecoveredFromDisk != null
-        && allBucketsRecoveredFromDisk.getCount() > 0) {
+
+    if (allBucketsRecoveredFromDisk != null && allBucketsRecoveredFromDisk.getCount() > 0) {
-    
-    Map<String, PartitionedRegion> colocatedRegions = ColocationHelper.getAllColocationRegions(this.prRegion);
-    
-    for(PartitionedRegion region : colocatedRegions.values()) {
+
+    Map<String, PartitionedRegion> colocatedRegions =
+        ColocationHelper.getAllColocationRegions(this.prRegion);
+
+    for (PartitionedRegion region : colocatedRegions.values()) {
-      if(redundancyProvider.allBucketsRecoveredFromDisk != null
+      if (redundancyProvider.allBucketsRecoveredFromDisk != null
-    
+
-  private static class ManageBucketRsp  {
+  private static class ManageBucketRsp {
-    final static ManageBucketRsp NO_INITIALIZING = 
-        new ManageBucketRsp("NO_INITIALIZING");
+    final static ManageBucketRsp NO_INITIALIZING = new ManageBucketRsp("NO_INITIALIZING");
-    
+
-      return this == NO || this == NO_INITIALIZING || this==CLOSED;
+      return this == NO || this == NO_INITIALIZING || this == CLOSED;
-    
+
-    
+
-    
+
-    
+
-    static ManageBucketRsp valueOf( boolean managed ) {
-      return managed? YES : NO;
+    static ManageBucketRsp valueOf(boolean managed) {
+      return managed ? YES : NO;
-  
+
+
-    
+
-      return "pDepart:"+problematicDeparture+" primary:"+primary;
+      return "pDepart:" + problematicDeparture + " primary:" + primary;
-  
+
-   
+
-      int profilesPresent =  
-        this.bucketToMonitor.getBucketAdvisor()
-        .addMembershipListenerAndAdviseGeneric(this).size();
+      int profilesPresent = this.bucketToMonitor.getBucketAdvisor()
+          .addMembershipListenerAndAdviseGeneric(this).size();
-    
+
-      this.bucketToMonitor.getBucketAdvisor().removeMembershipListener(this);      
+      this.bucketToMonitor.getBucketAdvisor().removeMembershipListener(this);
-      synchronized(this) {
+      synchronized (this) {
-    public void memberSuspect(InternalDistributedMember id,
-        InternalDistributedMember whoSuspected, String reason) {
-    }
-    
+    public void memberSuspect(InternalDistributedMember id, InternalDistributedMember whoSuspected,
+        String reason) {}
+
-      synchronized(this) {
+      synchronized (this) {
-     * Wait for expected number of owners to be recognized.  When the expected
-     * number have been seen, then fetch the primary and report it.  If while
-     * waiting for the owners to be recognized there is a departure which compromises
-     * redundancy
+     * Wait for expected number of owners to be recognized. When the expected number have been seen,
+     * then fetch the primary and report it. If while waiting for the owners to be recognized there
+     * is a departure which compromises redundancy
+     * 
-     * @return if no problematic departures are detected, the primary 
+     * @return if no problematic departures are detected, the primary
-    public BucketMembershipObserverResults waitForOwnersGetPrimary(
-        final int expectedCount, final Collection<InternalDistributedMember> expectedOwners, String partitionName)
-    throws InterruptedException  {
+    public BucketMembershipObserverResults waitForOwnersGetPrimary(final int expectedCount,
+        final Collection<InternalDistributedMember> expectedOwners, String partitionName)
+        throws InterruptedException {
-      synchronized(this) {
+      synchronized (this) {
-            if ( expectedOwners.isEmpty() ) {
+            if (expectedOwners.isEmpty()) {
-            //  reselect = true; // need to pick new victims
+            // reselect = true; // need to pick new victims
-            departures.set(false);    
-            if(problematicDeparture) {
+            departures.set(false);
+            if (problematicDeparture) {
-            logger.debug("Waiting for bucket {} to finish being created", prRegion.bucketStringForLogs(this.bucketToMonitor.getId()));
+            logger.debug("Waiting for bucket {} to finish being created",
+                prRegion.bucketStringForLogs(this.bucketToMonitor.getId()));
-          if (oldArrivals == arrivals.get() && 
-              oldDepartures == departures.get()) {
+          if (oldArrivals == arrivals.get() && oldDepartures == departures.get()) {
-                new Object[] {Integer.valueOf(creationWaitMillis), prRegion.getFullPath(), expectedOwners}));
+                new Object[] {Integer.valueOf(creationWaitMillis), prRegion.getFullPath(),
+                    expectedOwners}));
-      } 
+      }
-      if(primmy==null) {
+      if (primmy == null) {
-         * Handle a race where nobody has the bucket. We can't return a null member here because we haven't created the bucket, need to let
-         * the higher level code loop. 
+         * Handle a race where nobody has the bucket. We can't return a null member here because we
+         * haven't created the bucket, need to let the higher level code loop.
-        return new BucketMembershipObserverResults(false, 
-            primmy);
+        return new BucketMembershipObserverResults(false, primmy);
-        List<InternalDistributedMember> remaining) {
-    }
+        List<InternalDistributedMember> remaining) {}
-  
+
-   * This class extends MembershipListener to perform cleanup when a node leaves
-   * DistributedSystem.
+   * This class extends MembershipListener to perform cleanup when a node leaves DistributedSystem.
-    public void memberDeparted(final InternalDistributedMember id,
-        final boolean crashed)
-    {
+    public void memberDeparted(final InternalDistributedMember id, final boolean crashed) {
-        if(logger.isDebugEnabled()) {
-          logger.debug("MembershipListener invoked on DistributedMember = {} for failed memberId = {}", dmem, id);
+        if (logger.isDebugEnabled()) {
+          logger.debug(
+              "MembershipListener invoked on DistributedMember = {} for failed memberId = {}", dmem,
+              id);
-        if (! prRegion.isCacheClosing() && !prRegion.isDestroyed() &&
-            ! dmem.equals(id)) {
+        if (!prRegion.isCacheClosing() && !prRegion.isDestroyed() && !dmem.equals(id)) {
-          //Only schedule redundancy recovery if this not a fixed PR.
+          // Only schedule redundancy recovery if this not a fixed PR.
-                //After the metadata has been cleaned, recover redundancy.
+                // After the metadata has been cleaned, recover redundancy.
-          //Schedule clean up the metadata for the failed member.
+          // Schedule clean up the metadata for the failed member.
-      } catch(CancelException e) {
-        //ignore
+      } catch (CancelException e) {
+        // ignore
-    
-    public void memberSuspect(InternalDistributedMember id,
-        InternalDistributedMember whoSuspected, String reason) {
-    }
-    
-    public void memberJoined(InternalDistributedMember id)
-    {
+
+    public void memberSuspect(InternalDistributedMember id, InternalDistributedMember whoSuspected,
+        String reason) {}
+
+    public void memberJoined(InternalDistributedMember id) {
-    public void quorumLost(Set<InternalDistributedMember> failures, List<InternalDistributedMember> remaining) {
-    }
+    public void quorumLost(Set<InternalDistributedMember> failures,
+        List<InternalDistributedMember> remaining) {}
-  
+
-   * This class extends MembershipListener to start redundancy recovery
-   * when a persistent member is revoked
+   * This class extends MembershipListener to start redundancy recovery when a persistent member is
+   * revoked
-    
-    //TODO prpersist It seems like this might trigger recovery too often. For example, a rebalance
-    //can end up removing a bucket, which would trigger recovery here. We really need to only
-    //trigger this thing when a PR region is destroyed. And isn't that code already in there?
+
+    // TODO prpersist It seems like this might trigger recovery too often. For example, a rebalance
+    // can end up removing a bucket, which would trigger recovery here. We really need to only
+    // trigger this thing when a PR region is destroyed. And isn't that code already in there?
-      if(!revoked) {
+      if (!revoked) {
-      
+
-      if(logger.isDebugEnabled()) {
-        logger.debug("Persistent Membership Listener invoked on DistributedMember = {} for removed memberId = {}", dmem, persistentID);
+      if (logger.isDebugEnabled()) {
+        logger.debug(
+            "Persistent Membership Listener invoked on DistributedMember = {} for removed memberId = {}",
+            dmem, persistentID);
-      
-      if (! prRegion.isCacheClosing() && !prRegion.isDestroyed() && !prRegion.isFixedPartitionedRegion()) {
+
+      if (!prRegion.isCacheClosing() && !prRegion.isDestroyed()
+          && !prRegion.isFixedPartitionedRegion()) {
