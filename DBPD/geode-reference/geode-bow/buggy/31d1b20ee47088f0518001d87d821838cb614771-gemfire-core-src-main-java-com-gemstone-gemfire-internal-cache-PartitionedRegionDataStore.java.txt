Initial import of geode-1.0.0.0-SNAPSHOT-2.
All the new sub-project directories (like jvsd) were not imported.
A diff was done to confirm that this commit is exactly the same as
the open directory the snapshot was made from.

+import com.gemstone.gemfire.cache.hdfs.HDFSIOException;
+                if (getPartitionedRegion().isShadowPR()) {
+                  getPartitionedRegion().getColocatedWithRegion()
+                  .getRegionAdvisor()
+                  .getBucketAdvisor(possiblyFreeBucketId)
+                  .setShadowBucketDestroyed(false);
+                }
+            for (GatewaySenderEventImpl event : tempQueue) {
+              event.release();
+            }
+    factory.setOffHeap(this.partitionedRegion.getOffHeap());
-    return putLocally(bucketId, event, ifNew, ifOld, expectedOldValue,
-        requireOldValue, lastModified, br);
+    return putLocally(br, event, ifNew, ifOld, expectedOldValue,
+        requireOldValue, lastModified);
-  public boolean putLocally(final Integer bucketId,
+  public boolean putLocally(final BucketRegion bucketRegion,
-                            final long lastModified,
-                            final BucketRegion bucketRegion)
+                            final long lastModified)
-			  clientEvent, returnTombstones, false);
+			  clientEvent, returnTombstones, false, false);
-      boolean returnTombstones, boolean opScopeIsLocal) throws PrimaryBucketException,
+      boolean returnTombstones, boolean opScopeIsLocal, boolean allowReadFromHDFS) throws PrimaryBucketException,
-      logger.debug("getLocally:  key {}) bucketId={}{}{} region {} returnTombstones {}", key,
-          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, bucketRegion.getName(), returnTombstones);
+      logger.debug("getLocally:  key {}) bucketId={}{}{} region {} returnTombstones {} allowReadFromHDFS {}", key,
+          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, bucketRegion.getName(), returnTombstones, allowReadFromHDFS);
-      ret = bucketRegion.get(key, aCallbackArgument, true, disableCopyOnRead , preferCD, requestingClient, clientEvent, returnTombstones, opScopeIsLocal);
+      ret = bucketRegion.get(key, aCallbackArgument, true, disableCopyOnRead , preferCD, requestingClient, clientEvent, returnTombstones, opScopeIsLocal, allowReadFromHDFS, false);
-  public RawValue getSerializedLocally(KeyInfo keyInfo, boolean doNotLockEntry, EntryEventImpl clientEvent, boolean returnTombstones) throws PrimaryBucketException,
+  public RawValue getSerializedLocally(KeyInfo keyInfo, boolean doNotLockEntry, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) throws PrimaryBucketException,
-      RawValue result = bucketRegion.getSerialized(keyInfo, true, doNotLockEntry, clientEvent, returnTombstones);
+      RawValue result = bucketRegion.getSerialized(keyInfo, true, doNotLockEntry, clientEvent, returnTombstones, allowReadFromHDFS);
-      boolean access, boolean allowTombstones)
+      boolean access, boolean allowTombstones, boolean allowReadFromHDFS)
-      ent = bucketRegion.entries.getEntry(key);
+      if (allowReadFromHDFS) {
+        ent = bucketRegion.entries.getEntry(key);
+      }
+      else {
+        ent = bucketRegion.entries.getOperationalEntryInVM(key);
+      }
+        Set keys = r.keySet(allowTombstones);
+        if (getPartitionedRegion().isHDFSReadWriteRegion()) {
+          // hdfs regions can't copy all keys into memory
+          ret = keys;
+
+        } else  { 
+		}
-   * @param bucketId
-   *          the bucket id of the key
+   * @param bucketRegion
+   *          the bucket to do the create in
-  public boolean createLocally(Integer bucketId,
+  public boolean createLocally(final BucketRegion bucketRegion,
-    final BucketRegion bucketRegion = getInitializedBucketForId(event.getKey(), bucketId);
-
+  public Map<Integer, SizeEntry> getSizeEstimateForLocalPrimaryBuckets() {
+    return getSizeEstimateLocallyForBuckets(getAllLocalPrimaryBucketIds());
+  }
+
+    return getSizeLocallyForPrimary(bucketIds, false);
+  }
+
+  public Map<Integer, SizeEntry> getSizeEstimateLocallyForBuckets(Collection<Integer> bucketIds) {
+    return getSizeLocallyForPrimary(bucketIds, true);
+  }
+
+  private Map<Integer, SizeEntry> getSizeLocallyForPrimary(Collection<Integer> bucketIds, boolean estimate) {
-    BucketRegion r;
+    BucketRegion r = null;
-        mySizeMap.put(bucketId, new SizeEntry(r.size(), r.getBucketAdvisor().isPrimary()));
+        mySizeMap.put(bucketId, new SizeEntry(estimate ? r.sizeEstimate() : r.size(), r.getBucketAdvisor().isPrimary()));
+//        if (getLogWriter().fineEnabled() && r.getBucketAdvisor().isPrimary()) {
+//          r.verifyTombstoneCount();
+//        }
+      } catch (PrimaryBucketException skip) {
+        // sizeEstimate() will throw this exception as it will not try to read from HDFS on a secondary bucket,
+        // this bucket will be retried in PartitionedRegion.getSizeForHDFS() fixes bug 49033
+        continue;
+  /** a fast estimate of total bucket size */
+  public long getEstimatedLocalBucketSize(boolean primaryOnly) {
+    long size = 0;
+    for (BucketRegion br : localBucket2RegionMap.values()) {
+      if (!primaryOnly || br.getBucketAdvisor().isPrimary()) {
+        size += br.getEstimatedLocalSize();
+      }
+    }
+    return size;
+  }
+
+  /** a fast estimate of total bucket size */
+  public long getEstimatedLocalBucketSize(Set<Integer> bucketIds) {
+    long size = 0;
+    for (Integer bid : bucketIds) {
+      BucketRegion br = localBucket2RegionMap.get(bid);
+      if (br != null) {
+        size += br.getEstimatedLocalSize();
+      }
+    }
+    return size;
+  }
+
