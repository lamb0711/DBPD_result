Initial import of geode-1.0.0.0-SNAPSHOT-2.
All the new sub-project directories (like jvsd) were not imported.
A diff was done to confirm that this commit is exactly the same as
the open directory the snapshot was made from.

+import java.nio.ByteBuffer;
+import com.gemstone.gemfire.internal.cache.DiskEntry.Helper.ValueWrapper;
+import com.gemstone.gemfire.internal.offheap.OffHeapHelper;
+import com.gemstone.gemfire.internal.offheap.Releasable;
+import com.gemstone.gemfire.internal.offheap.SimpleMemoryAllocatorImpl;
+import com.gemstone.gemfire.internal.offheap.UnsafeMemoryChunk;
+import com.gemstone.gemfire.internal.offheap.SimpleMemoryAllocatorImpl.Chunk;
+import com.gemstone.gemfire.internal.offheap.StoredObject;
+import com.gemstone.gemfire.internal.offheap.annotations.Released;
+import com.gemstone.gemfire.internal.offheap.annotations.Retained;
+import com.gemstone.gemfire.internal.offheap.annotations.Unretained;
+  
+  /**
+   * In some cases we need to do something just before we drop the value
+   * from a DiskEntry that is being moved (i.e. overflowed) to disk.
+   * @param context
+   */
+  public void handleValueOverflow(RegionEntryContext context);
+  
+  /**
+   * In some cases we need to do something just after we unset the value
+   * from a DiskEntry that has been moved (i.e. overflowed) to disk.
+   * @param context
+   */
+  public void afterValueOverflow(RegionEntryContext context);
-      Object v = getOffHeapValueOnDiskOrBuffer(entry, dr, context);
+      @Released Object v = getOffHeapValueOnDiskOrBuffer(entry, dr, context);
+        if (v instanceof Chunk) {
+          @Released Chunk ohv = (Chunk) v;
+          try {
+            v = ohv.getDeserializedValue(null, null);
+            if (v == ohv) {
+              throw new IllegalStateException("sqlf tried to use getValueOnDiskOrBuffer");
+            }
+          } finally {
+            ohv.release(); // OFFHEAP the offheap ref is decremented here
+          }
+        } else {
+        }
+    @Retained
-            Object v = entry._getValueUse(context, true); // TODO:KIRK:OK Rusty had Object v = entry.getValueWithContext(context);
+            @Retained Object v = entry._getValueRetain(context, true); // TODO:KIRK:OK Rusty had Object v = entry.getValueWithContext(context);
-      Object v = null;
+      @Retained @Released Object v = null;
-        v = de._getValueUse(context, true); // OFFHEAP copied to heap entry; todo allow entry to refer to offheap since it will be copied to network.
+        SimpleMemoryAllocatorImpl.setReferenceCountOwner(entry);
+        v = de._getValueRetain(context, true); // OFFHEAP copied to heap entry; todo allow entry to refer to offheap since it will be copied to network.
+        SimpleMemoryAllocatorImpl.setReferenceCountOwner(null);
-          {
+        try {
+          if (v instanceof StoredObject && !((StoredObject) v).isSerialized()) {
+            entry.setSerialized(false);
+            entry.value = ((StoredObject) v).getDeserializedForReading();
+            
+            //For SQLFire we prefer eager deserialized
+//            if(v instanceof ByteSource) {
+//              entry.setEagerDeserialize();
+//            }
+          } else {
+            
+          //For SQLFire we prefer eager deserialized
+//            if(v instanceof ByteSource) {
+//              entry.setEagerDeserialize();
+//            }
+        } finally {
+          // If v == entry.value then v is assumed to be an OffHeapByteSource
+          // and release() will be called on v after the bytes have been read from
+          // off-heap.
+          if (v != entry.value) {
+            OffHeapHelper.releaseWithNoTracking(v);
+          }
+        }
-      } else {
+      }
+      else {
+        Object preparedValue = v;
+        if (preparedValue != null) {
+          preparedValue = AbstractRegionEntry.prepareValueForGII(preparedValue);
+          if (preparedValue == null) {
+            return false;
+          }
+        }
+      if (CachedDeserializableFactory.preferObject()) {
+        entry.value = preparedValue;
+        entry.setEagerDeserialize();
+      }
+      else {
-          BlobHelper.serializeTo(v, hdos);
+          BlobHelper.serializeTo(preparedValue, hdos);
+      }
-          entry.setValueWithContext(drv, AbstractRegionMap.prepareValueForCache((RegionEntryContext) r, re.getValue()));
+          entry.setValueWithContext(drv, entry.prepareValueForCache((RegionEntryContext) r,
+              re.getValue(), false));
+    private static final ValueWrapper INVALID_VW = new ByteArrayValueWrapper(true, INVALID_BYTES);
+    private static final ValueWrapper LOCAL_INVALID_VW = new ByteArrayValueWrapper(true, LOCAL_INVALID_BYTES);
+    private static final ValueWrapper TOMBSTONE_VW = new ByteArrayValueWrapper(true, TOMBSTONE_BYTES);
+    
+    public static interface ValueWrapper {
+      public boolean isSerialized();
+      public int getLength();
+      public byte getUserBits();
+      public void sendTo(ByteBuffer bb, Flushable flushable) throws IOException;
+      public String getBytesAsString();
+    }
+    public static interface Flushable {
+      public void flush() throws IOException;
+
+      public void flush(ByteBuffer bb, ByteBuffer chunkbb) throws IOException;
+    }
+    public static class ByteArrayValueWrapper implements ValueWrapper {
+      public final boolean isSerializedObject;
+      public final byte[] bytes;
+      
+     public ByteArrayValueWrapper(boolean isSerializedObject, byte[] bytes) {
+        this.isSerializedObject = isSerializedObject;
+        this.bytes = bytes;
+      }
+
+      @Override
+      public boolean isSerialized() {
+        return this.isSerializedObject;
+      }
+
+      @Override
+      public int getLength() {
+        return (this.bytes != null) ? this.bytes.length : 0;
+      }
+
+      private boolean isInvalidToken() {
+        return this == INVALID_VW;
+      }
+
+      private boolean isLocalInvalidToken() {
+        return this == LOCAL_INVALID_VW;
+      }
+
+      private boolean isTombstoneToken() {
+        return this == TOMBSTONE_VW;
+      }
+
+      @Override
+      public byte getUserBits() {
+        byte userBits = 0x0;
+        if (isSerialized()) {
+          if (isTombstoneToken()) {
+            userBits = EntryBits.setTombstone(userBits, true);
+          } else if (isInvalidToken()) {
+            userBits = EntryBits.setInvalid(userBits, true);
+          } else if (isLocalInvalidToken()) {
+            userBits = EntryBits.setLocalInvalid(userBits, true);
+          } else {
+            if (this.bytes == null) {
+              throw new IllegalStateException("userBits==1 and value is null");
+            } else if (this.bytes.length == 0) {
+              throw new IllegalStateException("userBits==1 and value is zero length");
+            }
+            userBits = EntryBits.setSerialized(userBits, true);
+          }
+        }
+        return userBits;
+      }
+
+      @Override
+      public void sendTo(ByteBuffer bb, Flushable flushable) throws IOException {
+        int offset = 0;
+        final int maxOffset = getLength();
+        while (offset < maxOffset) {
+          int bytesThisTime = maxOffset - offset;
+          boolean needsFlush = false;
+          if (bytesThisTime > bb.remaining()) {
+            needsFlush = true;
+            bytesThisTime = bb.remaining();
+          }
+          bb.put(this.bytes, offset, bytesThisTime);
+          offset += bytesThisTime;
+          if (needsFlush) {
+            flushable.flush();
+          }
+        }
+      }
+
+      @Override
+      public String getBytesAsString() {
+        if (this.bytes == null) {
+          return "null";
+        }
+        StringBuffer sb = new StringBuffer();
+        int len = getLength();
+        for (int i = 0; i < len; i++) {
+          sb.append(this.bytes[i]).append(", ");
+        }
+        return sb.toString();
+      }
+    }
+    
+    /**
+     * This class is a bit of a hack used by the compactor.
+     * For the compactor always copies to a byte[] so
+     * this class is just a simple wrapper.
+     * It is possible that the length of the byte array is greater
+     * than the actual length of the wrapped data.
+     * At the time we create this we are all done with isSerialized
+     * and userBits so those methods are not supported.
+     */
+    public static class CompactorValueWrapper extends ByteArrayValueWrapper {
+      private final int length;
+      
+      public CompactorValueWrapper(byte[] bytes, int length) {
+        super(false, bytes);
+        this.length = length;
+      }
+      
+      @Override
+      public boolean isSerialized() {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public int getLength() {
+        return this.length;
+      }
+
+      @Override
+      public byte getUserBits() {
+        throw new UnsupportedOperationException();
+      }
+    }
+    
+    /**
+     * Note that the Chunk this ValueWrapper is created with
+     * is unretained so it must be used before the owner of
+     * the chunk releases it.
+     * Since the RegionEntry that has the value we are writing to
+     * disk has it retained we are ok as long as this ValueWrapper's
+     * life ends before the RegionEntry sync is released.
+     * Note that this class is only used with uncompressed chunks.
+     */
+    public static class ChunkValueWrapper implements ValueWrapper {
+      private final @Unretained Chunk chunk;
+      public ChunkValueWrapper(Chunk c) {
+        assert !c.isCompressed();
+        this.chunk = c;
+      }
+      @Override
+      public boolean isSerialized() {
+        return this.chunk.isSerialized();
+      }
+      @Override
+      public int getLength() {
+        return this.chunk.getDataSize();
+      }
+      @Override
+      public byte getUserBits() {
+        byte userBits = 0x0;
+        if (isSerialized()) {
+          userBits = EntryBits.setSerialized(userBits, true);
+        }
+        return userBits;
+      }
+      @Override
+      public void sendTo(ByteBuffer bb, Flushable flushable) throws IOException {
+        final int maxOffset = getLength();
+        if (maxOffset == 0) {
+          return;
+        }
+        if (maxOffset > bb.capacity()) {
+          ByteBuffer chunkbb = this.chunk.createDirectByteBuffer();
+          if (chunkbb != null) {
+            flushable.flush(bb, chunkbb);
+            return;
+          }
+        }
+        final long bbAddress = Chunk.getDirectByteBufferAddress(bb);
+        if (bbAddress != 0L) {
+          int bytesRemaining = maxOffset;
+          int availableSpace = bb.remaining();
+          long addrToWrite = bbAddress + bb.position();
+          long addrToRead = this.chunk.getAddressForReading(0, maxOffset);
+          if (bytesRemaining > availableSpace) {
+            do {
+              UnsafeMemoryChunk.copyMemory(addrToRead, addrToWrite, availableSpace);
+              bb.position(bb.position()+availableSpace);
+              addrToRead += availableSpace;
+              bytesRemaining -= availableSpace;
+              flushable.flush();
+              addrToWrite = bbAddress + bb.position();
+              availableSpace = bb.remaining();
+            } while (bytesRemaining > availableSpace);
+          }
+          UnsafeMemoryChunk.copyMemory(addrToRead, addrToWrite, bytesRemaining);
+          bb.position(bb.position()+bytesRemaining);
+        } else {
+          long addr = this.chunk.getAddressForReading(0, maxOffset);
+          final long endAddr = addr + maxOffset;
+          while (addr != endAddr) {
+            bb.put(UnsafeMemoryChunk.readAbsoluteByte(addr));
+            addr++;
+            if (!bb.hasRemaining()) {
+              flushable.flush();
+            }
+          }
+        }
+      }
+      @Override
+      public String getBytesAsString() {
+        return this.chunk.getStringForm();
+      }
+    }
+
+    public static ValueWrapper createValueWrapper(Object value, EntryEventImpl event) {
+      if (value == Token.INVALID) {
+        // even though it is not serialized we say it is because
+        // bytes will never be an empty array when it is serialized
+        // so that gives us a way to specify the invalid value
+        // given a byte array and a boolean flag.
+        return INVALID_VW;
+      }
+      else if (value == Token.LOCAL_INVALID) {
+        // even though it is not serialized we say it is because
+        // bytes will never be an empty array when it is serialized
+        // so that gives us a way to specify the local-invalid value
+        // given a byte array and a boolean flag.
+        return LOCAL_INVALID_VW;
+      }
+      else if (value == Token.TOMBSTONE) {
+        return TOMBSTONE_VW;
+      }
+      else {
+        boolean isSerializedObject = true;
+        byte[] bytes;
+        if (value instanceof CachedDeserializable) {
+          CachedDeserializable proxy = (CachedDeserializable)value;
+          if (proxy instanceof Chunk) {
+            return new ChunkValueWrapper((Chunk) proxy);
+          }
+          if (proxy instanceof StoredObject) {
+            StoredObject ohproxy = (StoredObject) proxy;
+            isSerializedObject = ohproxy.isSerialized();
+            if (isSerializedObject) {
+              bytes = ohproxy.getSerializedValue();
+            } else {
+              bytes = (byte[]) ohproxy.getDeserializedForReading();
+            }
+          } else {
+            bytes = proxy.getSerializedValue();
+          }
+          if (event != null && isSerializedObject) {
+            event.setCachedSerializedNewValue(bytes);
+          }
+        }
+        else if (value instanceof byte[]) {
+          isSerializedObject = false;
+          bytes = (byte[])value;
+        }
+        else {
+          Assert.assertTrue(!Token.isRemovedFromDisk(value));
+          if (event != null && event.getCachedSerializedNewValue() != null) {
+            bytes = event.getCachedSerializedNewValue();
+          } else {
+            bytes = EntryEventImpl.serialize(value);
+            if (bytes.length == 0) {
+              throw new IllegalStateException("serializing <" + value + "> produced empty byte array");
+            }
+            if (event != null) {
+              event.setCachedSerializedNewValue(bytes);
+            }
+          }
+        }
+        return new ByteArrayValueWrapper(isSerializedObject, bytes);
+      }
+    }
+    public static ValueWrapper createValueWrapperFromEntry(DiskEntry entry, LocalRegion region, EntryEventImpl event) {
+      if (event != null) {
+        // For off-heap it should be faster to pass a reference to the
+        // StoredObject instead of using the cached byte[] (unless it is also compressed).
+        // Since NIO is used if the chunk of memory is large we can write it
+        // to the file with using the off-heap memory with no extra copying.
+        // So we give preference to getRawNewValue over getCachedSerializedNewValue
+        Object rawValue = null;
+        if (!event.hasDelta()) {
+          // We don't do this for the delta case because getRawNewValue returns delta
+          // and we want to write the entire new value to disk.
+          rawValue = event.getRawNewValue();
+          if (rawValue instanceof Chunk) {
+            return new ChunkValueWrapper((Chunk) rawValue);
+          }
+        }
+        if (event.getCachedSerializedNewValue() != null) {
+          return new ByteArrayValueWrapper(true, event.getCachedSerializedNewValue());
+        }
+        if (rawValue != null) {
+          return createValueWrapper(rawValue, event);
+        }
+      }
+      // TODO OFFHEAP: No need to retain since we hold the sync on entry but we need a flavor of _getValue that will decompress
+      @Retained Object value = entry._getValueRetain(region, true);
+      try {
+        return createValueWrapper(value, event);
+      } finally {
+        OffHeapHelper.release(value);
+      }
+    }
+    
+
-      DiskRegion dr = region.getDiskRegion();
-      byte[] bytes = null;
-      boolean isSerializedObject = true;
-
-      if (event != null && event.getCachedSerializedNewValue() != null) {
-        bytes = event.getCachedSerializedNewValue();
-      } else {
-        // If event != null then we could get the new value from it.
-        // Getting it from the entry is expensive on a compressed region.
-        Object value;
-        if (event != null && !event.hasDelta() && event.getRawNewValue() != null) {
-          // We don't do this for the delta case because getRawNewValue returns delta
-          // and we want to write the entire new value to disk.
-          value = event.getRawNewValue();
-        } else {
-          value = entry._getValueUse(region, true);
-        }
-
-      if (value == Token.INVALID) {
-        // even though it is not serialized we say it is because
-        // bytes will never be an empty array when it is serialized
-        // so that gives us a way to specify the invalid value
-        // given a byte array and a boolean flag.
-        bytes = INVALID_BYTES;
-      }
-      else if (value == Token.LOCAL_INVALID) {
-        // even though it is not serialized we say it is because
-        // bytes will never be an empty array when it is serialized
-        // so that gives us a way to specify the local-invalid value
-        // given a byte array and a boolean flag.
-        bytes = LOCAL_INVALID_BYTES;
-      }
-      else if (value == Token.TOMBSTONE) {
-        bytes = TOMBSTONE_BYTES;
-      }
-      else if (value instanceof byte[]) {
-        isSerializedObject = false;
-        bytes = (byte[])value;
-      }
-      else if (value instanceof CachedDeserializable) {
-        CachedDeserializable proxy = (CachedDeserializable)value;
-        bytes = proxy.getSerializedValue();
-        if (event != null) {
-          event.setCachedSerializedNewValue(bytes);
-        }
-      }
-      else {
-        Assert.assertTrue(!Token.isRemovedFromDisk(value));
-        if (event != null && event.getCachedSerializedNewValue() != null) {
-          bytes = event.getCachedSerializedNewValue();
-        } else {
-          bytes = EntryEventImpl.serialize(value);
-          if (bytes.length == 0) {
-            throw new IllegalStateException("serializing <" + value + "> produced empty byte array");
-          }
-          if (event != null) {
-            event.setCachedSerializedNewValue(bytes);
-          }
-        }
-      }
-      }
-
-      {
-        DiskId did = entry.getDiskId();
-        // @todo does the following unmark need to be called when an async
-        // write is scheduled or is it ok for doAsyncFlush to do it?
-        did.unmarkForWriting();
-        dr.put(entry, region, bytes, isSerializedObject, async);
-      }
+      writeBytesToDisk(entry, region, async, createValueWrapperFromEntry(entry, region, event));
+    }
+    
+    private static void writeBytesToDisk(DiskEntry entry, LocalRegion region, boolean async, ValueWrapper vw) throws RegionClearedException {
+      // @todo does the following unmark need to be called when an async
+      // write is scheduled or is it ok for doAsyncFlush to do it?
+      entry.getDiskId().unmarkForWriting();
+      region.getDiskRegion().put(entry, region, vw, async);
-              entry.setValueWithContext(region, null); // fixes bug 41119
+              try {
+                entry.handleValueOverflow(region);
+                entry.setValueWithContext(region, null); // fixes bug 41119
+              }finally {
+                entry.afterValueOverflow(region);
+              }
+              
-            entry.setValueWithContext(region, AbstractRegionMap.prepareValueForCache(region, re.getValue()));
+            entry.setValueWithContext(region, entry.prepareValueForCache(region, re.getValue(), false));
-          entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
+          //The new value in the entry needs to be set after the disk writing 
+          // has succeeded. If not , for GemFireXD , it is possible that other thread
+          // may pick this transient value from region entry ( which for 
+          //offheap will eventually be released ) as index key, 
+          //given that this operation is bound to fail in case of
+          //disk access exception.
+          
+          //entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
-              writeToDisk(entry, region, false, event);
+              //In case of compression the value is being set first 
+              // because atleast for now , GemFireXD does not support compression
+              // if and when it does support, this needs to be taken care of else
+              // we risk Bug 48965
+              if (AbstractRegionEntry.isCompressible(dr, newValue)) {
+                entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
+                
+                // newValue is prepared and compressed. We can't write compressed values to disk.
+                writeToDisk(entry, region, false, event);
+              } else {
+                writeBytesToDisk(entry, region, false, createValueWrapper(newValue, event));
+                entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
+              }
+              
+              entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
+              
+              entry.setValueWithContext(region, newValue); 
+            entry.setValueWithContext(region, newValue); // OFFHEAP newValue already prepared
+            
+          }else {
+            entry.setValueWithContext(region, newValue);
+          
-          entry.setValueWithContext(context, AbstractRegionMap.prepareValueForCache(drv, newValue.getValue()));
+          entry.setValueWithContext(context, entry.prepareValueForCache(drv, newValue.getValue(), 
+              false));
-            entry.setValueWithContext(context,null); // fixes bug 41119
+            try {
+              entry.handleValueOverflow(context);
+              entry.setValueWithContext(context,null); // fixes bug 41119
+            }finally {
+              entry.afterValueOverflow(context);
+            }
-      Object result = getValueOffHeapOrDiskWithoutFaultIn(entry, region);
+      Object result = OffHeapHelper.copyAndReleaseIfNeeded(getValueOffHeapOrDiskWithoutFaultIn(entry, region));
+      if (result instanceof StoredObject) {
+        ((StoredObject) result).release();
+        throw new IllegalStateException("sqlf tried to use getValueInVMOrDiskWithoutFaultIn");
+      }
+    @Retained
-      Object v = entry._getValueUse(region, true); // TODO:KIRK:OK Object v = entry.getValueWithContext(region);
+      @Retained Object v = entry._getValueRetain(region, true); // TODO:KIRK:OK Object v = entry.getValueWithContext(region);
-          v = entry._getValueUse(region, true); // TODO:KIRK:OK v = entry.getValueWithContext(region);
+          v = entry._getValueRetain(region, true); // TODO:KIRK:OK v = entry.getValueWithContext(region);
+//      } else if (v instanceof ByteSource) {
+//        // If the ByteSource contains a Delta or ListOfDelta then we want to deserialize it
+//        Object deserVal = ((CachedDeserializable)v).getDeserializedForReading();
+//        if (deserVal != v) {
+//          OffHeapHelper.release(v);
+//          v = deserVal;
+//        }
+
-
-    public static Object faultInValue(DiskEntry entry, LocalRegion region)
+    public static Object faultInValue(DiskEntry entry, LocalRegion region) {
+      return faultInValue(entry, region, false);
+    }
+    @Retained
+    public static Object faultInValueRetain(DiskEntry entry, LocalRegion region) {
+      return faultInValue(entry, region, true);
+    }
+    /**
+     * @param retainResult if true then the result may be a retained off-heap reference
+     */
+    @Retained
+    private static Object faultInValue(DiskEntry entry, LocalRegion region, boolean retainResult)
-      Object v = entry._getValueUse(region, true); // TODO:KIRK:OK Object v = entry.getValueWithContext(region);
+      @Retained Object v = entry._getValueRetain(region, true); // TODO:KIRK:OK Object v = entry.getValueWithContext(region);
+      try {
-          v = entry._getValueUse(region, true); // TODO:KIRK:OK v = entry.getValueWithContext(region);
+          v = entry._getValueRetain(region, true); // TODO:KIRK:OK v = entry.getValueWithContext(region);
-            v = readValueFromDisk(entry, region, -1, false, null);
+            v = readValueFromDisk(entry, region);
+      } finally {
+        if (!retainResult) {
+          v = OffHeapHelper.copyAndReleaseIfNeeded(v);
+          // At this point v should be either a heap object
+        }
+      }
-          Object v = readValueFromDisk(entry, recoveryStore, oplogId, true, in); // OFFHEAP: Off heap value ok since only used for token check
-          if (entry instanceof LRUEntry) {
-            if (v != null && !Token.isInvalid(v)) {
-              lruEntryFaultIn((LRUEntry) entry, recoveryStore);
-              
-              lruFaultedIn = true;
+          DiskId did = entry.getDiskId();
+          if (did != null) {
+            Object value = null;
+            DiskRecoveryStore region = recoveryStore;
+            DiskRegionView dr = region.getDiskRegionView();
+            dr.acquireReadLock();
+            try {
+              synchronized (did) {
+                // don't read if the oplog has changed.
+                if (oplogId == did.getOplogId()) {
+                  value = getValueFromDisk(dr, did, in);
+                  if (value != null) {
+                    setValueOnFaultIn(value, did, entry, dr, region);
+                  }
+                }
+              }
+            } finally {
+              dr.releaseReadLock();
+            }
+            if (entry instanceof LRUEntry) {
+              if (value != null && !Token.isInvalid(value)) {
+                lruEntryFaultIn((LRUEntry) entry, recoveryStore);
+                lruFaultedIn = true;
+              }
+    /**
+     *  Caller must have "did" synced.
+     */
+    private static Object getValueFromDisk(DiskRegionView dr, DiskId did, ByteArrayDataInput in) {
+      Object value;
+      if (dr.isBackup() && did.getKeyId() == DiskRegion.INVALID_ID) {
+        // must have been destroyed
+        value = null;
+      } else {
+        if (did.isKeyIdNegative()) {
+          did.setKeyId(- did.getKeyId());
+        }
+        // if a bucket region then create a CachedDeserializable here instead of object
+        value = dr.getRaw(did); // fix bug 40192
+        if (value instanceof BytesAndBits) {
+          BytesAndBits bb = (BytesAndBits)value;
+          if (EntryBits.isInvalid(bb.getBits())) {
+            value = Token.INVALID;
+          } else if (EntryBits.isLocalInvalid(bb.getBits())) {
+            value = Token.LOCAL_INVALID;
+          } else if (EntryBits.isTombstone(bb.getBits())) {
+            value = Token.TOMBSTONE;
+          } else if (EntryBits.isSerialized(bb.getBits())) {
+            value = readSerializedValue(bb.getBytes(), bb.getVersion(), in, false);
+          } else {
+            value = readRawValue(bb.getBytes(), bb.getVersion(), in);
+          }
+        }
+      }
+      return value;
+    }
+    
+     * return the result will only be off-heap if the value is a sqlf ByteSource. Otherwise result will be on-heap.
+     * Caller must have "entry" synced.
-    private static Object readValueFromDisk(DiskEntry entry,
-        DiskRecoveryStore region, long oplogId, boolean checkOplogId,
-        ByteArrayDataInput in) {
+    @Retained
+    private static Object readValueFromDisk(DiskEntry entry, DiskRecoveryStore region) {
-        // long id = diskId.getKeyId();
-        if (dr.isBackup() && did.getKeyId() == DiskRegion.INVALID_ID) {
-          return null; // must have been destroyed
+        Object value = getValueFromDisk(dr, did, null);
+        if (value == null) return null;
+        @Unretained Object preparedValue = setValueOnFaultIn(value, did, entry, dr, region);
+        // For Sqlfire we want to return the offheap representation.
+        // So we need to retain it for the caller to release.
+        /*if (preparedValue instanceof ByteSource) {
+          // This is the only case in which we return a retained off-heap ref.
+          ((ByteSource)preparedValue).retain();
+          return preparedValue;
+        } else */{
+          return value;
-        if(checkOplogId) {
-          // If we're only supposed to read from this specific oplog, don't
-          // read if the oplog has changed.
-          if(oplogId != did.getOplogId()) {
-            return null;
-          }
-        }
-        if (did.isKeyIdNegative()) {
-          did.setKeyId(- did.getKeyId());
-        }
-        // if a bucket region then create a CachedDeserializable here instead of object
-        Object value = dr.getRaw(did); // fix bug 40192
-        if (value instanceof BytesAndBits) {
-          BytesAndBits bb = (BytesAndBits)value;
-          if (EntryBits.isInvalid(bb.getBits())) {
-            value = Token.INVALID;
-          } else if (EntryBits.isLocalInvalid(bb.getBits())) {
-            value = Token.LOCAL_INVALID;
-          } else if (EntryBits.isTombstone(bb.getBits())) {
-            value = Token.TOMBSTONE;
-          } else if (EntryBits.isSerialized(bb.getBits())) {
-            value = readSerializedValue(bb.getBytes(), bb.getVersion(),
-                in, false);
-          } else {
-            value = readRawValue(bb.getBytes(), bb.getVersion(), in);
-          }
-        }
-        int bytesOnDisk = getValueLength(did);
-        Assert.assertTrue(value != null);
-        Object preparedValue = AbstractRegionMap.prepareValueForCache((RegionEntryContext) region, value);
-        region.updateSizeOnFaultIn(entry.getKey(), region.calculateValueSize(preparedValue), bytesOnDisk);
-        //did.setValueSerializedSize(0);
-        // I think the following assertion is true but need to run
-        // a regression with it. Reenable this post 6.5
-        //Assert.assertTrue(entry._getValue() == null);
-        entry.setValueWithContext((RegionEntryContext) region, preparedValue);
-        dr.incNumEntriesInVM(1L);
-        dr.incNumOverflowOnDisk(-1L);
-        incrementBucketStats(region, 1/*InVM*/, -1/*OnDisk*/, -bytesOnDisk);
-        return value; // OFFHEAP: off heap value return ok
+    
+    /**
+     * Caller must have "entry" and "did" synced and "dr" readLocked.
+     * @return the unretained result must be used by the caller before it releases the sync on "entry".
+     */
+    @Unretained
+    private static Object setValueOnFaultIn(Object value, DiskId did, DiskEntry entry, DiskRegionView dr, DiskRecoveryStore region) {
+//    dr.getOwner().getCache().getLogger().info("DEBUG: faulting in entry with key " + entry.getKey());
+      int bytesOnDisk = getValueLength(did);
+      // Retained by the prepareValueForCache call for the region entry.
+      // NOTE that we return this value unretained because the retain is owned by the region entry not the caller.
+      @Retained Object preparedValue = entry.prepareValueForCache((RegionEntryContext) region, value,
+          false);
+      region.updateSizeOnFaultIn(entry.getKey(), region.calculateValueSize(preparedValue), bytesOnDisk);
+      //did.setValueSerializedSize(0);
+      // I think the following assertion is true but need to run
+      // a regression with it. Reenable this post 6.5
+      //Assert.assertTrue(entry._getValue() == null);
+      entry.setValueWithContext((RegionEntryContext) region, preparedValue);
+      dr.incNumEntriesInVM(1L);
+      dr.incNumOverflowOnDisk(-1L);
+      incrementBucketStats(region, 1/*InVM*/, -1/*OnDisk*/, -bytesOnDisk);
+      return preparedValue;
+    }
-
+      
+      // Notify the SQLFire IndexManager if present
+     /* final IndexUpdater indexUpdater = region.getIndexUpdater();
+      if(indexUpdater != null && dr.isSync()) {
+        indexUpdater.onOverflowToDisk(entry);
+      }*/
+      
-          entry.setValueWithContext(region,null);
+          try {
+            entry.handleValueOverflow(region);
+            entry.setValueWithContext(region,null);
+          }finally {
+            entry.afterValueOverflow(region);
+          }
-                entry.setValueWithContext(region,null);
+                try {
+                  entry.handleValueOverflow(region);
+                  entry.setValueWithContext(region,null);
+                }finally {
+                  entry.afterValueOverflow(region);
+                }
+      if (region.isThisRegionBeingClosedOrDestroyed()) return;
+      if (region.isThisRegionBeingClosedOrDestroyed()) return;
+                if (region.isThisRegionBeingClosedOrDestroyed()) return;
-                entry.setValueWithContext(region,null);
+                try {
+                 entry.handleValueOverflow(region);
+                 entry.setValueWithContext(region,null);
+                }finally {
+                  entry.afterValueOverflow(region);
+                }
