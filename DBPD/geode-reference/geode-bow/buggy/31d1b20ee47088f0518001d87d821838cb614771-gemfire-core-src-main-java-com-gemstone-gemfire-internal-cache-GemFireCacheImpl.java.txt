Initial import of geode-1.0.0.0-SNAPSHOT-2.
All the new sub-project directories (like jvsd) were not imported.
A diff was done to confirm that this commit is exactly the same as
the open directory the snapshot was made from.

+import com.gemstone.gemfire.cache.CacheExistsException;
+import com.gemstone.gemfire.cache.asyncqueue.internal.AsyncEventQueueImpl;
+import com.gemstone.gemfire.cache.hdfs.HDFSStoreFactory;
+import com.gemstone.gemfire.cache.hdfs.internal.HDFSIntegrationUtil;
+import com.gemstone.gemfire.cache.hdfs.internal.HDFSStoreCreation;
+import com.gemstone.gemfire.cache.hdfs.internal.HDFSStoreFactoryImpl;
+import com.gemstone.gemfire.cache.hdfs.internal.HDFSStoreImpl;
+import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSFlushQueueFunction;
+import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSForceCompactionFunction;
+import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSLastCompactionTimeFunction;
+import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSRegionDirector;
+import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSStoreDirector;
+import com.gemstone.gemfire.cache.lucene.LuceneService;
+import com.gemstone.gemfire.cache.lucene.LuceneServiceProvider;
+import com.gemstone.gemfire.cache.util.ObjectSizer;
+import com.gemstone.gemfire.internal.cache.control.InternalResourceManager.ResourceType;
+import com.gemstone.gemfire.internal.cache.lru.OffHeapEvictor;
+import com.gemstone.gemfire.internal.cache.tier.sockets.ServerConnection;
+import com.gemstone.gemfire.internal.offheap.MemoryAllocator;
-import com.gemstone.gemfire.internal.util.ArrayUtils;
+import com.gemstone.gemfire.redis.GemFireRedisServer;
+import com.sun.jna.Native;
+import com.sun.jna.Platform;
-
-  
+
+  /**
+   * True if the user is allowed lock when memory resources appear to be overcommitted. 
+   */
+  public static final boolean ALLOW_MEMORY_LOCK_WHEN_OVERCOMMITTED = Boolean.getBoolean("gemfire.Cache.ALLOW_MEMORY_OVERCOMMIT");
+
+  
+  private OffHeapEvictor offHeapEvictor = null;
+  private final Object offHeapEvictorLock = new Object();
+
+   * Redis server is started when {@link DistributionConfig#getRedisPort()} is set
+   */
+  private GemFireRedisServer redisServer;
+  
+  /**
+  
+  private final LuceneService luceneService;
+  
+  private final static Boolean DISABLE_AUTO_EVICTION = Boolean.getBoolean("gemfire.disableAutoEviction");
+   * Invokes mlockall().  Locks  all pages mapped into the address space of the 
+   * calling process.  This includes the pages of the code, data and stack segment, 
+   * as well as shared libraries, user space kernel data, shared memory, and 
+   * memory-mapped files.  All mapped pages are guaranteed to be resident in RAM 
+   * when the call returns successfully; the pages are guaranteed to stay in RAM 
+   * until later unlocked.
+   * 
+   * @param flags
+   *    MCL_CURRENT 1 - Lock all pages which are currently mapped into the 
+   *    address space of the process.
+   *    
+   *    MCL_FUTURE  2 - Lock all pages which will become mapped into the address 
+   *    space of the process in the future.  These could be for instance new 
+   *    pages required by a growing heap and stack as well as new memory mapped 
+   *    files or shared memory regions.
+   *    
+   * @return 
+   *    0 if success, non-zero if error and errno set
+   *    
+   */
+  private static native int mlockall(int flags);
+
+  public static void lockMemory() {
+    int result = 0;
+    try {
+      Native.register(Platform.C_LIBRARY_NAME);
+      result = mlockall(1);
+      if (result == 0) {
+        return;
+      }
+    } catch (Throwable t) {
+      throw new IllegalStateException("Error trying to lock memory", t);
+    }
+
+    int errno = Native.getLastError();
+    String msg = "mlockall failed: " + errno;
+    if (errno == 1 || errno == 12) {  // EPERM || ENOMEM
+      msg = "Unable to lock memory due to insufficient free space or privileges.  " 
+          + "Please check the RLIMIT_MEMLOCK soft resource limit (ulimit -l) and " 
+          + "increase the available memory if needed";
+    }
+    throw new IllegalStateException(msg);
+  }
+  
+  /**
+  public static Cache create(DistributedSystem system, boolean existingOk, CacheConfig cacheConfig)
+  throws CacheExistsException, TimeoutException, CacheWriterException,
+  GatewayException,
+  RegionExistsException 
+  {
+    GemFireCacheImpl instance = getInstance();
+    if (instance != null && !instance.isClosed()) {
+      if (existingOk) {
+        // Check if cache configuration matches.
+        cacheConfig.validateCacheConfig(instance);
+
+        return instance;
+      } else {
+        // instance.creationStack argument is for debugging...
+        throw new CacheExistsException(instance, LocalizedStrings.CacheFactory_0_AN_OPEN_CACHE_ALREADY_EXISTS.toLocalizedString(instance), instance.creationStack);
+      }
+    }
+    return create(system, cacheConfig);
+  }
+  
+      
+      this.luceneService = LuceneServiceProvider.create(this);
+
-      getResourceManager().addResourceListener(getHeapEvictor());
+
+      getResourceManager().addResourceListener(ResourceType.HEAP_MEMORY, getHeapEvictor());
+      
+      /*
+       * Only bother creating an off-heap evictor if we have off-heap memory enabled.
+       */
+      if(null != getOffHeapStore()) {
+        getResourceManager().addResourceListener(ResourceType.OFFHEAP_MEMORY, getOffHeapEvictor());
+      }
+      
+      FunctionService.registerFunction(new HDFSLastCompactionTimeFunction());
+      FunctionService.registerFunction(new HDFSForceCompactionFunction());
+      FunctionService.registerFunction(new HDFSFlushQueueFunction());
+    startRedisServer();
+    
+  
+  private void startRedisServer() {
+    int port = system.getConfig().getRedisPort();
+    if (port != 0) {
+      String bindAddress = system.getConfig().getRedisBindAddress();
+      assert bindAddress != null;
+      if (bindAddress.equals(DistributionConfig.DEFAULT_REDIS_BIND_ADDRESS)) {
+        getLoggerI18n().info(LocalizedStrings.GemFireCacheImpl_STARTING_GEMFIRE_REDIS_SERVER_ON_PORT_0,
+            new Object[] { port });
+      } else {
+        getLoggerI18n().info(LocalizedStrings.GemFireCacheImpl_STARTING_GEMFIRE_REDIS_SERVER_ON_BIND_ADDRESS_0_PORT_1,
+            new Object[] { bindAddress, port });
+      }
+      this.redisServer = new GemFireRedisServer(bindAddress, port);
+      this.redisServer.start();
+    }
+  }
+
+  public OffHeapEvictor getOffHeapEvictor() {
+    synchronized (this.offHeapEvictorLock) {
+      stopper.checkCancelInProgress(null);
+      if (this.offHeapEvictor == null) {
+        this.offHeapEvictor = new OffHeapEvictor(this);
+      }
+      return this.offHeapEvictor;
+    }    
+  }
+  
+          
+          stopRedisServer();
+          closeHDFSStores();
+          
+          closeHDFSStores();
+  private void stopRedisServer() {
+    if (redisServer != null)
+      this.redisServer.shutdown();
+  }
+  
+    boolean stoppedBridgeServer = false;
+      stoppedBridgeServer = true;
+    }
+    if (stoppedBridgeServer) {
+      // now that all the bridge servers have stopped empty the static pool of commBuffers it might have used.
+      ServerConnection.emptyCommBufferPool();
+    final String regionPath = LocalRegion.calcFullPath(name, null);
+
+            HDFSIntegrationUtil.createAndAddAsyncQueue(regionPath, attrs, this);
+            attrs = setEvictionAttributesForLargeRegion(attrs);
-  public Region getRegion(String path) {
+  /**
+   * turn on eviction by default for HDFS regions
+   */
+  @SuppressWarnings("deprecation")
+  public <K, V> RegionAttributes<K, V> setEvictionAttributesForLargeRegion(
+      RegionAttributes<K, V> attrs) {
+    RegionAttributes<K, V> ra = attrs;
+    if (DISABLE_AUTO_EVICTION) {
+      return ra;
+    }
+    if (attrs.getDataPolicy().withHDFS()
+        || attrs.getHDFSStoreName() != null) {
+      // make the region overflow by default
+      EvictionAttributes evictionAttributes = attrs.getEvictionAttributes();
+      boolean hasNoEvictionAttrs = evictionAttributes == null
+          || evictionAttributes.getAlgorithm().isNone();
+      AttributesFactory<K, V> af = new AttributesFactory<K, V>(attrs);
+      String diskStoreName = attrs.getDiskStoreName();
+      // set the local persistent directory to be the same as that for
+      // HDFS store
+      if (attrs.getHDFSStoreName() != null) {
+        HDFSStoreImpl hdfsStore = findHDFSStore(attrs.getHDFSStoreName());
+        if (attrs.getPartitionAttributes().getLocalMaxMemory() != 0 && hdfsStore == null) {
+          // HDFS store expected to be found at this point
+          throw new IllegalStateException(
+              LocalizedStrings.HOPLOG_HDFS_STORE_NOT_FOUND
+                  .toLocalizedString(attrs.getHDFSStoreName()));
+        }
+        // if there is no disk store, use the one configured for hdfs queue
+        if (attrs.getPartitionAttributes().getLocalMaxMemory() != 0 && diskStoreName == null) {
+          diskStoreName = hdfsStore.getHDFSEventQueueAttributes().getDiskStoreName();
+        }
+      }
+      // set LRU heap eviction with overflow to disk for HDFS stores with
+      // local Oplog persistence
+      // set eviction attributes only if not set
+      if (hasNoEvictionAttrs) {
+        if (diskStoreName != null) {
+          af.setDiskStoreName(diskStoreName);
+        }
+        af.setEvictionAttributes(EvictionAttributes.createLRUHeapAttributes(
+            ObjectSizer.DEFAULT, EvictionAction.OVERFLOW_TO_DISK));
+      }
+      ra = af.create();
+    }
+    return ra;
+  }
+
+  public final Region getRegion(String path) {
+  /**
+   * @param returnDestroyedRegion
+   *          if true, okay to return a destroyed partitioned region
+   */
+  public final Region getPartitionedRegion(String path, boolean returnDestroyedRegion) {
+    stopper.checkCancelInProgress(null);
+    {
+      LocalRegion result = getRegionByPath(path);
+      // Do not waitOnInitialization() for PR
+      if (result != null) {
+        if (!(result instanceof PartitionedRegion)) {
+          return null;
+        } else {
+          return result;
+        }
+      }
+    }
+ 
+    String[] pathParts = parsePath(path);
+    LocalRegion root;
+    LogWriterI18n logger = getLoggerI18n();
+    synchronized (this.rootRegions) {
+      root = (LocalRegion) this.rootRegions.get(pathParts[0]);
+      if (root == null) {
+        if (logger.fineEnabled()) {
+          logger.fine("GemFireCache.getRegion, no region found for " + pathParts[0]);
+        }
+        stopper.checkCancelInProgress(null);
+        return null;
+      }
+      if (!returnDestroyedRegion && root.isDestroyed()) {
+        stopper.checkCancelInProgress(null);
+        return null;
+      }
+    }
+    if (logger.fineEnabled()) {
+      logger.fine("GemFireCache.getPartitionedRegion, calling getSubregion on root(" + pathParts[0] + "): " + pathParts[1]);
+    }
+    Region result = root.getSubregion(pathParts[1], returnDestroyedRegion);
+    if (result != null && !(result instanceof PartitionedRegion)) {
+      return null;
+    } else {
+      return result;
+    }
+  }
+
+
+  public void removeAsyncEventQueue(AsyncEventQueue asyncQueue) {
+    if (isClient()) {
+      throw new UnsupportedOperationException(
+          "operation is not supported on a client cache");
+    }
+    // first remove the gateway sender of the queue
+    if (asyncQueue instanceof AsyncEventQueueImpl) {
+      removeGatewaySender(((AsyncEventQueueImpl)asyncQueue).getSender());
+    }
+    // using gateway senders lock since async queue uses a gateway sender
+    synchronized (allGatewaySendersLock) {
+      this.allAsyncEventQueues.remove(asyncQueue);
+    }
+  }
-  
+
+  /**
+   * Wait for given sender queue to flush for given timeout.
+   * 
+   * @param id
+   *          ID of GatewaySender or AsyncEventQueue
+   * @param isAsyncListener
+   *          true if this is for an AsyncEventQueue and false if for a
+   *          GatewaySender
+   * @param maxWaitTime
+   *          maximum time to wait in seconds; zero or -ve means infinite wait
+   * 
+   * @return zero if maxWaitTime was not breached, -1 if queue could not be
+   *         found or is closed, and elapsed time if timeout was breached
+   */
+  public int waitForSenderQueueFlush(String id, boolean isAsyncListener,
+      int maxWaitTime) {
+    getCancelCriterion().checkCancelInProgress(null);
+    AbstractGatewaySender gatewaySender = null;
+    if (isAsyncListener) {
+      AsyncEventQueueImpl asyncQueue = (AsyncEventQueueImpl)
+          getAsyncEventQueue(id);
+      if (asyncQueue != null) {
+        gatewaySender = (AbstractGatewaySender) asyncQueue.getSender();
+      }
+    }
+    else {
+      gatewaySender = (AbstractGatewaySender)getGatewaySender(id);
+    }
+    RegionQueue rq;
+    final long startTime = System.currentTimeMillis();
+    long elapsedTime;
+    if (maxWaitTime <= 0) {
+      maxWaitTime = Integer.MAX_VALUE;
+    }
+    while (gatewaySender != null && gatewaySender.isRunning()
+        && (rq = gatewaySender.getQueue()) != null) {
+      if (rq.size() == 0) {
+        // return zero since it was not a timeout
+        return 0;
+      }
+      try {
+        Thread.sleep(500);
+        getCancelCriterion().checkCancelInProgress(null);
+      } catch (InterruptedException ie) {
+        Thread.currentThread().interrupt();
+        getCancelCriterion().checkCancelInProgress(ie);
+      }
+      // clear interrupted flag before retry
+      Thread.interrupted();
+      elapsedTime = System.currentTimeMillis() - startTime;
+      if (elapsedTime >= (maxWaitTime * 1000L)) {
+        // return elapsed time
+        return (int)(elapsedTime / 1000L);
+      }
+    }
+    return -1;
+  }
+
+      case PARTITION_HDFS: {
+    	  AttributesFactory af = new AttributesFactory();
+          af.setDataPolicy(DataPolicy.HDFS_PARTITION);
+          PartitionAttributesFactory paf = new PartitionAttributesFactory();
+          af.setPartitionAttributes(paf.create());
+          af.setEvictionAttributes(EvictionAttributes.createLRUHeapAttributes(null, EvictionAction.OVERFLOW_TO_DISK));
+          af.setHDFSWriteOnly(false);
+          c.setRegionAttributes(pra.toString(), af.create());
+          break;
+        }
+      case PARTITION_REDUNDANT_HDFS: {
+    	  AttributesFactory af = new AttributesFactory();
+          af.setDataPolicy(DataPolicy.HDFS_PARTITION);
+          PartitionAttributesFactory paf = new PartitionAttributesFactory();
+          paf.setRedundantCopies(1);
+          af.setPartitionAttributes(paf.create());
+          af.setEvictionAttributes(EvictionAttributes.createLRUHeapAttributes(null, EvictionAction.OVERFLOW_TO_DISK));
+          af.setHDFSWriteOnly(false);
+          c.setRegionAttributes(pra.toString(), af.create());
+          break;
+        }
+      case PARTITION_WRITEONLY_HDFS_STORE: {
+        AttributesFactory af = new AttributesFactory();
+          af.setDataPolicy(DataPolicy.HDFS_PARTITION);
+          PartitionAttributesFactory paf = new PartitionAttributesFactory();
+          af.setPartitionAttributes(paf.create());
+          af.setEvictionAttributes(EvictionAttributes.createLRUHeapAttributes(null, EvictionAction.OVERFLOW_TO_DISK));
+          af.setHDFSWriteOnly(true);
+          c.setRegionAttributes(pra.toString(), af.create());
+          break;
+        }
+      case PARTITION_REDUNDANT_WRITEONLY_HDFS_STORE: {
+        AttributesFactory af = new AttributesFactory();
+          af.setDataPolicy(DataPolicy.HDFS_PARTITION);
+          PartitionAttributesFactory paf = new PartitionAttributesFactory();
+          paf.setRedundantCopies(1);
+          af.setPartitionAttributes(paf.create());
+          af.setEvictionAttributes(EvictionAttributes.createLRUHeapAttributes(null, EvictionAction.OVERFLOW_TO_DISK));
+          af.setHDFSWriteOnly(true);
+          c.setRegionAttributes(pra.toString(), af.create());
+          break;
+        }
+  @Override
+  public HDFSStoreFactory createHDFSStoreFactory() {
+    // TODO Auto-generated method stub
+    return new HDFSStoreFactoryImpl(this);
+  }
+  
+  public HDFSStoreFactory createHDFSStoreFactory(HDFSStoreCreation creation) {
+    return new HDFSStoreFactoryImpl(this, creation);
+  }
+  public void addHDFSStore(HDFSStoreImpl hsi) {
+    HDFSStoreDirector.getInstance().addHDFSStore(hsi);
+    //TODO:HDFS Add a resource event for hdfs store creation as well 
+    // like the following disk store event
+    //system.handleResourceEvent(ResourceEvent.DISKSTORE_CREATE, dsi);
+  }
+
+  public void removeHDFSStore(HDFSStoreImpl hsi) {
+    //hsi.destroy();
+    HDFSStoreDirector.getInstance().removeHDFSStore(hsi.getName());
+    //TODO:HDFS Add a resource event for hdfs store as well 
+    // like the following disk store event
+    //system.handleResourceEvent(ResourceEvent.DISKSTORE_REMOVE, dsi);
+  }
+
+  public void closeHDFSStores() {
+    HDFSRegionDirector.reset();
+    HDFSStoreDirector.getInstance().closeHDFSStores();
+  }
+
+  
+  public HDFSStoreImpl findHDFSStore(String name) {
+    return HDFSStoreDirector.getInstance().getHDFSStore(name);
+  }
+  
+  public Collection<HDFSStoreImpl> getHDFSStores() {
+    return HDFSStoreDirector.getInstance().getAllHDFSStores();
+  }
+  
+  
-  
+
+  public MemoryAllocator getOffHeapStore() {
+    return this.getSystem().getOffHeapStore();
+  }
+
+  
+  /**
+   * get reference to LuceneService singleton
+   * @return LuceneService
+   */
+  public LuceneService getLuceneService() {
+    return this.luceneService;
+  }
