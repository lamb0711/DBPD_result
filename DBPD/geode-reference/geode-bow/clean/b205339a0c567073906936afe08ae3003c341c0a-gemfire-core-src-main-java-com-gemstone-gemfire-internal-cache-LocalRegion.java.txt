Merge branch 'feature/GEODE-8' into develop

+import static com.gemstone.gemfire.internal.offheap.annotations.OffHeapIdentifier.ENTRY_EVENT_NEW_VALUE;
+
+import com.gemstone.gemfire.LogWriter;
+import com.gemstone.gemfire.cache.hdfs.internal.HDFSBucketRegionQueue;
+import com.gemstone.gemfire.cache.hdfs.internal.HDFSIntegrationUtil;
+import com.gemstone.gemfire.cache.hdfs.internal.HoplogListenerForRegion;
+import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSRegionDirector;
+import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSRegionDirector.HdfsRegionManager;
+import com.gemstone.gemfire.internal.cache.control.InternalResourceManager.ResourceType;
+import com.gemstone.gemfire.internal.cache.control.MemoryThresholds;
+import com.gemstone.gemfire.internal.offheap.OffHeapHelper;
+import com.gemstone.gemfire.internal.offheap.SimpleMemoryAllocatorImpl;
+import com.gemstone.gemfire.internal.offheap.SimpleMemoryAllocatorImpl.Chunk;
+import com.gemstone.gemfire.internal.offheap.StoredObject;
+import com.gemstone.gemfire.internal.offheap.annotations.Released;
+import com.gemstone.gemfire.internal.offheap.annotations.Retained;
+import com.gemstone.gemfire.internal.offheap.annotations.Unretained;
-  public final AtomicBoolean heapThresholdReached = new AtomicBoolean(false);
+  public final AtomicBoolean memoryThresholdReached = new AtomicBoolean(false);
+  
+  
+  protected HdfsRegionManager hdfsManager;
+  protected HoplogListenerForRegion hoplogListener;
-  static private String calcFullPath(String regionName, LocalRegion parentRegion) {
+  static String calcFullPath(String regionName, LocalRegion parentRegion) {
+    this.offHeap = attrs.getOffHeap() || Boolean.getBoolean(myName+":OFF_HEAP");
+    if (getOffHeap()) {
+      if (cache.getOffHeapStore() == null) {
+        throw new IllegalStateException("The region " + myName + " was configured to use off heap memory but no off heap memory was configured.");
+      }
+    }
+    this.hdfsManager = initHDFSManager();
-    // Create Listener only when Heap eviction is enabled, and BucketRegion is
-    // created
+    
+  }
+
+  private HdfsRegionManager initHDFSManager() {
+    HdfsRegionManager hdfsMgr = null;
+    if (this.getHDFSStoreName() != null) {
+      this.hoplogListener = new HoplogListenerForRegion();
+      HDFSRegionDirector.getInstance().setCache(cache);
+      hdfsMgr = HDFSRegionDirector.getInstance().manageRegion(this, 
+          this.getHDFSStoreName(), hoplogListener);
+    }
+    return hdfsMgr;
+	if ((internalRegionArgs.isReadWriteHDFSRegion()) && this.diskRegion != null) {
+      this.diskRegion.setEntriesMapIncompatible(true);
+    }
+        String regionPath = calcFullPath(subregionName, this);
+            // create the async queue for HDFS if required. 
+            HDFSIntegrationUtil.createAndAddAsyncQueue(regionPath,
+                regionAttributes, this.cache);
+            regionAttributes = cache.setEvictionAttributesForLargeRegion(
+                regionAttributes);
-              if(pr.isShadowPR()) {
-                newRegion = new BucketRegionQueue(subregionName, regionAttributes,
-                    this, this.cache, internalRegionArgs);
-              }else {
+              if (pr.isShadowPR()) {
+                if (!pr.isShadowPRForHDFS()) {
+                    newRegion = new BucketRegionQueue(subregionName, regionAttributes,
+                      this, this.cache, internalRegionArgs);
+                }
+                else {
+                   newRegion = new HDFSBucketRegionQueue(subregionName, regionAttributes,
+                      this, this.cache, internalRegionArgs);
+                }
+                
+              } else {
-        //register the region with resource manager to get heap events
+        //register the region with resource manager to get memory events
-            cache.getResourceManager().addResourceListener(newRegion);
+            cache.getResourceManager().addResourceListener(ResourceType.MEMORY, newRegion);
+            
+            if (!newRegion.getOffHeap()) {
+              newRegion.initialCriticalMembers(cache.getResourceManager().getHeapMonitor().getState().isCritical(), cache
+                  .getResourceAdvisor().adviseCritialMembers());
+            } else {
+              newRegion.initialCriticalMembers(cache.getResourceManager().getHeapMonitor().getState().isCritical()
+                  || cache.getResourceManager().getOffHeapMonitor().getState().isCritical(), cache.getResourceAdvisor()
+                  .adviseCritialMembers());
+            }
+    // TODO OFFHEAP: validatedCreate calls freeOffHeapResources
-    if (event.getEventId() == null && generateEventID()) {
-      event.setNewEventId(cache.getDistributedSystem());
-    }
-    //Fix for 42448 - Only make create with null a local invalidate for
-    //normal regions. Otherwise, it will become a distributed invalidate.
-    if(getDataPolicy() == DataPolicy.NORMAL) {
-      event.setLocalInvalid(true);
-    }
-    discoverJTA();
-    if (!basicPut(event,
-                  true,  // ifNew
-                  false, // ifOld
-                  null,  // expectedOldValue
-                  true // requireOldValue TODO txMerge why is oldValue required for create? I think so that the EntryExistsException will have it.
-                  )) {
-      throw new EntryExistsException(event.getKey().toString(),
-          event.getOldValue());
-    }
-    else {
-      if (! getDataView().isDeferredStats()) {
-        getCachePerfStats().endPut(startPut, false);
+    try {
+      if (event.getEventId() == null && generateEventID()) {
+        event.setNewEventId(cache.getDistributedSystem());
+      assert event.isFetchFromHDFS() : "validatedPut() should have been called";
+      // Fix for 42448 - Only make create with null a local invalidate for
+      // normal regions. Otherwise, it will become a distributed invalidate.
+      if (getDataPolicy() == DataPolicy.NORMAL) {
+        event.setLocalInvalid(true);
+      }
+      discoverJTA();
+      if (!basicPut(event, true, // ifNew
+          false, // ifOld
+          null, // expectedOldValue
+          true // requireOldValue TODO txMerge why is oldValue required for
+               // create? I think so that the EntryExistsException will have it.
+      )) {
+        throw new EntryExistsException(event.getKey().toString(),
+            event.getOldValue());
+      } else {
+        if (!getDataView().isDeferredStats()) {
+          getCachePerfStats().endPut(startPut, false);
+        }
+      }
+    } finally {
+
+      event.release();
+
-    return new EntryEventImpl(this, Operation.CREATE, key,
+    return EntryEventImpl.create(this, Operation.CREATE, key,
-      return validatedDestroy(key, event);
+    return validatedDestroy(key, event);
+    // TODO OFFHEAP: validatedDestroy calls freeOffHeapResources
-  {
-    if (event.getEventId() == null && generateEventID()) {
-      event.setNewEventId(cache.getDistributedSystem());
+ {
+    try {
+      if (event.getEventId() == null && generateEventID()) {
+        event.setNewEventId(cache.getDistributedSystem());
+      }
+      basicDestroy(event, true, // cacheWrite
+          null); // expectedOldValue
+      if (event.isOldValueOffHeap()) {
+        return null;
+      } else {
+        return handleNotAvailable(event.getOldValue());
+      }
+    } finally {
+      event.release();
-    basicDestroy(event,
-                 true,  // cacheWrite
-                 null); // expectedOldValue
-    return handleNotAvailable(event.getOldValue());
-    return new EntryEventImpl(this, Operation.DESTROY, key,
+    return EntryEventImpl.create(this, Operation.DESTROY, key,
+   * @param retainResult if true then the result may be a retained off-heap reference
-  public final Object getDeserializedValue(final KeyInfo keyInfo, final boolean updateStats, boolean disableCopyOnRead, boolean preferCD, EntryEventImpl clientEvent, boolean returnTombstones) {
+  public final Object getDeserializedValue(RegionEntry re, final KeyInfo keyInfo, final boolean updateStats, boolean disableCopyOnRead, 
+  boolean preferCD, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS, boolean retainResult) {
-      RegionEntry re = this.entries.getEntry(keyInfo.getKey());
+      if (re == null) {
+        if (allowReadFromHDFS) {
+          re = this.entries.getEntry(keyInfo.getKey());
+        } else {
+          re = this.entries.getOperationalEntryInVM(keyInfo.getKey());
+        }
+      }
-          value = getDeserialized(re, updateStats, disableCopyOnRead, preferCD);
+          value = getDeserialized(re, updateStats, disableCopyOnRead, preferCD, retainResult);
-        value = getDeserialized(re, updateStats, disableCopyOnRead, preferCD);
+        value = getDeserialized(re, updateStats, disableCopyOnRead, preferCD, retainResult);
+   * @param retainResult if true then the result may be a retained off-heap reference
-  protected final Object getDeserialized(RegionEntry re, boolean updateStats, boolean disableCopyOnRead, boolean preferCD) {
+  @Retained
+  protected final Object getDeserialized(RegionEntry re, boolean updateStats, boolean disableCopyOnRead, boolean preferCD, boolean retainResult) {
+    assert !retainResult || preferCD;
-      Object v = null;
+      @Retained Object v = null;
-         v = re.getValue(this); // OFFHEAP: incrc, deserialize, decrc TODO: optimize if preferCD but need to track down when to decrc in that case
+        if (retainResult) {
+          v = re.getValueRetain(this);
+        } else {
+          v = re.getValue(this);
+        }
-                v = ((CachedDeserializable)v).getDeserializedForReading();
+              v = ((CachedDeserializable)v).getDeserializedForReading();
-    Object result = get(key, aCallbackArgument, generateCallbacks, false, false, null, clientEvent, false);
+    Object result = get(key, aCallbackArgument, generateCallbacks, false, false, null, clientEvent, false, true/*allowReadFromHDFS*/);
-	      ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones) throws TimeoutException, CacheLoaderException {
+	      ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) throws TimeoutException, CacheLoaderException {
-		      generateCallbacks, disableCopyOnRead, preferCD,requestingClient, clientEvent, returnTombstones, false);
+		      generateCallbacks, disableCopyOnRead, preferCD,requestingClient, clientEvent, returnTombstones, false, allowReadFromHDFS, false);
+   * The result of this operation may be an off-heap reference that the caller must release
+   */
+  @Retained
+  public Object getRetained(Object key, Object aCallbackArgument,
+      boolean generateCallbacks, boolean disableCopyOnRead,
+      ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones) throws TimeoutException, CacheLoaderException {
+    return getRetained(key, aCallbackArgument,
+              generateCallbacks, disableCopyOnRead, requestingClient, clientEvent, returnTombstones, false);
+  }
+
+  /**
+   * The result of this operation may be an off-heap reference that the caller must release.
+  @Retained
+  public Object getRetained(Object key, Object aCallbackArgument,
+      boolean generateCallbacks, boolean disableCopyOnRead,
+      ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones, boolean opScopeIsLocal) throws TimeoutException, CacheLoaderException {
+    return get(key, aCallbackArgument, generateCallbacks, disableCopyOnRead, true, requestingClient, clientEvent, returnTombstones, opScopeIsLocal, true, false);
+  }
+  /**
+   * @param opScopeIsLocal if true then just check local storage for a value; if false then try to find the value if it is not local
+   * @param retainResult if true then the result may be a retained off-heap reference.
+   */
-      ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones, boolean opScopeIsLocal) throws TimeoutException, CacheLoaderException
+      ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones, 
+	  boolean opScopeIsLocal, boolean allowReadFromHDFS, boolean retainResult) throws TimeoutException, CacheLoaderException
+    assert !retainResult || preferCD;
-      Object value = getDataView().getDeserializedValue(getKeyInfo(key), this, true, disableCopyOnRead, preferCD, clientEvent, returnTombstones);
+      KeyInfo keyInfo = getKeyInfo(key, aCallbackArgument);
+      Object value = getDataView().getDeserializedValue(keyInfo, this, true, disableCopyOnRead, preferCD, clientEvent, returnTombstones, allowReadFromHDFS, retainResult);
-            && (getScope().isDistributed()
+            && ((getScope().isDistributed() && !isHDFSRegion())
-          value = getDataView().findObject(getKeyInfo(key, aCallbackArgument),
+          // TODO OFFHEAP OPTIMIZE: findObject can be enhanced to use the retainResult flag
+          value = getDataView().findObject(keyInfo,
-              preferCD, requestingClient, clientEvent, returnTombstones);
+              preferCD, requestingClient, clientEvent, returnTombstones, false/*allowReadFromHDFS*/);
-    if (re == null && !isTX()) {
+    if (re == null && !isTX() && !isHDFSRegion()) {
+   * @return true if this region has been configured for HDFS persistence
+   */
+  public boolean isHDFSRegion() {
+    return false;
+  }
+
+  /**
+   * @return true if this region is configured to read and write data from HDFS
+   */
+  public boolean isHDFSReadWriteRegion() {
+    return false;
+  }
+
+  /**
+   * @return true if this region is configured to only write to HDFS
+   */
+  protected boolean isHDFSWriteOnly() {
+    return false;
+  }
+
+  /**
+   * FOR TESTING ONLY
+   */
+  public HoplogListenerForRegion getHoplogListener() {
+    return hoplogListener;
+  }
+  
+  /**
+   * FOR TESTING ONLY
+   */
+  public HdfsRegionManager getHdfsRegionManager() {
+    return hdfsManager;
+  }
+  
+  /**
+  @Retained
-      EntryEventImpl clientEvent, boolean returnTombstones) 
+      EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) 
+    final Object key = keyInfo.getKey();
+  
-    Object result = null;
+    @Retained Object result = null;
+           
+          //For sqlf since the deserialized value is nothing but chunk
+          // before returning the found value increase its use count
+          if(GemFireCacheImpl.sqlfSystem() && result instanceof Chunk) {
+            if(!((Chunk)result).retain()) {
+              return null;
+            }
+          }
-      localValue = getDeserializedValue(keyInfo, isCreate, disableCopyOnRead, preferCD, clientEvent, false);
+      localValue = getDeserializedValue(null, keyInfo, isCreate, disableCopyOnRead, preferCD, clientEvent, false, false/*allowReadFromHDFS*/, false);
+      // TODO verify that this method is not used for PR or BR and hence allowReadFromHDFS does not matter
-          localValue, disableCopyOnRead, preferCD, null, clientEvent, returnTombstones);
+          localValue, disableCopyOnRead, preferCD, null, clientEvent, returnTombstones, false/*allowReadFromHDFS*/);
+      && ! getOffHeap()
-      return validatedPut(event, startPut);
+     //Since Sqlfire directly calls validatedPut, the freeing is done in
+    // validatedPut
+     return validatedPut(event, startPut);
+     // TODO OFFHEAP: validatedPut calls freeOffHeapResources
+    
-    if (event.getEventId() == null && generateEventID()) {
-      event.setNewEventId(cache.getDistributedSystem());
-    }
-    Object oldValue = null;
-    //Sqlf changes begin
-    // see #40294.
-    
-    //Rahul: this has to be an update.
-    // so executing it as an update.
-    boolean forceUpdateForDelta = event.hasDelta();
-    // Sqlf Changes end.
-    if (basicPut(event,
-                   false, // ifNew
-                   forceUpdateForDelta, // ifOld
-                   null,  // expectedOldValue
-                   false // requireOldValue
-                   )) {
-      // don't copy it to heap just to return from put.
-      // TODO: come up with a better way to do this.
-      oldValue = event.getOldValue();
-      if (!getDataView().isDeferredStats()) {
-        getCachePerfStats().endPut(startPut, false);
+    try {
+      if (event.getEventId() == null && generateEventID()) {
+        event.setNewEventId(cache.getDistributedSystem());
+      Object oldValue = null;
+      // Sqlf changes begin
+      // see #40294.
+
+      // Rahul: this has to be an update.
+      // so executing it as an update.
+      boolean forceUpdateForDelta = event.hasDelta();
+      // Sqlf Changes end.
+      if (basicPut(event, false, // ifNew
+          forceUpdateForDelta, // ifOld
+          null, // expectedOldValue
+          false // requireOldValue
+      )) {
+        if (!event.isOldValueOffHeap()) {
+          // don't copy it to heap just to return from put.
+          // TODO: come up with a better way to do this.
+          oldValue = event.getOldValue();
+        }
+        if (!getDataView().isDeferredStats()) {
+          getCachePerfStats().endPut(startPut, false);
+        }
+      }
+      return handleNotAvailable(oldValue);
+    } finally {
+      event.release();
-    return handleNotAvailable(oldValue);
-    final EntryEventImpl event = new EntryEventImpl(
+    final EntryEventImpl event = EntryEventImpl.create(
+    boolean eventReturned = false;
+    try {
+    eventReturned = true;
+    } finally {
+      if (!eventReturned) event.release();
+    }
-
+  /**
+   * Creates an EntryEventImpl that is optimized to not fetch data from HDFS.
+   * This is meant to be used by PUT dml from GemFireXD.
+   */
+  public final EntryEventImpl newPutEntryEvent(Object key, Object value,
+      Object aCallbackArgument) {
+    EntryEventImpl ev = newUpdateEntryEvent(key, value, aCallbackArgument);
+    ev.setFetchFromHDFS(false);
+    ev.setPutDML(true);
+    return ev;
+  }
-  /**
+  protected boolean includeHDFSResults() {
+    return isUsedForPartitionedRegionBucket() 
+        && isHDFSReadWriteRegion() 
+        && getPartitionedRegion().includeHDFSResults();
+  }
+  
+
+  /** a fast estimate of total number of entries locally in the region */
+  public long getEstimatedLocalSize() {
+    RegionMap rm;
+    if (!this.isDestroyed) {
+      long size;
+      if (isHDFSReadWriteRegion() && this.initialized) {
+        // this size is not used by HDFS region iterators
+        // fixes bug 49239
+        return 0;
+      }
+      // if region has not been initialized yet, then get the estimate from
+      // disk region's recovery map if available
+      if (!this.initialized && this.diskRegion != null
+          && (rm = this.diskRegion.getRecoveredEntryMap()) != null
+          && (size = rm.size()) > 0) {
+        return size;
+      }
+      if ((rm = getRegionMap()) != null) {
+        return rm.size();
+      }
+    }
+    return 0;
+  }
+    /**
+  /**
+   * Returns true if this region is or has been closed or destroyed.
+   * Note that unlike {@link #isDestroyed()} this method will not
+   * return true if the cache is closing but has not yet started closing
+   * this region.
+   */
+  public boolean isThisRegionBeingClosedOrDestroyed() {
+    return this.isDestroyed;
+  }
+  
+        SimpleMemoryAllocatorImpl.skipRefCountTracking();
-        // No need to deserialize because of Bruce's fix in r30960 for bug 42162. See bug 42732.
-
+        if (val instanceof StoredObject) {
+          OffHeapHelper.release(val);
+          SimpleMemoryAllocatorImpl.unskipRefCountTracking();
+          return true;
+        }
+        SimpleMemoryAllocatorImpl.unskipRefCountTracking();
+        // No need to to check CachedDeserializable because of Bruce's fix in r30960 for bug 42162. See bug 42732.
-  public int entryCount()
-  {
+  public final int entryCount() {
+    return entryCount(null);
+  }
+
+  public int entryCount(Set<Integer> buckets) {
+    return entryCount(buckets, false);
+  }
+
+  protected int entryCount( Set<Integer> buckets, boolean estimate) {
+    assert buckets == null: "unexpected buckets " + buckets + " for region "
+        + toString();
+
+  public int entryCountEstimate(final TXStateInterface tx, Set<Integer> buckets, boolean entryCountEstimate) {
+    return entryCount(buckets, entryCountEstimate);
+  }
+
+	if (includeHDFSResults()) {
+      return result;
+    }
-    EntryEventImpl event = new EntryEventImpl(
+    EntryEventImpl event = EntryEventImpl.create(
+    try {
+    } finally {
+      event.release();
+    }
-    EntryEventImpl event = new EntryEventImpl(
+    EntryEventImpl event = EntryEventImpl.create(
+    } finally {
+      event.release();
-    EntryEventImpl event = new EntryEventImpl(
+    EntryEventImpl event = EntryEventImpl.create(
+    try {
+    } finally {
+      event.release();
+    }
-        cache.getResourceManager().addResourceListener(this);
+        cache.getResourceManager().addResourceListener(ResourceType.MEMORY, this);
+    Set<Index> prIndexes = new HashSet<Index>();
-                icd.getIndexImportString(), externalContext, icd.getPartitionedIndex(), !isOverflowToDisk));           
+                icd.getIndexImportString(), externalContext, icd.getPartitionedIndex(), !isOverflowToDisk));
+            prIndexes.add(icd.getPartitionedIndex());
+      //due to bug #52096, the pr index populate flags were not being set 
+      //we should revisit and clean up the index creation code paths
+      this.indexManager.setPopulateFlagForIndexes(prIndexes);
-      this.entries.clear(null);
+      closeEntries();
+  
+  public void closeEntries() {
+    this.entries.close();
+  }
+  public Set<VersionSource> clearEntries(RegionVersionVector rvv) {
+    return this.entries.clear(rvv);
+  }
+   * This method should be called when the caller cannot locate an entry and that condition
+   * is unexpected.  This will first double check the cache and region state before throwing
+   * an EntryNotFoundException.  EntryNotFoundException should be a last resort exception.
+   * 
+   * @param entryKey the missing entry's key.
+   */
+  public void checkEntryNotFound(Object entryKey) {
+    checkReadiness();
+    // Localized string for partitioned region is generic enough for general use
+    throw new EntryNotFoundException(LocalizedStrings.PartitionedRegion_ENTRY_NOT_FOUND_FOR_KEY_0.toLocalizedString(entryKey));    
+  }
+  
+  /**
-   * @see DistributedRegion#findObjectInSystem(KeyInfo, boolean, TXStateInterface, boolean, Object, boolean, boolean, ClientProxyMembershipID, EntryEventImpl, boolean)
+   * @see DistributedRegion#findObjectInSystem(KeyInfo, boolean, TXStateInterface, boolean, Object, boolean, boolean, ClientProxyMembershipID, EntryEventImpl, boolean, boolean )
-      EntryEventImpl clientEvent, boolean returnTombstones)
+      EntryEventImpl clientEvent, boolean returnTombstones,  boolean allowReadFromHDFS)
-        value = mySRP.get(key, aCallbackArgument, holder);
-        fromServer = value != null;
+        try {
+          value = mySRP.get(key, aCallbackArgument, holder);
+          fromServer = value != null;
+        } finally {
+          holder.release();
+        }
-    if (!fromServer) {
+    if (!fromServer || value == Token.TOMBSTONE) {
+          fromServer = false;
+    // don't allow tombstones into a client cache if it doesn't
+    // have concurrency checks enabled
+    if (fromServer && 
+        value == Token.TOMBSTONE && !this.concurrencyChecksEnabled) {
+      value = null;
+    }
-    if (value != null && !isHeapThresholdReachedForLoad()) {
+    if (value != null && !isMemoryThresholdReachedForLoad()) {
-        = new EntryEventImpl(this, op, key, value, aCallbackArgument,
+        = EntryEventImpl.create(this, op, key, value, aCallbackArgument,
+      try {
-          if (fromServer && (event.getNewValue() == Token.TOMBSTONE)) {
+          if (fromServer && (event.getRawNewValue() == Token.TOMBSTONE)) {
+      } finally {     
+          event.release();        
+      }
-  protected boolean isHeapThresholdReachedForLoad() {
-    return this.heapThresholdReached.get();
+  protected boolean isMemoryThresholdReachedForLoad() {
+    return this.memoryThresholdReached.get();
+    @Unretained(ENTRY_EVENT_NEW_VALUE)
+        // serverPut is called by cacheWriteBeforePut so the new value will not yet be off-heap
+        // TODO OFFHEAP: verify that the above assertion is true
+  @Retained
-      throw new EntryNotFoundException(keyInfo.getKey().toString());
+      checkEntryNotFound(keyInfo.getKey());
-      throw new EntryNotFoundException(keyInfo.getKey().toString());
+      checkEntryNotFound(keyInfo.getKey());
-    EntryEventImpl event = new EntryEventImpl(this, Operation.LOCAL_DESTROY,
+    EntryEventImpl event = EntryEventImpl.create(this, Operation.LOCAL_DESTROY,
+    } finally {
+      event.release();
-            localDestroyNoCallbacks(currentKey);
-              //TODO - shouldn't we skip this value and not put it in the region
-              //then? I think we're putting an Exception object in the region.
+              localDestroyNoCallbacks(currentKey);
+              continue;
+              if (tag == null) { // no version checks
+                localDestroyNoCallbacks(currentKey);
+              }
+  @Retained
+  if (this.hdfsStoreName != null) {
+    notifyGatewaySender(eventType, event);
+    }
-    final EntryEventImpl event = new EntryEventImpl(this, Operation.CREATE, key,
+    final EntryEventImpl event = EntryEventImpl.create(this, Operation.CREATE, key,
+    try {
+    } finally {
+      event.release();
+    }
-    final EntryEventImpl event = new EntryEventImpl(this, Operation.UPDATE, key,
+    final EntryEventImpl event = EntryEventImpl.create(this, Operation.UPDATE, key,
+    try {
+    } finally {
+      event.release();
+    }
-        new EntryEventImpl(this,
+        EntryEventImpl.create(this,
+      try {
+      } finally {
+        event.release();
+      }
-        new EntryEventImpl(this,
+        EntryEventImpl.create(this,
+      try {
+      } finally {
+        event.release();
+      }
-    final EntryEventImpl event = new EntryEventImpl(this, Operation.DESTROY, key,
+    final EntryEventImpl event = EntryEventImpl.create(this, Operation.DESTROY, key,
+    try {
+    } finally {
+      event.release();
+    }
-    final EntryEventImpl event = new EntryEventImpl(this, Operation.INVALIDATE, key,
+    final EntryEventImpl event = EntryEventImpl.create(this, Operation.INVALIDATE, key,
+    try {
+    } finally {
+      event.release();
+    }
-    EntryEventImpl event = new EntryEventImpl(this, Operation.UPDATE_VERSION_STAMP, key,
+    EntryEventImpl event = EntryEventImpl.create(this, Operation.UPDATE_VERSION_STAMP, key,
+      event.release();
-    if (!InternalResourceManager.isLowMemoryExceptionDisabled()) {
+    if (!MemoryThresholds.isLowMemoryExceptionDisabled()) {
-        return event.getOldValue() == null;
+        return !event.hasOldValue();
-    if (heapThresholdReached.get()) {
-      Set<DistributedMember> htrm = getHeapThresholdReachedMembers();
+    if (memoryThresholdReached.get()) {
+      Set<DistributedMember> htrm = getMemoryThresholdReachedMembers();
-      InternalResourceManager.getInternalResourceManager(cache).triggerMemoryEvent();
-      
+      InternalResourceManager.getInternalResourceManager(cache).getHeapMonitor().updateStateAndSendEvent();
+
+    try {
+    } finally {
+      updateTimeStampEvent.release();
+    }
+   public VersionTag findVersionTagForGatewayEvent(EventID eventId) {
+     if (this.eventTracker != null) {
+       return this.eventTracker.findVersionTagForGateway(eventId);
+     }
+     return null;
+   }
+   
+  /**
+   * Returns true if this region notifies multiple serial gateways.
+   */
+  public boolean notifiesMultipleSerialGateways() {
+    if (isPdxTypesRegion()) {
+      return false;
+    }
+    int serialGatewayCount = 0;
+    Set<String> allGatewaySenderIds = getAllGatewaySenderIds();
+    if (!allGatewaySenderIds.isEmpty()) {
+      List<Integer> allRemoteDSIds = getRemoteDsIds(allGatewaySenderIds);
+      if (allRemoteDSIds != null) {
+        for (GatewaySender sender : getCache().getAllGatewaySenders()) {
+          if (allGatewaySenderIds.contains(sender.getId())) {
+            if (!sender.isParallel()) {
+              serialGatewayCount++;
+              if (serialGatewayCount > 1) {
+                return true;
+              }
+            }
+          }
+        }
+      }
+    }
+    return false;
+  }
+  
+      final LogWriter logWriter = cache.getLogger();
+      float evictionPercentage = DEFAULT_HEAPLRU_EVICTION_HEAP_PERCENTAGE;
-      // we make sure that the EvictionHeapPercentage is enabled.
+      // we make sure that the eviction percentage is enabled.
-      if (!rm.hasEvictionThreshold()) { // fix for bug 42130
-        float chp = rm.getCriticalHeapPercentage();
-        if (chp > 0.0f) {
-          if (chp >= 10.0f) {
-            chp -= 5.0f;
+      if (!getOffHeap()) {
+        if (!rm.getHeapMonitor().hasEvictionThreshold()) {
+          float criticalPercentage = rm.getCriticalHeapPercentage();
+          if (criticalPercentage > 0.0f) {
+            if (criticalPercentage >= 10.f) {
+              evictionPercentage = criticalPercentage - 5.0f;
+            } else {
+              evictionPercentage = criticalPercentage;
+            }
-          rm.setEvictionHeapPercentage(chp);
-        } else {
-          rm.setEvictionHeapPercentage(DEFAULT_HEAPLRU_EVICTION_HEAP_PERCENTAGE);
+          rm.setEvictionHeapPercentage(evictionPercentage);
+          if (logWriter.fineEnabled()) {
+            logWriter.fine("Enabled heap eviction at " + evictionPercentage + " percent for LRU region");
+          }
+        }
+      } else {
+        if (!rm.getOffHeapMonitor().hasEvictionThreshold()) {
+          float criticalPercentage = rm.getCriticalOffHeapPercentage();
+          if (criticalPercentage > 0.0f) {
+            if (criticalPercentage >= 10.f) {
+              evictionPercentage = criticalPercentage - 5.0f;
+            } else {
+              evictionPercentage = criticalPercentage;
+            }
+          }
+          rm.setEvictionOffHeapPercentage(evictionPercentage);
+          if (logWriter.fineEnabled()) {
+            logWriter.fine("Enabled off-heap eviction at " + evictionPercentage + " percent for LRU region");
+          }
-    
+      
-    EntryEventImpl event = new EntryEventImpl(
+    EntryEventImpl event = EntryEventImpl.create(
+    event.setFetchFromHDFS(false);
+    return event;
+  }
+    protected EntryEventImpl generateCustomEvictDestroyEvent(final Object key) {
+    EntryEventImpl event =  EntryEventImpl.create(
+        this, Operation.CUSTOM_EVICT_DESTROY, key, null/* newValue */,
+        null, false, getMyId());
+    
+    // Fix for bug#36963
+    if (generateEventID()) {
+      event.setNewEventId(cache.getDistributedSystem());
+    }
+    event.setFetchFromHDFS(false);
+    } finally {
+      event.release();
-
+        
+          final EventDispatcher ed = new EventDispatcher(event, op);
-            this.cache.getEventThreadPool().execute(
-                new EventDispatcher(event, op));
+            this.cache.getEventThreadPool().execute(ed);
+            ed.release();
+    // TODO OFFHEAP MERGE: to fix 49905 asif commented out isDestroyed being set.
+    // But in xd it was set after closeEntries was called.
+    // Here it is set before and it fixed 49555.
-    this.entries.clear(null); //fixes bug 41333
+    closeEntries(); //fixes bug 41333
-    Iterator it = this.entries.regionEntries().iterator();
+    Iterator it = this.entries.regionEntriesInVM().iterator();
-        EntryEventImpl event = new EntryEventImpl(
+        EntryEventImpl event = EntryEventImpl.create(
+        try {
+        } finally {
+          event.release();
+        }
-                               getCompressor());
+                               getCompressor(), getOffHeap());
-      Object value = this.region.getDeserialized(getCheckedRegionEntry(), false, false, false);
+      Object value = this.region.getDeserialized(getCheckedRegionEntry(), false, false, false, false);
-  public final TXStateInterface getTXState() {
+  public final TXStateProxy getTXState() {
+    
-    }
+    }    
+                
+      if (LocalRegion.this.offHeap && event instanceof EntryEventImpl) {
+        // Make a copy that has its own off-heap refcount so fix bug 48837
+        event = new EntryEventImpl( (EntryEventImpl)event);   
+      }
-      dispatchEvent(LocalRegion.this, this.event, this.op);
+      try {
+        dispatchEvent(LocalRegion.this, this.event, this.op);
+      }finally {
+        this.release();
+      }
+    }
+    
+    public void release() {
+      if (LocalRegion.this.offHeap && this.event instanceof EntryEventImpl) {
+        ((EntryEventImpl)this.event).release();
+      }      
+  
+  
-        Object value = getDeserialized(this.basicGetEntry(), false, ignoreCopyOnRead, false);
+        Object value = getDeserialized(this.basicGetEntry(), false, ignoreCopyOnRead, false, false);
+   * returns an estimate of the number of entries in this region. This method
+   * should be prefered over size() for hdfs regions where an accurate size is
+   * not needed. This method is not supported on a client
+   * 
+   * @return the estimated size of this region
+   */
+  public int sizeEstimate() {
+    boolean isClient = this.imageState.isClient();
+    if (isClient) {
+      throw new UnsupportedOperationException(
+          "Method not supported on a client");
+    }
+    return entryCount(null, true);
+  }
+
+  /**
-      Set<VersionSource> remainingIDs = this.entries.clear(rvv);
+      Set<VersionSource> remainingIDs = clearEntries(rvv);
+    clearHDFSData();
+    
+  /**Clear HDFS data, if present */
+  protected void clearHDFSData() {
+    //do nothing, clear is implemented for subclasses like BucketRegion.
+  }
+
+    final boolean isTraceEnabled = logger.isTraceEnabled();
-          EntryEventImpl event = new EntryEventImpl(
+          EntryEventImpl event = EntryEventImpl.create(
+          try {
-            if (isDebugEnabled) {
-              logger.debug("Added remote result for getAll request: {}, {}", key, value);
+            if (isTraceEnabled) {
+              logger.trace("Added remote result for getAll request: {}, {}", key, value);
+          } finally {
+            event.release();
+          }
-      if (!InternalResourceManager.isLowMemoryExceptionDisabled()) {
+      if (!MemoryThresholds.isLowMemoryExceptionDisabled()) {
-    final EntryEventImpl event = new EntryEventImpl(this, Operation.PUTALL_CREATE, null,
+    final EntryEventImpl event = EntryEventImpl.create(this, Operation.PUTALL_CREATE, null,
+    try {
+    try {
+    } finally {
+      putAllOp.freeOffHeapResources();
+    }
+    } finally {
+      event.release();
+    }
-    final EntryEventImpl event = new EntryEventImpl(this, Operation.REMOVEALL_DESTROY, null,
+    final EntryEventImpl event = EntryEventImpl.create(this, Operation.REMOVEALL_DESTROY, null,
+    try {
+    try {
+    } finally {
+      removeAllOp.freeOffHeapResources();
+    }
+    } finally {
+      event.release();
+    }
-    EntryEventImpl event = new EntryEventImpl(this, Operation.PUTALL_CREATE,
+    EntryEventImpl event = EntryEventImpl.create(this, Operation.PUTALL_CREATE,
+    try {
+    try {
+    } finally {
+      putAllOp.freeOffHeapResources();
+    }
+    } finally {
+      event.release();
+    }
+      try {
+      } finally {
+        putAllOp.getBaseEvent().release();
+        putAllOp.freeOffHeapResources();
+      }
+      try {
+      } finally {
+        op.getBaseEvent().release();
+        op.freeOffHeapResources();
+      }
+                try {
+                } finally {
+                  tagHolder.release();
+                }
+      putAllOp.getBaseEvent().release();
+      putAllOp.freeOffHeapResources();
+      removeAllOp.getBaseEvent().release();
+      removeAllOp.freeOffHeapResources();
-    final EntryEventImpl event = new EntryEventImpl(this,
+    final EntryEventImpl event = EntryEventImpl.create(this,
-    return new DistributedPutAllOperation(event, map.size(), false);
+
+    event.disallowOffHeapValues();
+    DistributedPutAllOperation dpao = new DistributedPutAllOperation(event, map.size(), false);
+    return dpao;
+    public final DistributedPutAllOperation newPutAllForPUTDmlOperation(Map<?, ?> map, Object callbackArg) {
+    DistributedPutAllOperation dpao = newPutAllOperation(map, callbackArg);
+    dpao.getEvent().setFetchFromHDFS(false);
+    dpao.getEvent().setPutDML(true);
+    return dpao;
+  }
+
-    final EntryEventImpl event = new EntryEventImpl(this, Operation.REMOVEALL_DESTROY, null,
+    final EntryEventImpl event = EntryEventImpl.create(this, Operation.REMOVEALL_DESTROY, null,
+    event.disallowOffHeapValues();
+
+    try {
+	event.setFetchFromHDFS(putallOp.getEvent().isFetchFromHDFS());
+    event.setPutDML(putallOp.getEvent().isPutDML());
+    
+    } finally {
+      event.release();
+    }
+    try {
+    } finally {
+      event.release();
+    }
-    if (function.optimizeForWrite() && heapThresholdReached.get() &&
-        !InternalResourceManager.isLowMemoryExceptionDisabled()) {
-      Set<DistributedMember> htrm = getHeapThresholdReachedMembers();
+    if (function.optimizeForWrite() && memoryThresholdReached.get() &&
+        !MemoryThresholds.isLowMemoryExceptionDisabled()) {
+      Set<DistributedMember> htrm = getMemoryThresholdReachedMembers();
-  public Set<DistributedMember> getHeapThresholdReachedMembers() {
+  public Set<DistributedMember> getMemoryThresholdReachedMembers() {
-    setHeapThresholdFlag(event);
+    setMemoryThresholdFlag(event);
-  protected void setHeapThresholdFlag(MemoryEvent event) {
+  protected void setMemoryThresholdFlag(MemoryEvent event) {
-      if (event.getType().isCriticalUp()) {
-        //start rejecting operations
-        heapThresholdReached.set(true);
-      } else if (event.getType().isCriticalDown() || event.getType().isCriticalDisabled()) {
-        //stop rejecting operations
-        heapThresholdReached.set(false);
+      if (event.getState().isCritical()
+          && !event.getPreviousState().isCritical()
+          && (event.getType() == ResourceType.HEAP_MEMORY || (event.getType() == ResourceType.OFFHEAP_MEMORY && getOffHeap()))) {
+        // start rejecting operations
+        memoryThresholdReached.set(true);
+      } else if (!event.getState().isCritical()
+          && event.getPreviousState().isCritical()
+          && (event.getType() == ResourceType.HEAP_MEMORY || (event.getType() == ResourceType.OFFHEAP_MEMORY && getOffHeap()))) {
+        memoryThresholdReached.set(false);
-   * called when registering using {@link InternalResourceManager#addResourceListener(ResourceListener)}.
+   * called when registering using {@link InternalResourceManager#addResourceListener(ResourceType, ResourceListener)}.
-   * @param localHeapIsCritical true if the local heap is in a critical state
-   * @param critialMembers set of members whose heaps are in a critical state
-   * @see ResourceManager#setCriticalHeapPercentage(float)
+   * @param localMemoryIsCritical true if the local memory is in a critical state
+   * @param critialMembers set of members whose memory is in a critical state
+   * @see ResourceManager#setCriticalHeapPercentage(float) and ResourceManager#setCriticalOffHeapPercentage(float)
-  public void initialCriticalMembers(boolean localHeapIsCritical,
+  public void initialCriticalMembers(boolean localMemoryIsCritical,
-    if (localHeapIsCritical) {
-      heapThresholdReached.set(true);
+    if (localMemoryIsCritical) {
+      memoryThresholdReached.set(true);
-    EntryEventImpl event = new EntryEventImpl(
+    EntryEventImpl event = EntryEventImpl.create(
+    try {
+    } finally {
+      event.release();
+    }
-     EntryEventImpl event = new EntryEventImpl(
+     EntryEventImpl event = EntryEventImpl.create(
+     } finally {
+       event.release();
-    EntryEventImpl event = new EntryEventImpl(this,
+    EntryEventImpl event = EntryEventImpl.create(this,
+    } finally {
+      event.release();
-    EntryEventImpl event = new EntryEventImpl(this,
+    EntryEventImpl event = EntryEventImpl.create(this,
+    } finally {
+      event.release();
-    EntryEventImpl event = new EntryEventImpl(this,
+    EntryEventImpl event = EntryEventImpl.create(this,
+    } finally {
+      event.release();
-    final EntryEventImpl event = new EntryEventImpl(this, Operation.PUT_IF_ABSENT, key,
+    final EntryEventImpl event = EntryEventImpl.create(this, Operation.PUT_IF_ABSENT, key,
+    try {
-    Object oldValue = event.getRawOldValue();
+    Object oldValue = event.getRawOldValueAsHeapObject();
+    } finally {
+      event.release();
+    }
-    final EntryEventImpl event = new EntryEventImpl(this, Operation.REPLACE, key,
+    final EntryEventImpl event = EntryEventImpl.create(this, Operation.REPLACE, key,
+    try {
+    } finally {
+      event.release();
+    }
-    final EntryEventImpl event = new EntryEventImpl(this, Operation.REPLACE, key,
+    final EntryEventImpl event = EntryEventImpl.create(this, Operation.REPLACE, key,
+    try {
-      Object oldValue = event.getRawOldValue();
+      Object oldValue = event.getRawOldValueAsHeapObject();
+    } finally {
+      event.release();
+    }
-    final EntryEventImpl event = new EntryEventImpl(this, Operation.REMOVE, key,
+    final EntryEventImpl event = EntryEventImpl.create(this, Operation.REMOVE, key,
+    try {
+    } finally {
+      event.release();
+    }
+  private boolean isTest = false;
+  protected static boolean simulateClearForTests = false;
+
+  private AtomicInteger countNotFoundInLocal = null; 
+  public void setIsTest() {
+    isTest = true;
+    countNotFoundInLocal = new AtomicInteger();
+  }
+  public boolean isTest() {
+    return isTest;
+  } 
+  public void incCountNotFoundInLocal() {
+    countNotFoundInLocal.incrementAndGet();
+  }
+  
+  public Integer getCountNotFoundInLocal() {
+    return countNotFoundInLocal.get();
+  }
+  /// End of Variables and methods for test Hook for HDFS ///////
+  public void forceHDFSCompaction(boolean isMajor, Integer maxWaitTime) {
+    throw new UnsupportedOperationException(
+        LocalizedStrings.HOPLOG_DOES_NOT_USE_HDFSSTORE
+            .toLocalizedString(getName()));
+  }
+
+  public void flushHDFSQueue(int maxWaitTime) {
+    throw new UnsupportedOperationException(
+        LocalizedStrings.HOPLOG_DOES_NOT_USE_HDFSSTORE
+            .toLocalizedString(getName()));
+  }
+  
+  public long lastMajorHDFSCompaction() {
+    throw new UnsupportedOperationException();
+  }
+
+  public static void simulateClearForTests(boolean flag) {
+    simulateClearForTests = flag;
+    
+  }
+  
