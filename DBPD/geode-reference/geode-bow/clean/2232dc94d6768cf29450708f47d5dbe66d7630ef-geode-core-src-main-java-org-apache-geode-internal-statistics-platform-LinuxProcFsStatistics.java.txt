Revert "GEODE-2113 Implement SSL over NIO"

Regression testing encountered some hangs when using message "chunking"
in TCPConduit.  This needs to be investigated and fixed.

This reverts commit a075b0e1a13a7a57378973bbfb7f14a63f29bf87.

-  static SpaceTokenizer tokenizer;
+  static SpaceTokenizer st;
-    tokenizer = new SpaceTokenizer();
+    st = new SpaceTokenizer();
+    st = null;
-      tokenizer.setString(line);
-      tokenizer.skipTokens(22);
-      ints[LinuxProcessStats.imageSizeINT] = (int) (tokenizer.nextTokenAsLong() / OneMeg);
-      ints[LinuxProcessStats.rssSizeINT] =
-          (int) ((tokenizer.nextTokenAsLong() * pageSize) / OneMeg);
+      st.setString(line);
+      st.skipTokens(22);
+      ints[LinuxProcessStats.imageSizeINT] = (int) (st.nextTokenAsLong() / OneMeg);
+      ints[LinuxProcessStats.rssSizeINT] = (int) ((st.nextTokenAsLong() * pageSize) / OneMeg);
-      tokenizer.releaseResources();
+      st.releaseResources();
-    if (cpuStatSingleton == null) {
-      // stats have been closed or haven't been properly initialized
-      return;
-    }
-    tokenizer.releaseResources();
+    st.releaseResources();
-      tokenizer.setString(line);
-      doubles[LinuxSystemStats.loadAverage1DOUBLE] = tokenizer.nextTokenAsDouble();
-      doubles[LinuxSystemStats.loadAverage5DOUBLE] = tokenizer.nextTokenAsDouble();
-      doubles[LinuxSystemStats.loadAverage15DOUBLE] = tokenizer.nextTokenAsDouble();
+      st.setString(line);
+      doubles[LinuxSystemStats.loadAverage1DOUBLE] = st.nextTokenAsDouble();
+      doubles[LinuxSystemStats.loadAverage5DOUBLE] = st.nextTokenAsDouble();
+      doubles[LinuxSystemStats.loadAverage15DOUBLE] = st.nextTokenAsDouble();
-      tokenizer.releaseResources();
+      st.releaseResources();
-            tokenizer.setString(line);
-            tokenizer.skipToken(); // Burn initial token
-            ints[LinuxSystemStats.physicalMemoryINT] = (int) (tokenizer.nextTokenAsLong() / 1024);
+            st.setString(line);
+            st.skipToken(); // Burn initial token
+            ints[LinuxSystemStats.physicalMemoryINT] = (int) (st.nextTokenAsLong() / 1024);
-            tokenizer.setString(line);
-            tokenizer.skipToken(); // Burn initial token
-            ints[LinuxSystemStats.freeMemoryINT] = (int) (tokenizer.nextTokenAsLong() / 1024);
+            st.setString(line);
+            st.skipToken(); // Burn initial token
+            ints[LinuxSystemStats.freeMemoryINT] = (int) (st.nextTokenAsLong() / 1024);
-            tokenizer.setString(line);
-            tokenizer.skipToken(); // Burn initial token
-            ints[LinuxSystemStats.sharedMemoryINT] = (int) (tokenizer.nextTokenAsLong() / 1024);
+            st.setString(line);
+            st.skipToken(); // Burn initial token
+            ints[LinuxSystemStats.sharedMemoryINT] = (int) (st.nextTokenAsLong() / 1024);
-            tokenizer.setString(line);
-            tokenizer.nextToken(); // Burn initial token
-            ints[LinuxSystemStats.bufferMemoryINT] = (int) (tokenizer.nextTokenAsLong() / 1024);
+            st.setString(line);
+            st.nextToken(); // Burn initial token
+            ints[LinuxSystemStats.bufferMemoryINT] = (int) (st.nextTokenAsLong() / 1024);
-            tokenizer.setString(line);
-            tokenizer.skipToken(); // Burn initial token
-            ints[LinuxSystemStats.allocatedSwapINT] = (int) (tokenizer.nextTokenAsLong() / 1024);
+            st.setString(line);
+            st.skipToken(); // Burn initial token
+            ints[LinuxSystemStats.allocatedSwapINT] = (int) (st.nextTokenAsLong() / 1024);
-            tokenizer.setString(line);
-            tokenizer.skipToken(); // Burn initial token
-            ints[LinuxSystemStats.unallocatedSwapINT] = (int) (tokenizer.nextTokenAsLong() / 1024);
+            st.setString(line);
+            st.skipToken(); // Burn initial token
+            ints[LinuxSystemStats.unallocatedSwapINT] = (int) (st.nextTokenAsLong() / 1024);
-            tokenizer.setString(line);
-            tokenizer.skipToken(); // Burn initial token
-            ints[LinuxSystemStats.cachedMemoryINT] = (int) (tokenizer.nextTokenAsLong() / 1024);
+            st.setString(line);
+            st.skipToken(); // Burn initial token
+            ints[LinuxSystemStats.cachedMemoryINT] = (int) (st.nextTokenAsLong() / 1024);
-            tokenizer.setString(line);
-            tokenizer.skipToken(); // Burn initial token
-            ints[LinuxSystemStats.dirtyMemoryINT] = (int) (tokenizer.nextTokenAsLong() / 1024);
+            st.setString(line);
+            st.skipToken(); // Burn initial token
+            ints[LinuxSystemStats.dirtyMemoryINT] = (int) (st.nextTokenAsLong() / 1024);
-            tokenizer.setString(line);
-            tokenizer.skipToken(); // Burn initial token
-            ints[LinuxSystemStats.dirtyMemoryINT] = (int) (tokenizer.nextTokenAsLong() / 1024);
+            st.setString(line);
+            st.skipToken(); // Burn initial token
+            ints[LinuxSystemStats.dirtyMemoryINT] = (int) (st.nextTokenAsLong() / 1024);
-      tokenizer.releaseResources();
+      st.releaseResources();
-    try (InputStreamReader isr = new InputStreamReader(new FileInputStream("/proc/net/netstat"))) {
-      BufferedReader br = new BufferedReader(isr);
+    InputStreamReader isr;
+    BufferedReader br = null;
+    try {
+      isr = new InputStreamReader(new FileInputStream("/proc/net/netstat"));
+      br = new BufferedReader(isr);
-      tokenizer.setString(line);
-      tokenizer.skipTokens(1);
-      long tcpSyncookiesSent = tokenizer.nextTokenAsLong();
-      long tcpSyncookiesRecv = tokenizer.nextTokenAsLong();
-      tokenizer.skipTokens(17);
-      long tcpListenOverflows = tokenizer.nextTokenAsLong();
-      long tcpListenDrops = tokenizer.nextTokenAsLong();
+      st.setString(line);
+      st.skipTokens(1);
+      long tcpSyncookiesSent = st.nextTokenAsLong();
+      long tcpSyncookiesRecv = st.nextTokenAsLong();
+      st.skipTokens(17);
+      long tcpListenOverflows = st.nextTokenAsLong();
+      long tcpListenDrops = st.nextTokenAsLong();
-      br.close();
-      br = null;
-        try (InputStreamReader soMaxConnIsr =
-            new InputStreamReader(new FileInputStream("/proc/sys/net/core/somaxconn"))) {
-          BufferedReader br2 = new BufferedReader(soMaxConnIsr);
-          line = br2.readLine();
-          tokenizer.setString(line);
-          soMaxConn = tokenizer.nextTokenAsInt();
-          soMaxConnProcessed = true;
-        }
+        br.close();
+        isr = new InputStreamReader(new FileInputStream("/proc/sys/net/core/somaxconn"));
+        br = new BufferedReader(isr);
+        line = br.readLine();
+        st.setString(line);
+        soMaxConn = st.nextTokenAsInt();
+        soMaxConnProcessed = true;
-      tokenizer.releaseResources();
+      st.releaseResources();
+      if (br != null) {
+        try {
+          br.close();
+        } catch (IOException ignore) {
+        }
+      }
-        tokenizer.setString(line.substring(index + 1).trim());
-        long recv_bytes = tokenizer.nextTokenAsLong();
-        long recv_packets = tokenizer.nextTokenAsLong();
-        long recv_errs = tokenizer.nextTokenAsLong();
-        long recv_drop = tokenizer.nextTokenAsLong();
-        tokenizer.skipTokens(4); // fifo, frame, compressed, multicast
-        long xmit_bytes = tokenizer.nextTokenAsLong();
-        long xmit_packets = tokenizer.nextTokenAsLong();
-        long xmit_errs = tokenizer.nextTokenAsLong();
-        long xmit_drop = tokenizer.nextTokenAsLong();
-        tokenizer.skipToken(); // fifo
-        long xmit_colls = tokenizer.nextTokenAsLong();
+        st.setString(line.substring(index + 1).trim());
+        long recv_bytes = st.nextTokenAsLong();
+        long recv_packets = st.nextTokenAsLong();
+        long recv_errs = st.nextTokenAsLong();
+        long recv_drop = st.nextTokenAsLong();
+        st.skipTokens(4); // fifo, frame, compressed, multicast
+        long xmit_bytes = st.nextTokenAsLong();
+        long xmit_packets = st.nextTokenAsLong();
+        long xmit_errs = st.nextTokenAsLong();
+        long xmit_drop = st.nextTokenAsLong();
+        st.skipToken(); // fifo
+        long xmit_colls = st.nextTokenAsLong();
-      tokenizer.releaseResources();
+      st.releaseResources();
-        tokenizer.setString(line);
+        st.setString(line);
-          String tok = tokenizer.nextToken();
+          String tok = st.nextToken();
-            tok = tokenizer.nextToken();
+            tok = st.nextToken();
-          tok = tokenizer.nextToken();
+          tok = st.nextToken();
-          tok = tokenizer.nextToken();
+          tok = st.nextToken();
-            tok = tokenizer.nextToken();
+            tok = st.nextToken();
-        long tmp_readsCompleted = tokenizer.nextTokenAsLong();
-        long tmp_readsMerged = tokenizer.nextTokenAsLong();
-        long tmp_sectorsRead = tokenizer.nextTokenAsLong();
-        long tmp_timeReading = tokenizer.nextTokenAsLong();
-        if (tokenizer.hasMoreTokens()) {
+        long tmp_readsCompleted = st.nextTokenAsLong();
+        long tmp_readsMerged = st.nextTokenAsLong();
+        long tmp_sectorsRead = st.nextTokenAsLong();
+        long tmp_timeReading = st.nextTokenAsLong();
+        if (st.hasMoreTokens()) {
-          long tmp_writesCompleted = tokenizer.nextTokenAsLong();
-          long tmp_writesMerged = tokenizer.nextTokenAsLong();
-          long tmp_sectorsWritten = tokenizer.nextTokenAsLong();
-          long tmp_timeWriting = tokenizer.nextTokenAsLong();
-          long tmp_iosInProgress = tokenizer.nextTokenAsLong();
-          long tmp_timeIosInProgress = tokenizer.nextTokenAsLong();
-          long tmp_ioTime = tokenizer.nextTokenAsLong();
+          long tmp_writesCompleted = st.nextTokenAsLong();
+          long tmp_writesMerged = st.nextTokenAsLong();
+          long tmp_sectorsWritten = st.nextTokenAsLong();
+          long tmp_timeWriting = st.nextTokenAsLong();
+          long tmp_iosInProgress = st.nextTokenAsLong();
+          long tmp_timeIosInProgress = st.nextTokenAsLong();
+          long tmp_ioTime = st.nextTokenAsLong();
-      tokenizer.releaseResources();
+      st.releaseResources();
-      tokenizer.setString(newStatLine);
-      tokenizer.skipToken(); // cpu name
+      st.setString(newStatLine);
+      st.skipToken(); // cpu name
-      while (tokenizer.hasMoreTokens()) {
-        newStats.add(tokenizer.nextTokenAsLong());
+      while (st.hasMoreTokens()) {
+        newStats.add(st.nextTokenAsLong());
