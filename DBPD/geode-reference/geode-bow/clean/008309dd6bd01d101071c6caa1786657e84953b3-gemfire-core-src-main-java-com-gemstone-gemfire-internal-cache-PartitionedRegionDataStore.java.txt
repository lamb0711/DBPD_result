Merge branch 'release/1.0.0-incubating.M1'

- * ========================================================================= 
- * Copyright (c) 2002-2014 Pivotal Software, Inc. All Rights Reserved. 
- * This product is protected by U.S. and international copyright
- * and intellectual property laws. Pivotal products are covered by
- * more patents listed at http://www.pivotal.io/patents.
- * =========================================================================
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+import com.gemstone.gemfire.cache.hdfs.HDFSIOException;
-import com.gemstone.org.jgroups.util.StringId;
+import com.gemstone.gemfire.i18n.StringId;
-public final class PartitionedRegionDataStore implements HasCachePerfStats
+public class PartitionedRegionDataStore implements HasCachePerfStats
+                if (getPartitionedRegion().isShadowPR()) {
+                  getPartitionedRegion().getColocatedWithRegion()
+                  .getRegionAdvisor()
+                  .getBucketAdvisor(possiblyFreeBucketId)
+                  .setShadowBucketDestroyed(false);
+                }
+            for (GatewaySenderEventImpl event : tempQueue) {
+              event.release();
+            }
+    factory.setOffHeap(this.partitionedRegion.getOffHeap());
-    return putLocally(bucketId, event, ifNew, ifOld, expectedOldValue,
-        requireOldValue, lastModified, br);
+    return putLocally(br, event, ifNew, ifOld, expectedOldValue,
+        requireOldValue, lastModified);
-  public boolean putLocally(final Integer bucketId,
+  public boolean putLocally(final BucketRegion bucketRegion,
-                            final long lastModified,
-                            final BucketRegion bucketRegion)
+                            final long lastModified)
-			  clientEvent, returnTombstones, false);
+			  clientEvent, returnTombstones, false, false);
-      boolean returnTombstones, boolean opScopeIsLocal) throws PrimaryBucketException,
+      boolean returnTombstones, boolean opScopeIsLocal, boolean allowReadFromHDFS) throws PrimaryBucketException,
-      logger.debug("getLocally:  key {}) bucketId={}{}{} region {} returnTombstones {}", key,
-          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, bucketRegion.getName(), returnTombstones);
+      logger.debug("getLocally:  key {}) bucketId={}{}{} region {} returnTombstones {} allowReadFromHDFS {}", key,
+          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, bucketRegion.getName(), returnTombstones, allowReadFromHDFS);
-      ret = bucketRegion.get(key, aCallbackArgument, true, disableCopyOnRead , preferCD, requestingClient, clientEvent, returnTombstones, opScopeIsLocal);
+      ret = bucketRegion.get(key, aCallbackArgument, true, disableCopyOnRead , preferCD, requestingClient, clientEvent, returnTombstones, opScopeIsLocal, allowReadFromHDFS, false);
-  public RawValue getSerializedLocally(KeyInfo keyInfo, boolean doNotLockEntry, EntryEventImpl clientEvent, boolean returnTombstones) throws PrimaryBucketException,
+  public RawValue getSerializedLocally(KeyInfo keyInfo, boolean doNotLockEntry, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) throws PrimaryBucketException,
-      RawValue result = bucketRegion.getSerialized(keyInfo, true, doNotLockEntry, clientEvent, returnTombstones);
+      RawValue result = bucketRegion.getSerialized(keyInfo, true, doNotLockEntry, clientEvent, returnTombstones, allowReadFromHDFS);
-      boolean access, boolean allowTombstones)
+      boolean access, boolean allowTombstones, boolean allowReadFromHDFS)
-      ent = bucketRegion.entries.getEntry(key);
+      if (allowReadFromHDFS) {
+        ent = bucketRegion.entries.getEntry(key);
+      }
+      else {
+        ent = bucketRegion.entries.getOperationalEntryInVM(key);
+      }
+        Set keys = r.keySet(allowTombstones);
+        if (getPartitionedRegion().isHDFSReadWriteRegion()) {
+          // hdfs regions can't copy all keys into memory
+          ret = keys;
+
+        } else  { 
+		}
-   * @param bucketId
-   *          the bucket id of the key
+   * @param bucketRegion
+   *          the bucket to do the create in
-  public boolean createLocally(Integer bucketId,
+  public boolean createLocally(final BucketRegion bucketRegion,
-    final BucketRegion bucketRegion = getInitializedBucketForId(event.getKey(), bucketId);
-
+  public Map<Integer, SizeEntry> getSizeEstimateForLocalPrimaryBuckets() {
+    return getSizeEstimateLocallyForBuckets(getAllLocalPrimaryBucketIds());
+  }
+
+    return getSizeLocallyForPrimary(bucketIds, false);
+  }
+
+  public Map<Integer, SizeEntry> getSizeEstimateLocallyForBuckets(Collection<Integer> bucketIds) {
+    return getSizeLocallyForPrimary(bucketIds, true);
+  }
+
+  private Map<Integer, SizeEntry> getSizeLocallyForPrimary(Collection<Integer> bucketIds, boolean estimate) {
-    BucketRegion r;
+    BucketRegion r = null;
-        mySizeMap.put(bucketId, new SizeEntry(r.size(), r.getBucketAdvisor().isPrimary()));
+        mySizeMap.put(bucketId, new SizeEntry(estimate ? r.sizeEstimate() : r.size(), r.getBucketAdvisor().isPrimary()));
+//        if (getLogWriter().fineEnabled() && r.getBucketAdvisor().isPrimary()) {
+//          r.verifyTombstoneCount();
+//        }
+      } catch (PrimaryBucketException skip) {
+        // sizeEstimate() will throw this exception as it will not try to read from HDFS on a secondary bucket,
+        // this bucket will be retried in PartitionedRegion.getSizeForHDFS() fixes bug 49033
+        continue;
+  /** a fast estimate of total bucket size */
+  public long getEstimatedLocalBucketSize(boolean primaryOnly) {
+    long size = 0;
+    for (BucketRegion br : localBucket2RegionMap.values()) {
+      if (!primaryOnly || br.getBucketAdvisor().isPrimary()) {
+        size += br.getEstimatedLocalSize();
+      }
+    }
+    return size;
+  }
+
+  /** a fast estimate of total bucket size */
+  public long getEstimatedLocalBucketSize(Set<Integer> bucketIds) {
+    long size = 0;
+    for (Integer bid : bucketIds) {
+      BucketRegion br = localBucket2RegionMap.get(bid);
+      if (br != null) {
+        size += br.getEstimatedLocalSize();
+      }
+    }
+    return size;
+  }
+
