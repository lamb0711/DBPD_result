GEODE-1072: Removing HDFS related code

Removing all HDFS and EvictionCriteria created code. This code will be
reinstated on a branch to be cleaned up and merged as a separate module.

-import java.util.concurrent.Future;
-import com.gemstone.gemfire.cache.asyncqueue.internal.AsyncEventQueueStats;
-import com.gemstone.gemfire.cache.hdfs.internal.HDFSStoreFactoryImpl;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.CompactionStatus;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSFlushQueueFunction;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSForceCompactionArgs;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSForceCompactionFunction;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSForceCompactionResultCollector;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSLastCompactionTimeFunction;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSRegionDirector;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HoplogOrganizer;
-import com.gemstone.gemfire.internal.cache.partitioned.RegionAdvisor.BucketVisitor;
-import com.gemstone.gemfire.internal.util.concurrent.FutureResult;
-  private boolean isShadowPRForHDFS = false;
-  
+
-  private final ThreadLocal<Boolean> queryHDFS = new ThreadLocal<Boolean>() {
-    @Override
-    protected Boolean initialValue() {
-      return false;
-    }
-  };
-  
-    // add an async queue for the region if the store name is not null. 
-    if (this.getHDFSStoreName() != null) {
-      String eventQueueName = getHDFSEventQueueName();
-      super.addAsyncEventQueueId(eventQueueName);
-    }
-
-      if (internalRegionArgs.isUsedForHDFSParallelGatewaySenderQueue())
-        this.isShadowPRForHDFS = true;
-  @Override
-  public final boolean isHDFSRegion() {
-    return this.getHDFSStoreName() != null;
-  }
-
-  @Override
-  public final boolean isHDFSReadWriteRegion() {
-    return isHDFSRegion() && !getHDFSWriteOnly();
-  }
-
-  @Override
-  protected final boolean isHDFSWriteOnly() {
-    return isHDFSRegion() && getHDFSWriteOnly();
-  }
-
-  public final void setQueryHDFS(boolean includeHDFS) {
-    queryHDFS.set(includeHDFS);
-  }
-
-  @Override
-  public final boolean includeHDFSResults() {
-    return queryHDFS.get();
-  }
-
-  public final boolean isShadowPRForHDFS() {
-    return isShadowPRForHDFS;
-  }
-  
-          ret = this.dataStore.getEntryLocally(bucketId, key, access, allowTombstones, true);
+          ret = this.dataStore.getEntryLocally(bucketId, key, access, allowTombstones);
-          // For HDFS region, we will recover key, so allow bucket creation
-          if (!this.dataPolicy.withHDFS() && event.hasDelta()) {
+          if (event.hasDelta()) {
-      boolean generateCallbacks, boolean disableCopyOnRead, boolean preferCD,
-      ClientProxyMembershipID requestingClient,
-      EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) throws TimeoutException, CacheLoaderException
+                    boolean generateCallbacks, boolean disableCopyOnRead, boolean preferCD,
+                    ClientProxyMembershipID requestingClient,
+                    EntryEventImpl clientEvent, boolean returnTombstones) throws TimeoutException, CacheLoaderException
-                                      null /*no local value*/, disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, allowReadFromHDFS);
+                                      null /*no local value*/, disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones);
-    if (isTX() || this.hdfsStoreName != null) {
+    if (isTX()) {
-    if (!this.haveCacheLoader && (this.hdfsStoreName == null)) {
+    if (!this.haveCacheLoader) {
-  protected Object findObjectInSystem(KeyInfo keyInfo, boolean isCreate,
-      TXStateInterface tx, boolean generateCallbacks, Object localValue, boolean disableCopyOnRead, boolean preferCD, ClientProxyMembershipID requestingClient,
-      EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS)
+  protected Object findObjectInSystem(KeyInfo keyInfo,
+                                      boolean isCreate,
+                                      TXStateInterface tx,
+                                      boolean generateCallbacks,
+                                      Object localValue,
+                                      boolean disableCopyOnRead,
+                                      boolean preferCD,
+                                      ClientProxyMembershipID requestingClient,
+                                      EntryEventImpl clientEvent,
+                                      boolean returnTombstones)
-      obj = getFromBucket(targetNode, bucketId, key, aCallbackArgument, disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, allowRetry, allowReadFromHDFS);
+      obj = getFromBucket(targetNode, bucketId, key, aCallbackArgument, disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, allowRetry);
-   * @param preferCD 
+   * @param preferCD
-      int bucketId, final Object key, final Object aCallbackArgument,
-      boolean disableCopyOnRead, boolean preferCD, ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowRetry, boolean allowReadFromHDFS) {
+                               int bucketId,
+                               final Object key,
+                               final Object aCallbackArgument,
+                               boolean disableCopyOnRead,
+                               boolean preferCD,
+                               ClientProxyMembershipID requestingClient,
+                               EntryEventImpl clientEvent,
+                               boolean returnTombstones,
+                               boolean allowRetry) {
-              disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, false, allowReadFromHDFS);
+              disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, false);
-            else if (this.haveCacheLoader || this.hdfsStoreName != null) {
+            else if (this.haveCacheLoader) {
-                  disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, allowReadFromHDFS))) {
+                  disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones))) {
-          obj = getRemotely(retryNode, bucketId, key, aCallbackArgument, preferCD, requestingClient, clientEvent, returnTombstones, allowReadFromHDFS);
+          obj = getRemotely(retryNode, bucketId, key, aCallbackArgument, preferCD, requestingClient, clientEvent, returnTombstones);
-		final Object aCallbackArgument, boolean disableCopyOnRead,
-		boolean preferCD, ClientProxyMembershipID requestingClient,
-		EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS)
+                                   final Object aCallbackArgument, boolean disableCopyOnRead,
+                                   boolean preferCD, ClientProxyMembershipID requestingClient,
+                                   EntryEventImpl clientEvent, boolean returnTombstones)
-      disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, true, allowReadFromHDFS))) {
+      disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, true))) {
-      int bucketId, final Object key, final Object aCallbackArgument, boolean preferCD, ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) throws PrimaryBucketException,
+                            int bucketId,
+                            final Object key,
+                            final Object aCallbackArgument,
+                            boolean preferCD,
+                            ClientProxyMembershipID requestingClient,
+                            EntryEventImpl clientEvent,
+                            boolean returnTombstones) throws PrimaryBucketException,
-        aCallbackArgument, requestingClient, returnTombstones, allowReadFromHDFS);
+        aCallbackArgument, requestingClient, returnTombstones);
- 	if (isHDFSReadWriteRegion() && (includeHDFSResults() || estimate)) {
-      bucketSizes = getSizeForHDFS( buckets, estimate);
-	} else {
- 	}
-  private Map<Integer, SizeEntry> getSizeForHDFS(final Set<Integer> buckets, boolean estimate) {
-    // figure out which buckets to include
-    Map<Integer, SizeEntry> bucketSizes = new HashMap<Integer, SizeEntry>();
-    getRegionAdvisor().accept(new BucketVisitor<Map<Integer, SizeEntry>>() {
-      @Override
-      public boolean visit(RegionAdvisor advisor, ProxyBucketRegion pbr,
-          Map<Integer, SizeEntry> map) {
-        if (buckets == null || buckets.contains(pbr.getBucketId())) {
-          map.put(pbr.getBucketId(), null);
-          // ensure that the bucket has been created
-          pbr.getPartitionedRegion().getOrCreateNodeForBucketWrite(pbr.getBucketId(), null);
-        }
-        return true;
-      }
-    }, bucketSizes);
-    RetryTimeKeeper retry = new RetryTimeKeeper(retryTimeout);
-
-    while (true) {
-      // get the size from local buckets
-      if (dataStore != null) {
-        Map<Integer, SizeEntry> localSizes;
-        if (estimate) {
-          localSizes = dataStore.getSizeEstimateForLocalPrimaryBuckets();
-        } else {
-          localSizes = dataStore.getSizeForLocalPrimaryBuckets();
-        }
-        for (Map.Entry<Integer, SizeEntry> me : localSizes.entrySet()) {
-          if (bucketSizes.containsKey(me.getKey())) {
-            bucketSizes.put(me.getKey(), me.getValue());
-          }
-        }
-      }
-      // all done
-      int count = 0;
-      Iterator it = bucketSizes.values().iterator();
-      while (it.hasNext()) {
-        if (it.next() != null) count++;
-      }
-      if (bucketSizes.size() == count) {
-        return bucketSizes;
-      }
-      
-      Set<InternalDistributedMember> remotes = getRegionAdvisor().adviseDataStore(true);
-      remotes.remove(getMyId());
-      
-      // collect remote sizes
-      if (!remotes.isEmpty()) {
-        Map<Integer, SizeEntry> remoteSizes = new HashMap<Integer, PartitionedRegion.SizeEntry>();
-        try {
-          remoteSizes = getSizeRemotely(remotes, estimate);
-        } catch (ReplyException e) {
-          // Remote member will never throw ForceReattemptException or
-          // PrimaryBucketException, so any exception on the remote member
-          // should be re-thrown
-          e.handleAsUnexpected();
-        }
-        for (Map.Entry<Integer, SizeEntry> me : remoteSizes.entrySet()) {
-          Integer k = me.getKey();
-          if (bucketSizes.containsKey(k) && me.getValue().isPrimary()) {
-            bucketSizes.put(k, me.getValue());
-          }
-        }
-      }
-      
-      if (retry.overMaximum()) {
-        checkReadiness();
-        PRHARedundancyProvider.timedOut(this, null, null, "calculate size", retry.getRetryTime());
-      }
-      
-      // throttle subsequent attempts
-      retry.waitForBucketsRecovery();
-    }
-  }
-  
-	  .append("; hdfsStoreName=").append(getHDFSStoreName())
-      .append("; hdfsWriteOnly=").append(getHDFSWriteOnly())
-      
+
-    //For HDFS regions, we need a data store
-    //to do the global destroy so that it can delete
-    //the data from HDFS as well.
-    if(!isDataStore() && this.dataPolicy.withHDFS()) {
-      if(destroyOnDataStore(aCallbackArgument)) {
-        //If we were able to find a data store to do the destroy,
-        //stop here.
-        //otherwise go ahead and destroy the region from this member
-        return;
-      }
-    }
-
-    AsyncEventQueueImpl hdfsQueue = getHDFSEventQueue();
-    
-    if(hdfsQueue != null) {
-      hdfsQueue.destroy();
-      cache.removeAsyncEventQueue(hdfsQueue);
-    }
-	if(!isClose) {
-      destroyHDFSData();
-    }
-    HDFSRegionDirector.getInstance().clear(getFullPath());
-    
-      //hoplogs - pause HDFS dispatcher while we 
-      //clear the buckets to avoid missing some files
-      //during the clear
-      pauseHDFSDispatcher();
-
-        resumeHDFSDispatcher();
-  
-  /**Destroy all data in HDFS, if this region is using HDFS persistence.*/
-  private void destroyHDFSData() {
-    if(getHDFSStoreName() == null) {
-      return;
-    }
-    
-    try {
-      hdfsManager.destroyData();
-    } catch (IOException e) {
-      logger.warn(LocalizedStrings.HOPLOG_UNABLE_TO_DELETE_HDFS_DATA, e);
-    }
-  }
-
-  private void pauseHDFSDispatcher() {
-    if(!isHDFSRegion()) {
-      return;
-    }
-    AbstractGatewaySenderEventProcessor eventProcessor = getHDFSEventProcessor();
-    if (eventProcessor == null) return;
-    eventProcessor.pauseDispatching();
-    eventProcessor.waitForDispatcherToPause();
-  }
-  
-  /**
-   * Get the statistics for the HDFS event queue associated with this region,
-   * if any
-   */
-  public AsyncEventQueueStats getHDFSEventQueueStats() {
-    AsyncEventQueueImpl asyncQ = getHDFSEventQueue();
-    if(asyncQ == null) {
-      return null;
-    }
-    return asyncQ.getStatistics();
-  }
-  
-  protected AbstractGatewaySenderEventProcessor getHDFSEventProcessor() {
-    final AsyncEventQueueImpl asyncQ = getHDFSEventQueue();
-    final AbstractGatewaySender gatewaySender = (AbstractGatewaySender)asyncQ.getSender();
-    AbstractGatewaySenderEventProcessor eventProcessor = gatewaySender.getEventProcessor();
-    return eventProcessor;
-  }
-
-  public AsyncEventQueueImpl getHDFSEventQueue() {
-    String asyncQId = getHDFSEventQueueName();
-    if(asyncQId == null) {
-      return null;
-    }
-    final AsyncEventQueueImpl asyncQ =  (AsyncEventQueueImpl)this.getCache().getAsyncEventQueue(asyncQId);
-    return asyncQ;
-  }
-  
-  private void resumeHDFSDispatcher() {
-    if(!isHDFSRegion()) {
-      return;
-    }
-    AbstractGatewaySenderEventProcessor eventProcessor = getHDFSEventProcessor();
-    if (eventProcessor == null) return;
-    eventProcessor.resumeDispatching();
-  }
-
-  protected String getHDFSEventQueueName() {
-    if (!this.getDataPolicy().withHDFS()) return null;
-    String colocatedWith = this.getPartitionAttributes().getColocatedWith();
-    String eventQueueName;
-    if (colocatedWith != null) {
-      PartitionedRegion leader = ColocationHelper.getLeaderRegionName(this);
-      eventQueueName = HDFSStoreFactoryImpl.getEventQueueName(leader
-          .getFullPath());
-    }
-    else {
-      eventQueueName = HDFSStoreFactoryImpl.getEventQueueName(getFullPath());
-    }
-    return eventQueueName;
-  }
-
-  /**
-   * schedules compaction on all members where this region is hosted.
-   * 
-   * @param isMajor
-   *          true for major compaction
-   * @param maxWaitTime
-   *          time to wait for the operation to complete, 0 will wait forever
-   */
-  @Override
-  public void forceHDFSCompaction(boolean isMajor, Integer maxWaitTime) {
-    if (!this.isHDFSReadWriteRegion()) {
-      if (this.isHDFSRegion()) {
-        throw new UnsupportedOperationException(
-            LocalizedStrings.HOPLOG_CONFIGURED_AS_WRITEONLY
-                .toLocalizedString(getName()));
-      }
-      throw new UnsupportedOperationException(
-          LocalizedStrings.HOPLOG_DOES_NOT_USE_HDFSSTORE
-              .toLocalizedString(getName()));
-    }
-    // send request to remote data stores
-    long start = System.currentTimeMillis();
-    int waitTime = maxWaitTime * 1000;
-    HDFSForceCompactionArgs args = new HDFSForceCompactionArgs(getRegionAdvisor().getBucketSet(), isMajor, waitTime);
-    HDFSForceCompactionResultCollector rc = new HDFSForceCompactionResultCollector();
-    AbstractExecution execution = (AbstractExecution) FunctionService.onRegion(this).withArgs(args).withCollector(rc);
-    execution.setWaitOnExceptionFlag(true); // wait for all exceptions
-    if (logger.isDebugEnabled()) {
-      logger.debug("HDFS: ForceCompat invoking function with arguments "+args);
-    }
-    execution.execute(HDFSForceCompactionFunction.ID);
-    List<CompactionStatus> result = rc.getResult();
-    Set<Integer> successfulBuckets = rc.getSuccessfulBucketIds();
-    if (rc.shouldRetry()) {
-      int retries = 0;
-      while (retries < HDFSForceCompactionFunction.FORCE_COMPACTION_MAX_RETRIES) {
-        waitTime -= System.currentTimeMillis() - start;
-        if (maxWaitTime > 0 && waitTime < 0) {
-          break;
-        }
-        start = System.currentTimeMillis();
-        retries++;
-        Set<Integer> retryBuckets = new HashSet<Integer>(getRegionAdvisor().getBucketSet());
-        retryBuckets.removeAll(successfulBuckets);
-        
-        for (int bucketId : retryBuckets) {
-          getNodeForBucketWrite(bucketId, new PartitionedRegion.RetryTimeKeeper(waitTime));
-          long now = System.currentTimeMillis();
-          waitTime -= now - start;
-          start = now;
-        }
-        
-        args = new HDFSForceCompactionArgs(retryBuckets, isMajor, waitTime);
-        rc = new HDFSForceCompactionResultCollector();
-        execution = (AbstractExecution) FunctionService.onRegion(this).withArgs(args).withCollector(rc);
-        execution.setWaitOnExceptionFlag(true); // wait for all exceptions
-        if (logger.isDebugEnabled()) {
-          logger.debug("HDFS: ForceCompat re-invoking function with arguments "+args+" filter:"+retryBuckets);
-        }
-        execution.execute(HDFSForceCompactionFunction.ID);
-        result = rc.getResult();
-        successfulBuckets.addAll(rc.getSuccessfulBucketIds());
-      }
-    }
-    if (successfulBuckets.size() != getRegionAdvisor().getBucketSet().size()) {
-      checkReadiness();
-      Set<Integer> uncessfulBuckets = new HashSet<Integer>(getRegionAdvisor().getBucketSet());
-      uncessfulBuckets.removeAll(successfulBuckets);
-      throw new FunctionException("Could not run compaction on following buckets:"+uncessfulBuckets);
-    }
-  }
-
-  /**
-   * Schedules compaction on local buckets
-   * @param buckets the set of buckets to compact
-   * @param isMajor true for major compaction
-   * @param time TODO use this
-   * @return a list of futures for the scheduled compaction tasks
-   */
-  public List<Future<CompactionStatus>> forceLocalHDFSCompaction(Set<Integer> buckets, boolean isMajor, long time) {
-    List<Future<CompactionStatus>> futures = new ArrayList<Future<CompactionStatus>>();
-    if (!isDataStore() || hdfsManager == null || buckets == null || buckets.isEmpty()) {
-      if (logger.isDebugEnabled()) {
-        logger.debug(
-            "HDFS: did not schedule local " + (isMajor ? "Major" : "Minor") + " compaction");
-      }
-      // nothing to do
-      return futures;
-    }
-    if (logger.isDebugEnabled()) {
-      logger.debug(
-          "HDFS: scheduling local " + (isMajor ? "Major" : "Minor") + " compaction for buckets:"+buckets);
-    }
-    Collection<HoplogOrganizer> organizers = hdfsManager.getBucketOrganizers(buckets);
-    
-    for (HoplogOrganizer hoplogOrganizer : organizers) {
-      Future<CompactionStatus> f = hoplogOrganizer.forceCompaction(isMajor);
-      futures.add(f);
-    }
-    return futures;
-  }
-  
-  @Override
-  public void flushHDFSQueue(int maxWaitTime) {
-    if (!this.isHDFSRegion()) {
-      throw new UnsupportedOperationException(
-          LocalizedStrings.HOPLOG_DOES_NOT_USE_HDFSSTORE
-              .toLocalizedString(getName()));
-    }
-    HDFSFlushQueueFunction.flushQueue(this, maxWaitTime);
-  }
-  
-  @Override
-  public long lastMajorHDFSCompaction() {
-    if (!this.isHDFSReadWriteRegion()) {
-      if (this.isHDFSRegion()) {
-        throw new UnsupportedOperationException(
-            LocalizedStrings.HOPLOG_CONFIGURED_AS_WRITEONLY
-                .toLocalizedString(getName()));
-      }
-      throw new UnsupportedOperationException(
-          LocalizedStrings.HOPLOG_DOES_NOT_USE_HDFSSTORE
-              .toLocalizedString(getName()));
-    }
-    List<Long> result = (List<Long>) FunctionService.onRegion(this)
-        .execute(HDFSLastCompactionTimeFunction.ID)
-        .getResult();
-    if (logger.isDebugEnabled()) {
-      logger.debug("HDFS: Result of LastCompactionTimeFunction "+result);
-    }
-    long min = Long.MAX_VALUE;
-    for (long ts : result) {
-      if (ts !=0 && ts < min) {
-        min = ts;
-      }
-    }
-    min = min == Long.MAX_VALUE ? 0 : min;
-    return min;
-  }
-
-  public long lastLocalMajorHDFSCompaction() {
-    if (!isDataStore() || hdfsManager == null) {
-      // nothing to do
-      return 0;
-    }
-    if (logger.isDebugEnabled()) {
-      logger.debug(
-          "HDFS: getting local Major compaction time");
-    }
-    Collection<HoplogOrganizer> organizers = hdfsManager.getBucketOrganizers();
-    long minTS = Long.MAX_VALUE;
-    for (HoplogOrganizer hoplogOrganizer : organizers) {
-      long ts = hoplogOrganizer.getLastMajorCompactionTimestamp();
-      if (ts !=0 && ts < minTS) {
-        minTS = ts;
-      }
-    }
-    minTS = minTS == Long.MAX_VALUE ? 0 : minTS;
-    if (logger.isDebugEnabled()) {
-      logger.debug(
-          "HDFS: local Major compaction time: "+minTS);
-    }
-    return minTS;
-  }
-
