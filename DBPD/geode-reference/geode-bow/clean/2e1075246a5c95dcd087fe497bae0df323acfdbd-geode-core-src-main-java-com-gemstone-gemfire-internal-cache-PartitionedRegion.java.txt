Merge branch 'release/1.0.0-incubating.M3'

-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Hashtable;
-import java.util.Iterator;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.NoSuchElementException;
-import java.util.Random;
-import java.util.Set;
-import java.util.concurrent.Callable;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentMap;
-import java.util.concurrent.CopyOnWriteArrayList;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.Executors;
-import java.util.concurrent.Future;
-import java.util.concurrent.FutureTask;
-import java.util.concurrent.ScheduledExecutorService;
-import java.util.concurrent.ThreadFactory;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.locks.Lock;
-
-import org.apache.logging.log4j.Logger;
-
-import com.gemstone.gemfire.cache.AttributesFactory;
-import com.gemstone.gemfire.cache.AttributesMutator;
-import com.gemstone.gemfire.cache.Cache;
-import com.gemstone.gemfire.cache.CacheClosedException;
-import com.gemstone.gemfire.cache.CacheException;
-import com.gemstone.gemfire.cache.CacheListener;
-import com.gemstone.gemfire.cache.CacheLoader;
-import com.gemstone.gemfire.cache.CacheLoaderException;
-import com.gemstone.gemfire.cache.CacheStatistics;
-import com.gemstone.gemfire.cache.CacheWriter;
-import com.gemstone.gemfire.cache.CacheWriterException;
-import com.gemstone.gemfire.cache.CustomExpiry;
-import com.gemstone.gemfire.cache.DataPolicy;
-import com.gemstone.gemfire.cache.DiskAccessException;
-import com.gemstone.gemfire.cache.EntryExistsException;
-import com.gemstone.gemfire.cache.EntryNotFoundException;
-import com.gemstone.gemfire.cache.ExpirationAttributes;
-import com.gemstone.gemfire.cache.InterestPolicy;
-import com.gemstone.gemfire.cache.InterestRegistrationEvent;
-import com.gemstone.gemfire.cache.LoaderHelper;
-import com.gemstone.gemfire.cache.LowMemoryException;
-import com.gemstone.gemfire.cache.Operation;
-import com.gemstone.gemfire.cache.PartitionAttributes;
-import com.gemstone.gemfire.cache.PartitionResolver;
-import com.gemstone.gemfire.cache.PartitionedRegionDistributionException;
-import com.gemstone.gemfire.cache.PartitionedRegionStorageException;
-import com.gemstone.gemfire.cache.Region;
-import com.gemstone.gemfire.cache.RegionAttributes;
-import com.gemstone.gemfire.cache.RegionDestroyedException;
-import com.gemstone.gemfire.cache.RegionEvent;
-import com.gemstone.gemfire.cache.RegionExistsException;
-import com.gemstone.gemfire.cache.RegionMembershipListener;
+import com.gemstone.gemfire.cache.*;
-import com.gemstone.gemfire.cache.TransactionDataNotColocatedException;
-import com.gemstone.gemfire.cache.TransactionDataRebalancedException;
-import com.gemstone.gemfire.cache.TransactionException;
-import com.gemstone.gemfire.cache.asyncqueue.internal.AsyncEventQueueStats;
-import com.gemstone.gemfire.cache.execute.EmtpyRegionFunctionException;
-import com.gemstone.gemfire.cache.execute.Function;
-import com.gemstone.gemfire.cache.execute.FunctionContext;
-import com.gemstone.gemfire.cache.execute.FunctionException;
-import com.gemstone.gemfire.cache.execute.FunctionService;
-import com.gemstone.gemfire.cache.execute.ResultCollector;
-import com.gemstone.gemfire.cache.hdfs.internal.HDFSStoreFactoryImpl;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.CompactionStatus;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSFlushQueueFunction;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSForceCompactionArgs;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSForceCompactionFunction;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSForceCompactionResultCollector;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSLastCompactionTimeFunction;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HDFSRegionDirector;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HoplogOrganizer;
+import com.gemstone.gemfire.cache.execute.*;
-import com.gemstone.gemfire.cache.query.FunctionDomainException;
-import com.gemstone.gemfire.cache.query.Index;
-import com.gemstone.gemfire.cache.query.IndexCreationException;
-import com.gemstone.gemfire.cache.query.IndexExistsException;
-import com.gemstone.gemfire.cache.query.IndexInvalidException;
-import com.gemstone.gemfire.cache.query.IndexNameConflictException;
-import com.gemstone.gemfire.cache.query.IndexType;
-import com.gemstone.gemfire.cache.query.MultiIndexCreationException;
-import com.gemstone.gemfire.cache.query.NameResolutionException;
-import com.gemstone.gemfire.cache.query.QueryException;
-import com.gemstone.gemfire.cache.query.QueryInvocationTargetException;
-import com.gemstone.gemfire.cache.query.SelectResults;
-import com.gemstone.gemfire.cache.query.TypeMismatchException;
-import com.gemstone.gemfire.cache.query.internal.CompiledSelect;
-import com.gemstone.gemfire.cache.query.internal.DefaultQuery;
-import com.gemstone.gemfire.cache.query.internal.ExecutionContext;
-import com.gemstone.gemfire.cache.query.internal.QCompiler;
-import com.gemstone.gemfire.cache.query.internal.QueryExecutor;
-import com.gemstone.gemfire.cache.query.internal.ResultsBag;
-import com.gemstone.gemfire.cache.query.internal.ResultsCollectionWrapper;
-import com.gemstone.gemfire.cache.query.internal.ResultsSet;
-import com.gemstone.gemfire.cache.query.internal.index.AbstractIndex;
-import com.gemstone.gemfire.cache.query.internal.index.IndexCreationData;
-import com.gemstone.gemfire.cache.query.internal.index.IndexManager;
-import com.gemstone.gemfire.cache.query.internal.index.IndexUtils;
-import com.gemstone.gemfire.cache.query.internal.index.PartitionedIndex;
+import com.gemstone.gemfire.cache.query.*;
+import com.gemstone.gemfire.cache.query.internal.*;
+import com.gemstone.gemfire.cache.query.internal.index.*;
-import com.gemstone.gemfire.distributed.internal.DM;
-import com.gemstone.gemfire.distributed.internal.DistributionAdvisee;
-import com.gemstone.gemfire.distributed.internal.DistributionAdvisor;
+import com.gemstone.gemfire.distributed.internal.*;
-import com.gemstone.gemfire.distributed.internal.DistributionManager;
-import com.gemstone.gemfire.distributed.internal.InternalDistributedSystem;
-import com.gemstone.gemfire.distributed.internal.MembershipListener;
-import com.gemstone.gemfire.distributed.internal.ProfileListener;
-import com.gemstone.gemfire.distributed.internal.ReplyException;
-import com.gemstone.gemfire.distributed.internal.ReplyProcessor21;
+import com.gemstone.gemfire.i18n.StringId;
-import com.gemstone.gemfire.internal.cache.execute.AbstractExecution;
-import com.gemstone.gemfire.internal.cache.execute.FunctionExecutionNodePruner;
-import com.gemstone.gemfire.internal.cache.execute.FunctionRemoteContext;
-import com.gemstone.gemfire.internal.cache.execute.InternalFunctionInvocationTargetException;
-import com.gemstone.gemfire.internal.cache.execute.LocalResultCollector;
-import com.gemstone.gemfire.internal.cache.execute.PartitionedRegionFunctionExecutor;
-import com.gemstone.gemfire.internal.cache.execute.PartitionedRegionFunctionResultSender;
-import com.gemstone.gemfire.internal.cache.execute.PartitionedRegionFunctionResultWaiter;
-import com.gemstone.gemfire.internal.cache.execute.RegionFunctionContextImpl;
-import com.gemstone.gemfire.internal.cache.execute.ServerToClientFunctionResultSender;
+import com.gemstone.gemfire.internal.cache.execute.*;
-import com.gemstone.gemfire.internal.cache.partitioned.ContainsKeyValueMessage;
+import com.gemstone.gemfire.internal.cache.partitioned.*;
-import com.gemstone.gemfire.internal.cache.partitioned.DestroyMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.DestroyRegionOnDataStoreMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.DumpAllPRConfigMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.DumpB2NRegion;
-import com.gemstone.gemfire.internal.cache.partitioned.DumpBucketsMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.FetchBulkEntriesMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.FetchEntriesMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.FetchEntryMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.FetchKeysMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.GetMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.IdentityRequestMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.IdentityUpdateMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.IndexCreationMsg;
-import com.gemstone.gemfire.internal.cache.partitioned.InterestEventMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.InvalidateMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.PREntriesIterator;
-import com.gemstone.gemfire.internal.cache.partitioned.PRLocallyDestroyedException;
-import com.gemstone.gemfire.internal.cache.partitioned.PRSanityCheckMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.PRUpdateEntryVersionMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.PartitionedRegionObserver;
-import com.gemstone.gemfire.internal.cache.partitioned.PartitionedRegionObserverHolder;
-import com.gemstone.gemfire.internal.cache.partitioned.PutAllPRMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.PutMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.RegionAdvisor;
-import com.gemstone.gemfire.internal.cache.partitioned.RegionAdvisor.BucketVisitor;
-import com.gemstone.gemfire.internal.cache.partitioned.RemoveAllPRMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.RemoveIndexesMessage;
-import com.gemstone.gemfire.internal.cache.partitioned.SizeMessage;
+import com.gemstone.gemfire.internal.offheap.annotations.Released;
-import com.gemstone.gemfire.internal.util.concurrent.FutureResult;
-import com.gemstone.gemfire.i18n.StringId;
+import org.apache.logging.log4j.Logger;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.io.Serializable;
+import java.util.*;
+import java.util.concurrent.*;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.locks.Lock;
-      "gemfire.PartitionedRegionRandomSeed", NanoTimer.getTime()).longValue());
+      DistributionConfig.GEMFIRE_PREFIX + "PartitionedRegionRandomSeed", NanoTimer.getTime()).longValue());
-      "gemfire.disablePartitionedRegionBucketAck");
+      DistributionConfig.GEMFIRE_PREFIX + "disablePartitionedRegionBucketAck");
-  static public final String RETRY_TIMEOUT_PROPERTY = 
-      "gemfire.partitionedRegionRetryTimeout";
+  static public final String RETRY_TIMEOUT_PROPERTY =
+      DistributionConfig.GEMFIRE_PREFIX + "partitionedRegionRetryTimeout";
-  private boolean isShadowPRForHDFS = false;
-  
+
-  private final ThreadLocal<Boolean> queryHDFS = new ThreadLocal<Boolean>() {
-    @Override
-    protected Boolean initialValue() {
-      return false;
-    }
-  };
-  
-    // add an async queue for the region if the store name is not null. 
-    if (this.getHDFSStoreName() != null) {
-      String eventQueueName = getHDFSEventQueueName();
-      super.addAsyncEventQueueId(eventQueueName);
-    }
-
-        "gemfire.mimimumPartitionedRegionWriteRedundancy", 0).intValue();
+        DistributionConfig.GEMFIRE_PREFIX + "mimimumPartitionedRegionWriteRedundancy", 0).intValue();
-        "gemfire.mimimumPartitionedRegionReadRedundancy", 0).intValue();
+        DistributionConfig.GEMFIRE_PREFIX + "mimimumPartitionedRegionReadRedundancy", 0).intValue();
-      if (internalRegionArgs.isUsedForHDFSParallelGatewaySenderQueue())
-        this.isShadowPRForHDFS = true;
-  @Override
-  public final boolean isHDFSRegion() {
-    return this.getHDFSStoreName() != null;
-  }
-
-  @Override
-  public final boolean isHDFSReadWriteRegion() {
-    return isHDFSRegion() && !getHDFSWriteOnly();
-  }
-
-  @Override
-  protected final boolean isHDFSWriteOnly() {
-    return isHDFSRegion() && getHDFSWriteOnly();
-  }
-
-  public final void setQueryHDFS(boolean includeHDFS) {
-    queryHDFS.set(includeHDFS);
-  }
-
-  @Override
-  public final boolean includeHDFSResults() {
-    return queryHDFS.get();
-  }
-
-  public final boolean isShadowPRForHDFS() {
-    return isShadowPRForHDFS;
-  }
-  
-  void distributeUpdatedProfileOnHubCreation()
-  {
-    if (!(this.isClosed || this.isLocallyDestroyed)) {
-      // tell others of the change in status
-      this.requiresNotification = true;
-      new UpdateAttributesProcessor(this).distribute(false);      
-    }
-  }
-
-  @Override
-      if (colocatedRegion != null && !prConfig.isColocationComplete() &&
-        // if the current node is marked uninitialized (SQLF DDL replay in
-        // progress) then colocation will definitely not be marked complete so
-        // avoid taking the expensive region lock
-          !getCache().isUnInitializedMember(getDistributionManager().getId())) {
+      if (colocatedRegion != null && !prConfig.isColocationComplete()) {
-          // check if all the nodes have been initialized (SQLF bug #42089)
-          for (Node node : nodes) {
-            if (getCache().isUnInitializedMember(node.getMemberId())) {
-              colocationComplete = false;
-              break;
-            }
-          }
-          if (colocationComplete) {
-            prConfig.setColocationComplete();
-          }
+          prConfig.setColocationComplete();
-          ret = this.dataStore.getEntryLocally(bucketId, key, access, allowTombstones, true);
+          ret = this.dataStore.getEntryLocally(bucketId, key, access, allowTombstones);
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.1
+   * @since GemFire 5.1
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-   * @since 5.0
+   * @since GemFire 5.0
-          // if this is a Delta update, then throw exception since the key doesn't
-          // exist if there is no bucket for it yet
-          // For HDFS region, we will recover key, so allow bucket creation
-          if (!this.dataPolicy.withHDFS() && event.hasDelta()) {
-            throw new EntryNotFoundException(LocalizedStrings.
-              PartitionedRegion_CANNOT_APPLY_A_DELTA_WITHOUT_EXISTING_ENTRY
-                .toLocalizedString());
-          }
-        EntryEventImpl firstEvent = prMsg.getFirstEvent(this);
+        @Released EntryEventImpl firstEvent = prMsg.getFirstEvent(this);
-        EntryEventImpl firstEvent = prMsg.getFirstEvent(this);
+        @Released EntryEventImpl firstEvent = prMsg.getFirstEvent(this);
-    EntryEventImpl event = prMsg.getFirstEvent(this);
+    @Released EntryEventImpl event = prMsg.getFirstEvent(this);
-    EntryEventImpl event = prMsg.getFirstEvent(this);
+    @Released EntryEventImpl event = prMsg.getFirstEvent(this);
+    try {
+    } finally {
+      event.release();
+    }
-      boolean generateCallbacks, boolean disableCopyOnRead, boolean preferCD,
-      ClientProxyMembershipID requestingClient,
-      EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) throws TimeoutException, CacheLoaderException
+                    boolean generateCallbacks, boolean disableCopyOnRead, boolean preferCD,
+                    ClientProxyMembershipID requestingClient,
+                    EntryEventImpl clientEvent, boolean returnTombstones) throws TimeoutException, CacheLoaderException
-                                      null /*no local value*/, disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, allowReadFromHDFS);
+                                      null /*no local value*/, disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones);
-    if (isTX() || this.hdfsStoreName != null) {
+    if (isTX()) {
-   * @since 5.7
+   * @since GemFire 5.7
-    if (!this.haveCacheLoader && (this.hdfsStoreName == null)) {
+    if (!this.haveCacheLoader) {
-  protected Object findObjectInSystem(KeyInfo keyInfo, boolean isCreate,
-      TXStateInterface tx, boolean generateCallbacks, Object localValue, boolean disableCopyOnRead, boolean preferCD, ClientProxyMembershipID requestingClient,
-      EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS)
+  protected Object findObjectInSystem(KeyInfo keyInfo,
+                                      boolean isCreate,
+                                      TXStateInterface tx,
+                                      boolean generateCallbacks,
+                                      Object localValue,
+                                      boolean disableCopyOnRead,
+                                      boolean preferCD,
+                                      ClientProxyMembershipID requestingClient,
+                                      EntryEventImpl clientEvent,
+                                      boolean returnTombstones)
-      obj = getFromBucket(targetNode, bucketId, key, aCallbackArgument, disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, allowRetry, allowReadFromHDFS);
+      obj = getFromBucket(targetNode, bucketId, key, aCallbackArgument, disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, allowRetry);
-   * @since 6.0
+   * @since GemFire 6.0
-    final boolean hasRoutingObjects = execution.hasRoutingObjects();
-        .groupByBucket(this, routingKeys, primaryMembersNeeded,
-            hasRoutingObjects, isBucketSetAsFilter);
+        .groupByBucket(this, routingKeys, primaryMembersNeeded, false, isBucketSetAsFilter);
-                    hasRoutingObjects, isBucketSetAsFilter);
+                    false, isBucketSetAsFilter);
-                hasRoutingObjects, isBucketSetAsFilter), execution.isReExecute(),
+                false, isBucketSetAsFilter), execution.isReExecute(),
-   * @since 6.0
+   * @since GemFire 6.0
-      if (execution.hasRoutingObjects()) {
-        bucketId = Integer.valueOf(PartitionedRegionHelper
-            .getHashKey(this, key));
-      } else {
-        // bucketId = Integer.valueOf(PartitionedRegionHelper.getHashKey(this,
-        // Operation.FUNCTION_EXECUTION, key, null));
-        bucketId = Integer.valueOf(PartitionedRegionHelper.getHashKey(this,
+      bucketId = Integer.valueOf(PartitionedRegionHelper.getHashKey(this,
-      }
-      throw new EmtpyRegionFunctionException(
+      throw new EmptyRegionFunctionException(
-   * @since 6.0
+   * @since GemFire 6.0
-      throw new EmtpyRegionFunctionException(LocalizedStrings.PartitionedRegion_FUNCTION_NOT_EXECUTED_AS_REGION_IS_EMPTY.toLocalizedString()
+      throw new EmptyRegionFunctionException(LocalizedStrings.PartitionedRegion_FUNCTION_NOT_EXECUTED_AS_REGION_IS_EMPTY.toLocalizedString()
-   * @param preferCD 
+   * @param preferCD
-      int bucketId, final Object key, final Object aCallbackArgument,
-      boolean disableCopyOnRead, boolean preferCD, ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowRetry, boolean allowReadFromHDFS) {
+                               int bucketId,
+                               final Object key,
+                               final Object aCallbackArgument,
+                               boolean disableCopyOnRead,
+                               boolean preferCD,
+                               ClientProxyMembershipID requestingClient,
+                               EntryEventImpl clientEvent,
+                               boolean returnTombstones,
+                               boolean allowRetry) {
-              disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, false, allowReadFromHDFS);
+              disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, false);
-            else if (this.haveCacheLoader || this.hdfsStoreName != null) {
+            else if (this.haveCacheLoader) {
-                  disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, allowReadFromHDFS))) {
+                  disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones))) {
-          obj = getRemotely(retryNode, bucketId, key, aCallbackArgument, preferCD, requestingClient, clientEvent, returnTombstones, allowReadFromHDFS);
+          obj = getRemotely(retryNode, bucketId, key, aCallbackArgument, preferCD, requestingClient, clientEvent, returnTombstones);
-		final Object aCallbackArgument, boolean disableCopyOnRead,
-		boolean preferCD, ClientProxyMembershipID requestingClient,
-		EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS)
+                                   final Object aCallbackArgument, boolean disableCopyOnRead,
+                                   boolean preferCD, ClientProxyMembershipID requestingClient,
+                                   EntryEventImpl clientEvent, boolean returnTombstones)
-      disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, true, allowReadFromHDFS))) {
+      disableCopyOnRead, preferCD, requestingClient, clientEvent, returnTombstones, true))) {
-      int bucketId, final Object key, final Object aCallbackArgument, boolean preferCD, ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) throws PrimaryBucketException,
+                            int bucketId,
+                            final Object key,
+                            final Object aCallbackArgument,
+                            boolean preferCD,
+                            ClientProxyMembershipID requestingClient,
+                            EntryEventImpl clientEvent,
+                            boolean returnTombstones) throws PrimaryBucketException,
-        aCallbackArgument, requestingClient, returnTombstones, allowReadFromHDFS);
+        aCallbackArgument, requestingClient, returnTombstones);
-  // !!!:ezoerner:20080321 made this function public and static.
-  // @todo should be moved to the Distributed System level as a general service
-  // for getting a unique id, with different "domains" for different
-  // contexts
-  // :soubhik:pr_func merge20914:21056: overloaded static and non-static version of generatePRId.
-  //   static version is used mainly with sqlf & non-static in gfe.
-  public static int generatePRId(InternalDistributedSystem sys, Cache cache) {
-    
-    GemFireCacheImpl gfcache = (GemFireCacheImpl) cache;
-    
-    if(gfcache == null) return 0;
-    
-    return _generatePRId(sys, gfcache.getPartitionedRegionLockService());
-  }
-  
-    
+
+    if (cacheServiceProfiles != null) {
+      for (CacheServiceProfile csp : cacheServiceProfiles.values()) {
+        profile.addCacheServiceProfile(csp);
+      }
+    }
+
-   * @since 5.0
+   * @since GemFire 5.0
-        if (this.cache.getCancelCriterion().cancelInProgress() == null) {
+        if (!cache.getCancelCriterion().isCancelInProgress()) {
-   * Currently used by SQLFabric to get a non-wrapped iterator for all entries
-   * for index consistency check.
-   */
-  public Set allEntries() {
-    return new PREntriesSet();
-  }
-
-
-  /**
-   * @since 5.1
+   * @since GemFire 5.1
-        if (myTX != null && !skipTxCheckInIteration) {
+        if (myTX != null) {
-   * @since 5.1
+   * @since GemFire 5.1
-   * @since 6.6
+   * @since GemFire 6.6
- 	if (isHDFSReadWriteRegion() && (includeHDFSResults() || estimate)) {
-      bucketSizes = getSizeForHDFS( buckets, estimate);
-	} else {
- 	}
-  private Map<Integer, SizeEntry> getSizeForHDFS(final Set<Integer> buckets, boolean estimate) {
-    // figure out which buckets to include
-    Map<Integer, SizeEntry> bucketSizes = new HashMap<Integer, SizeEntry>();
-    getRegionAdvisor().accept(new BucketVisitor<Map<Integer, SizeEntry>>() {
-      @Override
-      public boolean visit(RegionAdvisor advisor, ProxyBucketRegion pbr,
-          Map<Integer, SizeEntry> map) {
-        if (buckets == null || buckets.contains(pbr.getBucketId())) {
-          map.put(pbr.getBucketId(), null);
-          // ensure that the bucket has been created
-          pbr.getPartitionedRegion().getOrCreateNodeForBucketWrite(pbr.getBucketId(), null);
-        }
-        return true;
-      }
-    }, bucketSizes);
-    RetryTimeKeeper retry = new RetryTimeKeeper(retryTimeout);
-
-    while (true) {
-      // get the size from local buckets
-      if (dataStore != null) {
-        Map<Integer, SizeEntry> localSizes;
-        if (estimate) {
-          localSizes = dataStore.getSizeEstimateForLocalPrimaryBuckets();
-        } else {
-          localSizes = dataStore.getSizeForLocalPrimaryBuckets();
-        }
-        for (Map.Entry<Integer, SizeEntry> me : localSizes.entrySet()) {
-          if (bucketSizes.containsKey(me.getKey())) {
-            bucketSizes.put(me.getKey(), me.getValue());
-          }
-        }
-      }
-      // all done
-      int count = 0;
-      Iterator it = bucketSizes.values().iterator();
-      while (it.hasNext()) {
-        if (it.next() != null) count++;
-      }
-      if (bucketSizes.size() == count) {
-        return bucketSizes;
-      }
-      
-      Set<InternalDistributedMember> remotes = getRegionAdvisor().adviseDataStore(true);
-      remotes.remove(getMyId());
-      
-      // collect remote sizes
-      if (!remotes.isEmpty()) {
-        Map<Integer, SizeEntry> remoteSizes = new HashMap<Integer, PartitionedRegion.SizeEntry>();
-        try {
-          remoteSizes = getSizeRemotely(remotes, estimate);
-        } catch (ReplyException e) {
-          // Remote member will never throw ForceReattemptException or
-          // PrimaryBucketException, so any exception on the remote member
-          // should be re-thrown
-          e.handleAsUnexpected();
-        }
-        for (Map.Entry<Integer, SizeEntry> me : remoteSizes.entrySet()) {
-          Integer k = me.getKey();
-          if (bucketSizes.containsKey(k) && me.getValue().isPrimary()) {
-            bucketSizes.put(k, me.getValue());
-          }
-        }
-      }
-      
-      if (retry.overMaximum()) {
-        checkReadiness();
-        PRHARedundancyProvider.timedOut(this, null, null, "calculate size", retry.getRetryTime());
-      }
-      
-      // throttle subsequent attempts
-      retry.waitForBucketsRecovery();
-    }
-  }
-  
-   * @since 5.0
+   * @since GemFire 5.0
-	  .append("; hdfsStoreName=").append(getHDFSStoreName())
-      .append("; hdfsWriteOnly=").append(getHDFSWriteOnly())
-      
+
-    //For HDFS regions, we need a data store
-    //to do the global destroy so that it can delete
-    //the data from HDFS as well.
-    if(!isDataStore() && this.dataPolicy.withHDFS()) {
-      if(destroyOnDataStore(aCallbackArgument)) {
-        //If we were able to find a data store to do the destroy,
-        //stop here.
-        //otherwise go ahead and destroy the region from this member
-        return;
-      }
-    }
-
-    AsyncEventQueueImpl hdfsQueue = getHDFSEventQueue();
-    
-    if(hdfsQueue != null) {
-      hdfsQueue.destroy();
-      cache.removeAsyncEventQueue(hdfsQueue);
-    }
-  public void localDestroyRegion(Object aCallbackArgument) {
-    localDestroyRegion(aCallbackArgument, false);
-  }
-
-  /**
-   * Locally destroy a region.
-   * 
-   * SQLFabric change: The parameter "ignoreParent" has been added to allow
-   * skipping the check for parent colocated region. This is because SQLFabric
-   * DDLs are distributed in any case and are guaranteed to be atomic (i.e. no
-   * concurrent DMLs on that table). Without this it is quite ugly to implement
-   * "TRUNCATE TABLE" which first drops the table and recreates it.
-   */
-  public void localDestroyRegion(Object aCallbackArgument, boolean ignoreParent)
+  public void localDestroyRegion(Object aCallbackArgument)
-    if ((!ignoreParent && prName != null)
+    if ((prName != null)
-	if(!isClose) {
-      destroyHDFSData();
-    }
-    HDFSRegionDirector.getInstance().clear(getFullPath());
-    
-   * @since 5.1
+   * @since GemFire 5.1
-   * DO NOT use in product code else it will break SQLFabric that has cases
-   * where routing object is not part of only the key.
-   * DO NOT use in product code else it will break SQLFabric that has cases
-   * where routing object is not part of only the key.
-   * can be found within {@link com.gemstone.gemfire.distributed.internal.DistributionConfig#getMemberTimeout}.
+   * can be found within {@link DistributionConfig#getMemberTimeout}.
-    // [SQLFabric] use PartitionAttributes to get the the resolver
-    // since it may change after ALTER TABLE
-    // [SQLFabric] use PartitionAttributes to get colocated region
-    // since it may change after ALTER TABLE
-  // For SQLFabric ALTER TABLE. Need to set the colocated region using
-  // PartitionAttributesImpl and also reset the parentAdvisor for
-  // BucketAdvisors.
-  /**
-   * Set the colocated with region path and adjust the BucketAdvisor's. This
-   * should *only* be invoked when region is just newly created and has no data
-   * or existing buckets else will have undefined behaviour.
-   * 
-   * @since 6.5
-   */
-  public void setColocatedWith(String colocatedRegionFullPath) {
-    ((PartitionAttributesImpl)this.partitionAttributes)
-        .setColocatedWith(colocatedRegionFullPath);
-    this.getRegionAdvisor().resetBucketAdvisorParents();
-  }
-
-   * @since 5.7
+   * @since GemFire 5.7
-  /*
-   * This is an internal API for sqlFabric only <br>
-   * This is usefull to execute a function on set of nodes irrelevant of the
-   * routinKeys <br>
-   * notes : This API uses DefaultResultCollector. If you want your Custome
-   * Result collector, let me know
-   * 
-   * @param functionName
-   * @param args
-   * @param nodes
-   *                Set of DistributedMembers on which this function will be
-   *                executed
-   * @throws Exception
-   *//*
-  public ResultCollector executeFunctionOnNodes(String functionName,
-      Serializable args, Set nodes) throws Exception {
-    Assert.assertTrue(functionName != null, "Error: functionName is null");
-    Assert.assertTrue(nodes != null, "Error: nodes set is null");
-    Assert.assertTrue(nodes.size() != 0, "Error: empty nodes Set");
-    ResultCollector rc = new DefaultResultCollector();
-    boolean isSelf = nodes.remove(getMyId());
-    PartitionedRegionFunctionResponse response = null;
-    //TODO Yogesh: this API is broken after Resultsender implementation
-    //response = new PartitionedRegionFunctionResponse(this.getSystem(), nodes,
-    //    rc);
-    Iterator i = nodes.iterator();
-    while (i.hasNext()) {
-      InternalDistributedMember recip = (InternalDistributedMember)i.next();
-      PartitionedRegionFunctionMessage.send(recip, this, functionName, args,
-          null routingKeys , response, null);
-    }
-    if (isSelf) {
-      // execute locally and collect the result
-      if (this.dataStore != null) {
-        this.dataStore.executeOnDataStore(
-            null routingKeys , functionName, args, 0,null,rc,null);
-      }
-    }
-    return response;
-  }*/
-
-
-  /*
-   * This is an internal API for sqlFabric only <br>
-   * API for invoking a function using primitive ints as the routing objects
-   * (i.e. passing the hashcodes of the routing objects directly). <br>
-   * notes : This API uses DefaultResultCollector. If you want to pass your
-   * Custom Result collector, let me know
-   * 
-   * @param functionName
-   * @param args
-   * @param hashcodes
-   *          hashcodes of the routing objects
-   * @throws Exception
-   *//*
-  public ResultCollector executeFunctionUsingHashCodes(String functionName,
-      Serializable args, int hashcodes[]) throws Exception {
-    Assert.assertTrue(functionName != null, "Error: functionName is null");
-    Assert.assertTrue(hashcodes != null, "Error: hashcodes array is null");
-    Assert.assertTrue(hashcodes.length != 0, "Error: empty hashcodes array");
-    Set nodes = new HashSet();
-    for (int i = 0; i < hashcodes.length; i++) {
-      int bucketId = hashcodes[i] % getTotalNumberOfBuckets();
-      InternalDistributedMember n = getNodeForBucketRead(bucketId);
-      nodes.add(n);
-    }
-    return executeFunctionOnNodes(functionName, args, nodes);
-  }*/
-
-  /**
-   * This is an internal API for sqlFabric only <br>
-   * Given a array of routing objects, returns a set of members on which the (owner of each
-   * buckets)
-   * 
-   * @param routingObjects array of routing objects passed 
-   * @return Set of  InternalDistributedMembers
-   */
-  public Set getMembersFromRoutingObjects(Object[] routingObjects) {
-    Assert.assertTrue(routingObjects != null, "Error: null routingObjects ");
-    Assert.assertTrue(routingObjects.length != 0, "Error: empty routingObjects ");
-    Set nodeSet = new HashSet();
-    int bucketId;
-    for (int i = 0; i < routingObjects.length; i++) {
-      bucketId = PartitionedRegionHelper.getHashKey(routingObjects[i],
-                                                    getTotalNumberOfBuckets());
-      InternalDistributedMember lnode = getOrCreateNodeForBucketRead(bucketId);
-      if (lnode != null) {
-        nodeSet.add(lnode);
-      }
-    }
-    return nodeSet;
-  }
-   * @since 6.1.2.9
+   * @since GemFire 6.1.2.9
-   * Returns the local BucketRegion given the key and value. Particularly useful
-   * for SQLFabric where the routing object may be part of value and determining
-   * from key alone will require an expensive global index lookup.
+   * Returns the local BucketRegion given the key and value.
-   * @since 6.1.2.9
+   * @since GemFire 6.1.2.9
-  /**
-   * Clear local primary buckets.
-   * This is currently only used by gemfirexd truncate table
-   * to clear the partitioned region.
-   */
-  public void clearLocalPrimaries() {
- // rest of it should be done only if this is a store while RecoveryLock
-    // above still required even if this is an accessor
-    if (getLocalMaxMemory() > 0) {
-      // acquire the primary bucket locks
-      // do this in a loop to handle the corner cases where a primary
-      // bucket region ceases to be so when we actually take the lock
-      // (probably not required to do this in loop after the recovery lock)
-      // [sumedh] do we need both recovery lock and bucket locks?
-      boolean done = false;
-      Set<BucketRegion> lockedRegions = null;
-      while (!done) {
-        lockedRegions = getDataStore().getAllLocalPrimaryBucketRegions();
-        done = true;
-        for (BucketRegion br : lockedRegions) {
-          try {
-            br.doLockForPrimary(false);
-          } catch (RegionDestroyedException rde) {
-            done = false;
-            break;
-          } catch (PrimaryBucketException pbe) {
-            done = false;
-            break;
-          } catch (Exception e) {
-            // ignore any other exception
-            logger.debug(
-                "GemFireContainer#clear: ignoring exception "
-                    + "in bucket lock acquire", e);
-          }
-        }
-      }
-      
-      //hoplogs - pause HDFS dispatcher while we 
-      //clear the buckets to avoid missing some files
-      //during the clear
-      pauseHDFSDispatcher();
-
-      try {
-        // now clear the bucket regions; we go through the primary bucket
-        // regions so there is distribution for every bucket but that
-        // should be performant enough
-        for (BucketRegion br : lockedRegions) {
-          try {
-            br.clear();
-          } catch (Exception e) {
-            // ignore any other exception
-            logger.debug(
-                "GemFireContainer#clear: ignoring exception "
-                    + "in bucket clear", e);
-          }
-        }
-      } finally {
-        resumeHDFSDispatcher();
-        // release the bucket locks
-        for (BucketRegion br : lockedRegions) {
-          try {
-            br.doUnlockForPrimary();
-          } catch (Exception e) {
-            // ignore all exceptions at this stage
-            logger.debug(
-                "GemFireContainer#clear: ignoring exception "
-                    + "in bucket lock release", e);
-          }
-        }
-      }
-    }
-    
-  }
-  
-  /**Destroy all data in HDFS, if this region is using HDFS persistence.*/
-  private void destroyHDFSData() {
-    if(getHDFSStoreName() == null) {
-      return;
-    }
-    
-    try {
-      hdfsManager.destroyData();
-    } catch (IOException e) {
-      logger.warn(LocalizedStrings.HOPLOG_UNABLE_TO_DELETE_HDFS_DATA, e);
-    }
-  }
-
-  private void pauseHDFSDispatcher() {
-    if(!isHDFSRegion()) {
-      return;
-    }
-    AbstractGatewaySenderEventProcessor eventProcessor = getHDFSEventProcessor();
-    if (eventProcessor == null) return;
-    eventProcessor.pauseDispatching();
-    eventProcessor.waitForDispatcherToPause();
-  }
-  
-  /**
-   * Get the statistics for the HDFS event queue associated with this region,
-   * if any
-   */
-  public AsyncEventQueueStats getHDFSEventQueueStats() {
-    AsyncEventQueueImpl asyncQ = getHDFSEventQueue();
-    if(asyncQ == null) {
-      return null;
-    }
-    return asyncQ.getStatistics();
-  }
-  
-  protected AbstractGatewaySenderEventProcessor getHDFSEventProcessor() {
-    final AsyncEventQueueImpl asyncQ = getHDFSEventQueue();
-    final AbstractGatewaySender gatewaySender = (AbstractGatewaySender)asyncQ.getSender();
-    AbstractGatewaySenderEventProcessor eventProcessor = gatewaySender.getEventProcessor();
-    return eventProcessor;
-  }
-
-  public AsyncEventQueueImpl getHDFSEventQueue() {
-    String asyncQId = getHDFSEventQueueName();
-    if(asyncQId == null) {
-      return null;
-    }
-    final AsyncEventQueueImpl asyncQ =  (AsyncEventQueueImpl)this.getCache().getAsyncEventQueue(asyncQId);
-    return asyncQ;
-  }
-  
-  private void resumeHDFSDispatcher() {
-    if(!isHDFSRegion()) {
-      return;
-    }
-    AbstractGatewaySenderEventProcessor eventProcessor = getHDFSEventProcessor();
-    if (eventProcessor == null) return;
-    eventProcessor.resumeDispatching();
-  }
-
-  protected String getHDFSEventQueueName() {
-    if (!this.getDataPolicy().withHDFS()) return null;
-    String colocatedWith = this.getPartitionAttributes().getColocatedWith();
-    String eventQueueName;
-    if (colocatedWith != null) {
-      PartitionedRegion leader = ColocationHelper.getLeaderRegionName(this);
-      eventQueueName = HDFSStoreFactoryImpl.getEventQueueName(leader
-          .getFullPath());
-    }
-    else {
-      eventQueueName = HDFSStoreFactoryImpl.getEventQueueName(getFullPath());
-    }
-    return eventQueueName;
-  }
-
-  /**
-   * schedules compaction on all members where this region is hosted.
-   * 
-   * @param isMajor
-   *          true for major compaction
-   * @param maxWaitTime
-   *          time to wait for the operation to complete, 0 will wait forever
-   */
-  @Override
-  public void forceHDFSCompaction(boolean isMajor, Integer maxWaitTime) {
-    if (!this.isHDFSReadWriteRegion()) {
-      if (this.isHDFSRegion()) {
-        throw new UnsupportedOperationException(
-            LocalizedStrings.HOPLOG_CONFIGURED_AS_WRITEONLY
-                .toLocalizedString(getName()));
-      }
-      throw new UnsupportedOperationException(
-          LocalizedStrings.HOPLOG_DOES_NOT_USE_HDFSSTORE
-              .toLocalizedString(getName()));
-    }
-    // send request to remote data stores
-    long start = System.currentTimeMillis();
-    int waitTime = maxWaitTime * 1000;
-    HDFSForceCompactionArgs args = new HDFSForceCompactionArgs(getRegionAdvisor().getBucketSet(), isMajor, waitTime);
-    HDFSForceCompactionResultCollector rc = new HDFSForceCompactionResultCollector();
-    AbstractExecution execution = (AbstractExecution) FunctionService.onRegion(this).withArgs(args).withCollector(rc);
-    execution.setWaitOnExceptionFlag(true); // wait for all exceptions
-    if (logger.isDebugEnabled()) {
-      logger.debug("HDFS: ForceCompat invoking function with arguments "+args);
-    }
-    execution.execute(HDFSForceCompactionFunction.ID);
-    List<CompactionStatus> result = rc.getResult();
-    Set<Integer> successfulBuckets = rc.getSuccessfulBucketIds();
-    if (rc.shouldRetry()) {
-      int retries = 0;
-      while (retries < HDFSForceCompactionFunction.FORCE_COMPACTION_MAX_RETRIES) {
-        waitTime -= System.currentTimeMillis() - start;
-        if (maxWaitTime > 0 && waitTime < 0) {
-          break;
-        }
-        start = System.currentTimeMillis();
-        retries++;
-        Set<Integer> retryBuckets = new HashSet<Integer>(getRegionAdvisor().getBucketSet());
-        retryBuckets.removeAll(successfulBuckets);
-        
-        for (int bucketId : retryBuckets) {
-          getNodeForBucketWrite(bucketId, new PartitionedRegion.RetryTimeKeeper(waitTime));
-          long now = System.currentTimeMillis();
-          waitTime -= now - start;
-          start = now;
-        }
-        
-        args = new HDFSForceCompactionArgs(retryBuckets, isMajor, waitTime);
-        rc = new HDFSForceCompactionResultCollector();
-        execution = (AbstractExecution) FunctionService.onRegion(this).withArgs(args).withCollector(rc);
-        execution.setWaitOnExceptionFlag(true); // wait for all exceptions
-        if (logger.isDebugEnabled()) {
-          logger.debug("HDFS: ForceCompat re-invoking function with arguments "+args+" filter:"+retryBuckets);
-        }
-        execution.execute(HDFSForceCompactionFunction.ID);
-        result = rc.getResult();
-        successfulBuckets.addAll(rc.getSuccessfulBucketIds());
-      }
-    }
-    if (successfulBuckets.size() != getRegionAdvisor().getBucketSet().size()) {
-      checkReadiness();
-      Set<Integer> uncessfulBuckets = new HashSet<Integer>(getRegionAdvisor().getBucketSet());
-      uncessfulBuckets.removeAll(successfulBuckets);
-      throw new FunctionException("Could not run compaction on following buckets:"+uncessfulBuckets);
-    }
-  }
-
-  /**
-   * Schedules compaction on local buckets
-   * @param buckets the set of buckets to compact
-   * @param isMajor true for major compaction
-   * @param time TODO use this
-   * @return a list of futures for the scheduled compaction tasks
-   */
-  public List<Future<CompactionStatus>> forceLocalHDFSCompaction(Set<Integer> buckets, boolean isMajor, long time) {
-    List<Future<CompactionStatus>> futures = new ArrayList<Future<CompactionStatus>>();
-    if (!isDataStore() || hdfsManager == null || buckets == null || buckets.isEmpty()) {
-      if (logger.isDebugEnabled()) {
-        logger.debug(
-            "HDFS: did not schedule local " + (isMajor ? "Major" : "Minor") + " compaction");
-      }
-      // nothing to do
-      return futures;
-    }
-    if (logger.isDebugEnabled()) {
-      logger.debug(
-          "HDFS: scheduling local " + (isMajor ? "Major" : "Minor") + " compaction for buckets:"+buckets);
-    }
-    Collection<HoplogOrganizer> organizers = hdfsManager.getBucketOrganizers(buckets);
-    
-    for (HoplogOrganizer hoplogOrganizer : organizers) {
-      Future<CompactionStatus> f = hoplogOrganizer.forceCompaction(isMajor);
-      futures.add(f);
-    }
-    return futures;
-  }
-  
-  @Override
-  public void flushHDFSQueue(int maxWaitTime) {
-    if (!this.isHDFSRegion()) {
-      throw new UnsupportedOperationException(
-          LocalizedStrings.HOPLOG_DOES_NOT_USE_HDFSSTORE
-              .toLocalizedString(getName()));
-    }
-    HDFSFlushQueueFunction.flushQueue(this, maxWaitTime);
-  }
-  
-  @Override
-  public long lastMajorHDFSCompaction() {
-    if (!this.isHDFSReadWriteRegion()) {
-      if (this.isHDFSRegion()) {
-        throw new UnsupportedOperationException(
-            LocalizedStrings.HOPLOG_CONFIGURED_AS_WRITEONLY
-                .toLocalizedString(getName()));
-      }
-      throw new UnsupportedOperationException(
-          LocalizedStrings.HOPLOG_DOES_NOT_USE_HDFSSTORE
-              .toLocalizedString(getName()));
-    }
-    List<Long> result = (List<Long>) FunctionService.onRegion(this)
-        .execute(HDFSLastCompactionTimeFunction.ID)
-        .getResult();
-    if (logger.isDebugEnabled()) {
-      logger.debug("HDFS: Result of LastCompactionTimeFunction "+result);
-    }
-    long min = Long.MAX_VALUE;
-    for (long ts : result) {
-      if (ts !=0 && ts < min) {
-        min = ts;
-      }
-    }
-    min = min == Long.MAX_VALUE ? 0 : min;
-    return min;
-  }
-
-  public long lastLocalMajorHDFSCompaction() {
-    if (!isDataStore() || hdfsManager == null) {
-      // nothing to do
-      return 0;
-    }
-    if (logger.isDebugEnabled()) {
-      logger.debug(
-          "HDFS: getting local Major compaction time");
-    }
-    Collection<HoplogOrganizer> organizers = hdfsManager.getBucketOrganizers();
-    long minTS = Long.MAX_VALUE;
-    for (HoplogOrganizer hoplogOrganizer : organizers) {
-      long ts = hoplogOrganizer.getLastMajorCompactionTimestamp();
-      if (ts !=0 && ts < minTS) {
-        minTS = ts;
-      }
-    }
-    minTS = minTS == Long.MAX_VALUE ? 0 : minTS;
-    if (logger.isDebugEnabled()) {
-      logger.debug(
-          "HDFS: local Major compaction time: "+minTS);
-    }
-    return minTS;
-  }
-
-
+  
+  public Logger getLogger() {
+	return logger;
+  }
