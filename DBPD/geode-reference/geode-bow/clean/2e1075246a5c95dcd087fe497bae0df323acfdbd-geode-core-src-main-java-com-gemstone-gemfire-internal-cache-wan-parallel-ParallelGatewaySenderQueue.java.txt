Merge branch 'release/1.0.0-incubating.M3'

-import it.unimi.dsi.fastutil.objects.ObjectOpenHashSet;
-
+import it.unimi.dsi.fastutil.objects.ObjectOpenHashSet;
-      if(!isUsedForHDFS() && userPR.getDataPolicy().withPersistence() && !sender.isPersistenceEnabled()){
+      if(userPR.getDataPolicy().withPersistence() && !sender.isPersistenceEnabled()){
-            prQName, ra, sender, isUsedForHDFS());
+            prQName, ra, sender);
-                  .setInternalMetaRegion(meta).setDestroyLockFlag(true)
+                  .setInternalMetaRegion(meta).setDestroyLockFlag(true).setInternalRegion(true)
-          // if the current node is marked uninitialized (SQLF DDL replay in
-          // progress) then we cannot wait for buckets to recover, because
-          // bucket creation has been disabled until DDL replay is complete.
-          if(!prQ.getCache().isUnInitializedMember(prQ.getDistributionManager().getId())) {
-            //Wait for buckets to be recovered.
-            prQ.shadowPRWaitForBucketRecovery();
-          }
+          //Wait for buckets to be recovered.
+          prQ.shadowPRWaitForBucketRecovery();
-        } catch (IOException veryUnLikely) {
+        } catch (IOException | ClassNotFoundException veryUnLikely) {
-        } catch (ClassNotFoundException alsoUnlikely) {
-          logger.fatal(LocalizedMessage.create(LocalizedStrings.SingleWriteSingleReadRegionQueue_UNEXPECTED_EXCEPTION_DURING_INIT_OF_0,
-                  this.getClass()), alsoUnlikely);
-  protected boolean isUsedForHDFS()
-  {
-    return false;
-  }
-                  // TODO OFFHEAP is value refCount ok here?
-    List batch = new ArrayList();
+    List<GatewaySenderEventImpl> batch = new ArrayList<>();
-  private void addPeekedEvents(List batch, int batchSize) {
+  private void addPeekedEvents(List<GatewaySenderEventImpl> batch, int batchSize) {
+
+      //Remove all entries from peekedEvents for buckets that are not longer primary
+      //This will prevent repeatedly trying to dispatch non-primary events
+      for(Iterator<GatewaySenderEventImpl> iterator = peekedEvents.iterator(); iterator.hasNext(); ) {
+        GatewaySenderEventImpl event = iterator.next();
+        final int bucketId = event.getBucketId();
+        final PartitionedRegion region = (PartitionedRegion) event.getRegion();
+        if(!region.getRegionAdvisor().isPrimaryForBucket(bucketId)) {
+          iterator.remove();
+        }
+      }
+
-  private void addPreviouslyPeekedEvents(List batch, int batchSize) {
+  private void addPreviouslyPeekedEvents(List<GatewaySenderEventImpl> batch, int batchSize) {
-      if (cache.getCancelCriterion().cancelInProgress() != null) {
+      if (cache.getCancelCriterion().isCancelInProgress()) {
-      this( regionName, attrs, parentRegion, cache, pgSender, false);
-    }
-    public ParallelGatewaySenderQueueMetaRegion(String regionName,
-        RegionAttributes attrs, LocalRegion parentRegion,
-        GemFireCacheImpl cache, AbstractGatewaySender pgSender, boolean isUsedForHDFS) {
-              .setParallelGatewaySender((AbstractGatewaySender)pgSender)
-              .setIsUsedForHDFSParallelGatewaySenderQueue(isUsedForHDFS));
+              .setParallelGatewaySender((AbstractGatewaySender)pgSender));
-        GemFireCacheImpl cache, final String prQName, final RegionAttributes ra, AbstractGatewaySender sender, boolean isUsedForHDFS) {
+        GemFireCacheImpl cache, final String prQName, final RegionAttributes ra, AbstractGatewaySender sender) {
-          prQName, ra, null, cache, sender, isUsedForHDFS);
+          prQName, ra, null, cache, sender);
