Merge branch 'release/1.1.0'

- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license
+ * agreements. See the NOTICE file distributed with this work for additional information regarding
+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License. You may obtain a
+ * copy of the License at
- *      http://www.apache.org/licenses/LICENSE-2.0
+ * http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
+ * Unless required by applicable law or agreed to in writing, software distributed under the License
+ * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+ * or implied. See the License for the specific language governing permissions and limitations under
+ * the License.
- * Implementation of DataStore (DS) for a PartitionedRegion (PR). This will be
-import org.apache.geode.internal.cache.wan.AbstractGatewaySenderEventProcessor;
-import org.apache.geode.internal.cache.wan.GatewaySenderEventImpl;
-import org.apache.geode.internal.cache.wan.parallel.ParallelGatewaySenderImpl;
-import org.apache.geode.internal.cache.wan.parallel.ParallelGatewaySenderQueue;
- * accessed via accessor of PartitionedRegion or PartionService thread which
- * will handle remote calls to this DataStore from other nodes participating in
- * this PartitionedRegion.
+ * Implementation of DataStore (DS) for a PartitionedRegion (PR). This will be import
+ * org.apache.geode.internal.cache.wan.AbstractGatewaySenderEventProcessor; import
+ * org.apache.geode.internal.cache.wan.GatewaySenderEventImpl; import
+ * org.apache.geode.internal.cache.wan.parallel.ParallelGatewaySenderImpl; import
+ * org.apache.geode.internal.cache.wan.parallel.ParallelGatewaySenderQueue; accessed via accessor of
+ * PartitionedRegion or PartionService thread which will handle remote calls to this DataStore from
+ * other nodes participating in this PartitionedRegion.
-public class PartitionedRegionDataStore implements HasCachePerfStats
-{
+public class PartitionedRegionDataStore implements HasCachePerfStats {
-  
+
-   * Anyone creating a bucket must hold the read lock.  Anyone deleting
-   * a bucket must hold the write lock.
+   * Anyone creating a bucket must hold the read lock. Anyone deleting a bucket must hold the write
+   * lock.
-  
+
-   * Maps each bucket to a real region that contains actual key/value entries
-   * for this PR instance
+   * Maps each bucket to a real region that contains actual key/value entries for this PR instance
-   * Keys are instances of {@link Integer}.
-   * Values are instances of (@link BucketRegion}.
+   * Keys are instances of {@link Integer}. Values are instances of (@link BucketRegion}.
-  
+
-   * A counter of the number of concurrent bucket creates in progress on
-   * this node
+   * A counter of the number of concurrent bucket creates in progress on this node
-  
+
-  
+
-  private final long maximumLocalBytes; 
-  
+  private final long maximumLocalBytes;
+
-   * The keysOfInterest contains a set of all keys in which any client has
-   * interest in this PR.
+   * The keysOfInterest contains a set of all keys in which any client has interest in this PR.
-  private static final boolean UPDATE_ACCESS_TIME_ON_INTEREST = Boolean
-      .getBoolean(DistributionConfig.GEMFIRE_PREFIX + "updateAccessTimeOnClientInterest");
+  private static final boolean UPDATE_ACCESS_TIME_ON_INTEREST =
+      Boolean.getBoolean(DistributionConfig.GEMFIRE_PREFIX + "updateAccessTimeOnClientInterest");
-  
-  //Only for testing
+
+  // Only for testing
-  
+
-   * Creates PartitionedRegionDataStore for dataStorage of PR and starts a
-   * PartitionService to handle remote operations on this DataStore from other
-   * participating nodes.
+   * Creates PartitionedRegionDataStore for dataStorage of PR and starts a PartitionService to
+   * handle remote operations on this DataStore from other participating nodes.
-   * @param pr
-   *          PartitionedRegion associated with this DataStore.
+   * @param pr PartitionedRegion associated with this DataStore.
-//  this.maximumLocalBytes =  (long) (pr.getLocalMaxMemory() * PartitionedRegionHelper.BYTES_PER_MB
-//  * this.partitionedRegion.rebalanceThreshold);
-    this.maximumLocalBytes =  (pr.getLocalMaxMemory() * PartitionedRegionHelper.BYTES_PER_MB);
-    
-//    this.bucketStats = new CachePerfStats(pr.getSystem(), "partition-" + pr.getName());
-    this.bucketStats = new RegionPerfStats(pr.getCache(), pr.getCachePerfStats(), "partition-" + pr.getName());
+    // this.maximumLocalBytes = (long) (pr.getLocalMaxMemory() *
+    // PartitionedRegionHelper.BYTES_PER_MB
+    // * this.partitionedRegion.rebalanceThreshold);
+    this.maximumLocalBytes = (pr.getLocalMaxMemory() * PartitionedRegionHelper.BYTES_PER_MB);
+
+    // this.bucketStats = new CachePerfStats(pr.getSystem(), "partition-" + pr.getName());
+    this.bucketStats =
+        new RegionPerfStats(pr.getCache(), pr.getCachePerfStats(), "partition-" + pr.getName());
-   * This method creates a PartitionedRegionDataStore be invoking the PRDS
-   * Constructor.
+   * This method creates a PartitionedRegionDataStore be invoking the PRDS Constructor.
-   * @return @throws
-   *         PartitionedRegionException
+   * @return @throws PartitionedRegionException
-  static PartitionedRegionDataStore createDataStore(Cache cache,
-      PartitionedRegion pr, PartitionAttributes pa)
-      throws PartitionedRegionException
-  {
-    PartitionedRegionDataStore prd = new PartitionedRegionDataStore(pr);    
+  static PartitionedRegionDataStore createDataStore(Cache cache, PartitionedRegion pr,
+      PartitionAttributes pa) throws PartitionedRegionException {
+    PartitionedRegionDataStore prd = new PartitionedRegionDataStore(pr);
-//  /**
-//   * Checks whether there is room in this Map to accommodate more data without
-//   * pushing the Map over its rebalance threshold.
-//   * 
-//   * @param bytes
-//   *          the size to check in bytes
-//   */
-//  boolean canAccommodateMoreBytesSafely(long bytes)
-//  {
-//
-//    if (this.partitionedRegion.getLocalMaxMemory() == 0) {
-//      return false;
-//    }
-//    long allocatedMemory = currentAllocatedMemory();
-//    // precision coercion from int to long on bytes
-//    long newAllocatedSize = allocatedMemory + bytes;
-//    if (newAllocatedSize < (this.partitionedRegion.getLocalMaxMemory()
-//        * PartitionedRegionHelper.BYTES_PER_MB * this.partitionedRegion.rebalanceThreshold)) {
-//      return true;
-//    }
-//    else {
-//      return false;
-//    }
-//  }
+  // /**
+  // * Checks whether there is room in this Map to accommodate more data without
+  // * pushing the Map over its rebalance threshold.
+  // *
+  // * @param bytes
+  // * the size to check in bytes
+  // */
+  // boolean canAccommodateMoreBytesSafely(long bytes)
+  // {
+  //
+  // if (this.partitionedRegion.getLocalMaxMemory() == 0) {
+  // return false;
+  // }
+  // long allocatedMemory = currentAllocatedMemory();
+  // // precision coercion from int to long on bytes
+  // long newAllocatedSize = allocatedMemory + bytes;
+  // if (newAllocatedSize < (this.partitionedRegion.getLocalMaxMemory()
+  // * PartitionedRegionHelper.BYTES_PER_MB * this.partitionedRegion.rebalanceThreshold)) {
+  // return true;
+  // }
+  // else {
+  // return false;
+  // }
+  // }
-   * @param bucketId
-   *          the id of the bucket
+   * @param bucketId the id of the bucket
-  public boolean isManagingBucket(int bucketId)
-  {
-    BucketRegion buk = this.localBucket2RegionMap.get(Integer.valueOf(bucketId));    
+  public boolean isManagingBucket(int bucketId) {
+    BucketRegion buk = this.localBucket2RegionMap.get(Integer.valueOf(bucketId));
-  
+
-   * Report an estimate of the number of primary buckets managed locally
-   * The result of this method is not stable.
+   * Report an estimate of the number of primary buckets managed locally The result of this method
+   * is not stable.
-  
-  
+
+
-  final boolean isManagingAnyBucket()
-  {
+  final boolean isManagingAnyBucket() {
-  
-  /** Try to grab buckets for all the colocated regions
-  /* In case we can't grab buckets there is no going back
-   * @param creationRequestor 
+
+  /**
+   * Try to grab buckets for all the colocated regions /* In case we can't grab buckets there is no
+   * going back
+   * 
+   * @param creationRequestor
-  
-  protected CreateBucketResult grabFreeBucketRecursively(final int bucketId, 
-                                              final PartitionedRegion pr,     
-                                              final InternalDistributedMember moveSource, 
-                                              final boolean forceCreation, 
-                                              final boolean isRebalance,
-                                              final boolean replaceOfflineData, 
-                                              final InternalDistributedMember creationRequestor, 
-                                              final boolean isDiskRecovery) {
-    CreateBucketResult  grab;
+
+  protected CreateBucketResult grabFreeBucketRecursively(final int bucketId,
+      final PartitionedRegion pr, final InternalDistributedMember moveSource,
+      final boolean forceCreation, final boolean isRebalance, final boolean replaceOfflineData,
+      final InternalDistributedMember creationRequestor, final boolean isDiskRecovery) {
+    CreateBucketResult grab;
-    //make sure we force creation and ignore redundancy checks for the child region.
-    //if we created the parent bucket, we want to make sure we create the child bucket.
-    grab = pr.getDataStore().grabFreeBucket(bucketId, dm, 
-        null, true, isRebalance, true, replaceOfflineData, creationRequestor);
+    // make sure we force creation and ignore redundancy checks for the child region.
+    // if we created the parent bucket, we want to make sure we create the child bucket.
+    grab = pr.getDataStore().grabFreeBucket(bucketId, dm, null, true, isRebalance, true,
+        replaceOfflineData, creationRequestor);
-        logger.debug("Failed grab for bucketId = {}{}{}", pr.getPRId(), pr.BUCKET_ID_SEPARATOR, bucketId);
+        logger.debug("Failed grab for bucketId = {}{}{}", pr.getPRId(), pr.BUCKET_ID_SEPARATOR,
+            bucketId);
-    if(colocatedWithList != null) {
+    if (colocatedWithList != null) {
-        if((isDiskRecovery || coLocatedWithPr.isInitialized()) 
+        if ((isDiskRecovery || coLocatedWithPr.isInitialized())
-          grab = coLocatedWithPr.getDataStore().grabFreeBucketRecursively(
-              bucketId, coLocatedWithPr, moveSource, forceCreation, 
-              isRebalance, replaceOfflineData, creationRequestor, isDiskRecovery);
+          grab = coLocatedWithPr.getDataStore().grabFreeBucketRecursively(bucketId, coLocatedWithPr,
+              moveSource, forceCreation, isRebalance, replaceOfflineData, creationRequestor,
+              isDiskRecovery);
-              logger.debug("Failed grab for bucketId = {}{}{}", pr.getPRId(), pr.BUCKET_ID_SEPARATOR, bucketId);
+              logger.debug("Failed grab for bucketId = {}{}{}", pr.getPRId(),
+                  pr.BUCKET_ID_SEPARATOR, bucketId);
-  
+
-   * Attempts to map a bucket id to this node. Creates real storage for the
-   * bucket by adding a new Region to bucket2Map. Bucket creation is done under
-   * the d-lock on b2n region.
+   * Attempts to map a bucket id to this node. Creates real storage for the bucket by adding a new
+   * Region to bucket2Map. Bucket creation is done under the d-lock on b2n region.
-   * @param possiblyFreeBucketId  the identity of the bucket
-   + @param mustBeNew  boolean enforcing that the bucket must not already exist 
-   * @param sender  the member requesting the bucket 
+   * @param possiblyFreeBucketId the identity of the bucket + @param mustBeNew boolean enforcing
+   *        that the bucket must not already exist
+   * @param sender the member requesting the bucket
-   * @param isRebalance true if bucket creation is directed by rebalancing    
-   * @param replaceOffineData 
+   * @param isRebalance true if bucket creation is directed by rebalancing
+   * @param replaceOffineData
-  CreateBucketResult grabFreeBucket(final int possiblyFreeBucketId, 
-                         final DistributedMember sender,
-                         final InternalDistributedMember moveSource, 
-                         final boolean forceCreation, 
-                         final boolean isRebalance,
-                         final boolean lockRedundancyLock, 
-                         boolean replaceOffineData,
-                         InternalDistributedMember creationRequestor) {
+  CreateBucketResult grabFreeBucket(final int possiblyFreeBucketId, final DistributedMember sender,
+      final InternalDistributedMember moveSource, final boolean forceCreation,
+      final boolean isRebalance, final boolean lockRedundancyLock, boolean replaceOffineData,
+      InternalDistributedMember creationRequestor) {
-    
-    long startTime
-        = this.partitionedRegion.getPrStats().startBucketCreate(isRebalance);
+
+    long startTime = this.partitionedRegion.getPrStats().startBucketCreate(isRebalance);
-    
+
-          logger.debug("grabFreeBucket: VM {} already contains the bucket with bucketId={}{}{}", this.partitionedRegion.getMyId(),
-              partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, possiblyFreeBucketId);
+          logger.debug("grabFreeBucket: VM {} already contains the bucket with bucketId={}{}{}",
+              this.partitionedRegion.getMyId(), partitionedRegion.getPRId(),
+              PartitionedRegion.BUCKET_ID_SEPARATOR, possiblyFreeBucketId);
-      
+
-        lock.lock();  // prevent destruction while creating buckets
+        lock.lock(); // prevent destruction while creating buckets
-          //This counter is used by canAccomodateAnotherBucket to estimate if this member should
-          //accept another bucket
+          // This counter is used by canAccomodateAnotherBucket to estimate if this member should
+          // accept another bucket
-          
-  //        final boolean needsAllocation;
+
+          // final boolean needsAllocation;
-                PartitionedRegionHelper.printCollection(this.partitionedRegion.getRegionAdvisor().getBucketOwners(possiblyFreeBucketId)),
-                partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, possiblyFreeBucketId);
+                PartitionedRegionHelper.printCollection(this.partitionedRegion.getRegionAdvisor()
+                    .getBucketOwners(possiblyFreeBucketId)),
+                partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR,
+                possiblyFreeBucketId);
-          if (! forceCreation ) { 
-            // Currently the balancing check should only run when creating buckets, 
-            // as opposed to during bucket recovery... it is assumed that 
+          if (!forceCreation) {
+            // Currently the balancing check should only run when creating buckets,
+            // as opposed to during bucket recovery... it is assumed that
-            if (! canAccommodateAnotherBucket()) {
+            if (!canAccommodateAnotherBucket()) {
-          ProxyBucketRegion buk = partitionedRegion.getRegionAdvisor().getProxyBucketArray()[possiblyFreeBucketId];
-          //Prevent other threads on the same VM from creating this bucket.
-          //It doesn't look the the redundancy lock actually correctly
-          //handles multiple threads, and the isManagingBucket call
-          //also needs to be done under this lock
-          synchronized(buk) {
-            //DAN - this just needs to be done holding a lock for this particular bucket
-            if(!verifyBucketBeforeGrabbing(possiblyFreeBucketId)) {
+          ProxyBucketRegion buk =
+              partitionedRegion.getRegionAdvisor().getProxyBucketArray()[possiblyFreeBucketId];
+          // Prevent other threads on the same VM from creating this bucket.
+          // It doesn't look the the redundancy lock actually correctly
+          // handles multiple threads, and the isManagingBucket call
+          // also needs to be done under this lock
+          synchronized (buk) {
+            // DAN - this just needs to be done holding a lock for this particular bucket
+            if (!verifyBucketBeforeGrabbing(possiblyFreeBucketId)) {
-            
+
-              Integer possiblyFreeBucketIdInt = Integer
-              .valueOf(possiblyFreeBucketId);
-              
+              Integer possiblyFreeBucketIdInt = Integer.valueOf(possiblyFreeBucketId);
+
-              Object redundancyLock = lockRedundancyLock(moveSource,
-                  possiblyFreeBucketId, replaceOffineData);
-              //DAN - I hope this is ok to do without that bucket admin lock
+              Object redundancyLock =
+                  lockRedundancyLock(moveSource, possiblyFreeBucketId, replaceOffineData);
+              // DAN - I hope this is ok to do without that bucket admin lock
-                  getPartitionedRegion().getColocatedWithRegion()
-                  .getRegionAdvisor()
-                  .getBucketAdvisor(possiblyFreeBucketId)
-                  .setShadowBucketDestroyed(false);
+                  getPartitionedRegion().getColocatedWithRegion().getRegionAdvisor()
+                      .getBucketAdvisor(possiblyFreeBucketId).setShadowBucketDestroyed(false);
-                //Mark the bucket as hosting and distribute to peers
-                //before we release the dlock. This makes sure that our peers
-                //won't think they need to satisfy redundancy
+                // Mark the bucket as hosting and distribute to peers
+                // before we release the dlock. This makes sure that our peers
+                // won't think they need to satisfy redundancy
-                }
-                else {
+                } else {
-                if(bukReg == null) {
+                if (bukReg == null) {
-                  logger.debug("grabFreeBucket: mapped bucketId={}{}{} on node = {}", this.partitionedRegion.getPRId(),
-                      PartitionedRegion.BUCKET_ID_SEPARATOR, possiblyFreeBucketId, this.partitionedRegion.getMyId());
+                  logger.debug("grabFreeBucket: mapped bucketId={}{}{} on node = {}",
+                      this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR,
+                      possiblyFreeBucketId, this.partitionedRegion.getMyId());
-              } 
-              else {
+              } else {
-            }
-            else {
+            } else {
-              // the bucket and the creator may have died 
+              // the bucket and the creator may have died
-                logger.debug("grabFreeBucket: bucketId={}{}{} already mapped on VM = {}", this.partitionedRegion.getPRId(),
-                    PartitionedRegion.BUCKET_ID_SEPARATOR, possiblyFreeBucketId, partitionedRegion.getMyId());
+                logger.debug("grabFreeBucket: bucketId={}{}{} already mapped on VM = {}",
+                    this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR,
+                    possiblyFreeBucketId, partitionedRegion.getMyId());
-              logger.debug("grabFreeBucket: Mapped bucketId={}{}{}", this.partitionedRegion.getPRId(),
-                  PartitionedRegion.BUCKET_ID_SEPARATOR, possiblyFreeBucketId);
+              logger.debug("grabFreeBucket: Mapped bucketId={}{}{}",
+                  this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR,
+                  possiblyFreeBucketId);
-        }
-        catch (RegionDestroyedException rde) {
-          RegionDestroyedException rde2 = new RegionDestroyedException(toString(),
-              this.partitionedRegion.getFullPath());
+        } catch (RegionDestroyedException rde) {
+          RegionDestroyedException rde2 =
+              new RegionDestroyedException(toString(), this.partitionedRegion.getFullPath());
-        }
-        catch(RedundancyAlreadyMetException e) {
+        } catch (RedundancyAlreadyMetException e) {
-            logger.debug("Redundancy already met {}{}{} assignment {}", this.partitionedRegion.getPRId(),
-                PartitionedRegion.BUCKET_ID_SEPARATOR, possiblyFreeBucketId, localBucket2RegionMap.get(Integer.valueOf(possiblyFreeBucketId)));
+            logger.debug("Redundancy already met {}{}{} assignment {}",
+                this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR,
+                possiblyFreeBucketId,
+                localBucket2RegionMap.get(Integer.valueOf(possiblyFreeBucketId)));
-        }
-        finally {
+        } finally {
-          lock.unlock();  // prevent destruction while creating buckets
+          lock.unlock(); // prevent destruction while creating buckets
-      
+
-    
+
-      this.partitionedRegion.getPrStats().endBucketCreate(
-          startTime, createdBucket, isRebalance);
+      this.partitionedRegion.getPrStats().endBucketCreate(startTime, createdBucket, isRebalance);
-    List<PartitionedRegion> colocatedWithList = ColocationHelper
-        .getColocatedChildRegions(partitionedRegion);
+    List<PartitionedRegion> colocatedWithList =
+        ColocationHelper.getColocatedChildRegions(partitionedRegion);
-        AbstractGatewaySender sender = childRegion
-            .getParallelGatewaySender();
+        AbstractGatewaySender sender = childRegion.getParallelGatewaySender();
-        AbstractGatewaySenderEventProcessor eventProcessor = sender
-            .getEventProcessor();
+        AbstractGatewaySenderEventProcessor eventProcessor = sender.getEventProcessor();
-        ConcurrentParallelGatewaySenderQueue queue = (ConcurrentParallelGatewaySenderQueue)eventProcessor
-            .getQueue();
+        ConcurrentParallelGatewaySenderQueue queue =
+            (ConcurrentParallelGatewaySenderQueue) eventProcessor.getQueue();
-        BlockingQueue<GatewaySenderEventImpl> tempQueue = queue
-        		.getBucketTmpQueue(bucketId);
+        BlockingQueue<GatewaySenderEventImpl> tempQueue = queue.getBucketTmpQueue(bucketId);
-            tempQueue.clear();  
+            tempQueue.clear();
-  
-  public Object lockRedundancyLock(InternalDistributedMember moveSource, int bucketId, boolean replaceOffineData) {
-    //TODO prperist - Make this thing easier to find!
-    final PartitionedRegion.BucketLock bl = partitionedRegion
-        .getRegionAdvisor().getBucketAdvisor(bucketId).getProxyBucketRegion()
-        .getBucketLock();
+
+  public Object lockRedundancyLock(InternalDistributedMember moveSource, int bucketId,
+      boolean replaceOffineData) {
+    // TODO prperist - Make this thing easier to find!
+    final PartitionedRegion.BucketLock bl = partitionedRegion.getRegionAdvisor()
+        .getBucketAdvisor(bucketId).getProxyBucketRegion().getBucketLock();
-    boolean succeeded =false;
+    boolean succeeded = false;
-        if(logger.isDebugEnabled()) {
-          logger.debug("Redundancy already satisfied. current owners=", partitionedRegion.getRegionAdvisor().getBucketOwners(bucketId));
+        if (logger.isDebugEnabled()) {
+          logger.debug("Redundancy already satisfied. current owners=",
+              partitionedRegion.getRegionAdvisor().getBucketOwners(bucketId));
-      succeeded=true;
+      succeeded = true;
-      if(!succeeded) {
+      if (!succeeded) {
-    
+
-  
+
-      PartitionedRegion.BucketLock bl  = (BucketLock) lock;
+      PartitionedRegion.BucketLock bl = (BucketLock) lock;
-    PartitionedRegion colocatedRegion = 
-      ColocationHelper.getColocatedRegion(this.partitionedRegion);
-    StoppableReadLock parentLock = null; 
+    PartitionedRegion colocatedRegion = ColocationHelper.getColocatedRegion(this.partitionedRegion);
+    StoppableReadLock parentLock = null;
-   * Returns false if this region is colocated and parent bucket does not 
-   * exist.
+   * Returns false if this region is colocated and parent bucket does not exist.
+   * 
-    PartitionedRegion colocatedRegion = 
-      ColocationHelper.getColocatedRegion(this.partitionedRegion);
-    if (colocatedRegion != null &&
-        !colocatedRegion.getDataStore().isManagingBucket(bucketId)) {
+    PartitionedRegion colocatedRegion = ColocationHelper.getColocatedRegion(this.partitionedRegion);
+    if (colocatedRegion != null && !colocatedRegion.getDataStore().isManagingBucket(bucketId)) {
-    if(!isColocationComplete(bucketId)) {
+    if (!isColocationComplete(bucketId)) {
-    
+
-  
+
-    
-    if(!ColocationHelper.isColocationComplete(this.partitionedRegion)) {
-      ProxyBucketRegion pb = this.partitionedRegion.getRegionAdvisor().getProxyBucketArray()[bucketId];
+
+    if (!ColocationHelper.isColocationComplete(this.partitionedRegion)) {
+      ProxyBucketRegion pb =
+          this.partitionedRegion.getRegionAdvisor().getProxyBucketArray()[bucketId];
-      
-      //Don't worry about colocation if we're recovering a persistent
-      //bucket. The bucket must have been properly colocated earlier.
-      if(persistenceAdvisor != null && persistenceAdvisor.wasHosting()) {
+
+      // Don't worry about colocation if we're recovering a persistent
+      // bucket. The bucket must have been properly colocated earlier.
+      if (persistenceAdvisor != null && persistenceAdvisor.wasHosting()) {
-  
+
-   * This method creates bucket regions, based on redundancy level. If
-   * redundancy level is: a) = 1 it creates a local region b) >1 it creates a
-   * distributed region
+   * This method creates bucket regions, based on redundancy level. If redundancy level is: a) = 1
+   * it creates a local region b) >1 it creates a distributed region
-   * @return @throws
-   *         CacheException
+   * @return @throws CacheException
-  private BucketRegion createBucketRegion(int bucketId)
-  {
+  private BucketRegion createBucketRegion(int bucketId) {
-    
+
-    if(isPersistent) {
+    if (isPersistent) {
-    
-    if(PartitionedRegion.DISABLE_SECONDARY_BUCKET_ACK) {
+
+    if (PartitionedRegion.DISABLE_SECONDARY_BUCKET_ACK) {
-    }
-    else {
+    } else {
-    if(this.partitionedRegion.getValueConstraint() != null) {
+    if (this.partitionedRegion.getValueConstraint() != null) {
-    
+
-      if(ea.getAction() != ExpirationAction.DESTROY)
+      if (ea.getAction() != ExpirationAction.DESTROY)
-      if(ea.getAction() != ExpirationAction.DESTROY)
+      if (ea.getAction() != ExpirationAction.DESTROY)
-    CustomExpiry  ce = this.partitionedRegion.getAttributes().getCustomEntryIdleTimeout();
+    CustomExpiry ce = this.partitionedRegion.getAttributes().getCustomEntryIdleTimeout();
-    if (this.partitionedRegion.getStatisticsEnabled()) { 
-      factory.setStatisticsEnabled(true); 
+    if (this.partitionedRegion.getStatisticsEnabled()) {
+      factory.setStatisticsEnabled(true);
-    EvictionAttributesImpl eva = (EvictionAttributesImpl)this.partitionedRegion.getEvictionAttributes();
+    EvictionAttributesImpl eva =
+        (EvictionAttributesImpl) this.partitionedRegion.getEvictionAttributes();
-    	setDiskAttributes(factory);
+        setDiskAttributes(factory);
-    
+
-    
+
-    
+
-      bucketRegion = (BucketRegion)rootRegion.createSubregion(bucketRegionName,
-          attributes, new InternalRegionArguments()
-          .setPartitionedRegionBucketRedundancy(this.partitionedRegion.getRedundantCopies())
-          .setBucketAdvisor(proxyBucket.getBucketAdvisor())
-          .setPersistenceAdvisor(proxyBucket.getPersistenceAdvisor())
-          .setDiskRegion(proxyBucket.getDiskRegion())
-          .setCachePerfStatsHolder(this)
-          .setLoaderHelperFactory(this.partitionedRegion)
-          .setPartitionedRegion(this.partitionedRegion)
-          .setIndexes(getIndexes(rootRegion.getFullPath(), bucketRegionName)));
+      bucketRegion = (BucketRegion) rootRegion.createSubregion(bucketRegionName, attributes,
+          new InternalRegionArguments()
+              .setPartitionedRegionBucketRedundancy(this.partitionedRegion.getRedundantCopies())
+              .setBucketAdvisor(proxyBucket.getBucketAdvisor())
+              .setPersistenceAdvisor(proxyBucket.getPersistenceAdvisor())
+              .setDiskRegion(proxyBucket.getDiskRegion()).setCachePerfStatsHolder(this)
+              .setLoaderHelperFactory(this.partitionedRegion)
+              .setPartitionedRegion(this.partitionedRegion)
+              .setIndexes(getIndexes(rootRegion.getFullPath(), bucketRegionName)));
-    }
-    catch (RegionExistsException ex) {
+    } catch (RegionExistsException ex) {
-        logger.debug("PartitionedRegionDataStore#createBucketRegion: Bucket region already created for bucketId={}{}{}",
+        logger.debug(
+            "PartitionedRegionDataStore#createBucketRegion: Bucket region already created for bucketId={}{}{}",
-      bucketRegion = (BucketRegion)rootRegion.getSubregion(bucketRegionName);
-    }
-    catch (IOException ieo) {
+      bucketRegion = (BucketRegion) rootRegion.getSubregion(bucketRegionName);
+    } catch (IOException ieo) {
-    }
-    catch (ClassNotFoundException cne) {
+    } catch (ClassNotFoundException cne) {
-      Assert.assertTrue(false,
-          "ClassNotFoundException creating bucket Region: " + cne);
-    }catch (InternalGemFireError e) {
+      Assert.assertTrue(false, "ClassNotFoundException creating bucket Region: " + cne);
+    } catch (InternalGemFireError e) {
-        logger.info(LocalizedMessage.create(LocalizedStrings.PartitionedRegionDataStore_ASSERTION_ERROR_CREATING_BUCKET_IN_REGION), e);
+        logger.info(
+            LocalizedMessage.create(
+                LocalizedStrings.PartitionedRegionDataStore_ASSERTION_ERROR_CREATING_BUCKET_IN_REGION),
+            e);
-    } 
+    }
-      dumpBuckets(); 
+      dumpBuckets();
-    
-//      Iterator i = localRegion.entrySet().iterator();
-//      while (i.hasNext()) {
-//        try {
-//          NonTXEntry nte = (NonTXEntry) i.next();
-//          // updateBucket2Size(bucketId.longValue(), localRegion, null);
-//          // nte.getRegionEntry().getValueInVM();
-//        } catch (EntryDestroyedException ignore) {}
-//      }
+
+    // Iterator i = localRegion.entrySet().iterator();
+    // while (i.hasNext()) {
+    // try {
+    // NonTXEntry nte = (NonTXEntry) i.next();
+    // // updateBucket2Size(bucketId.longValue(), localRegion, null);
+    // // nte.getRegionEntry().getValueInVM();
+    // } catch (EntryDestroyedException ignore) {}
+    // }
-        Map.Entry indexEntry = (Map.Entry)it.next();
-        PartitionedIndex index = (PartitionedIndex)indexEntry.getValue();
-        IndexCreationData icd = new IndexCreationData(index.getName());        
+        Map.Entry indexEntry = (Map.Entry) it.next();
+        PartitionedIndex index = (PartitionedIndex) indexEntry.getValue();
+        IndexCreationData icd = new IndexCreationData(index.getName());
-        icd.setIndexData(index.getType(), index.getCanonicalizedFromClause(), 
+        icd.setIndexData(index.getType(), index.getCanonicalizedFromClause(),
-        logger.info(LocalizedMessage.create(LocalizedStrings.PartitionedRegionDataStore_EXCPETION__IN_BUCKET_INDEX_CREATION_, 
+        logger.info(LocalizedMessage.create(
+            LocalizedStrings.PartitionedRegionDataStore_EXCPETION__IN_BUCKET_INDEX_CREATION_,
-  
+
-  
+
-    final Object oldbukReg = this.localBucket2RegionMap.putIfAbsent(Integer.valueOf(bucketId), bukReg);
+    final Object oldbukReg =
+        this.localBucket2RegionMap.putIfAbsent(Integer.valueOf(bucketId), bukReg);
-    Assert.assertTrue(oldbukReg==null);
+    Assert.assertTrue(oldbukReg == null);
-  
- /*public void removeBucketRegion(int bucketId) {
-    Assert.assertHoldsLock(this.bucketAdminLock,true);    
-    this.localBucket2RegionMap.remove(Long.valueOf(bucketId));    
-  }*/
-  private CacheListener createDebugBucketListener()
-  {
+  /*
+   * public void removeBucketRegion(int bucketId) {
+   * Assert.assertHoldsLock(this.bucketAdminLock,true);
+   * this.localBucket2RegionMap.remove(Long.valueOf(bucketId)); }
+   */
+
+  private CacheListener createDebugBucketListener() {
-      public void afterCreate(EntryEvent event)
-      {
-        EntryEventImpl ee = (EntryEventImpl)event;
-        logger.debug("BucketListener: o={}, r={}, k={}, nv={}, dm={}", event.getOperation(), event.getRegion().getFullPath(),
-            event.getKey(), ee.getRawNewValue(), event.getDistributedMember());
+      public void afterCreate(EntryEvent event) {
+        EntryEventImpl ee = (EntryEventImpl) event;
+        logger.debug("BucketListener: o={}, r={}, k={}, nv={}, dm={}", event.getOperation(),
+            event.getRegion().getFullPath(), event.getKey(), ee.getRawNewValue(),
+            event.getDistributedMember());
-      public void afterUpdate(EntryEvent event)
-      {
-        EntryEventImpl ee = (EntryEventImpl)event;
-        logger.debug("BucketListener: o={}, r={}, k={}, ov={}, nv={}, dm={}", event.getOperation(), event.getRegion().getFullPath(),
-            event.getKey(), ee.getRawOldValue(), ee.getRawNewValue(), event.getDistributedMember());
+      public void afterUpdate(EntryEvent event) {
+        EntryEventImpl ee = (EntryEventImpl) event;
+        logger.debug("BucketListener: o={}, r={}, k={}, ov={}, nv={}, dm={}", event.getOperation(),
+            event.getRegion().getFullPath(), event.getKey(), ee.getRawOldValue(),
+            ee.getRawNewValue(), event.getDistributedMember());
-      public void afterInvalidate(EntryEvent event)
-      {
-        logger.debug("BucketListener: o={}, r={}, k={}, dm={}", event.getOperation(), event.getRegion().getFullPath(),
-            event.getKey(), event.getDistributedMember());
+      public void afterInvalidate(EntryEvent event) {
+        logger.debug("BucketListener: o={}, r={}, k={}, dm={}", event.getOperation(),
+            event.getRegion().getFullPath(), event.getKey(), event.getDistributedMember());
-      public void afterDestroy(EntryEvent event)
-      {
-        logger.debug("BucketListener: o={}, r={}, k={}, dm={}", event.getOperation(), event.getRegion().getFullPath(),
-            event.getKey(), event.getDistributedMember());
+      public void afterDestroy(EntryEvent event) {
+        logger.debug("BucketListener: o={}, r={}, k={}, dm={}", event.getOperation(),
+            event.getRegion().getFullPath(), event.getKey(), event.getDistributedMember());
+
+
+
+
+
+
-//  private void addBucketMapping(Integer bucketId, Node theNode)
-//  {
-//    VersionedArrayList list = (VersionedArrayList)this.partitionedRegion
-//        .getBucket2Node().get(bucketId);
-//    // Create a new list to avoid concurrent modification exceptions when
-//    // the array list is serialized e.g. GII
-//    if (list == null) {
-//      list = new VersionedArrayList(
-//          this.partitionedRegion.getRedundantCopies() + 1);
-//      list.add(theNode);
-//
-//    }
-//    else {
-//      for(Iterator itr =list.iterator(); itr.hasNext();) {
-//       	Node nd = (Node)itr.next();
-//       	if( !PartitionedRegionHelper.isMemberAlive(nd.getMemberId(), 
-//            this.partitionedRegion.cache) 
-//       	    && !this.partitionedRegion.isPresentInPRConfig(nd)) {       		      	 
-//       		list.remove(nd);
-//       	  if(list.size() ==0 ) {
-//       	    PartitionedRegionHelper.logForDataLoss(this.partitionedRegion, 
-//                bucketId.intValue(), "addBucketMapping");
-//          }
-//       	}
-//       	
-//      }
-//      if (!list.contains(theNode)) {
-//        list.add(theNode);
-//      }
-//    }
-//    this.partitionedRegion.checkClosed();
-//    this.partitionedRegion.checkReadiness();     
-//    this.partitionedRegion.getBucket2Node().put(bucketId, list);    
-//  }
+  // private void addBucketMapping(Integer bucketId, Node theNode)
+  // {
+  // VersionedArrayList list = (VersionedArrayList)this.partitionedRegion
+  // .getBucket2Node().get(bucketId);
+  // // Create a new list to avoid concurrent modification exceptions when
+  // // the array list is serialized e.g. GII
+  // if (list == null) {
+  // list = new VersionedArrayList(
+  // this.partitionedRegion.getRedundantCopies() + 1);
+  // list.add(theNode);
+  //
+  // }
+  // else {
+  // for(Iterator itr =list.iterator(); itr.hasNext();) {
+  // Node nd = (Node)itr.next();
+  // if( !PartitionedRegionHelper.isMemberAlive(nd.getMemberId(),
+  // this.partitionedRegion.cache)
+  // && !this.partitionedRegion.isPresentInPRConfig(nd)) {
+  // list.remove(nd);
+  // if(list.size() ==0 ) {
+  // PartitionedRegionHelper.logForDataLoss(this.partitionedRegion,
+  // bucketId.intValue(), "addBucketMapping");
+  // }
+  // }
+  //
+  // }
+  // if (!list.contains(theNode)) {
+  // list.add(theNode);
+  // }
+  // }
+  // this.partitionedRegion.checkClosed();
+  // this.partitionedRegion.checkReadiness();
+  // this.partitionedRegion.getBucket2Node().put(bucketId, list);
+  // }
-  public CacheLoader getCacheLoader()
-  {
+  public CacheLoader getCacheLoader() {
-  
+
-   * Gets the total amount of memory in bytes allocated for all values for this
-   * PR in this VM. This is the current memory (MB) watermark for data in this
-   * PR.
+   * Gets the total amount of memory in bytes allocated for all values for this PR in this VM. This
+   * is the current memory (MB) watermark for data in this PR.
-   * If eviction to disk is enabled, this does not reflect the size
-   * of entries on disk.
+   * If eviction to disk is enabled, this does not reflect the size of entries on disk.
-  public long currentAllocatedMemory()
-  {
+  public long currentAllocatedMemory() {
-   * Checks if this PartitionedRegionDataStore has the capacity to handle the
-   * bucket creation request. If so, creates the real storage for the bucket.
+   * Checks if this PartitionedRegionDataStore has the capacity to handle the bucket creation
+   * request. If so, creates the real storage for the bucket.
-   * @param bucketId
-   *          the bucket id
-   * @param size
-   *          the size in bytes of the bucket to create locally
-   * @param forceCreation ignore local maximum buckets check  
-   * @return true if already managing the bucket or if the bucket
-   *         has been created
+   * @param bucketId the bucket id
+   * @param size the size in bytes of the bucket to create locally
+   * @param forceCreation ignore local maximum buckets check
+   * @return true if already managing the bucket or if the bucket has been created
-  public boolean handleManageBucketRequest(int bucketId, 
-                                           int size, 
-                                           InternalDistributedMember sender, 
-                                           boolean forceCreation) {
-    
+  public boolean handleManageBucketRequest(int bucketId, int size, InternalDistributedMember sender,
+      boolean forceCreation) {
+
-    if (! canAccommodateMoreBytesSafely(size)) {
+    if (!canAccommodateMoreBytesSafely(size)) {
-        logger.debug("Partitioned Region {} has exceeded local maximum memory configuration {} Mb, current size is {} Mb",
+        logger.debug(
+            "Partitioned Region {} has exceeded local maximum memory configuration {} Mb, current size is {} Mb",
-    
-    if (! forceCreation && ! canAccommodateAnotherBucket()) {
+
+    if (!forceCreation && !canAccommodateAnotherBucket()) {
-    if(grabBucket(bucketId, null, forceCreation, false, true, sender, false).nowExists()) {
+    if (grabBucket(bucketId, null, forceCreation, false, true, sender, false).nowExists()) {
-        logger.debug("handleManageBucketRequest: successful, returning:{} bucketId={}{}{} for PR = {}", this.partitionedRegion.getMyId(),
-            this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, this.getName());
+        logger.debug(
+            "handleManageBucketRequest: successful, returning:{} bucketId={}{}{} for PR = {}",
+            this.partitionedRegion.getMyId(), this.partitionedRegion.getPRId(),
+            PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, this.getName());
-    }
-    else {
+    } else {
-   
+
-   * Determine if the ratio of buckets this VM should host is appropriate 
-   * given its localMaxMemory setting as compared to others
+   * Determine if the ratio of buckets this VM should host is appropriate given its localMaxMemory
+   * setting as compared to others
-    final int localMax = this.partitionedRegion.getLocalMaxMemory(); 
+    final int localMax = this.partitionedRegion.getLocalMaxMemory();
-    double totalMax = (double)this.partitionedRegion.getRegionAdvisor()
-          .adviseTotalMemoryAllocation()
-          + localMax;
+    double totalMax =
+        (double) this.partitionedRegion.getRegionAdvisor().adviseTotalMemoryAllocation() + localMax;
-    
+
-    
-    final int totalBucketInstances = this.partitionedRegion.getTotalNumberOfBuckets() 
-      * (this.partitionedRegion.getRedundantCopies() + 1);
-    
-    final double numBucketsToHostLocally = Math.ceil(memoryRatio 
-        * totalBucketInstances);
-    
+
+    final int totalBucketInstances = this.partitionedRegion.getTotalNumberOfBuckets()
+        * (this.partitionedRegion.getRedundantCopies() + 1);
+
+    final double numBucketsToHostLocally = Math.ceil(memoryRatio * totalBucketInstances);
+
-    //Pessimistically assume that all concurrent bucket creates will succeed.
-    //-1 because we've already incremented bucketCreatesInProgress to include this thread.
-    final int currentNumBuckets = this.localBucket2RegionMap.size() + bucketCreatesInProgress.intValue() - 1;
+    // Pessimistically assume that all concurrent bucket creates will succeed.
+    // -1 because we've already incremented bucketCreatesInProgress to include this thread.
+    final int currentNumBuckets =
+        this.localBucket2RegionMap.size() + bucketCreatesInProgress.intValue() - 1;
-      logger.debug("canAccomodateAnotherBucket: local VM can host {} does host {} concurrent creates {}",
-          numBucketsToHostLocally, this.localBucket2RegionMap.size(), (bucketCreatesInProgress.intValue() - 1));
+      logger.debug(
+          "canAccomodateAnotherBucket: local VM can host {} does host {} concurrent creates {}",
+          numBucketsToHostLocally, this.localBucket2RegionMap.size(),
+          (bucketCreatesInProgress.intValue() - 1));
-    
-    if (! ret) {
+
+    if (!ret) {
-        logger.debug("Partitioned Region {} potentially unbalanced; maximum number of buckets, {}, has been reached",
+        logger.debug(
+            "Partitioned Region {} potentially unbalanced; maximum number of buckets, {}, has been reached",
-    
+
-  
+
-   * Checks if this PartitionedRegionDataStore has the capacity to handle the
-   * rebalancing size.
+   * Checks if this PartitionedRegionDataStore has the capacity to handle the rebalancing size.
-   * @param size
-   *          the size in bytes of the bucket to be rebalanced
+   * @param size the size in bytes of the bucket to be rebalanced
-  boolean handleRemoteCanRebalance(long size)
-  {
+  boolean handleRemoteCanRebalance(long size) {
-   * @param bucketId
-   *          the id of the bucket to rebalance
-   * @param obj
-   *          the contents of the bucket
-   * @param regionName
-   *          the name of the PR
+   * @param bucketId the id of the bucket to rebalance
+   * @param obj the contents of the bucket
+   * @param regionName the name of the PR
-  boolean handleRemoteRebalance(int bucketId, Object obj, String regionName)
-  {
+  boolean handleRemoteRebalance(int bucketId, Object obj, String regionName) {
-   * Checks if this PR has the capacity to handle the rebalancing size. If so,
-   * creates the real storage for the bucket and a bucket2Node Region mapping.
-   * These two operations are done as a logical unit so that the node can
-   * immediately begin handling remote requests once the bucket2Node mapping
-   * becomes visible.
+   * Checks if this PR has the capacity to handle the rebalancing size. If so, creates the real
+   * storage for the bucket and a bucket2Node Region mapping. These two operations are done as a
+   * logical unit so that the node can immediately begin handling remote requests once the
+   * bucket2Node mapping becomes visible.
-   * @param bucketId
-   *          the bucket id
+   * @param bucketId the bucket id
-  boolean handleRemoteCreateBackupRegion(int bucketId)
-  {
+  boolean handleRemoteCreateBackupRegion(int bucketId) {
-  public long getBucketSize(int bucketId)
-  {
+  public long getBucketSize(int bucketId) {
-    // getInitializedBucketForId(Long.valueOf(bucketId)); // wait for the bucket to finish initializing
-    final BucketRegion bucketRegion = this.localBucket2RegionMap
-        .get(Integer.valueOf(bucketId));
-    if(bucketRegion == null) {
+    // getInitializedBucketForId(Long.valueOf(bucketId)); // wait for the bucket to finish
+    // initializing
+    final BucketRegion bucketRegion = this.localBucket2RegionMap.get(Integer.valueOf(bucketId));
+    if (bucketRegion == null) {
-  
+
+   * 
-  public void queryLocalNode(DefaultQuery query, Object[] parameters, List buckets, PRQueryResultCollector resultCollector)
-    throws QueryException, InterruptedException
-  {
-    Assert.assertTrue(!buckets.isEmpty(), "bucket list can not be empty. ");
-    invokeBucketReadHook();
-
-    // Check if QueryMonitor is enabled, if so add query to be monitored.
-    QueryMonitor queryMonitor = null;
-    if( GemFireCacheImpl.getInstance() != null)
-    {
-      queryMonitor = GemFireCacheImpl.getInstance().getQueryMonitor();
-    }
-    
-    try {
-      if (queryMonitor != null) {
-        // Add current thread to be monitored by QueryMonitor.
-        queryMonitor.monitorQueryThread(Thread.currentThread(), query);
-      }
-      new PRQueryProcessor(this, query, parameters, buckets).executeQuery(resultCollector);
-    } finally {
-      if (queryMonitor != null) {
-        queryMonitor.stopMonitoringQueryThread(Thread.currentThread(), query);
-      }
-    }
-  }
-  */
+   * public void queryLocalNode(DefaultQuery query, Object[] parameters, List buckets,
+   * PRQueryResultCollector resultCollector) throws QueryException, InterruptedException {
+   * Assert.assertTrue(!buckets.isEmpty(), "bucket list can not be empty. ");
+   * invokeBucketReadHook();
+   * 
+   * // Check if QueryMonitor is enabled, if so add query to be monitored. QueryMonitor queryMonitor
+   * = null; if( GemFireCacheImpl.getInstance() != null) { queryMonitor =
+   * GemFireCacheImpl.getInstance().getQueryMonitor(); }
+   * 
+   * try { if (queryMonitor != null) { // Add current thread to be monitored by QueryMonitor.
+   * queryMonitor.monitorQueryThread(Thread.currentThread(), query); } new PRQueryProcessor(this,
+   * query, parameters, buckets).executeQuery(resultCollector); } finally { if (queryMonitor !=
+   * null) { queryMonitor.stopMonitoringQueryThread(Thread.currentThread(), query); } } }
+   */
-  private String getName()
-  {
+  private String getName() {
-  
-   * 3) If it finds the bucket region from step 1, it tries to put the key-value
-   * on the region. <br>
-   * 4) updateBucket2Size if bucket is on more than 1 node or else bucket
-   * listeners would take care of size update. <br>
+   * 3) If it finds the bucket region from step 1, it tries to put the key-value on the region. <br>
+   * 4) updateBucket2Size if bucket is on more than 1 node or else bucket listeners would take care
+   * of size update. <br>
-   * @throws ForceReattemptException
-   *           if bucket region is null
+   * @throws ForceReattemptException if bucket region is null
-  public boolean putLocally(final Integer bucketId,
-                            final EntryEventImpl event,
-                            boolean ifNew,
-                            boolean ifOld,
-                            Object expectedOldValue,
-                            boolean requireOldValue,
-                            final long lastModified)
-  throws PrimaryBucketException, ForceReattemptException {
+  public boolean putLocally(final Integer bucketId, final EntryEventImpl event, boolean ifNew,
+      boolean ifOld, Object expectedOldValue, boolean requireOldValue, final long lastModified)
+      throws PrimaryBucketException, ForceReattemptException {
-    return putLocally(br, event, ifNew, ifOld, expectedOldValue,
-        requireOldValue, lastModified);
+    return putLocally(br, event, ifNew, ifOld, expectedOldValue, requireOldValue, lastModified);
-  public boolean putLocally(final BucketRegion bucketRegion,
-                            final EntryEventImpl event,
-                            boolean ifNew,
-                            boolean ifOld,
-                            Object expectedOldValue,
-                            boolean requireOldValue,
-                            final long lastModified)
-  throws PrimaryBucketException, ForceReattemptException {
-    boolean didPut=false; // false if entry put fails
-    
-//    final BucketRegion bucketRegion = getInitializedBucketForId(event.getKey(), bucketId);
-    
-    try{
+  public boolean putLocally(final BucketRegion bucketRegion, final EntryEventImpl event,
+      boolean ifNew, boolean ifOld, Object expectedOldValue, boolean requireOldValue,
+      final long lastModified) throws PrimaryBucketException, ForceReattemptException {
+    boolean didPut = false; // false if entry put fails
+
+    // final BucketRegion bucketRegion = getInitializedBucketForId(event.getKey(), bucketId);
+
+    try {
-        didPut = bucketRegion.basicUpdate(event, ifNew, ifOld, lastModified,
-            false);
-      }
-      else {
+        didPut = bucketRegion.basicUpdate(event, ifNew, ifOld, lastModified, false);
+      } else {
-        didPut = bucketRegion.virtualPut(event,
-                                         ifNew,
-                                         ifOld,
-                                         expectedOldValue,
-                                         requireOldValue,
-                                         lastModified,
-                                         false);
+        didPut = bucketRegion.virtualPut(event, ifNew, ifOld, expectedOldValue, requireOldValue,
+            lastModified, false);
-    }
-    catch(RegionDestroyedException rde){
+    } catch (RegionDestroyedException rde) {
-    } 
-    
+    }
+
-  
+
-    return UPDATE_ACCESS_TIME_ON_INTEREST
-        && this.keysOfInterest.containsKey(event.getKey());
+    return UPDATE_ACCESS_TIME_ON_INTEREST && this.keysOfInterest.containsKey(event.getKey());
-  
-  
-    
+
+
+
-    if(!this.partitionedRegion.isEntryEvictionPossible()) {
+    if (!this.partitionedRegion.isEntryEvictionPossible()) {
-      //only check for exceeding local max memory if we're not evicting entries.
-      // TODO, investigate precision issues with cast to long 
+      // only check for exceeding local max memory if we're not evicting entries.
+      // TODO, investigate precision issues with cast to long
-          logStr = LocalizedStrings.PartitionedRegionDataStore_PARTITIONED_REGION_0_HAS_EXCEEDED_LOCAL_MAXIMUM_MEMORY_CONFIGURATION_2_MB_CURRENT_SIZE_IS_3_MB;
+          logStr =
+              LocalizedStrings.PartitionedRegionDataStore_PARTITIONED_REGION_0_HAS_EXCEEDED_LOCAL_MAXIMUM_MEMORY_CONFIGURATION_2_MB_CURRENT_SIZE_IS_3_MB;
-      }
-      else {
-        if (locBytes <= this.maximumLocalBytes) {  
+      } else {
+        if (locBytes <= this.maximumLocalBytes) {
-          logStr = LocalizedStrings.PartitionedRegionDataStore_PARTITIONED_REGION_0_IS_AT_OR_BELOW_LOCAL_MAXIMUM_MEMORY_CONFIGURATION_2_MB_CURRENT_SIZE_IS_3_MB;  
+          logStr =
+              LocalizedStrings.PartitionedRegionDataStore_PARTITIONED_REGION_0_IS_AT_OR_BELOW_LOCAL_MAXIMUM_MEMORY_CONFIGURATION_2_MB_CURRENT_SIZE_IS_3_MB;
-        Object[] logArgs = new Object[] {this.partitionedRegion.getFullPath(), logStr, Long.valueOf(this.partitionedRegion.getLocalMaxMemory()), Long.valueOf(locBytes / PartitionedRegionHelper.BYTES_PER_MB)}; 
+        Object[] logArgs = new Object[] {this.partitionedRegion.getFullPath(), logStr,
+            Long.valueOf(this.partitionedRegion.getLocalMaxMemory()),
+            Long.valueOf(locBytes / PartitionedRegionHelper.BYTES_PER_MB)};
-   * @param bytes
-   *          the size to check in bytes
+   * @param bytes the size to check in bytes
-  boolean canAccommodateMoreBytesSafely(int bytes)
-  {
+  boolean canAccommodateMoreBytesSafely(int bytes) {
-    
+
-    if(this.partitionedRegion.isEntryEvictionPossible()) {
-      //assume that since we're evicting entries, we're allowed to go over localMaxMemory and
-      //eviction will take care of keeping us from running out of memory.
+    if (this.partitionedRegion.isEntryEvictionPossible()) {
+      // assume that since we're evicting entries, we're allowed to go over localMaxMemory and
+      // eviction will take care of keeping us from running out of memory.
-      logger.debug("canAccomodateMoreBytes: bytes = {} allocatedMemory = {} newAllocatedSize = {} thresholdSize = ",
-          bytes, curBytes, (curBytes+bytes), this.maximumLocalBytes);
+      logger.debug(
+          "canAccomodateMoreBytes: bytes = {} allocatedMemory = {} newAllocatedSize = {} thresholdSize = ",
+          bytes, curBytes, (curBytes + bytes), this.maximumLocalBytes);
-    }
-    else {
+    } else {
-  public PartitionedRegion getPartitionedRegion()
-  {
+  public PartitionedRegion getPartitionedRegion() {
-   * 1) Locates the bucket region. If it doesnt find the actual bucket, it means
-   * that this bucket is remapped to some other node and remappedBucketException
-   * is thrown <br>
+   * 1) Locates the bucket region. If it doesnt find the actual bucket, it means that this bucket is
+   * remapped to some other node and remappedBucketException is thrown <br>
-   * 3) updateBucket2Size if bucket is on more than 1 node or else bucket
-   * listners would take care of size update.
+   * 3) updateBucket2Size if bucket is on more than 1 node or else bucket listners would take care
+   * of size update.
-   * @param bucketId
-   *          for the key
+   * @param bucketId for the key
-   * @throws EntryNotFoundException
-   *           if entry is not found for the key or expectedOldValue is
-   *           not null and current value is not equal to it
+   * @throws EntryNotFoundException if entry is not found for the key or expectedOldValue is not
+   *         null and current value is not equal to it
-  public Object destroyLocally(Integer bucketId,
-                               EntryEventImpl event,
-                               Object expectedOldValue)
-  throws EntryNotFoundException,
-         PrimaryBucketException,
-         ForceReattemptException {
+  public Object destroyLocally(Integer bucketId, EntryEventImpl event, Object expectedOldValue)
+      throws EntryNotFoundException, PrimaryBucketException, ForceReattemptException {
-          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId); 
+          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId);
-    }
-    catch (EntryNotFoundException enf) {
+    } catch (EntryNotFoundException enf) {
-            new RegionDestroyedException(LocalizedStrings.PartitionedRegionDataStore_REGION_HAS_BEEN_DESTROYED.toLocalizedString(), this.partitionedRegion.getFullPath()));
+            new RegionDestroyedException(
+                LocalizedStrings.PartitionedRegionDataStore_REGION_HAS_BEEN_DESTROYED
+                    .toLocalizedString(),
+                this.partitionedRegion.getFullPath()));
-      //                                   event.getKey());
+      // event.getKey());
-    }
-    catch(RegionDestroyedException rde){
+    } catch (RegionDestroyedException rde) {
-    //event.setRegion(this.partitionedRegion);
-    //this.partitionedRegion.notifyBridgeClients(EnumListenerEvent.AFTER_DESTROY,
-    //    event);
-    //this.partitionedRegion.notifyGatewayHub(EnumListenerEvent.AFTER_DESTROY,
-    //    event);
+    // event.setRegion(this.partitionedRegion);
+    // this.partitionedRegion.notifyBridgeClients(EnumListenerEvent.AFTER_DESTROY,
+    // event);
+    // this.partitionedRegion.notifyGatewayHub(EnumListenerEvent.AFTER_DESTROY,
+    // event);
- * This method does the cleaning up of the closed/locallyDestroyed PartitionedRegion.
- * This cleans up the reference of the close PartitionedRegion's node from the b2n region and locallyDestroys the b2n region (if removeBucketMapping is true).
- * It locallyDestroys the bucket region and cleans up the localBucketRegion map to avoid any stale references to locally destroyed bucket region. 
- * @param removeBucketMapping
- */
-  void cleanUp(boolean removeBucketMapping, boolean removeFromDisk)
-  {
+   * This method does the cleaning up of the closed/locallyDestroyed PartitionedRegion. This cleans
+   * up the reference of the close PartitionedRegion's node from the b2n region and locallyDestroys
+   * the b2n region (if removeBucketMapping is true). It locallyDestroys the bucket region and
+   * cleans up the localBucketRegion map to avoid any stale references to locally destroyed bucket
+   * region.
+   * 
+   * @param removeBucketMapping
+   */
+  void cleanUp(boolean removeBucketMapping, boolean removeFromDisk) {
-          logger.debug("cleanUp: Done destroyBucket2NodeRegionLocally for {}", this.partitionedRegion);
+          logger.debug("cleanUp: Done destroyBucket2NodeRegionLocally for {}",
+              this.partitionedRegion);
-      }
-      else {
+      } else {
-      
+
-        ProxyBucketRegion[] proxyBuckets = getPartitionedRegion().getRegionAdvisor().getProxyBucketArray();
-        if(proxyBuckets != null) {
-          for(ProxyBucketRegion pbr : proxyBuckets) {
+        ProxyBucketRegion[] proxyBuckets =
+            getPartitionedRegion().getRegionAdvisor().getProxyBucketArray();
+        if (proxyBuckets != null) {
+          for (ProxyBucketRegion pbr : proxyBuckets) {
-            // concurrent entry iterator does not guarantee value, key pairs  
-            if (buk != null) { 
+            // concurrent entry iterator does not guarantee value, key pairs
+            if (buk != null) {
-                if(removeFromDisk) {
+                if (removeFromDisk) {
-                //Fix for defect #49012
+                // Fix for defect #49012
-                    buk.getPartitionedRegion().getColocatedWithRegion()
-                        .getRegionAdvisor().getBucketAdvisor(bucketId)
-                        .setShadowBucketDestroyed(true);
+                    buk.getPartitionedRegion().getColocatedWithRegion().getRegionAdvisor()
+                        .getBucketAdvisor(bucketId).setShadowBucketDestroyed(true);
-              }
-              catch (RegionDestroyedException ignore) {
-              }
-              catch (Exception ex) {
+              } catch (RegionDestroyedException ignore) {
+              } catch (Exception ex) {
-                    new Object[] {this.partitionedRegion.getFullPath(), Integer.valueOf(buk.getId())}), ex);
+                    new Object[] {this.partitionedRegion.getFullPath(),
+                        Integer.valueOf(buk.getId())}),
+                    ex);
-              if(diskRegion != null) {
+              if (diskRegion != null) {
-      }
-      finally {
+      } finally {
-    }
-    catch (Exception ex) {
+    } catch (Exception ex) {
-        LocalizedStrings.PartitionedRegionDataStore_PARTITIONEDREGION_0_CAUGHT_UNEXPECTED_EXCEPTION_DURING_CLEANUP,
-        this.partitionedRegion.getFullPath()), ex);
-    }
-    finally {
+          LocalizedStrings.PartitionedRegionDataStore_PARTITIONEDREGION_0_CAUGHT_UNEXPECTED_EXCEPTION_DURING_CLEANUP,
+          this.partitionedRegion.getFullPath()), ex);
+    } finally {
-   * Removes a redundant bucket hosted by this data store. The rebalancer
-   * invokes this method directly or sends this member a message to invoke it.
+   * Removes a redundant bucket hosted by this data store. The rebalancer invokes this method
+   * directly or sends this member a message to invoke it.
-   * This operation is done by the rebalancer (REB) and can only be done on
-   * non-primary buckets. If you want to remove a primary bucket first send
-   * one of its peers "become primary" and then send it "unhost" (we could
-   * offer a "unhost" option on "become primary" or a "becomePrimary" option
-   * on "create redundant"). The member that has the bucket being removed is
-   * called the bucket host (BH).
+   * This operation is done by the rebalancer (REB) and can only be done on non-primary buckets. If
+   * you want to remove a primary bucket first send one of its peers "become primary" and then send
+   * it "unhost" (we could offer a "unhost" option on "become primary" or a "becomePrimary" option
+   * on "create redundant"). The member that has the bucket being removed is called the bucket host
+   * (BH).
-   * 1. REB sends an "unhostBucket" message to BH. This message will be
-   *    rejected if the member finds itself to be the primary or if he doesn't
-   *    host the bucket by sending a failure reply to REB.
-   * 2. BH marks itself as "not-hosting". This causes any read operations that
-   *    come in to not start and retry. BH also updates the advisor to know
-   *    that it is no longer hosting the bucket.
-   * 3. BH then waits for any in-progress reads (which read ops to wait for
-   *    are TBD) to complete.
-   * 4. BH then removes the bucket region from its cache.
-   * 5. BH then sends a success reply to REB.
+   * 1. REB sends an "unhostBucket" message to BH. This message will be rejected if the member finds
+   * itself to be the primary or if he doesn't host the bucket by sending a failure reply to REB. 2.
+   * BH marks itself as "not-hosting". This causes any read operations that come in to not start and
+   * retry. BH also updates the advisor to know that it is no longer hosting the bucket. 3. BH then
+   * waits for any in-progress reads (which read ops to wait for are TBD) to complete. 4. BH then
+   * removes the bucket region from its cache. 5. BH then sends a success reply to REB.
-   * This method is now also used by the PartitionManager.
-   * For the PartitionManager, it does remove the primary bucket.
+   * This method is now also used by the PartitionManager. For the PartitionManager, it does remove
+   * the primary bucket.
-   * @return true if the bucket was removed; false if unable to remove or if
-   * bucket is not hosted
+   * @return true if the bucket was removed; false if unable to remove or if bucket is not hosted
-    
-    //Don't allow the removal of a bucket if we haven't
-    //finished recovering from disk
-    if(!this.partitionedRegion.getRedundancyProvider().isPersistentRecoveryComplete()) {
+
+    // Don't allow the removal of a bucket if we haven't
+    // finished recovering from disk
+    if (!this.partitionedRegion.getRedundancyProvider().isPersistentRecoveryComplete()) {
-       logger.debug(
-                "Returning false from removeBucket because we have not finished recovering all colocated regions from disk");
+        logger.debug(
+            "Returning false from removeBucket because we have not finished recovering all colocated regions from disk");
-      BucketRegion bucketRegion = this.localBucket2RegionMap.get(Integer
-          .valueOf(bucketId));
+      BucketRegion bucketRegion = this.localBucket2RegionMap.get(Integer.valueOf(bucketId));
-          logger.debug("Returning true from removeBucket because we don't have the bucket we've been told to remove");
+          logger.debug(
+              "Returning true from removeBucket because we don't have the bucket we've been told to remove");
-        
+
-            bucketRegion.getPartitionedRegion().getColocatedWithRegion()
-                .getRegionAdvisor().getBucketAdvisor(bucketId)
-                .setShadowBucketDestroyed(true);
+            bucketRegion.getPartitionedRegion().getColocatedWithRegion().getRegionAdvisor()
+                .getBucketAdvisor(bucketId).setShadowBucketDestroyed(true);
-      }
-      finally {
+      } finally {
-      InternalDistributedMember myId = this.partitionedRegion
-          .getDistributionManager().getDistributionManagerId();
+      InternalDistributedMember myId =
+          this.partitionedRegion.getDistributionManager().getDistributionManagerId();
-        }
-        catch (InterruptedException e) {
+        } catch (InterruptedException e) {
-      }
-      else {
+      } else {
-    }
-    finally {
+    } finally {
-   * Wait for in an progress backup. When we backup the whole DS, we need to make 
-   * sure we don't miss a bucket because it is in the process
-   * of rebalancing. This doesn't wait for the whole backup to complete,
-   * it only makes sure that this destroy will wait until the point
-   * when we know that we that this bucket won't be destroyed on this member
-   * in the backup unless it was backed up on the target member. 
+   * Wait for in an progress backup. When we backup the whole DS, we need to make sure we don't miss
+   * a bucket because it is in the process of rebalancing. This doesn't wait for the whole backup to
+   * complete, it only makes sure that this destroy will wait until the point when we know that we
+   * that this bucket won't be destroyed on this member in the backup unless it was backed up on the
+   * target member.
-    if(getPartitionedRegion().getDataPolicy().withPersistence() && backupManager != null) {
+    if (getPartitionedRegion().getDataPolicy().withPersistence() && backupManager != null) {
-    
+
-   * This calls removeBucket on every colocated child that is directly
-   * colocated to this bucket's PR. Those each in turn do the same to their
-   * child buckets and so on before returning.
+   * This calls removeBucket on every colocated child that is directly colocated to this bucket's
+   * PR. Those each in turn do the same to their child buckets and so on before returning.
-   * @param forceRemovePrimary true if we should remove the bucket, even if
-   * it is primary.
+   * @param forceRemovePrimary true if we should remove the bucket, even if it is primary.
-  private boolean removeBucketForColocatedChildren(int bucketId, 
-      boolean forceRemovePrimary) {
+  private boolean removeBucketForColocatedChildren(int bucketId, boolean forceRemovePrimary) {
-    
+
-    List<PartitionedRegion> colocatedChildPRs = 
+    List<PartitionedRegion> colocatedChildPRs =
-        removedChildBucket = 
+        removedChildBucket =
-    
+
-  
+
-   * Create a new backup of the bucket, allowing redundancy to be exceeded.
-   * All colocated child buckets will also be created.
+   * Create a new backup of the bucket, allowing redundancy to be exceeded. All colocated child
+   * buckets will also be created.
-  public CreateBucketResult createRedundantBucket(int bucketId, boolean isRebalance, InternalDistributedMember moveSource) {
-    // recurse down to create each tier of children after creating leader bucket 
+  public CreateBucketResult createRedundantBucket(int bucketId, boolean isRebalance,
+      InternalDistributedMember moveSource) {
+    // recurse down to create each tier of children after creating leader bucket
-  
+
-   * If the bucket is the leader bucket then it will recursively create all
-   * colocated children and then remove all colocated children as well from
-   * the <code>source</code> member.
+   * If the bucket is the leader bucket then it will recursively create all colocated children and
+   * then remove all colocated children as well from the <code>source</code> member.
-    BucketAdvisor bucketAdvisor = this.partitionedRegion.getRegionAdvisor().getBucketAdvisor(bucketId);
+    BucketAdvisor bucketAdvisor =
+        this.partitionedRegion.getRegionAdvisor().getBucketAdvisor(bucketId);
-    RemoveBucketResponse response = RemoveBucketMessage.send(source,
-        this.partitionedRegion, bucketId, false);
+    RemoveBucketResponse response =
+        RemoveBucketMessage.send(source, this.partitionedRegion, bucketId, false);
-          logger.debug("Successfully created bucket {} in {} but failed to remove it from {}", bucketId, this, source);
+          logger.debug("Successfully created bucket {} in {} but failed to remove it from {}",
+              bucketId, this, source);
-  
+
+   * 
-   * @param bucketId the bucket to  fetch
+   * @param bucketId the bucket to fetch
-  public BucketRegion getInitializedBucketForId(Object key, Integer bucketId) 
+  public BucketRegion getInitializedBucketForId(Object key, Integer bucketId)
-            this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, this.partitionedRegion);
+            this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId,
+            this.partitionedRegion);
-      ForceReattemptException fre = new BucketNotFoundException(LocalizedStrings.PartitionedRegionDataStore_BUCKET_ID_0_NOT_FOUND_ON_VM_1.toLocalizedString(new Object[] {this.partitionedRegion.bucketStringForLogs(bucketId.intValue()), this.partitionedRegion.getMyId()}));
+      ForceReattemptException fre = new BucketNotFoundException(
+          LocalizedStrings.PartitionedRegionDataStore_BUCKET_ID_0_NOT_FOUND_ON_VM_1
+              .toLocalizedString(
+                  new Object[] {this.partitionedRegion.bucketStringForLogs(bucketId.intValue()),
+                      this.partitionedRegion.getMyId()}));
-   * Returns the local BucketRegion given an bucketId.
-   * Returns null if no BucketRegion exists.
+   * Returns the local BucketRegion given an bucketId. Returns null if no BucketRegion exists.
+   * 
-  
-  
+
+
-    Integer bucketId = Integer.valueOf(PartitionedRegionHelper.getHashKey(this.partitionedRegion,
-        null, key, null, null));
+    Integer bucketId = Integer
+        .valueOf(PartitionedRegionHelper.getHashKey(this.partitionedRegion, null, key, null, null));
-   * Test hook to return the per entry overhead for a bucket region.
-   * PRECONDITION: a bucket must exist and be using LRU.
+   * Test hook to return the per entry overhead for a bucket region. PRECONDITION: a bucket must
+   * exist and be using LRU.
+   * 
-    AbstractLRURegionMap map = (AbstractLRURegionMap)br.getRegionMap();
+    AbstractLRURegionMap map = (AbstractLRURegionMap) br.getRegionMap();
-  
+
-   * Fetch a BucketRegion, but do not return until it is initialized
-   * and the primary is known.
+   * Fetch a BucketRegion, but do not return until it is initialized and the primary is known.
+   * 
-  public BucketRegion getInitializedBucketWithKnownPrimaryForId(Object key,
-      Integer bucketId) throws ForceReattemptException {
+  public BucketRegion getInitializedBucketWithKnownPrimaryForId(Object key, Integer bucketId)
+      throws ForceReattemptException {
-   * @param bucketId
-   *          for the key
-   * @param key
-   *          the key, whose value needs to be checks
-   * @throws ForceReattemptException
-   *           if bucket region is null
+   * @param bucketId for the key
+   * @param key the key, whose value needs to be checks
+   * @throws ForceReattemptException if bucket region is null
-   * @throws PRLocallyDestroyedException if the PartitionRegion is locally destroyed 
+   * @throws PRLocallyDestroyedException if the PartitionRegion is locally destroyed
-  public boolean containsValueForKeyLocally(Integer bucketId, Object key) 
-      throws PrimaryBucketException, ForceReattemptException,
-      PRLocallyDestroyedException 
-  {
+  public boolean containsValueForKeyLocally(Integer bucketId, Object key)
+      throws PrimaryBucketException, ForceReattemptException, PRLocallyDestroyedException {
-    boolean ret=false;
+    boolean ret = false;
-    }
-    catch (RegionDestroyedException rde) {
-      if (this.partitionedRegion.isLocallyDestroyed
-          || this.partitionedRegion.isClosed) {
+    } catch (RegionDestroyedException rde) {
+      if (this.partitionedRegion.isLocallyDestroyed || this.partitionedRegion.isClosed) {
-      }
-      else {
+      } else {
-          throw new RegionDestroyedException(LocalizedStrings.PartitionedRegionDataStore_UNABLE_TO_GET_VALUE_FOR_KEY.toLocalizedString(), this.partitionedRegion.toString(), rde);
+          throw new RegionDestroyedException(
+              LocalizedStrings.PartitionedRegionDataStore_UNABLE_TO_GET_VALUE_FOR_KEY
+                  .toLocalizedString(),
+              this.partitionedRegion.toString(), rde);
-    return ret; 
+    return ret;
-  
+
-   * @param bucketId
-   *          the bucketId for the key
-   * @param key
-   *          the key to look for
-   * @throws ForceReattemptException
-   *           if bucket region is null
+   * @param bucketId the bucketId for the key
+   * @param key the key to look for
+   * @throws ForceReattemptException if bucket region is null
-   * @throws PRLocallyDestroyedException if the PartitionRegion is locally destroyed 
+   * @throws PRLocallyDestroyedException if the PartitionRegion is locally destroyed
-  public boolean containsKeyLocally(Integer bucketId, Object key) 
-    throws PrimaryBucketException, ForceReattemptException,
-           PRLocallyDestroyedException 
-  { 
+  public boolean containsKeyLocally(Integer bucketId, Object key)
+      throws PrimaryBucketException, ForceReattemptException, PRLocallyDestroyedException {
-            this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, bucketRegion.getName(), ret);
+            this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId,
+            bucketRegion.getName(), ret);
-      if (this.partitionedRegion.isLocallyDestroyed
-          || this.partitionedRegion.isClosed) {
+      if (this.partitionedRegion.isLocallyDestroyed || this.partitionedRegion.isClosed) {
-      }
-      else {
+      } else {
-                                             this.partitionedRegion.toString(), rde);
+              this.partitionedRegion.toString(), rde);
-   * Test hook that will be invoked before any bucket read. It is invoked after
-   * the bucket is acquired but before the bucket is locked and before the read
-   * operation is done.
+   * Test hook that will be invoked before any bucket read. It is invoked after the bucket is
+   * acquired but before the bucket is locked and before the read operation is done.
+
+
-  
+
-   * @param key
-   *          the key to look for
-   * @param preferCD 
-   * @param requestingClient the client making the request, or null
-   * @param clientEvent client's event (for returning version tag)
-   * @param returnTombstones whether tombstones should be returned
-   * @throws ForceReattemptException
-   *           if bucket region is null
-   * @return value from the bucket region
-   * @throws PrimaryBucketException if the locally managed bucket is not primary
-   * @throws PRLocallyDestroyedException if the PartitionRegion is locally destroyed
-   */
-  public Object getLocally(int bucketId, final Object key,
-      final Object aCallbackArgument, boolean disableCopyOnRead, boolean preferCD, ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones) throws PrimaryBucketException,
-      ForceReattemptException, PRLocallyDestroyedException
-  {
-	  return getLocally(bucketId, key,aCallbackArgument, disableCopyOnRead, preferCD, requestingClient, 
-			  clientEvent, returnTombstones, false);
-  }
-  /**
-   * Returns value corresponding to this key.
-   * @param key
-   *          the key to look for
+   * 
+   * @param key the key to look for
-   * @param opScopeIsLocal if true then just check local storage for a value; if false then try to find the value if it is not local
-   * @throws ForceReattemptException
-   *           if bucket region is null
+   * @throws ForceReattemptException if bucket region is null
-  public Object getLocally(int bucketId,
-                           final Object key,
-                           final Object aCallbackArgument,
-                           boolean disableCopyOnRead,
-                           boolean preferCD,
-                           ClientProxyMembershipID requestingClient,
-                           EntryEventImpl clientEvent,
-                           boolean returnTombstones,
-                           boolean opScopeIsLocal) throws PrimaryBucketException,
-      ForceReattemptException, PRLocallyDestroyedException
-  {
+  public Object getLocally(int bucketId, final Object key, final Object aCallbackArgument,
+      boolean disableCopyOnRead, boolean preferCD, ClientProxyMembershipID requestingClient,
+      EntryEventImpl clientEvent, boolean returnTombstones)
+      throws PrimaryBucketException, ForceReattemptException, PRLocallyDestroyedException {
+    return getLocally(bucketId, key, aCallbackArgument, disableCopyOnRead, preferCD,
+        requestingClient, clientEvent, returnTombstones, false);
+  }
+
+  /**
+   * Returns value corresponding to this key.
+   * 
+   * @param key the key to look for
+   * @param preferCD
+   * @param requestingClient the client making the request, or null
+   * @param clientEvent client's event (for returning version tag)
+   * @param returnTombstones whether tombstones should be returned
+   * @param opScopeIsLocal if true then just check local storage for a value; if false then try to
+   *        find the value if it is not local
+   * @throws ForceReattemptException if bucket region is null
+   * @return value from the bucket region
+   * @throws PrimaryBucketException if the locally managed bucket is not primary
+   * @throws PRLocallyDestroyedException if the PartitionRegion is locally destroyed
+   */
+  public Object getLocally(int bucketId, final Object key, final Object aCallbackArgument,
+      boolean disableCopyOnRead, boolean preferCD, ClientProxyMembershipID requestingClient,
+      EntryEventImpl clientEvent, boolean returnTombstones, boolean opScopeIsLocal)
+      throws PrimaryBucketException, ForceReattemptException, PRLocallyDestroyedException {
-    //  check for primary (when a loader is present) done deeper in the BucketRegion
-    Object ret=null;
+    // check for primary (when a loader is present) done deeper in the BucketRegion
+    Object ret = null;
-          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, bucketRegion.getName(), returnTombstones);
+          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId,
+          bucketRegion.getName(), returnTombstones);
-      ret = bucketRegion.get(key, aCallbackArgument, true, disableCopyOnRead , preferCD, requestingClient, clientEvent, returnTombstones, opScopeIsLocal,
-        false);
+      ret = bucketRegion.get(key, aCallbackArgument, true, disableCopyOnRead, preferCD,
+          requestingClient, clientEvent, returnTombstones, opScopeIsLocal, false);
-    }
-    catch (RegionDestroyedException rde) {
-      if (this.partitionedRegion.isLocallyDestroyed
-          || this.partitionedRegion.isClosed) {
+    } catch (RegionDestroyedException rde) {
+      if (this.partitionedRegion.isLocallyDestroyed || this.partitionedRegion.isClosed) {
-      }
-      else {
+      } else {
-          throw new InternalGemFireError("Got region destroyed message, but neither bucket nor PR was destroyed", rde);
+          throw new InternalGemFireError(
+              "Got region destroyed message, but neither bucket nor PR was destroyed", rde);
-    return ret; 
+    return ret;
+   * 
-   * @throws ForceReattemptException
-   *           if bucket region is null
+   * @throws ForceReattemptException if bucket region is null
-   * @see #getLocally(int, Object, Object, boolean, boolean, ClientProxyMembershipID, EntryEventImpl, boolean)
+   * @see #getLocally(int, Object, Object, boolean, boolean, ClientProxyMembershipID,
+   *      EntryEventImpl, boolean)
-  public RawValue getSerializedLocally(KeyInfo keyInfo,
-                                       boolean doNotLockEntry,
-                                       ClientProxyMembershipID requestingClient,
-                                       EntryEventImpl clientEvent,
-                                       boolean returnTombstones) throws PrimaryBucketException,
-      ForceReattemptException {
-    final BucketRegion bucketRegion = getInitializedBucketForId(keyInfo.getKey(), keyInfo.getBucketId());
-    //  check for primary (when loader is present) done deeper in the BucketRegion
+  public RawValue getSerializedLocally(KeyInfo keyInfo, boolean doNotLockEntry,
+      ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent,
+      boolean returnTombstones) throws PrimaryBucketException, ForceReattemptException {
+    final BucketRegion bucketRegion =
+        getInitializedBucketForId(keyInfo.getKey(), keyInfo.getBucketId());
+    // check for primary (when loader is present) done deeper in the BucketRegion
-          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, keyInfo.getBucketId(), bucketRegion.getName());
+          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR,
+          keyInfo.getBucketId(), bucketRegion.getName());
-      RawValue result = bucketRegion.getSerialized(keyInfo, true, doNotLockEntry, requestingClient, clientEvent, returnTombstones);
+      RawValue result = bucketRegion.getSerialized(keyInfo, true, doNotLockEntry, requestingClient,
+          clientEvent, returnTombstones);
-        if (bucketRegion.isBucketDestroyed()) {
-          // bucket moved by rebalance
-          throw new ForceReattemptException("Bucket removed during get", rde);
-        } else {
-          throw rde;
-        }
-    }
-    catch (IOException e) {
-      throw new ForceReattemptException(LocalizedStrings.PartitionedRegionDataStore_UNABLE_TO_SERIALIZE_VALUE.toLocalizedString(), e);
+      if (bucketRegion.isBucketDestroyed()) {
+        // bucket moved by rebalance
+        throw new ForceReattemptException("Bucket removed during get", rde);
+      } else {
+        throw rde;
+      }
+    } catch (IOException e) {
+      throw new ForceReattemptException(
+          LocalizedStrings.PartitionedRegionDataStore_UNABLE_TO_SERIALIZE_VALUE.toLocalizedString(),
+          e);
-   * Finds the local bucket corresponding to the given key and retrieves the
-   * key's Region.Entry
-   * @param key
-   *          the key to look for
-   * @param access
-   *          true if caller wants last accessed time updated
+   * Finds the local bucket corresponding to the given key and retrieves the key's Region.Entry
+   * 
+   * @param key the key to look for
+   * @param access true if caller wants last accessed time updated
-   * @throws ForceReattemptException
-   *           if bucket region is not present in this process
-   * @return a RegionEntry for the given key, which will be null if the key is
-   *         not in the bucket
-   * @throws EntryNotFoundException
-   *           TODO-javadocs
-   * @throws PRLocallyDestroyedException
-   *           if the PartitionRegion is locally destroyed
+   * @throws ForceReattemptException if bucket region is not present in this process
+   * @return a RegionEntry for the given key, which will be null if the key is not in the bucket
+   * @throws EntryNotFoundException TODO-javadocs
+   * @throws PRLocallyDestroyedException if the PartitionRegion is locally destroyed
-  public EntrySnapshot getEntryLocally(int bucketId, final Object key,
-                                       boolean access, boolean allowTombstones)
-      throws EntryNotFoundException, PrimaryBucketException,
-      ForceReattemptException, PRLocallyDestroyedException
-  {
+  public EntrySnapshot getEntryLocally(int bucketId, final Object key, boolean access,
+      boolean allowTombstones) throws EntryNotFoundException, PrimaryBucketException,
+      ForceReattemptException, PRLocallyDestroyedException {
-          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, bucketRegion.getName());
+          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId,
+          bucketRegion.getName());
-        throw new EntryNotFoundException(LocalizedStrings.PartitionedRegionDataStore_ENTRY_NOT_FOUND.toLocalizedString());
+        throw new EntryNotFoundException(
+            LocalizedStrings.PartitionedRegionDataStore_ENTRY_NOT_FOUND.toLocalizedString());
-      // TODO:KIRK:OK } else if ((ent.isTombstone() && allowTombstones) || !Token.isRemoved(ent.getValueInVM(getPartitionedRegion()))) {
+        // TODO:KIRK:OK } else if ((ent.isTombstone() && allowTombstones) ||
+        // !Token.isRemoved(ent.getValueInVM(getPartitionedRegion()))) {
-        res = new EntrySnapshot(ent, bucketRegion,partitionedRegion, allowTombstones);
+        res = new EntrySnapshot(ent, bucketRegion, partitionedRegion, allowTombstones);
-    }
-    catch (RegionDestroyedException rde) {
-      if (this.partitionedRegion.isLocallyDestroyed
-          || this.partitionedRegion.isClosed) {
+    } catch (RegionDestroyedException rde) {
+      if (this.partitionedRegion.isLocallyDestroyed || this.partitionedRegion.isClosed) {
-      }
-      else {
+      } else {
-          throw new RegionDestroyedException(LocalizedStrings.PartitionedRegionDataStore_UNABLE_TO_GET_ENTRY.toLocalizedString(), this.partitionedRegion.toString(), rde);
+          throw new RegionDestroyedException(
+              LocalizedStrings.PartitionedRegionDataStore_UNABLE_TO_GET_ENTRY.toLocalizedString(),
+              this.partitionedRegion.toString(), rde);
-   * @return The <code>Set</code> of keys for bucketId or
-   *         {@link Collections#EMPTY_SET}if no keys are present
-   *@throws PRLocallyDestroyedException if the PartitionRegion is locally destroyed         
+   * @return The <code>Set</code> of keys for bucketId or {@link Collections#EMPTY_SET}if no keys
+   *         are present
+   * @throws PRLocallyDestroyedException if the PartitionRegion is locally destroyed
-  public Set handleRemoteGetKeys(Integer bucketId, int interestType,
-      Object interestArg, boolean allowTombstones) throws PRLocallyDestroyedException, ForceReattemptException
-  {
+  public Set handleRemoteGetKeys(Integer bucketId, int interestType, Object interestArg,
+      boolean allowTombstones) throws PRLocallyDestroyedException, ForceReattemptException {
-          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, allowTombstones);
+          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId,
+          allowTombstones);
-    try{
-    if (r != null) {
-      invokeBucketReadHook();
-      if (!r.isEmpty() || (allowTombstones && r.getTombstoneCount() > 0)) {
-        ret = r.getKeysWithInterest(interestType, interestArg, allowTombstones);
+    try {
+      if (r != null) {
+        invokeBucketReadHook();
+        if (!r.isEmpty() || (allowTombstones && r.getTombstoneCount() > 0)) {
+          ret = r.getKeysWithInterest(interestType, interestArg, allowTombstones);
+        }
+        checkIfBucketMoved(r);
-      checkIfBucketMoved(r);
-    }
-    }
-    catch (RegionDestroyedException rde) {
-      if (this.partitionedRegion.isLocallyDestroyed
-          || this.partitionedRegion.isClosed) {
+    } catch (RegionDestroyedException rde) {
+      if (this.partitionedRegion.isLocallyDestroyed || this.partitionedRegion.isClosed) {
-      }
-      else {
+      } else {
-          throw new RegionDestroyedException(LocalizedStrings.PartitionedRegionDataStore_UNABLE_TO_FETCH_KEYS_ON_0.toLocalizedString(this.partitionedRegion.toString()), this.partitionedRegion.getFullPath(), rde);
+          throw new RegionDestroyedException(
+              LocalizedStrings.PartitionedRegionDataStore_UNABLE_TO_FETCH_KEYS_ON_0
+                  .toLocalizedString(this.partitionedRegion.toString()),
+              this.partitionedRegion.getFullPath(), rde);
-   * Get the local keys for a given bucket.  This operation should be as efficient as possible,
-   * by avoiding making copies of the returned set.  The returned set can and should
-   * reflect concurrent changes (no ConcurrentModificationExceptions).
+   * Get the local keys for a given bucket. This operation should be as efficient as possible, by
+   * avoiding making copies of the returned set. The returned set can and should reflect concurrent
+   * changes (no ConcurrentModificationExceptions).
-   * @return The <code>Set</code> of keys for bucketId or
-   *         {@link Collections#EMPTY_SET} if no keys are present
-   *@throws PRLocallyDestroyedException if the PartitionRegion is locally destroyed         
+   * @return The <code>Set</code> of keys for bucketId or {@link Collections#EMPTY_SET} if no keys
+   *         are present
+   * @throws PRLocallyDestroyedException if the PartitionRegion is locally destroyed
-  public Set getKeysLocally(Integer bucketId, boolean allowTombstones) throws PRLocallyDestroyedException, ForceReattemptException
-  {
+  public Set getKeysLocally(Integer bucketId, boolean allowTombstones)
+      throws PRLocallyDestroyedException, ForceReattemptException {
-      logger.debug("handleRemoteGetKeys: bucketId: {}{}{}", this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId);
+      logger.debug("handleRemoteGetKeys: bucketId: {}{}{}", this.partitionedRegion.getPRId(),
+          PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId);
-    try{
+    try {
-    }
-    catch (RegionDestroyedException rde) {
-      if (this.partitionedRegion.isLocallyDestroyed
-          || this.partitionedRegion.isClosed) {
+    } catch (RegionDestroyedException rde) {
+      if (this.partitionedRegion.isLocallyDestroyed || this.partitionedRegion.isClosed) {
-      }
-      else {
+      } else {
-            LocalizedStrings.PartitionedRegionDataStore_UNABLE_TO_FETCH_KEYS_ON_0
-              .toLocalizedString(this.partitionedRegion),
-            this.partitionedRegion.getFullPath(), rde);
+              LocalizedStrings.PartitionedRegionDataStore_UNABLE_TO_FETCH_KEYS_ON_0
+                  .toLocalizedString(this.partitionedRegion),
+              this.partitionedRegion.getFullPath(), rde);
-  
+
-  public String toString()
-  {
+  public String toString() {
-      + System.identityHashCode(this) + " name: " + rName 
-      + " bucket count: " + this.localBucket2RegionMap.size();
-    } 
+          + System.identityHashCode(this) + " name: " + rName + " bucket count: "
+          + this.localBucket2RegionMap.size();
+    }
-   * 3) If it finds the bucket region from step 1, it tries to creates the entry
-   * on the region. <br>
-   * 4) If entry already exists, for the key, step 3 would throw
-   * EntryExistsException or else it will create an entry <br>
-   * 5) updateBucket2Size if bucket is on more than 1 node or else bucket
-   * listners would take care of size update. <br>
+   * 3) If it finds the bucket region from step 1, it tries to creates the entry on the region. <br>
+   * 4) If entry already exists, for the key, step 3 would throw EntryExistsException or else it
+   * will create an entry <br>
+   * 5) updateBucket2Size if bucket is on more than 1 node or else bucket listners would take care
+   * of size update. <br>
-   * @param bucketRegion
-   *          the bucket to do the create in
-   * @param event
-   *          the particulars of the operation
-   * @param ifNew
-   *          whether a new entry can be created
-   * @param ifOld
-   *          whether an existing entry can be updated
-   * @param lastModified
-   *          timestamp
-   * @throws ForceReattemptException
-   *           if bucket region is null 
-   * @throws EntryExistsException
-   *           if an entry with this key already exists
+   * @param bucketRegion the bucket to do the create in
+   * @param event the particulars of the operation
+   * @param ifNew whether a new entry can be created
+   * @param ifOld whether an existing entry can be updated
+   * @param lastModified timestamp
+   * @throws ForceReattemptException if bucket region is null
+   * @throws EntryExistsException if an entry with this key already exists
-  public boolean createLocally(final BucketRegion bucketRegion,
-                               final EntryEventImpl event,
-                               boolean ifNew,
-                               boolean ifOld,
-                               boolean requireOldValue,
-                               final long lastModified)
-  throws ForceReattemptException {
+  public boolean createLocally(final BucketRegion bucketRegion, final EntryEventImpl event,
+      boolean ifNew, boolean ifOld, boolean requireOldValue, final long lastModified)
+      throws ForceReattemptException {
-    try{
+    try {
-        result = bucketRegion.basicUpdate(event, ifNew, ifOld,
-            lastModified, true);
-      }
-      else {
+        result = bucketRegion.basicUpdate(event, ifNew, ifOld, lastModified, true);
+      } else {
-        result = bucketRegion.virtualPut(event,
-                                         ifNew,
-                                         ifOld,
-                                         null, // expectedOldValue
-                                         requireOldValue,
-                                         lastModified,
-                                         false);
+        result = bucketRegion.virtualPut(event, ifNew, ifOld, null, // expectedOldValue
+            requireOldValue, lastModified, false);
-//      if (shouldThrowExists && !posDup) {
-//        throw new EntryExistsException(event.getKey().toString());
-//      }
+      // if (shouldThrowExists && !posDup) {
+      // throw new EntryExistsException(event.getKey().toString());
+      // }
-    }
-    catch(RegionDestroyedException rde){
+    } catch (RegionDestroyedException rde) {
-    } 
-    
+    }
+
-    //event.setRegion(this.partitionedRegion);
-    //this.partitionedRegion.notifyBridgeClients(EnumListenerEvent.AFTER_CREATE,
-    //    event);
-    //this.partitionedRegion.notifyGatewayHub(EnumListenerEvent.AFTER_CREATE,
-    //    event);
+    // event.setRegion(this.partitionedRegion);
+    // this.partitionedRegion.notifyBridgeClients(EnumListenerEvent.AFTER_CREATE,
+    // event);
+    // this.partitionedRegion.notifyGatewayHub(EnumListenerEvent.AFTER_CREATE,
+    // event);
-   * 1) Locates the bucket region. If it doesnt find the actual bucket, it means
-   * that this bucket is remapped to some other node and remappedBucketException
-   * is thrown <br>
+   * 1) Locates the bucket region. If it doesnt find the actual bucket, it means that this bucket is
+   * remapped to some other node and remappedBucketException is thrown <br>
-   * 4) if step 2 returns non-null value, perform invalidate on the bucket
-   * region and use value from step 2 in step 5 <br>
-   * 5) updateBucket2Size if bucket is on more than 1 node or else bucket
-   * listners would take care of size update. <br>
+   * 4) if step 2 returns non-null value, perform invalidate on the bucket region and use value from
+   * step 2 in step 5 <br>
+   * 5) updateBucket2Size if bucket is on more than 1 node or else bucket listners would take care
+   * of size update. <br>
-   * @param bucketId
-   *          the bucketId for the key
-   * @param event
-   *          the event that prompted this action
-   * @throws ForceReattemptException
-   *           if bucket region is null 
-   * @throws EntryNotFoundException
-   *           if entry is not found for the key
+   * @param bucketId the bucketId for the key
+   * @param event the event that prompted this action
+   * @throws ForceReattemptException if bucket region is null
+   * @throws EntryNotFoundException if entry is not found for the key
-      throws EntryNotFoundException, PrimaryBucketException,
-      ForceReattemptException
-  {
+      throws EntryNotFoundException, PrimaryBucketException, ForceReattemptException {
-          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId,  event.getKey());
+          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId,
+          event.getKey());
-    
-    // bug 34361: don't send a reply if bucket was destroyed during the op
-    bucketRegion.checkReadiness();
-    // this is now done elsewhere
-    //event.setRegion(this.partitionedRegion);
-    //this.partitionedRegion.notifyBridgeClients(
-    //    EnumListenerEvent.AFTER_INVALIDATE, event);
-    } catch(RegionDestroyedException rde){
+      // bug 34361: don't send a reply if bucket was destroyed during the op
+      bucketRegion.checkReadiness();
+
+      // this is now done elsewhere
+      // event.setRegion(this.partitionedRegion);
+      // this.partitionedRegion.notifyBridgeClients(
+      // EnumListenerEvent.AFTER_INVALIDATE, event);
+    } catch (RegionDestroyedException rde) {
-    } 
+    }
+
-   * This method iterates over localBucket2RegionMap and returns collective size
-   * of the bucket regions. <br>
+   * This method iterates over localBucket2RegionMap and returns collective size of the bucket
+   * regions. <br>
-   * 2) If localBucket2RegionMap is not empty, get keyset of all these bucket
-   * IDs. <br>
+   * 2) If localBucket2RegionMap is not empty, get keyset of all these bucket IDs. <br>
-   * 4) If first node from the node list is current node, increment the size
-   * counter. <br>
-   * 5) Step#4 takes care of the problem of recounting the size of redundant
-   * buckets. <br>
+   * 4) If first node from the node list is current node, increment the size counter. <br>
+   * 5) Step#4 takes care of the problem of recounting the size of redundant buckets. <br>
-   * @return the map of bucketIds and their associated sizes, or
-   *         {@link Collections#EMPTY_MAP}when the size is zero
+   * @return the map of bucketIds and their associated sizes, or {@link Collections#EMPTY_MAP}when
+   *         the size is zero
-  
+
-  public Map<Integer, Integer> getSizeLocally(boolean primaryOnly)
-  {
+  public Map<Integer, Integer> getSizeLocally(boolean primaryOnly) {
-    for (Iterator<Map.Entry<Integer, BucketRegion>> itr = this.localBucket2RegionMap.entrySet().iterator(); itr.hasNext(); ) {
+    for (Iterator<Map.Entry<Integer, BucketRegion>> itr =
+        this.localBucket2RegionMap.entrySet().iterator(); itr.hasNext();) {
-        if(null != r) { // fix for bug#35497
+        if (null != r) { // fix for bug#35497
-  
+
-   * This method iterates over localBucket2RegionMap and returns collective size
-   * of the primary bucket regions.  
+   * This method iterates over localBucket2RegionMap and returns collective size of the primary
+   * bucket regions.
-   * @return the map of bucketIds and their associated sizes, or
-   *         {@link Collections#EMPTY_MAP}when the size is zero
+   * @return the map of bucketIds and their associated sizes, or {@link Collections#EMPTY_MAP}when
+   *         the size is zero
-  public Map<Integer, SizeEntry> getSizeForLocalBuckets()
-  {
+  public Map<Integer, SizeEntry> getSizeForLocalBuckets() {
-  
+
-  private Map<Integer, SizeEntry> getSizeLocallyForPrimary(Collection<Integer> bucketIds, boolean estimate) {
+  private Map<Integer, SizeEntry> getSizeLocallyForPrimary(Collection<Integer> bucketIds,
+      boolean estimate) {
-    for(Integer bucketId : bucketIds) {
+    for (Integer bucketId : bucketIds) {
-        mySizeMap.put(bucketId, new SizeEntry(estimate ? r.sizeEstimate() : r.size(), r.getBucketAdvisor().isPrimary()));
-//        if (getLogWriter().fineEnabled() && r.getBucketAdvisor().isPrimary()) {
-//          r.verifyTombstoneCount();
-//        }
+        mySizeMap.put(bucketId, new SizeEntry(estimate ? r.sizeEstimate() : r.size(),
+            r.getBucketAdvisor().isPrimary()));
+        // if (getLogWriter().fineEnabled() && r.getBucketAdvisor().isPrimary()) {
+        // r.verifyTombstoneCount();
+        // }
-        // sizeEstimate() will throw this exception as it will not try to read from HDFS on a secondary bucket,
+        // sizeEstimate() will throw this exception as it will not try to read from HDFS on a
+        // secondary bucket,
-      } catch(RegionDestroyedException skip) {
+      } catch (RegionDestroyedException skip) {
-    
+  public int getSizeOfLocalBuckets(boolean includeSecondary) {
+    int sizeOfLocal = 0;
+    Set<BucketRegion> primaryBuckets = getAllLocalBucketRegions();
+    for (BucketRegion br : primaryBuckets) {
+      sizeOfLocal += br.size();
+    }
+    return sizeOfLocal;
+  }
+
+
-   * Interface for visiting buckets 
+   * Interface for visiting buckets
+
-      for (Iterator i = this.localBucket2RegionMap.entrySet().iterator(); i.hasNext(); ) {
+      for (Iterator i = this.localBucket2RegionMap.entrySet().iterator(); i.hasNext();) {
-        // of an entry.  Specifically, getValue() performs a CHM.get() and as a result
-        // may return null if the entry was removed, but yet always returns a key 
-        // under the same circumstances... Ouch.  Due to the fact that entries are 
+        // of an entry. Specifically, getValue() performs a CHM.get() and as a result
+        // may return null if the entry was removed, but yet always returns a key
+        // under the same circumstances... Ouch. Due to the fact that entries are
-        // to protect BucketVisitors, in the event iteration occurs during data 
-        // store cleanup.  Bug fix 38680.
+        // to protect BucketVisitors, in the event iteration occurs during data
+        // store cleanup. Bug fix 38680.
-    } 
+    }
-  private void visitBucket(final Integer bucketId, final LocalRegion bucket, final EntryVisitor ev) {
+
+  private void visitBucket(final Integer bucketId, final LocalRegion bucket,
+      final EntryVisitor ev) {
-      for (Iterator ei = bucket.entrySet().iterator(); ei.hasNext(); ) {
+      for (Iterator ei = bucket.entrySet().iterator(); ei.hasNext();) {
-    } catch (CacheRuntimeException ignore) {}
+    } catch (CacheRuntimeException ignore) {
+    }
-  
+
-   * Test class and method for visiting Entries
-   * NOTE: This class will only give a partial view if a visited bucket is moved
-   * by a rebalance while a visit is in progress on that bucket.
+   * Test class and method for visiting Entries NOTE: This class will only give a partial view if a
+   * visited bucket is moved by a rebalance while a visit is in progress on that bucket.
-  protected static abstract class EntryVisitor  {
+  protected static abstract class EntryVisitor {
+
+
-      public void visit(Integer bucketId, Region buk)
-      {
+      public void visit(Integer bucketId, Region buk) {
-          ((LocalRegion)buk).waitForData();
-          for (Iterator ei = buk.entrySet().iterator(); ei.hasNext(); ) {
+          ((LocalRegion) buk).waitForData();
+          for (Iterator ei = buk.entrySet().iterator(); ei.hasNext();) {
-        } catch (CacheRuntimeException ignore) {}
-       knock.finishedVisiting();
+        } catch (CacheRuntimeException ignore) {
+        }
+        knock.finishedVisiting();
-  } 
-  
+  }
+
-   * <i>Test Method</i>
-   * Return the list of PartitionedRegion entries contained in this data store  
+   * <i>Test Method</i> Return the list of PartitionedRegion entries contained in this data store
+   * 
-      public void visit(Integer bucketId, Entry re)
-      {
+      public void visit(Integer bucketId, Entry re) {
+
-  
+
-   * <i>Test Method</i>
-   * Dump all the entries in all the buckets to the logger, validate that the 
+   * <i>Test Method</i> Dump all the entries in all the buckets to the logger, validate that the
+   * 
-      
+
-  
+
-      public void visit(Integer bucketId, Region buk)
-      {
+      public void visit(Integer bucketId, Region buk) {
-          LocalRegion lbuk = (LocalRegion)buk;
+          LocalRegion lbuk = (LocalRegion) buk;
-              logger.debug("Size is not consistent with keySet size! size={} but keySet size={} region={}", size, keySetSize, lbuk);
+              logger.debug(
+                  "Size is not consistent with keySet size! size={} but keySet size={} region={}",
+                  size, keySetSize, lbuk);
-        } catch (CacheRuntimeException ignore) {}
+        } catch (CacheRuntimeException ignore) {
+        }
-  
+
-   * <i>Test Method</i>
-   * Dump all the bucket names in this data store to the logger 
+   * <i>Test Method</i> Dump all the bucket names in this data store to the logger
-      public void visit(Integer bucketId, Region r)
-      {
-        buf.append("bucketId: ").append(partitionedRegion.bucketStringForLogs(bucketId.intValue())).append(" bucketName: ").append(r).append("\n");
+      public void visit(Integer bucketId, Region r) {
+        buf.append("bucketId: ").append(partitionedRegion.bucketStringForLogs(bucketId.intValue()))
+            .append(" bucketName: ").append(r).append("\n");
-  
+
-   * <i>Test Method</i> Return the list of all the bucket names in this data
-   * store.
+   * <i>Test Method</i> Return the list of all the bucket names in this data store.
-  
+
-   * <i>Test Method</i> Return the list of all the primary bucket ids in this
-   * data store.
+   * <i>Test Method</i> Return the list of all the primary bucket ids in this data store.
-        BucketRegion br = (BucketRegion)r;
-        BucketAdvisor ba = (BucketAdvisor)br.getDistributionAdvisor();
+        BucketRegion br = (BucketRegion) r;
+        BucketAdvisor ba = (BucketAdvisor) br.getDistributionAdvisor();
-  
+
-   * <i>Test Method</i> Return the list of all the non primary bucket ids in this
-   * data store.
+   * <i>Test Method</i> Return the list of all the non primary bucket ids in this data store.
-        BucketRegion br = (BucketRegion)r;
-        BucketAdvisor ba = (BucketAdvisor)br.getDistributionAdvisor();
+        BucketRegion br = (BucketRegion) r;
+        BucketAdvisor ba = (BucketAdvisor) br.getDistributionAdvisor();
-  
+
-   * <i>Test Method</i>
-   * Dump the entries in this given bucket to the logger
+   * <i>Test Method</i> Dump the entries in this given bucket to the logger
+   * 
+
-      public void visit(Integer bid, Entry re)
-      {
-        buf.append(re.getKey()).append(" => ").append(re.getValue()).append("\n"); 
+      public void visit(Integer bid, Entry re) {
+        buf.append(re.getKey()).append(" => ").append(re.getValue()).append("\n");
-  } 
-  
+  }
+
-   * Fetch the entries for the given bucket 
+   * Fetch the entries for the given bucket
+   * 
-   * @return  a Map containing all the entries
+   * @return a Map containing all the entries
-  public BucketRegion handleRemoteGetEntries(int bucketId)
-      throws ForceReattemptException  {
+  public BucketRegion handleRemoteGetEntries(int bucketId) throws ForceReattemptException {
-      logger.debug("handleRemoteGetEntries: bucketId: {}{}{}", this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId); 
+      logger.debug("handleRemoteGetEntries: bucketId: {}{}{}", this.partitionedRegion.getPRId(),
+          PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId);
-  public CachePerfStats getCachePerfStats()
-  {
+  public CachePerfStats getCachePerfStats() {
-  
+
-   * Return a set of local buckets.  Iterators may include entries with null 
-   * values (but non-null keys).  
+   * Return a set of local buckets. Iterators may include entries with null values (but non-null
+   * keys).
-  
+
-   * Returns a set of local buckets. 
+   * Returns a set of local buckets.
+   * 
-  
-  public boolean isLocalBucketRegionPresent(){
+
+  public boolean isLocalBucketRegionPresent() {
-  
-  public Set<Integer> getAllLocalPrimaryBucketIdsBetweenProvidedIds(int low,
-      int high) {
+
+  public Set<Integer> getAllLocalPrimaryBucketIdsBetweenProvidedIds(int low, int high) {
-  public Object getLocalValueInVM(final Object key, int bucketId)
-  {
+  public Object getLocalValueInVM(final Object key, int bucketId) {
-    }
-    catch (ForceReattemptException e) {
+    } catch (ForceReattemptException e) {
-  
+
-   * This method is intended for testing purposes only.
-   * DO NOT use in product code.
+   * This method is intended for testing purposes only. DO NOT use in product code.
-  public Object getLocalValueOnDisk(final Object key, int bucketId)
-  {
+  public Object getLocalValueOnDisk(final Object key, int bucketId) {
-    }
-    catch (ForceReattemptException e) {
+    } catch (ForceReattemptException e) {
-  
-  public Object getLocalValueOnDiskOrBuffer(final Object key, int bucketId)
-  {
+
+  public Object getLocalValueOnDiskOrBuffer(final Object key, int bucketId) {
-    }
-    catch (ForceReattemptException e) {
+    } catch (ForceReattemptException e) {
-  
+
-   * Checks for RegionDestroyedException 
-   * in case of remoteEvent & localDestroy OR isClosed
-   * throws a ForceReattemptException
+   * Checks for RegionDestroyedException in case of remoteEvent & localDestroy OR isClosed throws a
+   * ForceReattemptException
+   * 
-   * @param  isOriginRemote true the event we are processing has a remote origin.
-   * @param  rde
-   *  
+   * @param isOriginRemote true the event we are processing has a remote origin.
+   * @param rde
+   * 
-  public void checkRegionDestroyedOnBucket(final BucketRegion br,
-                                            final boolean isOriginRemote,
-                                            RegionDestroyedException rde)
-  throws ForceReattemptException {
-    if (isOriginRemote){
+  public void checkRegionDestroyedOnBucket(final BucketRegion br, final boolean isOriginRemote,
+      RegionDestroyedException rde) throws ForceReattemptException {
+    if (isOriginRemote) {
-        throw new ForceReattemptException("Operation failed due to RegionDestroyedException :"+rde , rde);
+        throw new ForceReattemptException(
+            "Operation failed due to RegionDestroyedException :" + rde, rde);
-    throw new InternalGemFireError("Got region destroyed message, but neither bucket nor PR was destroyed", rde);
+    throw new InternalGemFireError(
+        "Got region destroyed message, but neither bucket nor PR was destroyed", rde);
-   * @param bucketId
-   *          the id of the bucket to create
-   * @param moveSource
-   *          the member id of where the bucket is being copied from, if this is
-   *          a bucket move. Setting this field means that we are allowed to go
-   *          over redundancy.
-   * @param forceCreation
-   *          force the bucket creation, even if it would provide better balance
-   *          if the bucket was placed on another node.
-   * @param replaceOffineData
-   *          create the bucket, even if redundancy is satisfied when
-   *          considering offline members.
-   * @param isRebalance
-   *          true if this is a rebalance
-   * @param creationRequestor
-   *          the id of the member that is atomically creating this bucket on
-   *          all members, if this is an atomic bucket creation.
+   * @param bucketId the id of the bucket to create
+   * @param moveSource the member id of where the bucket is being copied from, if this is a bucket
+   *        move. Setting this field means that we are allowed to go over redundancy.
+   * @param forceCreation force the bucket creation, even if it would provide better balance if the
+   *        bucket was placed on another node.
+   * @param replaceOffineData create the bucket, even if redundancy is satisfied when considering
+   *        offline members.
+   * @param isRebalance true if this is a rebalance
+   * @param creationRequestor the id of the member that is atomically creating this bucket on all
+   *        members, if this is an atomic bucket creation.
-  public CreateBucketResult grabBucket(final int bucketId, 
-                            final InternalDistributedMember moveSource,
-                            final boolean forceCreation,
-                            final boolean replaceOffineData,
-                            final boolean isRebalance,
-                            final InternalDistributedMember creationRequestor,
-                            final boolean isDiskRecovery) {
-    CreateBucketResult grab = grabFreeBucket(bucketId, partitionedRegion.getMyId(), 
-        moveSource, forceCreation, isRebalance, true, replaceOffineData, creationRequestor);
+  public CreateBucketResult grabBucket(final int bucketId,
+      final InternalDistributedMember moveSource, final boolean forceCreation,
+      final boolean replaceOffineData, final boolean isRebalance,
+      final InternalDistributedMember creationRequestor, final boolean isDiskRecovery) {
+    CreateBucketResult grab = grabFreeBucket(bucketId, partitionedRegion.getMyId(), moveSource,
+        forceCreation, isRebalance, true, replaceOffineData, creationRequestor);
-        logger.debug("Failed grab for bucketId = {}{}{}", this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId);
+        logger.debug("Failed grab for bucketId = {}{}{}", this.partitionedRegion.getPRId(),
+            PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId);
-      //   Assert.assertTrue(nList.contains(partitionedRegion.getNode().getMemberId()) , 
+      // Assert.assertTrue(nList.contains(partitionedRegion.getNode().getMemberId()) ,
-        PartitionedRegion pr = (PartitionedRegion)itr.next();
+        PartitionedRegion pr = (PartitionedRegion) itr.next();
-              bucketId, pr.isInitialized(), pr.getDataStore().isColocationComplete(bucketId), pr.getFullPath());
+              bucketId, pr.isInitialized(), pr.getDataStore().isColocationComplete(bucketId),
+              pr.getFullPath());
-            && (pr.getDataStore().isColocationComplete(
-                bucketId))) {
-          grab = pr.getDataStore().grabFreeBucketRecursively(
-              bucketId, pr, moveSource, forceCreation, replaceOffineData, isRebalance, creationRequestor, isDiskRecovery);
+            && (pr.getDataStore().isColocationComplete(bucketId))) {
+          grab = pr.getDataStore().grabFreeBucketRecursively(bucketId, pr, moveSource,
+              forceCreation, replaceOffineData, isRebalance, creationRequestor, isDiskRecovery);
-              logger.debug("Failed grab for bucketId = {}{}{}", this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId);
+              logger.debug("Failed grab for bucketId = {}{}{}", this.partitionedRegion.getPRId(),
+                  PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId);
-      logger.debug("Grab attempt on bucketId={}{}{}; grab:{}",this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, grab); 
+      logger.debug("Grab attempt on bucketId={}{}{}; grab:{}", this.partitionedRegion.getPRId(),
+          PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId, grab);
-   * Checks consistency of bucket and meta data before attempting to grab the
-   * bucket.
+   * Checks consistency of bucket and meta data before attempting to grab the bucket.
-   * @return false if bucket should not be grabbed, else true.
-   * TODO prpersist - move this to BucketRegion
+   * @return false if bucket should not be grabbed, else true. TODO prpersist - move this to
+   *         BucketRegion
-    final boolean isNodeInMetaData = partitionedRegion.getRegionAdvisor()
-        .isBucketLocal(buckId);
+    final boolean isNodeInMetaData = partitionedRegion.getRegionAdvisor().isBucketLocal(buckId);
-        Set owners = partitionedRegion.getRegionAdvisor().getBucketOwners(
-            buckId);
+        Set owners = partitionedRegion.getRegionAdvisor().getBucketOwners(buckId);
-                LocalizedStrings.PartitionedRegionDataStore_VERIFIED_NODELIST_FOR_BUCKETID_0_IS_1,
-                new Object[] { partitionedRegion.bucketStringForLogs(buckId),
-                    PartitionedRegionHelper.printCollection(owners) }));
-        Assert.assertTrue(false, " This node " + partitionedRegion.getNode()
-            + " is managing the bucket with bucketId= "
-            + partitionedRegion.bucketStringForLogs(buckId)
-            + " but doesn't have an entry in " + "b2n region for PR "
-            + partitionedRegion);
+            LocalizedStrings.PartitionedRegionDataStore_VERIFIED_NODELIST_FOR_BUCKETID_0_IS_1,
+            new Object[] {partitionedRegion.bucketStringForLogs(buckId),
+                PartitionedRegionHelper.printCollection(owners)}));
+        Assert.assertTrue(false,
+            " This node " + partitionedRegion.getNode() + " is managing the bucket with bucketId= "
+                + partitionedRegion.bucketStringForLogs(buckId) + " but doesn't have an entry in "
+                + "b2n region for PR " + partitionedRegion);
-        logger.debug("BR#verifyBucketBeforeGrabbing We already host {}{}{}", this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, buckId);
+        logger.debug("BR#verifyBucketBeforeGrabbing We already host {}{}{}",
+            this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, buckId);
-      //It's ok to return true here, we do another check later
-      //to make sure we don't host the bucket.
+      // It's ok to return true here, we do another check later
+      // to make sure we don't host the bucket.
-    }
-    else {
-      if (partitionedRegion.isDestroyed()
-          || partitionedRegion.getGemFireCache().isClosed()) {
+    } else {
+      if (partitionedRegion.isDestroyed() || partitionedRegion.getGemFireCache().isClosed()) {
-          logger.debug("PartitionedRegionDataStore: grabBackupBuckets: This node is not managing the bucket with Id = {} but has an entry in the b2n region for PartitionedRegion {} because destruction of this PartitionedRegion is initiated on other node",
+          logger.debug(
+              "PartitionedRegionDataStore: grabBackupBuckets: This node is not managing the bucket with Id = {} but has an entry in the b2n region for PartitionedRegion {} because destruction of this PartitionedRegion is initiated on other node",
-  public void executeOnDataStore(final Set localKeys, final Function function,
-      final Object object, final int prid, final Set<Integer> bucketSet,
-      final boolean isReExecute,
-      final PartitionedRegionFunctionStreamingMessage msg, long time,
-      ServerConnection servConn, int transactionID){
-    
+  public void executeOnDataStore(final Set localKeys, final Function function, final Object object,
+      final int prid, final Set<Integer> bucketSet, final boolean isReExecute,
+      final PartitionedRegionFunctionStreamingMessage msg, long time, ServerConnection servConn,
+      int transactionID) {
+
-          LocalizedStrings.FunctionService_BUCKET_MIGRATED_TO_ANOTHER_NODE
-              .toLocalizedString());
+          LocalizedStrings.FunctionService_BUCKET_MIGRATED_TO_ANOTHER_NODE.toLocalizedString());
-    
-    ResultSender resultSender = new PartitionedRegionFunctionResultSender(dm,
-        this.partitionedRegion, time, msg, function, bucketSet);  
-    final RegionFunctionContextImpl prContext = new RegionFunctionContextImpl(
-        function.getId(), this.partitionedRegion, object, localKeys,
-        ColocationHelper.constructAndGetAllColocatedLocalDataSet(
-            this.partitionedRegion, bucketSet), bucketSet, resultSender,
-        isReExecute);
+    ResultSender resultSender = new PartitionedRegionFunctionResultSender(dm,
+        this.partitionedRegion, time, msg, function, bucketSet);
+
+    final RegionFunctionContextImpl prContext =
+        new RegionFunctionContextImpl(function.getId(),
+            this.partitionedRegion, object, localKeys, ColocationHelper
+                .constructAndGetAllColocatedLocalDataSet(this.partitionedRegion, bucketSet),
+            bucketSet, resultSender, isReExecute);
-        logger.debug("Executing Function: {} on Remote Node with context: ", function.getId(), prContext);
+        logger.debug("Executing Function: {} on Remote Node with context: ", function.getId(),
+            prContext);
-    }
-    catch (FunctionException functionException) {
+    } catch (FunctionException functionException) {
-        logger.debug("FunctionException occured on remote node while executing Function: {}", function.getId(), functionException);
+        logger.debug("FunctionException occured on remote node while executing Function: {}",
+            function.getId(), functionException);
-      if(functionException.getCause() instanceof QueryInvalidException) {
+      if (functionException.getCause() instanceof QueryInvalidException) {
-      } 
-      throw functionException ;
+      }
+      throw functionException;
-  
+
-//    boolean arr[] = new boolean[]{false, true, false, true , false , false , false , false };
-//    Random random = new Random();
-//    int num = random.nextInt(7);
-//    System.out.println("PRDS.verifyBuckets returning " + arr[num]);
-//    return arr[num];
+    // boolean arr[] = new boolean[]{false, true, false, true , false , false , false , false };
+    // Random random = new Random();
+    // int num = random.nextInt(7);
+    // System.out.println("PRDS.verifyBuckets returning " + arr[num]);
+    // return arr[num];
-      if (!this.partitionedRegion.getRegionAdvisor().getBucketAdvisor(bucket)
-          .isHosting()) {
+      if (!this.partitionedRegion.getRegionAdvisor().getBucketAdvisor(bucket).isHosting()) {
-  
-  
+
+
-  
+
-      logger.debug("PartitionedRegionDataStore for {} handling {}", this.partitionedRegion.getFullPath(), event);
+      logger.debug("PartitionedRegionDataStore for {} handling {}",
+          this.partitionedRegion.getFullPath(), event);
-        AtomicInteger references = (AtomicInteger)this.keysOfInterest.get(key);
+        AtomicInteger references = (AtomicInteger) this.keysOfInterest.get(key);
-            logger.debug("PartitionedRegionDataStore for {} adding interest for: ", this.partitionedRegion.getFullPath(), key);
+            logger.debug("PartitionedRegionDataStore for {} adding interest for: ",
+                this.partitionedRegion.getFullPath(), key);
-        }
-        else {
+        } else {
-            logger.debug("PartitionedRegionDataStore for {} removing interest for: ", this.partitionedRegion.getFullPath(), key);
+            logger.debug("PartitionedRegionDataStore for {} removing interest for: ",
+                this.partitionedRegion.getFullPath(), key);
-          logger.debug("PartitionedRegionDataStore for {} now has {} client(s) interested in key {}",
+          logger.debug(
+              "PartitionedRegionDataStore for {} now has {} client(s) interested in key {}",
-  
+
-    protected void setBucketRegion(boolean b){
+    protected void setBucketRegion(boolean b) {
-    }    
+    }
-  
+
-    
+
-    
+
-    
+
-  public void updateEntryVersionLocally(Integer bucketId, EntryEventImpl event) throws ForceReattemptException {
+  public void updateEntryVersionLocally(Integer bucketId, EntryEventImpl event)
+      throws ForceReattemptException {
-      logger.debug("updateEntryVersionLocally: bucketId={}{}{} for key={}", this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR,
-          bucketId, event.getKey());
+      logger.debug("updateEntryVersionLocally: bucketId={}{}{} for key={}",
+          this.partitionedRegion.getPRId(), PartitionedRegion.BUCKET_ID_SEPARATOR, bucketId,
+          event.getKey());
-    
+
-    } catch(RegionDestroyedException rde){
+    } catch (RegionDestroyedException rde) {
-    } 
+    }
