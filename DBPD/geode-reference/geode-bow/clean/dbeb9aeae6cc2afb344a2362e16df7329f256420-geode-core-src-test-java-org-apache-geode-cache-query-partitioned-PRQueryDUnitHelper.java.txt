Merge branch 'release/1.1.0'

- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license
+ * agreements. See the NOTICE file distributed with this work for additional information regarding
+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License. You may obtain a
+ * copy of the License at
- *      http://www.apache.org/licenses/LICENSE-2.0
+ * http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
+ * Unless required by applicable law or agreed to in writing, software distributed under the License
+ * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+ * or implied. See the License for the specific language governing permissions and limitations under
+ * the License.
+import org.apache.geode.cache.DiskStore;
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-        }
-        catch (IllegalStateException ex) {
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .warning(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreate: Creation caught IllegalStateException",
-                  ex);
+        } catch (IllegalStateException ex) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().warning(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreate: Creation caught IllegalStateException",
+              ex);
-                + regionName + " not in cache", cache.getRegion(regionName));
+                + regionName + " not in cache",
+            cache.getRegion(regionName));
-    return (CacheSerializableRunnable)createPrRegion;
+    return (CacheSerializableRunnable) createPrRegion;
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-        }
-        catch (IllegalStateException ex) {
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .warning(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreate: Creation caught IllegalStateException",
-                  ex);
+        } catch (IllegalStateException ex) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().warning(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreate: Creation caught IllegalStateException",
+              ex);
-                + regionName + " not in cache", cache.getRegion(regionName));
+                + regionName + " not in cache",
+            cache.getRegion(regionName));
-    return (CacheSerializableRunnable)createPrRegion;
+    return (CacheSerializableRunnable) createPrRegion;
-    final String regionName) {
+      final String regionName) {
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-        }
-        catch (IllegalStateException ex) {
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .warning(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreate: Creation caught IllegalStateException",
-                  ex);
+        } catch (IllegalStateException ex) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().warning(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreate: Creation caught IllegalStateException",
+              ex);
-                + regionName + " not in cache", cache.getRegion(regionName));
+                + regionName + " not in cache",
+            cache.getRegion(regionName));
-    return (CacheSerializableRunnable)createPrRegion;
+    return (CacheSerializableRunnable) createPrRegion;
-   * This function creates a appropriate region PR given the scope & the
-   * redundancy parameters *
+   * This function creates a appropriate region PR given the scope & the redundancy parameters *
-  public CacheSerializableRunnable getCacheSerializableRunnableForPRCreate(
-    final String regionName, final int redundancy, final Class constraint) {
-      
+  public CacheSerializableRunnable getCacheSerializableRunnableForPRCreate(final String regionName,
+      final int redundancy, final Class constraint) {
+
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-        PartitionAttributes prAttr = paf.setRedundantCopies(redundancy)
-            .create();
+        PartitionAttributes prAttr = paf.setRedundantCopies(redundancy).create();
-                + regionName + " not in cache", cache.getRegion(regionName));
+                + regionName + " not in cache",
+            cache.getRegion(regionName));
-    return (CacheSerializableRunnable)createPrRegion;
+    return (CacheSerializableRunnable) createPrRegion;
+  }
+
+  /**
+   * This function creates a colocated pair of PR's given the scope & the redundancy parameters for
+   * the parent *
+   *
+   * @param regionName
+   * @param redundancy
+   * @param constraint
+   * @param makePersistent
+   * @return cacheSerializable object
+   */
+  public CacheSerializableRunnable getCacheSerializableRunnableForColocatedPRCreate(
+      final String regionName, final int redundancy, final Class constraint,
+      boolean makePersistent) {
+
+    final String childRegionName = regionName + "Child";
+    final String diskName = "disk";
+    SerializableRunnable createPrRegion;
+    createPrRegion = new CacheSerializableRunnable(regionName) {
+      @Override
+      public void run2() throws CacheException {
+
+        Cache cache = getCache();
+        Region partitionedregion = null;
+        Region childRegion = null;
+        AttributesFactory attr = new AttributesFactory();
+        attr.setValueConstraint(constraint);
+        if (makePersistent) {
+          DiskStore ds = cache.findDiskStore(diskName);
+          if (ds == null) {
+            ds = cache.createDiskStoreFactory().setDiskDirs(JUnit4CacheTestCase.getDiskDirs())
+                .create(diskName);
+          }
+          attr.setDataPolicy(DataPolicy.PERSISTENT_PARTITION);
+          attr.setDiskStoreName(diskName);
+        } else {
+          attr.setDataPolicy(DataPolicy.PARTITION);
+          attr.setDiskStoreName(null);
+        }
+
+        PartitionAttributesFactory paf = new PartitionAttributesFactory();
+        paf.setRedundantCopies(redundancy);
+        attr.setPartitionAttributes(paf.create());
+
+        // parent region
+        partitionedregion = cache.createRegion(regionName, attr.create());
+        assertNotNull(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region "
+                + regionName + " not in cache",
+            cache.getRegion(regionName));
+        assertNotNull(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref null",
+            partitionedregion);
+        assertTrue(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref claims to be destroyed",
+            !partitionedregion.isDestroyed());
+
+        // child region
+        attr.setValueConstraint(constraint);
+        paf.setColocatedWith(regionName);
+        attr.setPartitionAttributes(paf.create());
+        childRegion = cache.createRegion(childRegionName, attr.create());
+      }
+    };
+
+    return (CacheSerializableRunnable) createPrRegion;
+  }
+
+  /**
+   * This function creates the parent region of colocated pair of PR's given the scope & the
+   * redundancy parameters for the parent *
+   *
+   * @param regionName
+   * @param redundancy
+   * @param constraint
+   * @param makePersistent
+   * @return cacheSerializable object
+   */
+  public CacheSerializableRunnable getCacheSerializableRunnableForColocatedParentCreate(
+      final String regionName, final int redundancy, final Class constraint,
+      boolean makePersistent) {
+
+    final String childRegionName = regionName + "Child";
+    final String diskName = "disk";
+    SerializableRunnable createPrRegion;
+    createPrRegion = new CacheSerializableRunnable(regionName + "-NoChildRegion") {
+      @Override
+      public void run2() throws CacheException {
+
+        Cache cache = getCache();
+        Region partitionedregion = null;
+        Region childRegion = null;
+        AttributesFactory attr = new AttributesFactory();
+        attr.setValueConstraint(constraint);
+        if (makePersistent) {
+          DiskStore ds = cache.findDiskStore(diskName);
+          if (ds == null) {
+            ds = cache.createDiskStoreFactory().setDiskDirs(JUnit4CacheTestCase.getDiskDirs())
+                .create(diskName);
+          }
+          attr.setDataPolicy(DataPolicy.PERSISTENT_PARTITION);
+          attr.setDiskStoreName(diskName);
+        } else {
+          attr.setDataPolicy(DataPolicy.PARTITION);
+          attr.setDiskStoreName(null);
+        }
+
+        PartitionAttributesFactory paf = new PartitionAttributesFactory();
+        paf.setRedundantCopies(redundancy);
+        attr.setPartitionAttributes(paf.create());
+
+        // parent region
+        partitionedregion = cache.createRegion(regionName, attr.create());
+        assertNotNull(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region "
+                + regionName + " not in cache",
+            cache.getRegion(regionName));
+        assertNotNull(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref null",
+            partitionedregion);
+        assertTrue(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref claims to be destroyed",
+            !partitionedregion.isDestroyed());
+      }
+    };
+
+    return (CacheSerializableRunnable) createPrRegion;
+  }
+
+  /**
+   * This function creates the parent region of colocated pair of PR's given the scope & the
+   * redundancy parameters for the parent *
+   *
+   * @param regionName
+   * @param redundancy
+   * @param constraint
+   * @param isPersistent
+   * @return cacheSerializable object
+   */
+  public CacheSerializableRunnable getCacheSerializableRunnableForColocatedChildCreate(
+      final String regionName, final int redundancy, final Class constraint, boolean isPersistent) {
+
+    final String childRegionName = regionName + "Child";
+    final String diskName = "disk";
+    SerializableRunnable createPrRegion;
+    createPrRegion = new CacheSerializableRunnable(regionName + "-ChildRegion") {
+      @Override
+      public void run2() throws CacheException {
+
+        Cache cache = getCache();
+        Region partitionedregion = null;
+        Region childRegion = null;
+        AttributesFactory attr = new AttributesFactory();
+        attr.setValueConstraint(constraint);
+        if (isPersistent) {
+          DiskStore ds = cache.findDiskStore(diskName);
+          if (ds == null) {
+            // ds = cache.createDiskStoreFactory().setDiskDirs(getDiskDirs())
+            ds = cache.createDiskStoreFactory()
+                .setDiskDirs(
+                    org.apache.geode.test.dunit.cache.internal.JUnit4CacheTestCase.getDiskDirs())
+                .create(diskName);
+          }
+          attr.setDataPolicy(DataPolicy.PERSISTENT_PARTITION);
+          attr.setDiskStoreName(diskName);
+        } else {
+          attr.setDataPolicy(DataPolicy.PARTITION);
+          attr.setDiskStoreName(null);
+        }
+
+        PartitionAttributesFactory paf = new PartitionAttributesFactory();
+        paf.setRedundantCopies(redundancy);
+        attr.setPartitionAttributes(paf.create());
+
+        // skip parent region creation
+        // partitionedregion = cache.createRegion(regionName, attr.create());
+
+        // child region
+        attr.setValueConstraint(constraint);
+        paf.setColocatedWith(regionName);
+        attr.setPartitionAttributes(paf.create());
+        childRegion = cache.createRegion(childRegionName, attr.create());
+      }
+    };
+
+    return (CacheSerializableRunnable) createPrRegion;
-        
-      SerializableRunnable createPrRegion;
-      createPrRegion = new CacheSerializableRunnable(regionName) {
-        @Override
-        public void run2() throws CacheException
-        {
-          Cache cache = getCache();
-          Region partitionedregion = null;
-          try {
-            AttributesFactory attr = new AttributesFactory();
-            PartitionAttributesFactory paf = new PartitionAttributesFactory();
-            PartitionAttributes prAttr = paf.setRedundantCopies(redundancy).setTotalNumBuckets(buckets)
-                .create();
-            attr.setPartitionAttributes(prAttr);
-            partitionedregion = cache.createRegion(regionName, attr.create());
-          }
-          catch (IllegalStateException ex) {
-            org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-                .warning(
-                    "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Creation caught IllegalStateException",
-                    ex);
-          }
-          assertNotNull(
-              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region "
-                  + regionName + " not in cache", cache.getRegion(regionName));
-          assertNotNull(
-              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref null",
-              partitionedregion);
-          assertTrue(
-              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref claims to be destroyed",
-              !partitionedregion.isDestroyed());
+    SerializableRunnable createPrRegion;
+    createPrRegion = new CacheSerializableRunnable(regionName) {
+      @Override
+      public void run2() throws CacheException {
+
+        Cache cache = getCache();
+        Region partitionedregion = null;
+        try {
+          AttributesFactory attr = new AttributesFactory();
+          PartitionAttributesFactory paf = new PartitionAttributesFactory();
+          PartitionAttributes prAttr =
+              paf.setRedundantCopies(redundancy).setTotalNumBuckets(buckets).create();
+          attr.setPartitionAttributes(prAttr);
+          partitionedregion = cache.createRegion(regionName, attr.create());
+        } catch (IllegalStateException ex) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().warning(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Creation caught IllegalStateException",
+              ex);
-      };
+        assertNotNull(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region "
+                + regionName + " not in cache",
+            cache.getRegion(regionName));
+        assertNotNull(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref null",
+            partitionedregion);
+        assertTrue(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref claims to be destroyed",
+            !partitionedregion.isDestroyed());
+      }
+    };
-      return (CacheSerializableRunnable)createPrRegion;
-    }
+    return (CacheSerializableRunnable) createPrRegion;
+  }
-        
-      SerializableRunnable createPrRegion;
-      createPrRegion = new CacheSerializableRunnable(regionName) {
-        @Override
-        public void run2() throws CacheException
-        {
-          Cache cache = getCache();
-          Region partitionedregion = null;
-          try {
-            cache.createDiskStoreFactory().setDiskDirs(JUnit4CacheTestCase.getDiskDirs()).create("diskstore");
-            AttributesFactory attr = new AttributesFactory();
-            attr.setValueConstraint(constraint);
-            attr.setDataPolicy(DataPolicy.PERSISTENT_PARTITION);
-            attr.setDiskStoreName("diskstore");
+    SerializableRunnable createPrRegion;
+    createPrRegion = new CacheSerializableRunnable(regionName) {
+      @Override
+      public void run2() throws CacheException {
-            PartitionAttributesFactory paf = new PartitionAttributesFactory();
-            PartitionAttributes prAttr = paf.setRedundantCopies(redundancy)
-                .create();
+        Cache cache = getCache();
+        Region partitionedregion = null;
+        try {
+          cache.createDiskStoreFactory().setDiskDirs(JUnit4CacheTestCase.getDiskDirs())
+              .create("diskstore");
+          AttributesFactory attr = new AttributesFactory();
+          attr.setValueConstraint(constraint);
+          attr.setDataPolicy(DataPolicy.PERSISTENT_PARTITION);
+          attr.setDiskStoreName("diskstore");
-            attr.setPartitionAttributes(prAttr);
+          PartitionAttributesFactory paf = new PartitionAttributesFactory();
+          PartitionAttributes prAttr = paf.setRedundantCopies(redundancy).create();
-            partitionedregion = cache.createRegion(regionName, attr.create());
-          }
-          catch (IllegalStateException ex) {
-            org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-                .warning(
-                    "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Creation caught IllegalStateException",
-                    ex);
-          }
-          assertNotNull(
-              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region "
-                  + regionName + " not in cache", cache.getRegion(regionName));
-          assertNotNull(
-              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref null",
-              partitionedregion);
-          assertTrue(
-              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref claims to be destroyed",
-              !partitionedregion.isDestroyed());
+          attr.setPartitionAttributes(prAttr);
+
+          partitionedregion = cache.createRegion(regionName, attr.create());
+        } catch (IllegalStateException ex) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().warning(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Creation caught IllegalStateException",
+              ex);
-      };
+        assertNotNull(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region "
+                + regionName + " not in cache",
+            cache.getRegion(regionName));
+        assertNotNull(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref null",
+            partitionedregion);
+        assertTrue(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref claims to be destroyed",
+            !partitionedregion.isDestroyed());
+      }
+    };
-      return (CacheSerializableRunnable)createPrRegion;
-    }
+    return (CacheSerializableRunnable) createPrRegion;
+  }
-    /**
-     * This function creates a colocated region PR given the oher colocated region.
-     * 
-     * @param regionName
-     * @param redundancy
-     * @param coloRegionName
-     * @return cacheSerializable object
-     */
-    
-    public CacheSerializableRunnable getCacheSerializableRunnableForPRColocatedCreate(
+  /**
+   * This function creates a colocated region PR given the oher colocated region.
+   * 
+   * @param regionName
+   * @param redundancy
+   * @param coloRegionName
+   * @return cacheSerializable object
+   */
+
+  public CacheSerializableRunnable getCacheSerializableRunnableForPRColocatedCreate(
-        
-      SerializableRunnable createPrRegion;
-      createPrRegion = new CacheSerializableRunnable(regionName) {
-        @Override
-        public void run2() throws CacheException
-        {
-  
-          Cache cache = getCache();
-          Region partitionedregion = null;
-          try {
-            Region colocatedRegion = cache.getRegion(coloRegionName);
-            assertNotNull(colocatedRegion);
-            AttributesFactory attr = new AttributesFactory();
-            
-            PartitionAttributesFactory paf = new PartitionAttributesFactory();
-            PartitionAttributes prAttr = paf.setRedundantCopies(redundancy).setColocatedWith(colocatedRegion.getFullPath())
-                .create();
-            attr.setPartitionAttributes(prAttr);
-  
-            partitionedregion = cache.createRegion(regionName, attr.create());
-          }
-          catch (IllegalStateException ex) {
-            org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-                .warning(
-                    "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Creation caught IllegalStateException",
-                    ex);
-          }
-          assertNotNull(
-              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region "
-                  + regionName + " not in cache", cache.getRegion(regionName));
-          assertNotNull(
-              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref null",
-              partitionedregion);
-          assertTrue(
-              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref claims to be destroyed",
-              !partitionedregion.isDestroyed());
+
+    SerializableRunnable createPrRegion;
+    createPrRegion = new CacheSerializableRunnable(regionName) {
+      @Override
+      public void run2() throws CacheException {
+
+        Cache cache = getCache();
+        Region partitionedregion = null;
+        try {
+          Region colocatedRegion = cache.getRegion(coloRegionName);
+          assertNotNull(colocatedRegion);
+          AttributesFactory attr = new AttributesFactory();
+
+          PartitionAttributesFactory paf = new PartitionAttributesFactory();
+          PartitionAttributes prAttr = paf.setRedundantCopies(redundancy)
+              .setColocatedWith(colocatedRegion.getFullPath()).create();
+          attr.setPartitionAttributes(prAttr);
+
+          partitionedregion = cache.createRegion(regionName, attr.create());
+        } catch (IllegalStateException ex) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().warning(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Creation caught IllegalStateException",
+              ex);
-      };
-  
-      return (CacheSerializableRunnable)createPrRegion;
-    }
-  
+        assertNotNull(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region "
+                + regionName + " not in cache",
+            cache.getRegion(regionName));
+        assertNotNull(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref null",
+            partitionedregion);
+        assertTrue(
+            "PRQueryDUnitHelper#getCacheSerializableRunnableForPRCreateWithRedundancy: Partitioned Region ref claims to be destroyed",
+            !partitionedregion.isDestroyed());
+      }
+    };
+
+    return (CacheSerializableRunnable) createPrRegion;
+  }
+
-  public CacheSerializableRunnable getCacheSerializableRunnableForPRPuts(
-      final String regionName, final Object[] portfolio, final int from,
-      final int to)
-  {
+  public CacheSerializableRunnable getCacheSerializableRunnableForPRPuts(final String regionName,
+      final Object[] portfolio, final int from, final int to) {
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-    return (CacheSerializableRunnable)prPuts;
+    return (CacheSerializableRunnable) prPuts;
-  
+
-   * This function puts portfolio objects into the created Region (PR or
-   * RR). Also, other operation like, invalidate, destroy and create are
-   * performed in random manner based on {@link Random#nextInt(int)}.
+   * This function puts portfolio objects into the created Region (PR or RR). Also, other operation
+   * like, invalidate, destroy and create are performed in random manner based on
+   * {@link Random#nextInt(int)}.
-        
+
-              case 0:
-                // Put operation
-                region.put(new Integer(j), new Portfolio(j));
-                break;
-              case 1:
-                // invalidate
-                if (region.containsKey(new Integer(j))) {
-                  region.invalidate(new Integer(j));
-                }
-                break;
-              case 2:
-                if (region.containsKey(new Integer(j))) {
-                  region.destroy(new Integer(j));
-                }
-                break;
-              case 3:
+                case 0:
+                  // Put operation
+                  region.put(new Integer(j), new Portfolio(j));
+                  break;
+                case 1:
+                  // invalidate
+                  if (region.containsKey(new Integer(j))) {
+                    region.invalidate(new Integer(j));
+                  }
+                  break;
+                case 2:
+                  if (region.containsKey(new Integer(j))) {
+                    region.destroy(new Integer(j));
+                  }
+                  break;
+                case 3:
-                if (!region.containsKey(new Integer(j))) {
-                  region.create(new Integer(j), null);
-                }
+                  if (!region.containsKey(new Integer(j))) {
+                    region.create(new Integer(j), null);
+                  }
-                break;
-              default:
-                break;
+                  break;
+                default:
+                  break;
-              org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info("EntryExistsException was thrown for key "+ j);
+              org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
+                  .info("EntryExistsException was thrown for key " + j);
-              org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info("EntryNotFoundException was thrown for key "+ j);
+              org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
+                  .info("EntryNotFoundException was thrown for key " + j);
-      final String regionName, final Object[] portfolio, final int from,
-      final int to)
-  {
+      final String regionName, final Object[] portfolio, final int from, final int to) {
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-        for (int j = from, i = to ; j < to; j++, i++)
+        for (int j = from, i = to; j < to; j++, i++)
-    return (CacheSerializableRunnable)prPuts;
+    return (CacheSerializableRunnable) prPuts;
+
-      final String regionName, final Object[] portfolio, final int from,
-      final int to)
-  {
+      final String regionName, final Object[] portfolio, final int from, final int to) {
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-    return (CacheSerializableRunnable)prPuts;
+    return (CacheSerializableRunnable) prPuts;
-   * 1. Creates & executes a query with Logical Operators on the given PR Region
-   * 2. Executes the same query on the local region <br>
+   * 1. Creates & executes a query with Logical Operators on the given PR Region 2. Executes the
+   * same query on the local region <br>
-    final String regionName, final String localRegion, final boolean fullQueryOnPortfolioPositions) {
+      final String regionName, final String localRegion,
+      final boolean fullQueryOnPortfolioPositions) {
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-        
+
-          queries = new String[] { "import org.apache.geode.cache.\"query\".data.Position;"
-            + "select distinct r.ID, status, mktValue "
-            + "from $1 r, r.positions.values pVal TYPE Position "
-            + "where r.status = 'active' AND pVal.mktValue >= 25.00",
-            
-            "import org.apache.geode.cache.\"query\".data.Position;"
-            + "select distinct * "
-            + "from $1 r, r.positions.values pVal TYPE Position "
-            + "where r.status = 'active' AND pVal.mktValue >= 25.00",
+          queries = new String[] {"import org.apache.geode.cache.\"query\".data.Position;"
+              + "select distinct r.ID, status, mktValue "
+              + "from $1 r, r.positions.values pVal TYPE Position "
+              + "where r.status = 'active' AND pVal.mktValue >= 25.00",
-            "import org.apache.geode.cache.\"query\".data.Position;"
-            + "select distinct ID "
-            + "from $1 r, r.positions.values pVal TYPE Position "
-            + "where r.status = 'active' AND pVal.mktValue >= 25.00",
+              "import org.apache.geode.cache.\"query\".data.Position;" + "select distinct * "
+                  + "from $1 r, r.positions.values pVal TYPE Position "
+                  + "where r.status = 'active' AND pVal.mktValue >= 25.00",
-            "select distinct * "
-            + "from $1 "
-            + "where status = 'active'",
-            
-            "import org.apache.geode.cache.\"query\".data.Position;"
-            + "select distinct r from $1 r, "
-            + "r.positions.values pVal TYPE Position where pVal.mktValue < $2",
-            
-            "select p.positions.get('acc') from $1 p",
-            
+              "import org.apache.geode.cache.\"query\".data.Position;" + "select distinct ID "
+                  + "from $1 r, r.positions.values pVal TYPE Position "
+                  + "where r.status = 'active' AND pVal.mktValue >= 25.00",
+
+              "select distinct * " + "from $1 " + "where status = 'active'",
+
+              "import org.apache.geode.cache.\"query\".data.Position;"
+                  + "select distinct r from $1 r, "
+                  + "r.positions.values pVal TYPE Position where pVal.mktValue < $2",
+
+              "select p.positions.get('acc') from $1 p",
+
-        }
-        else {
-          queries = new String[] { "ID = 0 OR ID = 1", "ID > 4 AND ID < 9", "ID = 5",
-            "ID < 5 ", "ID <= 5" };
+        } else {
+          queries = new String[] {"ID = 0 OR ID = 1", "ID > 4 AND ID < 9", "ID = 5", "ID < 5 ",
+              "ID <= 5"};
-        
-        final String [] expectedExceptions = new String[] {
-            RegionDestroyedException.class.getName(),
-            ReplyException.class.getName(),
-            CacheClosedException.class.getName(),
-            ForceReattemptException.class.getName(),
-            QueryInvocationTargetException.class.getName()
-            };
-        for (int i=0; i<expectedExceptions.length; i++) {
+        final String[] expectedExceptions =
+            new String[] {RegionDestroyedException.class.getName(), ReplyException.class.getName(),
+                CacheClosedException.class.getName(), ForceReattemptException.class.getName(),
+                QueryInvocationTargetException.class.getName()};
+
+        for (int i = 0; i < expectedExceptions.length; i++) {
-            "<ExpectedException action=add>" + expectedExceptions[i]
-                + "</ExpectedException>");
+              "<ExpectedException action=add>" + expectedExceptions[i] + "</ExpectedException>");
-                params = new Object[] { local, new Double((j % 25) * 1.0 + 1) };
+                params = new Object[] {local, new Double((j % 25) * 1.0 + 1)};
-              }
-              else {
+              } else {
-                params = new Object[] { region, new Double((j % 25) * 1.0 + 1) };
+                params = new Object[] {region, new Double((j % 25) * 1.0 + 1)};
-              }
-              else {
+              } else {
-             }
-           }
+            }
+          }
-        }
-        catch (QueryInvocationTargetException e) {
+        } catch (QueryInvocationTargetException e) {
-            // throw an unchecked exception so the controller can examine the cause and see whether or not it's okay
-            throw new TestException("PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception"
-                , e);
+            // throw an unchecked exception so the controller can examine the cause and see whether
+            // or not it's okay
+            throw new TestException(
+                "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception",
+                e);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .error(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught QueryException while querying"
-                      + e, e);
-          throw new TestException("PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception"
-              , e);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught QueryException while querying"
+                  + e,
+              e);
+          throw new TestException(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception",
+              e);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a RegionDestroyedException while querying as expected ",
-                  rde);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a RegionDestroyedException while querying as expected ",
+              rde);
-        }
-        catch (CancelException cce) {
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a CancelException while querying as expected ",
-                  cce);
+        } catch (CancelException cce) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a CancelException while querying as expected ",
+              cce);
-        }
-        finally {
+        } finally {
-              "<ExpectedException action=remove>" + expectedException
-                + "</ExpectedException>");
+                "<ExpectedException action=remove>" + expectedException + "</ExpectedException>");
-    return (CacheSerializableRunnable)PrRegion;
+    return (CacheSerializableRunnable) PrRegion;
-  
+
-      final String regionName, final String localRegion) { 
+      final String regionName, final String localRegion) {
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-        String[] queries = new String[]{
-          "p.status from /REGION_NAME p order by p.status",
-          "* from /REGION_NAME order by status, ID desc",      
-          "status, ID from /REGION_NAME order by status",
-          "p.status, p.ID from /REGION_NAME p order by p.status",      
-          "p.position1.secId, p.ID from /REGION_NAME p order by p.position1.secId",
-          "key from /REGION_NAME.keys key order by key.status",
-          "key.ID from /REGION_NAME.keys key order by key.ID",
-          "key.ID, key.status from /REGION_NAME.keys key order by key.status",
-          "key.ID, key.status from /REGION_NAME.keys key order by key.status, key.ID",
-          "key.ID, key.status from /REGION_NAME.keys key order by key.status desc, key.ID",
-          "key.ID, key.status from /REGION_NAME.keys key order by key.status, key.ID desc",
-          "p.status, p.ID from /REGION_NAME p order by p.status asc, p.ID",
-          "* from /REGION_NAME p order by p.status, p.ID",
-          "p.ID from /REGION_NAME p, p.positions.values order by p.ID",
-          "* from /REGION_NAME p, p.positions.values order by p.ID",
-          "p.ID, p.status from /REGION_NAME p, p.positions.values order by p.status",
-          "pos.secId from /REGION_NAME p, p.positions.values pos order by pos.secId",
-          "p.ID, pos.secId from /REGION_NAME p, p.positions.values pos order by pos.secId",
-          "* from /REGION_NAME p order by p.iD",
-          "p.iD from /REGION_NAME p order by p.iD",
-          "p.iD, p.status from /REGION_NAME p order by p.iD",
-          "iD, status from /REGION_NAME order by iD",
-          "* from /REGION_NAME p order by p.getID()",
-          "p.getID() from /REGION_NAME p order by p.getID()",
-          "* from /REGION_NAME p order by p.names[1]",
-          "* from /REGION_NAME p order by p.getP1().secId",
-          "* from /REGION_NAME p order by p.getP1().getSecId()",
-          "* from /REGION_NAME p order by p.position1.secId",
-          "p.ID, p.position1.secId from /REGION_NAME p order by p.position1.secId",
-          "p.position1.secId, p.ID from /REGION_NAME p order by p.position1.secId",
-          "e.key.ID from /REGION_NAME.entries e order by e.key.ID",
-          "e.key.ID, e.value.status from /REGION_NAME.entries e order by e.key.ID",
-          "e.key.ID, e.value.status from /REGION_NAME.entrySet e order by e.key.ID, e.value.status desc",
-          "e.key, e.value from /REGION_NAME.entrySet e order by e.key.ID, e.value.status desc",
-          "e.key from /REGION_NAME.entrySet e order by e.key.ID, e.key.pkid desc",
-          "p, pos from /REGION_NAME p, p.positions.values pos order by p.ID",
-          "p, pos from /REGION_NAME p, p.positions.values pos order by pos.secId",
-          "p, pos from /REGION_NAME p, p.positions.values pos order by p.ID, pos.secId",
-        };
+        String[] queries = new String[] {"p.status from /REGION_NAME p order by p.status",
+            "* from /REGION_NAME order by status, ID desc",
+            "status, ID from /REGION_NAME order by status",
+            "p.status, p.ID from /REGION_NAME p order by p.status",
+            "p.position1.secId, p.ID from /REGION_NAME p order by p.position1.secId",
+            "key from /REGION_NAME.keys key order by key.status",
+            "key.ID from /REGION_NAME.keys key order by key.ID",
+            "key.ID, key.status from /REGION_NAME.keys key order by key.status",
+            "key.ID, key.status from /REGION_NAME.keys key order by key.status, key.ID",
+            "key.ID, key.status from /REGION_NAME.keys key order by key.status desc, key.ID",
+            "key.ID, key.status from /REGION_NAME.keys key order by key.status, key.ID desc",
+            "p.status, p.ID from /REGION_NAME p order by p.status asc, p.ID",
+            "* from /REGION_NAME p order by p.status, p.ID",
+            "p.ID from /REGION_NAME p, p.positions.values order by p.ID",
+            "* from /REGION_NAME p, p.positions.values order by p.ID",
+            "p.ID, p.status from /REGION_NAME p, p.positions.values order by p.status",
+            "pos.secId from /REGION_NAME p, p.positions.values pos order by pos.secId",
+            "p.ID, pos.secId from /REGION_NAME p, p.positions.values pos order by pos.secId",
+            "* from /REGION_NAME p order by p.iD", "p.iD from /REGION_NAME p order by p.iD",
+            "p.iD, p.status from /REGION_NAME p order by p.iD",
+            "iD, status from /REGION_NAME order by iD", "* from /REGION_NAME p order by p.getID()",
+            "p.getID() from /REGION_NAME p order by p.getID()",
+            "* from /REGION_NAME p order by p.names[1]",
+            "* from /REGION_NAME p order by p.getP1().secId",
+            "* from /REGION_NAME p order by p.getP1().getSecId()",
+            "* from /REGION_NAME p order by p.position1.secId",
+            "p.ID, p.position1.secId from /REGION_NAME p order by p.position1.secId",
+            "p.position1.secId, p.ID from /REGION_NAME p order by p.position1.secId",
+            "e.key.ID from /REGION_NAME.entries e order by e.key.ID",
+            "e.key.ID, e.value.status from /REGION_NAME.entries e order by e.key.ID",
+            "e.key.ID, e.value.status from /REGION_NAME.entrySet e order by e.key.ID, e.value.status desc",
+            "e.key, e.value from /REGION_NAME.entrySet e order by e.key.ID, e.value.status desc",
+            "e.key from /REGION_NAME.entrySet e order by e.key.ID, e.key.pkid desc",
+            "p, pos from /REGION_NAME p, p.positions.values pos order by p.ID",
+            "p, pos from /REGION_NAME p, p.positions.values pos order by pos.secId",
+            "p, pos from /REGION_NAME p, p.positions.values pos order by p.ID, pos.secId",};
-        final String [] expectedExceptions = new String[] {
-            RegionDestroyedException.class.getName(),
-            ReplyException.class.getName(),
-            CacheClosedException.class.getName(),
-            ForceReattemptException.class.getName(),
-            QueryInvocationTargetException.class.getName()
-        };
+        final String[] expectedExceptions =
+            new String[] {RegionDestroyedException.class.getName(), ReplyException.class.getName(),
+                CacheClosedException.class.getName(), ForceReattemptException.class.getName(),
+                QueryInvocationTargetException.class.getName()};
-          getCache().getLogger().info(
-            "<ExpectedException action=add>" + expectedException
-              + "</ExpectedException>");
+          getCache().getLogger()
+              .info("<ExpectedException action=add>" + expectedException + "</ExpectedException>");
-     
-        
+
+
-              qStr = (distinct + queries[j].replace("REGION_NAME", localRegion)); 
+              qStr = (distinct + queries[j].replace("REGION_NAME", localRegion));
-              qStr = (distinct + queries[j].replace("REGION_NAME", regionName)); 
+              qStr = (distinct + queries[j].replace("REGION_NAME", regionName));
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-          .info(
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
-          StructSetOrResultsSet ssORrs = new  StructSetOrResultsSet();
-          ssORrs.CompareQueryResultsWithoutAndWithIndexes(r, queries.length,queries);
-          
-        }
-        catch (QueryInvocationTargetException e) {
-          // throw an unchecked exception so the controller can examine the cause and see whether or not it's okay
-          throw new TestException("PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception"
-              , e);
+          StructSetOrResultsSet ssORrs = new StructSetOrResultsSet();
+          ssORrs.CompareQueryResultsWithoutAndWithIndexes(r, queries.length, queries);
+
+        } catch (QueryInvocationTargetException e) {
+          // throw an unchecked exception so the controller can examine the cause and see whether or
+          // not it's okay
+          throw new TestException(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception",
+              e);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-          .error(
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
-              + e, e);
-          throw new TestException("PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception"
-              , e);
+                  + e,
+              e);
+          throw new TestException(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception",
+              e);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-          .info(
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
-        }
-        catch (CancelException cce) {
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-          .info(
+        } catch (CancelException cce) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
-        }
-        finally {
+        } finally {
-              "<ExpectedException action=remove>" + expectedException
-                + "</ExpectedException>");
+                "<ExpectedException action=remove>" + expectedException + "</ExpectedException>");
-    return (CacheSerializableRunnable)PrRegion;
+    return (CacheSerializableRunnable) PrRegion;
-      final String regionName, final String localRegion) { 
+      final String regionName, final String localRegion) {
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-        String[] queries = new String[]{
-          "p.status from /REGION_NAME p order by p.status",
-          "status, ID from /REGION_NAME order by status, ID",
-          "p.status, p.ID from /REGION_NAME p order by p.status, p.ID",      
-          "key.ID from /REGION_NAME.keys key order by key.ID",
-          "key.ID, key.status from /REGION_NAME.keys key order by key.status, key.ID",
-          "key.ID, key.status from /REGION_NAME.keys key order by key.status desc, key.ID",
-          "key.ID, key.status from /REGION_NAME.keys key order by key.status, key.ID desc",
-          "p.status, p.ID from /REGION_NAME p order by p.status asc, p.ID",
-          "p.ID, p.status from /REGION_NAME p order by p.ID desc, p.status asc",
-          "p.ID from /REGION_NAME p, p.positions.values order by p.ID",
-          "p.ID, p.status from /REGION_NAME p, p.positions.values order by p.status, p.ID",
-          "pos.secId from /REGION_NAME p, p.positions.values pos order by pos.secId",
-          "p.ID, pos.secId from /REGION_NAME p, p.positions.values pos order by pos.secId, p.ID",
-          "p.iD from /REGION_NAME p order by p.iD",
-          "p.iD, p.status from /REGION_NAME p order by p.iD",
-          "iD, status from /REGION_NAME order by iD",
-          "p.getID() from /REGION_NAME p order by p.getID()",
-          "p.names[1] from /REGION_NAME p order by p.names[1]",
-          "p.position1.secId, p.ID from /REGION_NAME p order by p.position1.secId desc, p.ID",
-          "p.ID, p.position1.secId from /REGION_NAME p order by p.position1.secId, p.ID",
-          "e.key.ID from /REGION_NAME.entries e order by e.key.ID",
-          "e.key.ID, e.value.status from /REGION_NAME.entries e order by e.key.ID",
-          "e.key.ID, e.value.status from /REGION_NAME.entrySet e order by e.key.ID desc , e.value.status desc",
-          "e.key, e.value from /REGION_NAME.entrySet e order by e.key.ID, e.value.status desc",
-          "e.key from /REGION_NAME.entrySet e order by e.key.ID desc, e.key.pkid desc",
-          "p.ID, pos.secId from /REGION_NAME p, p.positions.values pos order by p.ID, pos.secId",
-          "p.ID, pos.secId from /REGION_NAME p, p.positions.values pos order by p.ID desc, pos.secId desc",
-          "p.ID, pos.secId from /REGION_NAME p, p.positions.values pos order by p.ID desc, pos.secId",
-        };
+        String[] queries = new String[] {"p.status from /REGION_NAME p order by p.status",
+            "status, ID from /REGION_NAME order by status, ID",
+            "p.status, p.ID from /REGION_NAME p order by p.status, p.ID",
+            "key.ID from /REGION_NAME.keys key order by key.ID",
+            "key.ID, key.status from /REGION_NAME.keys key order by key.status, key.ID",
+            "key.ID, key.status from /REGION_NAME.keys key order by key.status desc, key.ID",
+            "key.ID, key.status from /REGION_NAME.keys key order by key.status, key.ID desc",
+            "p.status, p.ID from /REGION_NAME p order by p.status asc, p.ID",
+            "p.ID, p.status from /REGION_NAME p order by p.ID desc, p.status asc",
+            "p.ID from /REGION_NAME p, p.positions.values order by p.ID",
+            "p.ID, p.status from /REGION_NAME p, p.positions.values order by p.status, p.ID",
+            "pos.secId from /REGION_NAME p, p.positions.values pos order by pos.secId",
+            "p.ID, pos.secId from /REGION_NAME p, p.positions.values pos order by pos.secId, p.ID",
+            "p.iD from /REGION_NAME p order by p.iD",
+            "p.iD, p.status from /REGION_NAME p order by p.iD",
+            "iD, status from /REGION_NAME order by iD",
+            "p.getID() from /REGION_NAME p order by p.getID()",
+            "p.names[1] from /REGION_NAME p order by p.names[1]",
+            "p.position1.secId, p.ID from /REGION_NAME p order by p.position1.secId desc, p.ID",
+            "p.ID, p.position1.secId from /REGION_NAME p order by p.position1.secId, p.ID",
+            "e.key.ID from /REGION_NAME.entries e order by e.key.ID",
+            "e.key.ID, e.value.status from /REGION_NAME.entries e order by e.key.ID",
+            "e.key.ID, e.value.status from /REGION_NAME.entrySet e order by e.key.ID desc , e.value.status desc",
+            "e.key, e.value from /REGION_NAME.entrySet e order by e.key.ID, e.value.status desc",
+            "e.key from /REGION_NAME.entrySet e order by e.key.ID desc, e.key.pkid desc",
+            "p.ID, pos.secId from /REGION_NAME p, p.positions.values pos order by p.ID, pos.secId",
+            "p.ID, pos.secId from /REGION_NAME p, p.positions.values pos order by p.ID desc, pos.secId desc",
+            "p.ID, pos.secId from /REGION_NAME p, p.positions.values pos order by p.ID desc, pos.secId",};
-        final String [] expectedExceptions = new String[] {
-            RegionDestroyedException.class.getName(),
-            ReplyException.class.getName(),
-            CacheClosedException.class.getName(),
-            ForceReattemptException.class.getName(),
-            QueryInvocationTargetException.class.getName()
-        };
+        final String[] expectedExceptions =
+            new String[] {RegionDestroyedException.class.getName(), ReplyException.class.getName(),
+                CacheClosedException.class.getName(), ForceReattemptException.class.getName(),
+                QueryInvocationTargetException.class.getName()};
-          getCache().getLogger().info(
-            "<ExpectedException action=add>" + expectedException
-              + "</ExpectedException>");
+          getCache().getLogger()
+              .info("<ExpectedException action=add>" + expectedException + "</ExpectedException>");
-        StructSetOrResultsSet ssORrs = new  StructSetOrResultsSet();
-        
+        StructSetOrResultsSet ssORrs = new StructSetOrResultsSet();
+
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-          .info(
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
-        }
-        catch (QueryInvocationTargetException e) {
-          // throw an unchecked exception so the controller can examine the cause and see whether or not it's okay
-          throw new TestException("PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception"
-              , e);
+        } catch (QueryInvocationTargetException e) {
+          // throw an unchecked exception so the controller can examine the cause and see whether or
+          // not it's okay
+          throw new TestException(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception",
+              e);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-          .error(
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
-              + e, e);
-          throw new TestException("PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception"
-              , e);
+                  + e,
+              e);
+          throw new TestException(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception",
+              e);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-          .info(
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
-        }
-        catch (CancelException cce) {
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-          .info(
+        } catch (CancelException cce) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
-        }
-        finally {
+        } finally {
-              "<ExpectedException action=remove>" + expectedException
-                + "</ExpectedException>");
+                "<ExpectedException action=remove>" + expectedException + "</ExpectedException>");
-    return (CacheSerializableRunnable)PrRegion;
+    return (CacheSerializableRunnable) PrRegion;
- public CacheSerializableRunnable getCacheSerializableRunnableForPROrderByQueryWithLimit(
-      final String regionName, final String localRegion) { 
+  public CacheSerializableRunnable getCacheSerializableRunnableForPROrderByQueryWithLimit(
+      final String regionName, final String localRegion) {
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-        String[] queries = new String[]{
-          "status as st from /REGION_NAME order by status",
-          "p.status from /REGION_NAME p order by p.status",      
-          "p.position1.secId, p.ID from /REGION_NAME p order by p.position1.secId, p.ID desc",
-          "key from /REGION_NAME.keys key order by key.status, key.ID",
-          "key.ID from /REGION_NAME.keys key order by key.ID",
-          "key.ID, key.status from /REGION_NAME.keys key order by key.status, key.ID asc",
-          "key.ID, key.status from /REGION_NAME.keys key order by key.status desc, key.ID",
-          "p.status, p.ID from /REGION_NAME p order by p.status asc, p.ID",
-          "p.ID from /REGION_NAME p, p.positions.values order by p.ID",
-          "* from /REGION_NAME p, p.positions.values val order by p.ID, val.secId",
-          "p.iD, p.status from /REGION_NAME p order by p.iD",
-          "iD, status from /REGION_NAME order by iD",
-          "* from /REGION_NAME p order by p.getID()",
-          "* from /REGION_NAME p order by p.getP1().secId, p.ID desc, p.ID",
-          " p.position1.secId , p.ID as st from /REGION_NAME p order by p.position1.secId, p.ID",
-          "e.key.ID, e.value.status from /REGION_NAME.entrySet e order by e.key.ID, e.value.status desc",
-          "e.key from /REGION_NAME.entrySet e order by e.key.ID, e.key.pkid desc",
-          "p, pos from /REGION_NAME p, p.positions.values pos order by p.ID, pos.secId desc",
-          "p, pos from /REGION_NAME p, p.positions.values pos order by pos.secId, p.ID",          
-          "status , ID as ied from /REGION_NAME where ID > 0 order by status, ID desc",
-          "p.status as st, p.ID as id from /REGION_NAME p where ID > 0 and status = 'inactive' order by p.status, p.ID desc",      
-          "p.position1.secId as st, p.ID as ied from /REGION_NAME p where p.ID > 0 and p.position1.secId != 'IBM' order by p.position1.secId, p.ID",        
-          " key.status as st, key.ID from /REGION_NAME.keys key where key.ID > 5 order by key.status, key.ID desc",
-          " key.ID, key.status as st from /REGION_NAME.keys key where key.status = 'inactive' order by key.status desc, key.ID",
-          
+        String[] queries = new String[] {"status as st from /REGION_NAME order by status",
+            "p.status from /REGION_NAME p order by p.status",
+            "p.position1.secId, p.ID from /REGION_NAME p order by p.position1.secId, p.ID desc",
+            "key from /REGION_NAME.keys key order by key.status, key.ID",
+            "key.ID from /REGION_NAME.keys key order by key.ID",
+            "key.ID, key.status from /REGION_NAME.keys key order by key.status, key.ID asc",
+            "key.ID, key.status from /REGION_NAME.keys key order by key.status desc, key.ID",
+            "p.status, p.ID from /REGION_NAME p order by p.status asc, p.ID",
+            "p.ID from /REGION_NAME p, p.positions.values order by p.ID",
+            "* from /REGION_NAME p, p.positions.values val order by p.ID, val.secId",
+            "p.iD, p.status from /REGION_NAME p order by p.iD",
+            "iD, status from /REGION_NAME order by iD", "* from /REGION_NAME p order by p.getID()",
+            "* from /REGION_NAME p order by p.getP1().secId, p.ID desc, p.ID",
+            " p.position1.secId , p.ID as st from /REGION_NAME p order by p.position1.secId, p.ID",
+            "e.key.ID, e.value.status from /REGION_NAME.entrySet e order by e.key.ID, e.value.status desc",
+            "e.key from /REGION_NAME.entrySet e order by e.key.ID, e.key.pkid desc",
+            "p, pos from /REGION_NAME p, p.positions.values pos order by p.ID, pos.secId desc",
+            "p, pos from /REGION_NAME p, p.positions.values pos order by pos.secId, p.ID",
+            "status , ID as ied from /REGION_NAME where ID > 0 order by status, ID desc",
+            "p.status as st, p.ID as id from /REGION_NAME p where ID > 0 and status = 'inactive' order by p.status, p.ID desc",
+            "p.position1.secId as st, p.ID as ied from /REGION_NAME p where p.ID > 0 and p.position1.secId != 'IBM' order by p.position1.secId, p.ID",
+            " key.status as st, key.ID from /REGION_NAME.keys key where key.ID > 5 order by key.status, key.ID desc",
+            " key.ID, key.status as st from /REGION_NAME.keys key where key.status = 'inactive' order by key.status desc, key.ID",
+
-        final String [] expectedExceptions = new String[] {
-            RegionDestroyedException.class.getName(),
-            ReplyException.class.getName(),
-            CacheClosedException.class.getName(),
-            ForceReattemptException.class.getName(),
-            QueryInvocationTargetException.class.getName()
-        };
+        final String[] expectedExceptions =
+            new String[] {RegionDestroyedException.class.getName(), ReplyException.class.getName(),
+                CacheClosedException.class.getName(), ForceReattemptException.class.getName(),
+                QueryInvocationTargetException.class.getName()};
-          getCache().getLogger().info(
-            "<ExpectedException action=add>" + expectedException
-              + "</ExpectedException>");
+          getCache().getLogger()
+              .info("<ExpectedException action=add>" + expectedException + "</ExpectedException>");
-             
+
-          for (int l=1; l <= 3; l++) {
+          for (int l = 1; l <= 3; l++) {
-                qStr += (" LIMIT " + (l*l));
+                qStr += (" LIMIT " + (l * l));
-                SelectResults sr = (SelectResults)qs.newQuery(qStr).execute();
+                SelectResults sr = (SelectResults) qs.newQuery(qStr).execute();
-                if(sr.asList().size() > l*l) {
-                  fail("The resultset size exceeds limit size. Limit size="+ l*l+", result size ="+ sr.asList().size());
+                if (sr.asList().size() > l * l) {
+                  fail("The resultset size exceeds limit size. Limit size=" + l * l
+                      + ", result size =" + sr.asList().size());
-                qStr += (" LIMIT " + (l*l));
+                qStr += (" LIMIT " + (l * l));
-                SelectResults srr = (SelectResults)qs.newQuery(qStr).execute();
+                SelectResults srr = (SelectResults) qs.newQuery(qStr).execute();
-                if(srr.size() > l*l) {
-                  fail("The resultset size exceeds limit size. Limit size="+ l*l+", result size ="+ srr.asList().size());
+                if (srr.size() > l * l) {
+                  fail("The resultset size exceeds limit size. Limit size=" + l * l
+                      + ", result size =" + srr.asList().size());
-                //assertIndexDetailsEquals("The resultset size is not same as limit size.", l*l, srr.asList().size());
+                // assertIndexDetailsEquals("The resultset size is not same as limit size.", l*l,
+                // srr.asList().size());
-//                getCache().getLogger().info("Finished executing PR query: " + qStr);
+                // getCache().getLogger().info("Finished executing PR query: " + qStr);
-            StructSetOrResultsSet ssORrs = new  StructSetOrResultsSet();
-            ssORrs.CompareQueryResultsWithoutAndWithIndexes(r, queries.length,true,rq);
+            StructSetOrResultsSet ssORrs = new StructSetOrResultsSet();
+            ssORrs.CompareQueryResultsWithoutAndWithIndexes(r, queries.length, true, rq);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-          .info(
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
-        }
-        catch (QueryInvocationTargetException e) {
-          // throw an unchecked exception so the controller can examine the cause and see whether or not it's okay
-          throw new TestException("PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception"
-              , e);
+        } catch (QueryInvocationTargetException e) {
+          // throw an unchecked exception so the controller can examine the cause and see whether or
+          // not it's okay
+          throw new TestException(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception",
+              e);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-          .error(
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
-              + e, e);
-          throw new TestException("PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception"
-              , e);
+                  + e,
+              e);
+          throw new TestException(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception",
+              e);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-          .info(
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
-        }
-        catch (CancelException cce) {
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-          .info(
+        } catch (CancelException cce) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
-        }
-        finally {
+        } finally {
-              "<ExpectedException action=remove>" + expectedException
-                + "</ExpectedException>");
+                "<ExpectedException action=remove>" + expectedException + "</ExpectedException>");
-    return (CacheSerializableRunnable)PrRegion;
+    return (CacheSerializableRunnable) PrRegion;
- public CacheSerializableRunnable getCacheSerializableRunnableForPRCountStarQueries(
-     final String regionName, final String localRegion) { 
-   SerializableRunnable PrRegion = new CacheSerializableRunnable("PRCountStarQuery") {
-     public void run2() throws CacheException
-     {
+  public CacheSerializableRunnable getCacheSerializableRunnableForPRCountStarQueries(
+      final String regionName, final String localRegion) {
+    SerializableRunnable PrRegion = new CacheSerializableRunnable("PRCountStarQuery") {
+      public void run2() throws CacheException {
-       Cache cache = getCache();
-       // Querying the localRegion and the PR region
+        Cache cache = getCache();
+        // Querying the localRegion and the PR region
-       String[] queries = new String[]{
-           "select COUNT(*) from /" + regionName,
-           "select COUNT(*) from /" + regionName + " where ID > 0",
-           "select COUNT(*) from /" + regionName + " where ID > 0 AND status='active'",
-           "select COUNT(*) from /" + regionName + " where ID > 0 OR status='active'",
-           "select COUNT(*) from /" + regionName + " where ID > 0 AND status LIKE 'act%'",
-           "select COUNT(*) from /" + regionName + " where ID > 0 OR status LIKE 'ina%'",
-           "select COUNT(*) from /" + regionName + " where ID IN SET(1, 2, 3, 4, 5)",
-           "select COUNT(*) from /" + regionName + " where NOT (ID > 5)",
-           "select DISTINCT COUNT(*) from /" + regionName + " where ID > 0",
-           "select DISTINCT COUNT(*) from /" + regionName + " where ID > 0 AND status='active'",
-           "select DISTINCT COUNT(*) from /" + regionName + " where ID > 0 OR status='active'",
-           "select DISTINCT COUNT(*) from /" + regionName + " where ID > 0 AND status LIKE 'act%'",
-           "select DISTINCT COUNT(*) from /" + regionName + " where ID > 0 OR status LIKE 'ina%'",
-           "select DISTINCT COUNT(*) from /" + regionName + " where ID IN SET(1, 2, 3, 4, 5)",
-           "select DISTINCT COUNT(*) from /" + regionName + " where NOT (ID > 5)",
-           "select COUNT(*) from /" + regionName + " p, p.positions.values pos where p.ID > 0 AND pos.secId = 'IBM'",
-           "select DISTINCT COUNT(*) from /" + regionName + " p, p.positions.values pos where p.ID > 0 AND pos.secId = 'IBM'",
-           "select COUNT(*) from /" + regionName + " p, p.positions.values pos where p.ID > 0 AND pos.secId = 'IBM' LIMIT 5",
-           "select DISTINCT COUNT(*) from /" + regionName + " p, p.positions.values pos where p.ID > 0 AND pos.secId = 'IBM' ORDER BY p.ID",
-           "select COUNT(*) from /" + regionName + " p, p.positions.values pos where p.ID > 0 AND p.status = 'active' AND pos.secId = 'IBM'",
-           "select COUNT(*) from /" + regionName + " p, p.positions.values pos where p.ID > 0 AND p.status = 'active' OR pos.secId = 'IBM'",
-           "select COUNT(*) from /" + regionName + " p, p.positions.values pos where p.ID > 0 OR p.status = 'active' OR pos.secId = 'IBM'",
-           "select COUNT(*) from /" + regionName + " p, p.positions.values pos where p.ID > 0 OR p.status = 'active' OR pos.secId = 'IBM' LIMIT 150",
-           //"select DISTINCT COUNT(*) from /" + regionName + " p, p.positions.values pos where p.ID > 0 OR p.status = 'active' OR pos.secId = 'IBM' ORDER BY p.ID",
-       };
+        String[] queries = new String[] {"select COUNT(*) from /" + regionName,
+            "select COUNT(*) from /" + regionName + " where ID > 0",
+            "select COUNT(*) from /" + regionName + " where ID > 0 AND status='active'",
+            "select COUNT(*) from /" + regionName + " where ID > 0 OR status='active'",
+            "select COUNT(*) from /" + regionName + " where ID > 0 AND status LIKE 'act%'",
+            "select COUNT(*) from /" + regionName + " where ID > 0 OR status LIKE 'ina%'",
+            "select COUNT(*) from /" + regionName + " where ID IN SET(1, 2, 3, 4, 5)",
+            "select COUNT(*) from /" + regionName + " where NOT (ID > 5)",
+            "select DISTINCT COUNT(*) from /" + regionName + " where ID > 0",
+            "select DISTINCT COUNT(*) from /" + regionName + " where ID > 0 AND status='active'",
+            "select DISTINCT COUNT(*) from /" + regionName + " where ID > 0 OR status='active'",
+            "select DISTINCT COUNT(*) from /" + regionName + " where ID > 0 AND status LIKE 'act%'",
+            "select DISTINCT COUNT(*) from /" + regionName + " where ID > 0 OR status LIKE 'ina%'",
+            "select DISTINCT COUNT(*) from /" + regionName + " where ID IN SET(1, 2, 3, 4, 5)",
+            "select DISTINCT COUNT(*) from /" + regionName + " where NOT (ID > 5)",
+            "select COUNT(*) from /" + regionName
+                + " p, p.positions.values pos where p.ID > 0 AND pos.secId = 'IBM'",
+            "select DISTINCT COUNT(*) from /" + regionName
+                + " p, p.positions.values pos where p.ID > 0 AND pos.secId = 'IBM'",
+            "select COUNT(*) from /" + regionName
+                + " p, p.positions.values pos where p.ID > 0 AND pos.secId = 'IBM' LIMIT 5",
+            "select DISTINCT COUNT(*) from /" + regionName
+                + " p, p.positions.values pos where p.ID > 0 AND pos.secId = 'IBM' ORDER BY p.ID",
+            "select COUNT(*) from /" + regionName
+                + " p, p.positions.values pos where p.ID > 0 AND p.status = 'active' AND pos.secId = 'IBM'",
+            "select COUNT(*) from /" + regionName
+                + " p, p.positions.values pos where p.ID > 0 AND p.status = 'active' OR pos.secId = 'IBM'",
+            "select COUNT(*) from /" + regionName
+                + " p, p.positions.values pos where p.ID > 0 OR p.status = 'active' OR pos.secId = 'IBM'",
+            "select COUNT(*) from /" + regionName
+                + " p, p.positions.values pos where p.ID > 0 OR p.status = 'active' OR pos.secId = 'IBM' LIMIT 150",
+            // "select DISTINCT COUNT(*) from /" + regionName + " p, p.positions.values pos where
+            // p.ID > 0 OR p.status = 'active' OR pos.secId = 'IBM' ORDER BY p.ID",
+        };
-       Object r[][] = new Object[queries.length][2];
-       Region region = cache.getRegion(regionName);
-       assertNotNull(region);
+        Object r[][] = new Object[queries.length][2];
+        Region region = cache.getRegion(regionName);
+        assertNotNull(region);
-       final String [] expectedExceptions = new String[] {
-           RegionDestroyedException.class.getName(),
-           ReplyException.class.getName(),
-           CacheClosedException.class.getName(),
-           ForceReattemptException.class.getName(),
-           QueryInvocationTargetException.class.getName()
-       };
+        final String[] expectedExceptions =
+            new String[] {RegionDestroyedException.class.getName(), ReplyException.class.getName(),
+                CacheClosedException.class.getName(), ForceReattemptException.class.getName(),
+                QueryInvocationTargetException.class.getName()};
-       for (final String expectedException : expectedExceptions) {
-         getCache().getLogger().info(
-           "<ExpectedException action=add>" + expectedException
-             + "</ExpectedException>");
-       }
+        for (final String expectedException : expectedExceptions) {
+          getCache().getLogger()
+              .info("<ExpectedException action=add>" + expectedException + "</ExpectedException>");
+        }
-       QueryService qs = getCache().getQueryService();
-       Object[] params;
-            
-       try {
+        QueryService qs = getCache().getQueryService();
+        Object[] params;
+
+        try {
-              SelectResults srr = (SelectResults) qs.newQuery(qStr.replace(regionName, localRegion)).execute();
+              SelectResults srr =
+                  (SelectResults) qs.newQuery(qStr.replace(regionName, localRegion)).execute();
-         org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-         .info(
-             "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Queries Executed successfully on Local region & PR Region");
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Queries Executed successfully on Local region & PR Region");
-         StructSetOrResultsSet ssORrs = new  StructSetOrResultsSet();
-         ssORrs.CompareCountStarQueryResultsWithoutAndWithIndexes(r, queries.length,true,queries);
-         
-       }
-       catch (QueryInvocationTargetException e) {
-         // throw an unchecked exception so the controller can examine the cause and see whether or not it's okay
-         throw new TestException("PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception"
-             , e);
-       }
+          StructSetOrResultsSet ssORrs = new StructSetOrResultsSet();
+          ssORrs.CompareCountStarQueryResultsWithoutAndWithIndexes(r, queries.length, true,
+              queries);
-       catch (QueryException e) {
-         org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-         .error(
-             "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught QueryException while querying"
-             + e, e);
-         throw new TestException("PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception"
-             , e);
-       }
+        } catch (QueryInvocationTargetException e) {
+          // throw an unchecked exception so the controller can examine the cause and see whether or
+          // not it's okay
+          throw new TestException(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception",
+              e);
+        }
-       catch (RegionDestroyedException rde) {
-         org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-         .info(
-             "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a RegionDestroyedException while querying as expected ",
-             rde);
+        catch (QueryException e) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught QueryException while querying"
+                  + e,
+              e);
+          throw new TestException(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught unexpected query exception",
+              e);
+        }
-       }
-       catch (CancelException cce) {
-         org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-         .info(
-             "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a CancelException while querying as expected ",
-             cce);
+        catch (RegionDestroyedException rde) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a RegionDestroyedException while querying as expected ",
+              rde);
-       }
-       finally {
-         for (final String expectedException : expectedExceptions) {
-           getCache().getLogger().info(
-             "<ExpectedException action=remove>" + expectedException
-               + "</ExpectedException>");
-         }
-       }
+        } catch (CancelException cce) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a CancelException while querying as expected ",
+              cce);
-     }
+        } finally {
+          for (final String expectedException : expectedExceptions) {
+            getCache().getLogger().info(
+                "<ExpectedException action=remove>" + expectedException + "</ExpectedException>");
+          }
+        }
-   };
-   return (CacheSerializableRunnable)PrRegion;
- }
+      }
+
+    };
+    return (CacheSerializableRunnable) PrRegion;
+  }
-        
+
-      
-          Collection indexes = qs.getIndexes();
-          Iterator it = indexes.iterator();
-          while(it.hasNext()) {
-            PartitionedIndex ind = (PartitionedIndex)it.next();
-            /*List bucketIndex = ind.getBucketIndexes();
-            int k = 0;
-            logger.info("Total number of bucket index : "+bucketIndex.size());
-            while ( k < bucketIndex.size() ){
-              Index bukInd = (Index)bucketIndex.get(k);
-              logger.info("Buket Index "+bukInd+"  usage : "+bukInd.getStatistics().getTotalUses());
-              // if number of quries on pr change in getCacheSerializableRunnableForPRQueryAndCompareResults
-              // literal 6  should change.
-              //Asif :  With the optmization of Range Queries a where clause
-              // containing something like ID > 4 AND ID < 9 will be evaluated 
-              //using a single index lookup, so accordingly modifying the 
-              //assert value from 7 to 6
-              // Anil : With aquiringReadLock during Index.getSizeEstimate(), the
-              // Index usage in case of "ID = 0 OR ID = 1" is increased by 3.
-              int indexUsageWithSizeEstimation = 3;
-              int expectedUse = 6;
-              long indexUse = bukInd.getStatistics().getTotalUses();
-              // Anil : With chnages to use single index for PR query evaluation, once the index
-              // is identified the same index is used on other PR buckets, the sieEstimation is
-              // done only once, which adds additional index use for only one bucket index.
-              if (!(indexUse == expectedUse || indexUse == (expectedUse + indexUsageWithSizeEstimation))){
-                fail ("Index usage is not as expected, expected it to be either " + 
-                    expectedUse + " or " + (expectedUse + indexUsageWithSizeEstimation) + 
-                    " it is: " + indexUse);
-                //assertIndexDetailsEquals(6 + indexUsageWithSizeEstimation, bukInd.getStatistics().getTotalUses());
-              }
-              k++;
-            }*/
-            //Shobhit: Now we dont need to check stats per bucket index,
-            //stats are accumulated in single pr index stats.
-            
-            // Anil : With aquiringReadLock during Index.getSizeEstimate(), the
-            // Index usage in case of "ID = 0 OR ID = 1" is increased by 3.
-            int indexUsageWithSizeEstimation = 3;
-            
-            logger.info("index uses for "+ind.getNumberOfIndexedBuckets()+" index "+ind.getName()+": "+ind.getStatistics().getTotalUses());
-            assertEquals(6, ind.getStatistics().getTotalUses());
-          }
-          
+
+        Collection indexes = qs.getIndexes();
+        Iterator it = indexes.iterator();
+        while (it.hasNext()) {
+          PartitionedIndex ind = (PartitionedIndex) it.next();
+          /*
+           * List bucketIndex = ind.getBucketIndexes(); int k = 0;
+           * logger.info("Total number of bucket index : "+bucketIndex.size()); while ( k <
+           * bucketIndex.size() ){ Index bukInd = (Index)bucketIndex.get(k);
+           * logger.info("Buket Index "+bukInd+"  usage : "+bukInd.getStatistics().getTotalUses());
+           * // if number of quries on pr change in
+           * getCacheSerializableRunnableForPRQueryAndCompareResults // literal 6 should change.
+           * //Asif : With the optmization of Range Queries a where clause // containing something
+           * like ID > 4 AND ID < 9 will be evaluated //using a single index lookup, so accordingly
+           * modifying the //assert value from 7 to 6 // Anil : With aquiringReadLock during
+           * Index.getSizeEstimate(), the // Index usage in case of "ID = 0 OR ID = 1" is increased
+           * by 3. int indexUsageWithSizeEstimation = 3; int expectedUse = 6; long indexUse =
+           * bukInd.getStatistics().getTotalUses(); // Anil : With chnages to use single index for
+           * PR query evaluation, once the index // is identified the same index is used on other PR
+           * buckets, the sieEstimation is // done only once, which adds additional index use for
+           * only one bucket index. if (!(indexUse == expectedUse || indexUse == (expectedUse +
+           * indexUsageWithSizeEstimation))){ fail
+           * ("Index usage is not as expected, expected it to be either " + expectedUse + " or " +
+           * (expectedUse + indexUsageWithSizeEstimation) + " it is: " + indexUse);
+           * //assertIndexDetailsEquals(6 + indexUsageWithSizeEstimation,
+           * bukInd.getStatistics().getTotalUses()); } k++; }
+           */
+          // Shobhit: Now we dont need to check stats per bucket index,
+          // stats are accumulated in single pr index stats.
+
+          // Anil : With aquiringReadLock during Index.getSizeEstimate(), the
+          // Index usage in case of "ID = 0 OR ID = 1" is increased by 3.
+          int indexUsageWithSizeEstimation = 3;
+
+          logger.info("index uses for " + ind.getNumberOfIndexedBuckets() + " index "
+              + ind.getName() + ": " + ind.getStatistics().getTotalUses());
+          assertEquals(6, ind.getStatistics().getTotalUses());
+        }
+
-          
-      
+
+
-      final String regionName, final String localRegion)
-  {
+      final String regionName, final String localRegion) {
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-        String[] query = { "TRUE", "FALSE", "UNDEFINED", "NULL" };
+        String[] query = {"TRUE", "FALSE", "UNDEFINED", "NULL"};
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryWithConstantsAndComparingResults: Queries Executed successfully on Local region & PR Region");
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryWithConstantsAndComparingResults: Queries Executed successfully on Local region & PR Region");
-        }
-        catch (QueryException e) {
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .error(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryWithConstantsAndComparingResults: Caught an Exception while querying Constants"
-                      + e, e);
-          fail("PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryWithConstantsAndComparingResults: Caught Exception while querying Constants. Exception is "
-              + e);
+        } catch (QueryException e) {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryWithConstantsAndComparingResults: Caught an Exception while querying Constants"
+                  + e,
+              e);
+          fail(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryWithConstantsAndComparingResults: Caught Exception while querying Constants. Exception is "
+                  + e);
-    return (CacheSerializableRunnable)PrRegion;
+    return (CacheSerializableRunnable) PrRegion;
-   * This function creates a Accessor node region on the given PR given the
-   * scope parameter.
+   * This function creates a Accessor node region on the given PR given the scope parameter.
-      final String regionName, final int redundancy, final Class constraint)
-  {
+      final String regionName, final int redundancy, final Class constraint) {
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-        PartitionAttributes prAttr = paf.setLocalMaxMemory(maxMem)
-        .setRedundantCopies(redundancy).create();
+        PartitionAttributes prAttr =
+            paf.setLocalMaxMemory(maxMem).setRedundantCopies(redundancy).create();
-                + regionName + " not in cache", cache.getRegion(regionName));
+                + regionName + " not in cache",
+            cache.getRegion(regionName));
-    return (CacheSerializableRunnable)createPrRegion;
+    return (CacheSerializableRunnable) createPrRegion;
-   * This function compares the two result sets passed based on <br> 1. Type
-   * <br> 2. Size <br> 3. Contents <br>
+   * This function compares the two result sets passed based on <br> 1. Type <br> 2. Size <br> 3.
+   * Contents <br>
-  public void compareTwoQueryResults(Object[][] r, int len)
-  {
+  public void compareTwoQueryResults(Object[][] r, int len) {
-        type1 = ((SelectResults)r[j][0]).getCollectionType().getElementType();
-        assertNotNull(
-            "PRQueryDUnitHelper#compareTwoQueryResults: Type 1 is NULL "
-                + type1, type1);
-        type2 = ((SelectResults)r[j][1]).getCollectionType().getElementType();
-        assertNotNull(
-            "PRQueryDUnitHelper#compareTwoQueryResults: Type 2 is NULL "
-                + type2, type2);
+        type1 = ((SelectResults) r[j][0]).getCollectionType().getElementType();
+        assertNotNull("PRQueryDUnitHelper#compareTwoQueryResults: Type 1 is NULL " + type1, type1);
+        type2 = ((SelectResults) r[j][1]).getCollectionType().getElementType();
+        assertNotNull("PRQueryDUnitHelper#compareTwoQueryResults: Type 2 is NULL " + type2, type2);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#compareTwoQueryResults: Both Search Results are of the same Type i.e.--> "
+                  + ((SelectResults) r[j][0]).getCollectionType().getElementType());
+
+        } else {
-              .info(
-                  "PRQueryDUnitHelper#compareTwoQueryResults: Both Search Results are of the same Type i.e.--> "
-                      + ((SelectResults)r[j][0]).getCollectionType()
-                          .getElementType());
+              .error("PRQueryDUnitHelper#compareTwoQueryResults: Classes are : "
+                  + type1.getClass().getName() + " " + type2.getClass().getName());
+          fail(
+              "PRQueryDUnitHelper#compareTwoQueryResults: FAILED:Search result Type is different in both the cases");
-        else {
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
-              "PRQueryDUnitHelper#compareTwoQueryResults: Classes are : "
-                  + type1.getClass().getName() + " "
-                  + type2.getClass().getName());
-
-          fail("PRQueryDUnitHelper#compareTwoQueryResults: FAILED:Search result Type is different in both the cases");
-        }
-        int size0 = ((SelectResults)r[j][0]).size();
-        int size1 = ((SelectResults)r[j][1]).size();
+        int size0 = ((SelectResults) r[j][0]).size();
+        int size1 = ((SelectResults) r[j][1]).size();
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#compareTwoQueryResults: Both Search Results are non-zero and are of Same Size i.e.  Size= "
-                      + size1 + ";j=" + j);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#compareTwoQueryResults: Both Search Results are non-zero and are of Same Size i.e.  Size= "
+                  + size1 + ";j=" + j);
+        } else {
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
+              "PRQueryDUnitHelper#compareTwoQueryResults: FAILED:Search resultSet size are different in both cases; size0="
+                  + size0 + ";size1=" + size1 + ";j=" + j);
+          fail(
+              "PRQueryDUnitHelper#compareTwoQueryResults: FAILED:Search resultSet size are different in both cases; size0="
+                  + size0 + ";size1=" + size1 + ";j=" + j);
-        else {
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .error(
-                  "PRQueryDUnitHelper#compareTwoQueryResults: FAILED:Search resultSet size are different in both cases; size0=" +
-                     size0 + ";size1=" + size1 + ";j=" + j);
-          fail("PRQueryDUnitHelper#compareTwoQueryResults: FAILED:Search resultSet size are different in both cases; size0=" + size0 + ";size1=" + size1 + ";j=" + j);
-        }
-        set2 = (((SelectResults)r[j][1]).asSet());
-        set1 = (((SelectResults)r[j][0]).asSet());
+        set2 = (((SelectResults) r[j][1]).asSet());
+        set1 = (((SelectResults) r[j][0]).asSet());
-                     + "result contents are not equal, ", set1, set2);
+            + "result contents are not equal, ", set1, set2);
-   * 1. Creates & executes a query with Logical Operators on the given PR Region
-   * 2. Executes the same query on the local region <br>
+   * 1. Creates & executes a query with Logical Operators on the given PR Region 2. Executes the
+   * same query on the local region <br>
-    final String regionName)
-  {
+      final String regionName) {
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-          fail("PRQueryDUnitHelper#getCacheSerializableRunnableForPRInvalidQuery: InvalidQueryException expected");
-        }
-        catch (QueryInvalidException e) {
+          fail(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRInvalidQuery: InvalidQueryException expected");
+        } catch (QueryInvalidException e) {
-        }
-        catch (QueryException qe) {
+        } catch (QueryException qe) {
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .error(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRInvalidQuery: Caught another Exception while querying , Exception is "
-                      + qe, qe);
-          fail("PRQueryDUnitHelper#getCacheSerializableRunnableForPRInvalidQuery: Caught another Exception while querying , Exception is "
-              + qe);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRInvalidQuery: Caught another Exception while querying , Exception is "
+                  + qe,
+              qe);
+          fail(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRInvalidQuery: Caught another Exception while querying , Exception is "
+                  + qe);
-    return (CacheSerializableRunnable)PrRegion;
+    return (CacheSerializableRunnable) PrRegion;
-      final String regionName, final int redundancy, final Class constraint)
-  {
+      final String regionName, final int redundancy, final Class constraint) {
-      public void run2() throws CacheException
-      {
+      public void run2() throws CacheException {
-        final String expectedRegionDestroyedException = RegionDestroyedException.class
-            .getName();
+        final String expectedRegionDestroyedException = RegionDestroyedException.class.getName();
+        getCache().getLogger().info("<ExpectedException action=add>"
+            + expectedRegionDestroyedException + "</ExpectedException>");
-            "<ExpectedException action=add>" + expectedRegionDestroyedException
-                + "</ExpectedException>");
-        getCache().getLogger().info(
-            "<ExpectedException action=add>" + expectedReplyException
-                + "</ExpectedException>");
+            "<ExpectedException action=add>" + expectedReplyException + "</ExpectedException>");
-        org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-            .info(
-                "PROperationWithQueryDUnitTest#getCacheSerializableRunnableForRegionClose: Closing region");
+        org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+            "PROperationWithQueryDUnitTest#getCacheSerializableRunnableForRegionClose: Closing region");
-        org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-            .info(
-                "PROperationWithQueryDUnitTest#getCacheSerializableRunnableForRegionClose: Region Closed on VM ");
+        org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+            "PROperationWithQueryDUnitTest#getCacheSerializableRunnableForRegionClose: Region Closed on VM ");
-        PartitionAttributes prAttr = paf.setRedundantCopies(redundancy)
-            .create();
+        PartitionAttributes prAttr = paf.setRedundantCopies(redundancy).create();
-        org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-            .info(
-                "PROperationWithQueryDUnitTest#getCacheSerializableRunnableForRegionClose: Region Recreated on VM ");
+        org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+            "PROperationWithQueryDUnitTest#getCacheSerializableRunnableForRegionClose: Region Recreated on VM ");
-            "<ExpectedException action=remove>" + expectedReplyException
-                + "</ExpectedException>");
-        getCache().getLogger().info(
-            "<ExpectedException action=remove>"
-                + expectedRegionDestroyedException + "</ExpectedException>");
+            "<ExpectedException action=remove>" + expectedReplyException + "</ExpectedException>");
+        getCache().getLogger().info("<ExpectedException action=remove>"
+            + expectedRegionDestroyedException + "</ExpectedException>");
-    return (CacheSerializableRunnable)PrRegion;
+    return (CacheSerializableRunnable) PrRegion;
-   * NOTE: Closing of the cache must be done from the test case rather than in PRQueryDUintHelper
+   *         NOTE: Closing of the cache must be done from the test case rather than in
+   *         PRQueryDUintHelper
-      final String regionName, final int redundancy, final Class constraint)
-  {
+      final String regionName, final int redundancy, final Class constraint) {
-      public void run2() throws CacheException
-      {
-        final String expectedCacheClosedException = CacheClosedException.class
-            .getName();
+      public void run2() throws CacheException {
+        final String expectedCacheClosedException = CacheClosedException.class.getName();
+        getCache().getLogger().info("<ExpectedException action=add>" + expectedCacheClosedException
+            + "</ExpectedException>");
-            "<ExpectedException action=add>" + expectedCacheClosedException
-                + "</ExpectedException>");
-        getCache().getLogger().info(
-            "<ExpectedException action=add>" + expectedReplyException
-                + "</ExpectedException>");
+            "<ExpectedException action=add>" + expectedReplyException + "</ExpectedException>");
-        org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-            .info(
-                "PROperationWithQueryDUnitTest#getCacheSerializableRunnableForCacheClose: Recreating the cache ");
+        org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+            "PROperationWithQueryDUnitTest#getCacheSerializableRunnableForCacheClose: Recreating the cache ");
-        PartitionAttributes prAttr = paf.setRedundantCopies(redundancy)
-            .create();
+        PartitionAttributes prAttr = paf.setRedundantCopies(redundancy).create();
-          //Wait for recovery to finish
+          // Wait for recovery to finish
-        org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-            .info(
-                "PROperationWithQueryDUnitTest#getCacheSerializableRunnableForCacheClose: cache Recreated on VM ");
+        org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+            "PROperationWithQueryDUnitTest#getCacheSerializableRunnableForCacheClose: cache Recreated on VM ");
-            "<ExpectedException action=remove>" + expectedReplyException
-                + "</ExpectedException>");
-        getCache().getLogger().info(
-            "<ExpectedException action=remove>" + expectedCacheClosedException
-                + "</ExpectedException>");
+            "<ExpectedException action=remove>" + expectedReplyException + "</ExpectedException>");
+        getCache().getLogger().info("<ExpectedException action=remove>"
+            + expectedCacheClosedException + "</ExpectedException>");
-    return (CacheSerializableRunnable)PrRegion;
+    return (CacheSerializableRunnable) PrRegion;
-    /**
-   * This function creates a appropriate index on a  PR given the name and 
-   * other parameters.
+  /**
+   * This function creates a appropriate index on a PR given the name and other parameters.
-      final String prRegionName, final String indexName,
-      final String indexedExpression, final String fromClause, final String alias)
-  {
+      final String prRegionName, final String indexName, final String indexedExpression,
+      final String fromClause, final String alias) {
-    SerializableRunnable prIndexCreator = new CacheSerializableRunnable(
-        "PartitionedIndexCreator") {
+    SerializableRunnable prIndexCreator = new CacheSerializableRunnable("PartitionedIndexCreator") {
-      public void run2()
-      {
+      public void run2() {
-          if (null != fromClause) { 
-            logger.info("Test Creating index with Name : [ "+indexName+" ] " +
-                "IndexedExpression : [ "+indexedExpression+" ] Alias : [ "+alias+" ] FromClause : [ "+fromClause+ " " + alias+" ] " );
-            Index parIndex = qs.createIndex(indexName, IndexType.FUNCTIONAL,
-                indexedExpression, fromClause);
-            logger.info(
-                "Index creted on partitioned region : " + parIndex);
-          }
-          else {
-          logger.info("Test Creating index with Name : [ "+indexName+" ] " +
-                        "IndexedExpression : [ "+indexedExpression+" ] Alias : [ "+alias+" ] FromClause : [ "+region.getFullPath() + " " + alias+" ] " );
-          Index parIndex = qs.createIndex(indexName, IndexType.FUNCTIONAL,
-              indexedExpression, region.getFullPath() + " " + alias);
-          logger.info(
-              "Index creted on partitioned region : " + parIndex);
-          logger.info(
-              "Number of buckets indexed in the partitioned region locally : "
-                  + "" + ((PartitionedIndex)parIndex).getNumberOfIndexedBuckets()
-                  + " and remote buckets indexed : "
-                  + ((PartitionedIndex)parIndex).getNumRemoteBucketsIndexed());
+          if (null != fromClause) {
+            logger.info("Test Creating index with Name : [ " + indexName + " ] "
+                + "IndexedExpression : [ " + indexedExpression + " ] Alias : [ " + alias
+                + " ] FromClause : [ " + fromClause + " " + alias + " ] ");
+            Index parIndex =
+                qs.createIndex(indexName, IndexType.FUNCTIONAL, indexedExpression, fromClause);
+            logger.info("Index creted on partitioned region : " + parIndex);
+          } else {
+            logger.info("Test Creating index with Name : [ " + indexName + " ] "
+                + "IndexedExpression : [ " + indexedExpression + " ] Alias : [ " + alias
+                + " ] FromClause : [ " + region.getFullPath() + " " + alias + " ] ");
+            Index parIndex = qs.createIndex(indexName, IndexType.FUNCTIONAL, indexedExpression,
+                region.getFullPath() + " " + alias);
+            logger.info("Index creted on partitioned region : " + parIndex);
+            logger.info("Number of buckets indexed in the partitioned region locally : " + ""
+                + ((PartitionedIndex) parIndex).getNumberOfIndexedBuckets()
+                + " and remote buckets indexed : "
+                + ((PartitionedIndex) parIndex).getNumRemoteBucketsIndexed());
-           * assertIndexDetailsEquals("Max num of buckets in the partiotion regions and
-           * the " + "buckets indexed should be equal",
+           * assertIndexDetailsEquals("Max num of buckets in the partiotion regions and the
+           * " + "buckets indexed should be equal",
-           * (((PartionedIndex)parIndex).getNumberOfIndexedBucket()+((PartionedIndex)parIndex).getNumRemtoeBucketsIndexed()));
-           * should put all the assetion in a seperate function.
+           * (((PartionedIndex)parIndex).getNumberOfIndexedBucket()+((PartionedIndex)parIndex).
+           * getNumRemtoeBucketsIndexed())); should put all the assetion in a seperate function.
-        }
-        catch (Exception ex) {
+        } catch (Exception ex) {
-    return (CacheSerializableRunnable)prIndexCreator;
+    return (CacheSerializableRunnable) prIndexCreator;
-  
- /**
-  * This function defines a appropriate index on a  PR given the name and 
-  * other parameters.
-  */
+
+  /**
+   * This function defines a appropriate index on a PR given the name and other parameters.
+   */
-        final ArrayList<String> indexedExpression) {
-    return getCacheSerializableRunnableForDefineIndex(prRegionName, indexName, indexedExpression, null);
+      final ArrayList<String> indexedExpression) {
+    return getCacheSerializableRunnableForDefineIndex(prRegionName, indexName, indexedExpression,
+        null);
-  
+
-    final String prRegionName, final ArrayList<String> indexName,
+      final String prRegionName, final ArrayList<String> indexName,
-    SerializableRunnable prIndexCreator = new CacheSerializableRunnable(
-        "PartitionedIndexCreator") {
+    SerializableRunnable prIndexCreator = new CacheSerializableRunnable("PartitionedIndexCreator") {
-          for(int i = 0 ; i < indexName.size(); i++) {
-            qs.defineIndex(indexName.get(i), indexedExpression.get(i), fromClause == null ? region.getFullPath() : fromClause.get(i));
+          for (int i = 0; i < indexName.size(); i++) {
+            qs.defineIndex(indexName.get(i), indexedExpression.get(i),
+                fromClause == null ? region.getFullPath() : fromClause.get(i));
-          if(ex instanceof MultiIndexCreationException) {
+          if (ex instanceof MultiIndexCreationException) {
-            for(Exception e: ((MultiIndexCreationException) ex).getExceptionsMap().values()) {
-               sb.append(e.getMessage()).append("\n");
+            for (Exception e : ((MultiIndexCreationException) ex).getExceptionsMap().values()) {
+              sb.append(e.getMessage()).append("\n");
-           Assert.fail("Creating Index in this vm failed : ", ex);
+            Assert.fail("Creating Index in this vm failed : ", ex);
-      final String rrRegionName, final String indexName,
-      final String indexedExpression, final String fromClause, final String alias)
-  {
+      final String rrRegionName, final String indexName, final String indexedExpression,
+      final String fromClause, final String alias) {
-    SerializableRunnable prIndexCreator = new CacheSerializableRunnable(
-        "ReplicatedRegionIndexCreator") {
-      public void run2()
-      {
-        try {
-          Cache cache = getCache();
-          QueryService qs = cache.getQueryService();
-          Region region = cache.getRegion(rrRegionName);
-          LogWriter logger = cache.getLogger();
-          if (null != fromClause) { 
-            logger.info("Test Creating index with Name : [ "+indexName+" ] " +
-                "IndexedExpression : [ "+indexedExpression+" ] Alias : [ "+alias+" ] FromClause : [ "+fromClause+ " " + alias+" ] " );
-            Index parIndex = qs.createIndex(indexName, IndexType.FUNCTIONAL,
-                indexedExpression, fromClause);
-            logger.info(
-                "Index creted on replicated region : " + parIndex);
-                  
-          } 
-          else {
-          logger.info("Test Creating index with Name : [ "+indexName+" ] " +
-                        "IndexedExpression : [ "+indexedExpression+" ] Alias : [ "+alias+" ] FromClause : [ "+region.getFullPath() + " " + alias+" ] " );
-          Index parIndex = qs.createIndex(indexName, IndexType.FUNCTIONAL,
-              indexedExpression, region.getFullPath() + " " + alias);
-          logger.info(
-              "Index creted on replicated region : " + parIndex);
+    SerializableRunnable prIndexCreator =
+        new CacheSerializableRunnable("ReplicatedRegionIndexCreator") {
+          public void run2() {
+            try {
+              Cache cache = getCache();
+              QueryService qs = cache.getQueryService();
+              Region region = cache.getRegion(rrRegionName);
+              LogWriter logger = cache.getLogger();
+              if (null != fromClause) {
+                logger.info("Test Creating index with Name : [ " + indexName + " ] "
+                    + "IndexedExpression : [ " + indexedExpression + " ] Alias : [ " + alias
+                    + " ] FromClause : [ " + fromClause + " " + alias + " ] ");
+                Index parIndex =
+                    qs.createIndex(indexName, IndexType.FUNCTIONAL, indexedExpression, fromClause);
+                logger.info("Index creted on replicated region : " + parIndex);
+
+              } else {
+                logger.info("Test Creating index with Name : [ " + indexName + " ] "
+                    + "IndexedExpression : [ " + indexedExpression + " ] Alias : [ " + alias
+                    + " ] FromClause : [ " + region.getFullPath() + " " + alias + " ] ");
+                Index parIndex = qs.createIndex(indexName, IndexType.FUNCTIONAL, indexedExpression,
+                    region.getFullPath() + " " + alias);
+                logger.info("Index creted on replicated region : " + parIndex);
+              }
+
+
+            } catch (Exception ex) {
+              Assert.fail("Creating Index in this vm failed : ", ex);
+            }
-          
-          
-        }
-        catch (Exception ex) {
-          Assert.fail("Creating Index in this vm failed : ", ex);
-        }
-      }
-    };
-    return (CacheSerializableRunnable)prIndexCreator;
+        };
+    return (CacheSerializableRunnable) prIndexCreator;
-  public CacheSerializableRunnable getCacheSerializableRunnableForPRCreate(final String regionName)
-  {
-    SerializableRunnable prIndexCreator = new CacheSerializableRunnable(
-        "PrRegionCreator") {
+  public CacheSerializableRunnable getCacheSerializableRunnableForPRCreate(
+      final String regionName) {
+    SerializableRunnable prIndexCreator = new CacheSerializableRunnable("PrRegionCreator") {
-      public void run2()
-      {
+      public void run2() {
-          PartitionedRegion region = (PartitionedRegion)cache
-            .getRegion(regionName);
+          PartitionedRegion region = (PartitionedRegion) cache.getRegion(regionName);
-            Map.Entry entry = (Map.Entry)it.next();
-            Index index = (Index)entry.getValue();
-            logger.info("The partitioned index created on this region "
-              + " " + index);
+            Map.Entry entry = (Map.Entry) it.next();
+            Index index = (Index) entry.getValue();
+            logger.info("The partitioned index created on this region " + " " + index);
-              + ((PartitionedIndex)index).getNumberOfIndexedBuckets());
-        }
-        }
-        finally {
+                + ((PartitionedIndex) index).getNumberOfIndexedBuckets());
+          }
+        } finally {
-               
+
-    return (CacheSerializableRunnable)prIndexCreator;
+    return (CacheSerializableRunnable) prIndexCreator;
-  
-  
-  public File findFile(String fileName)
-  {
+
+
+  public File findFile(String fileName) {
-  
+
-      final String name)
-  {
-    return new CacheSerializableRunnable(
-        "PrIndexCreationCheck") {
+      final String name) {
+    return new CacheSerializableRunnable("PrIndexCreationCheck") {
-      public void run2()
-      {
+      public void run2() {
-          Map.Entry entry = (Map.Entry)it.next();
-          Index index = (Index)entry.getValue();
-          logger.info("the partitioned index created on this region "
-              + " " + index);
+          Map.Entry entry = (Map.Entry) it.next();
+          Index index = (Index) entry.getValue();
+          logger.info("the partitioned index created on this region " + " " + index);
-              + ((PartitionedIndex)index).getNumberOfIndexedBuckets());
+              + ((PartitionedIndex) index).getNumberOfIndexedBuckets());
-  
+
-   * This function creates a duplicate index should throw an IndexNameConflictException
-   * and if not the test should fail.
+   * This function creates a duplicate index should throw an IndexNameConflictException and if not
+   * the test should fail.
-      final String prRegionName, final String indexName,
-      final String indexedExpression,final String fromClause, final String alias)
-  {
-    SerializableRunnable prIndexCreator = new CacheSerializableRunnable(
-        "DuplicatePartitionedIndexCreator") {
-      @Override
-      public void run2()
-      {
-        Cache cache = getCache();
-        LogWriter logger = cache.getLogger();
-        QueryService qs = cache.getQueryService();
-        Region region = cache.getRegion(prRegionName);
-        try {
-          if (null != fromClause){
-            qs.createIndex(indexName, IndexType.FUNCTIONAL,
-                indexedExpression, fromClause);
-            throw new RuntimeException("Should throw an exception because "
-                + "the index with name : " + indexName + " should already exists");
-          }
-          else {
-            qs.createIndex(indexName, IndexType.FUNCTIONAL,
-              indexedExpression, region.getFullPath() + " "+alias);
-            throw new RuntimeException("Should throw an exception because "
-              + "the index with name : " + indexName + " should already exists");
-          }
-        }
-        catch (IndexExistsException e) {
-          logger.info("Index Exists Excetpiont righteously throw ", e);
-        }
-        catch (IndexNameConflictException ex) {
-          logger.info("Gott the right exception");
-        }
-        catch (RegionNotFoundException exx) {
-          // TODO Auto-generated catch block
-          Assert.fail("Region Not found in this vm ", exx);
-        }
+      final String prRegionName, final String indexName, final String indexedExpression,
+      final String fromClause, final String alias) {
+    SerializableRunnable prIndexCreator =
+        new CacheSerializableRunnable("DuplicatePartitionedIndexCreator") {
+          @Override
+          public void run2() {
+            Cache cache = getCache();
+            LogWriter logger = cache.getLogger();
+            QueryService qs = cache.getQueryService();
+            Region region = cache.getRegion(prRegionName);
+            try {
+              if (null != fromClause) {
+                qs.createIndex(indexName, IndexType.FUNCTIONAL, indexedExpression, fromClause);
+                throw new RuntimeException("Should throw an exception because "
+                    + "the index with name : " + indexName + " should already exists");
+              } else {
+                qs.createIndex(indexName, IndexType.FUNCTIONAL, indexedExpression,
+                    region.getFullPath() + " " + alias);
+                throw new RuntimeException("Should throw an exception because "
+                    + "the index with name : " + indexName + " should already exists");
+              }
+            } catch (IndexExistsException e) {
+              logger.info("Index Exists Excetpiont righteously throw ", e);
+            } catch (IndexNameConflictException ex) {
+              logger.info("Gott the right exception");
+            } catch (RegionNotFoundException exx) {
+              // TODO Auto-generated catch block
+              Assert.fail("Region Not found in this vm ", exx);
+            }
-      }
-    };
-    return (CacheSerializableRunnable)prIndexCreator;
+          }
+        };
+    return (CacheSerializableRunnable) prIndexCreator;
-  
+
-   * Cacheserializable runnable which removes all the index on a partitioned
-   * region
+   * Cacheserializable runnable which removes all the index on a partitioned region
-   * @param name
-   *          name of the partitioned regions
+   * @param name name of the partitioned regions
-  public CacheSerializableRunnable getCacheSerializableRunnableForRemoveIndex(
-      final String name, final boolean random)
-  {
-    return new CacheSerializableRunnable(
-        "PrRemoveIndex") {
+  public CacheSerializableRunnable getCacheSerializableRunnableForRemoveIndex(final String name,
+      final boolean random) {
+    return new CacheSerializableRunnable("PrRemoveIndex") {
-      public void run2()
-      {
+      public void run2() {
-        logger.info("Got the following cache : "+ cache1);
+        logger.info("Got the following cache : " + cache1);
-          logger.info("Removed all the index on this paritioned regions : "
-              + parRegion);
+          logger.info("Removed all the index on this paritioned regions : " + parRegion);
-          assertEquals(0, ((LocalRegion)parRegion).getIndexManager().getIndexes().size());
+          assertEquals(0, ((LocalRegion) parRegion).getIndexManager().getIndexes().size());
-          assertEquals(3, ((LocalRegion)parRegion).getIndexManager().getIndexes().size());
+          assertEquals(3, ((LocalRegion) parRegion).getIndexManager().getIndexes().size());
-              Index in = (Index)it.next();
+              Index in = (Index) it.next();
-          assertEquals(0, ((LocalRegion)parRegion).getIndexManager().getIndexes().size());
+          assertEquals(0, ((LocalRegion) parRegion).getIndexManager().getIndexes().size());
-        String[] queries = new String[] {
-            "r1.ID = r2.id",
-            "r1.ID = r2.id AND r1.ID > 5",
+        String[] queries = new String[] {"r1.ID = r2.id", "r1.ID = r2.id AND r1.ID > 5",
-            "r1.ID = r2.id ORDER BY r1.ID",
-            "r1.ID = r2.id ORDER BY r2.id",
-            "r1.ID = r2.id ORDER BY r2.status",
-            "r1.ID = r2.id AND r1.status != r2.status",
+            "r1.ID = r2.id ORDER BY r1.ID", "r1.ID = r2.id ORDER BY r2.id",
+            "r1.ID = r2.id ORDER BY r2.status", "r1.ID = r2.id AND r1.status != r2.status",
-            "r1.ID = r2.id AND (r1.positions.size < r2.positions.size OR r1.positions.size > 0)", };
+            "r1.ID = r2.id AND (r1.positions.size < r2.positions.size OR r1.positions.size > 0)",};
-        final String[] expectedExceptions = new String[] {
-            RegionDestroyedException.class.getName(),
-            ReplyException.class.getName(),
-            CacheClosedException.class.getName(),
-            ForceReattemptException.class.getName(),
-            QueryInvocationTargetException.class.getName() };
+        final String[] expectedExceptions =
+            new String[] {RegionDestroyedException.class.getName(), ReplyException.class.getName(),
+                CacheClosedException.class.getName(), ForceReattemptException.class.getName(),
+                QueryInvocationTargetException.class.getName()};
-          getCache().getLogger().info(
-            "<ExpectedException action=add>" + expectedException
-              + "</ExpectedException>");
+          getCache().getLogger()
+              .info("<ExpectedException action=add>" + expectedException + "</ExpectedException>");
-            getCache().getLogger().info(
-                "About to execute local query: " + queries[j]);
+            getCache().getLogger().info("About to execute local query: " + queries[j]);
-                .onRegion(
-                    (getCache().getRegion(name) instanceof PartitionedRegion) ? getCache()
-                        .getRegion(name) : getCache().getRegion(coloName))
-                .withArgs(
-                    "<trace> Select "
-                        + (queries[j].contains("ORDER BY") ? "DISTINCT" : "")
-                        + " * from /" + name + " r1, /" + coloName
-                        + " r2 where " + queries[j]).execute(func).getResult();
+                .onRegion((getCache().getRegion(name) instanceof PartitionedRegion)
+                    ? getCache().getRegion(name) : getCache().getRegion(coloName))
+                .withArgs("<trace> Select " + (queries[j].contains("ORDER BY") ? "DISTINCT" : "")
+                    + " * from /" + name + " r1, /" + coloName + " r2 where " + queries[j])
+                .execute(func).getResult();
-            getCache().getLogger().info(
-                "About to execute local query: " + queries[j]);
+            getCache().getLogger().info("About to execute local query: " + queries[j]);
-            SelectResults r2 = (SelectResults) qs.newQuery(
-                "Select " + (queries[j].contains("ORDER BY") ? "DISTINCT" : "")
-                    + " * from /" + localName + " r1, /" + coloLocalName
-                    + " r2 where " + queries[j]).execute();
+            SelectResults r2 = (SelectResults) qs
+                .newQuery(
+                    "Select " + (queries[j].contains("ORDER BY") ? "DISTINCT" : "") + " * from /"
+                        + localName + " r1, /" + coloLocalName + " r2 where " + queries[j])
+                .execute();
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Queries Executed successfully on Local region & PR Region");
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Queries Executed successfully on Local region & PR Region");
-          ssORrs.CompareQueryResultsAsListWithoutAndWithIndexes(r,
-              queries.length, false, false, queries);
+          ssORrs.CompareQueryResultsAsListWithoutAndWithIndexes(r, queries.length, false, false,
+              queries);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .error(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught QueryException while querying"
-                      + e, e);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught QueryException while querying"
+                  + e,
+              e);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a RegionDestroyedException while querying as expected ",
-                  rde);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a RegionDestroyedException while querying as expected ",
+              rde);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a CancelException while querying as expected ",
-                  cce);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a CancelException while querying as expected ",
+              cce);
-              "<ExpectedException action=remove>" + expectedException
-                + "</ExpectedException>");
+                "<ExpectedException action=remove>" + expectedException + "</ExpectedException>");
-        String[] queries = new String[] {
-            "r1.ID = r2.id",
-            "r1.ID = r2.id AND r1.ID > 5",
+        String[] queries = new String[] {"r1.ID = r2.id", "r1.ID = r2.id AND r1.ID > 5",
-            "r1.ID = r2.id ORDER BY r1.ID",
-            "r1.ID = r2.id ORDER BY r2.id",
-            "r1.ID = r2.id ORDER BY r2.status",
-            "r1.ID = r2.id AND r1.status != r2.status",
+            "r1.ID = r2.id ORDER BY r1.ID", "r1.ID = r2.id ORDER BY r2.id",
+            "r1.ID = r2.id ORDER BY r2.status", "r1.ID = r2.id AND r1.status != r2.status",
-            "r1.ID = r2.id AND (r1.positions.size < r2.positions.size OR r1.positions.size > 0)", };
+            "r1.ID = r2.id AND (r1.positions.size < r2.positions.size OR r1.positions.size > 0)",};
-        final String[] expectedExceptions = new String[] {
-            RegionDestroyedException.class.getName(),
-            ReplyException.class.getName(),
-            CacheClosedException.class.getName(),
-            ForceReattemptException.class.getName(),
-            QueryInvocationTargetException.class.getName() };
+        final String[] expectedExceptions =
+            new String[] {RegionDestroyedException.class.getName(), ReplyException.class.getName(),
+                CacheClosedException.class.getName(), ForceReattemptException.class.getName(),
+                QueryInvocationTargetException.class.getName()};
-          getCache().getLogger().info(
-            "<ExpectedException action=add>" + expectedException
-              + "</ExpectedException>");
+          getCache().getLogger()
+              .info("<ExpectedException action=add>" + expectedException + "</ExpectedException>");
-            getCache().getLogger().info(
-                "About to execute local query: " + queries[j]);
+            getCache().getLogger().info("About to execute local query: " + queries[j]);
-                .onRegion(
-                    (getCache().getRegion(name) instanceof PartitionedRegion) ? getCache()
-                        .getRegion(name) : getCache().getRegion(coloName))
-                .withArgs(
-                    "<trace> Select "
-                        + (queries[j].contains("ORDER BY") ? "DISTINCT" : "")
-                        + " * from /" + name + " r1, /" + coloName
-                        + " r2, r2.positions.values pos2 where " + queries[j]).execute(func).getResult();
+                .onRegion((getCache().getRegion(name) instanceof PartitionedRegion)
+                    ? getCache().getRegion(name) : getCache().getRegion(coloName))
+                .withArgs("<trace> Select " + (queries[j].contains("ORDER BY") ? "DISTINCT" : "")
+                    + " * from /" + name + " r1, /" + coloName
+                    + " r2, r2.positions.values pos2 where " + queries[j])
+                .execute(func).getResult();
-            getCache().getLogger().info(
-                "About to execute local query: " + queries[j]);
+            getCache().getLogger().info("About to execute local query: " + queries[j]);
-            SelectResults r2 = (SelectResults) qs.newQuery(
-                "Select " + (queries[j].contains("ORDER BY") ? "DISTINCT" : "")
-                    + " * from /" + localName + " r1, /" + coloLocalName
-                    + " r2, r2.positions.values pos2 where " + queries[j]).execute();
+            SelectResults r2 = (SelectResults) qs.newQuery("Select "
+                + (queries[j].contains("ORDER BY") ? "DISTINCT" : "") + " * from /" + localName
+                + " r1, /" + coloLocalName + " r2, r2.positions.values pos2 where " + queries[j])
+                .execute();
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Queries Executed successfully on Local region & PR Region");
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Queries Executed successfully on Local region & PR Region");
-          ssORrs.CompareQueryResultsAsListWithoutAndWithIndexes(r,
-              queries.length, false, false, queries);
+          ssORrs.CompareQueryResultsAsListWithoutAndWithIndexes(r, queries.length, false, false,
+              queries);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .error(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught QueryException while querying"
-                      + e, e);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught QueryException while querying"
+                  + e,
+              e);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a RegionDestroyedException while querying as expected ",
-                  rde);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a RegionDestroyedException while querying as expected ",
+              rde);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a CancelException while querying as expected ",
-                  cce);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a CancelException while querying as expected ",
+              cce);
-              "<ExpectedException action=remove>" + expectedException
-                + "</ExpectedException>");
+                "<ExpectedException action=remove>" + expectedException + "</ExpectedException>");
-        String[] queries = new String[] {
-            "r1.ID = pos2.id",
-            "r1.ID = pos2.id AND r1.ID > 5",
-            "r1.ID = pos2.id AND r1.status = 'active'",
-            "r1.ID = pos2.id ORDER BY r1.ID",
-            "r1.ID = pos2.id ORDER BY pos2.id",
-            "r1.ID = pos2.id ORDER BY r2.status",
+        String[] queries = new String[] {"r1.ID = pos2.id", "r1.ID = pos2.id AND r1.ID > 5",
+            "r1.ID = pos2.id AND r1.status = 'active'", "r1.ID = pos2.id ORDER BY r1.ID",
+            "r1.ID = pos2.id ORDER BY pos2.id", "r1.ID = pos2.id ORDER BY r2.status",
-            "r1.ID = pos2.id AND (r1.positions.size < r2.positions.size OR r1.positions.size > 0)", };
+            "r1.ID = pos2.id AND (r1.positions.size < r2.positions.size OR r1.positions.size > 0)",};
-        final String[] expectedExceptions = new String[] {
-            RegionDestroyedException.class.getName(),
-            ReplyException.class.getName(),
-            CacheClosedException.class.getName(),
-            ForceReattemptException.class.getName(),
-            QueryInvocationTargetException.class.getName() };
+        final String[] expectedExceptions =
+            new String[] {RegionDestroyedException.class.getName(), ReplyException.class.getName(),
+                CacheClosedException.class.getName(), ForceReattemptException.class.getName(),
+                QueryInvocationTargetException.class.getName()};
-          getCache().getLogger().info(
-            "<ExpectedException action=add>" + expectedException
-              + "</ExpectedException>");
+          getCache().getLogger()
+              .info("<ExpectedException action=add>" + expectedException + "</ExpectedException>");
-            getCache().getLogger().info(
-                "About to execute local query: " + queries[j]);
+            getCache().getLogger().info("About to execute local query: " + queries[j]);
-                .onRegion(
-                    (getCache().getRegion(name) instanceof PartitionedRegion) ? getCache()
-                        .getRegion(name) : getCache().getRegion(coloName))
-                .withArgs(
-                    "<trace> Select "
-                        + (queries[j].contains("ORDER BY") ? "DISTINCT" : "")
-                        + " * from /" + name + " r1, /" + coloName
-                        + " r2, r2.positions.values pos2 where " + queries[j]).execute(func).getResult();
+                .onRegion((getCache().getRegion(name) instanceof PartitionedRegion)
+                    ? getCache().getRegion(name) : getCache().getRegion(coloName))
+                .withArgs("<trace> Select " + (queries[j].contains("ORDER BY") ? "DISTINCT" : "")
+                    + " * from /" + name + " r1, /" + coloName
+                    + " r2, r2.positions.values pos2 where " + queries[j])
+                .execute(func).getResult();
-            getCache().getLogger().info(
-                "About to execute local query: " + queries[j]);
+            getCache().getLogger().info("About to execute local query: " + queries[j]);
-            SelectResults r2 = (SelectResults) qs.newQuery(
-                "Select " + (queries[j].contains("ORDER BY") ? "DISTINCT" : "")
-                    + " * from /" + localName + " r1, /" + coloLocalName
-                    + " r2, r2.positions.values pos2 where " + queries[j]).execute();
+            SelectResults r2 = (SelectResults) qs.newQuery("Select "
+                + (queries[j].contains("ORDER BY") ? "DISTINCT" : "") + " * from /" + localName
+                + " r1, /" + coloLocalName + " r2, r2.positions.values pos2 where " + queries[j])
+                .execute();
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Queries Executed successfully on Local region & PR Region");
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Queries Executed successfully on Local region & PR Region");
-          ssORrs.CompareQueryResultsAsListWithoutAndWithIndexes(r,
-              queries.length, false, false, queries);
+          ssORrs.CompareQueryResultsAsListWithoutAndWithIndexes(r, queries.length, false, false,
+              queries);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .error(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught QueryException while querying"
-                      + e, e);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught QueryException while querying"
+                  + e,
+              e);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a RegionDestroyedException while querying as expected ",
-                  rde);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a RegionDestroyedException while querying as expected ",
+              rde);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a CancelException while querying as expected ",
-                  cce);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a CancelException while querying as expected ",
+              cce);
-              "<ExpectedException action=remove>" + expectedException
-                + "</ExpectedException>");
+                "<ExpectedException action=remove>" + expectedException + "</ExpectedException>");
-        String[] queries = new String[] {
-            "r1.ID = r2.id",
-            "r1.ID = r2.id AND r1.ID > 5",
+        String[] queries = new String[] {"r1.ID = r2.id", "r1.ID = r2.id AND r1.ID > 5",
-            "r1.ID = r2.id ORDER BY r1.ID",
-            "r1.ID = r2.id ORDER BY r2.id",
-            "r1.ID = r2.id ORDER BY r2.status",
-            "r1.ID = r2.id AND r1.status != r2.status",
+            "r1.ID = r2.id ORDER BY r1.ID", "r1.ID = r2.id ORDER BY r2.id",
+            "r1.ID = r2.id ORDER BY r2.status", "r1.ID = r2.id AND r1.status != r2.status",
-            "r1.ID = r2.id AND (r1.positions.size < r2.positions.size OR r1.positions.size > 0)", };
+            "r1.ID = r2.id AND (r1.positions.size < r2.positions.size OR r1.positions.size > 0)",};
-        final String[] expectedExceptions = new String[] {
-            RegionDestroyedException.class.getName(),
-            ReplyException.class.getName(),
-            CacheClosedException.class.getName(),
-            ForceReattemptException.class.getName(),
-            QueryInvocationTargetException.class.getName() };
+        final String[] expectedExceptions =
+            new String[] {RegionDestroyedException.class.getName(), ReplyException.class.getName(),
+                CacheClosedException.class.getName(), ForceReattemptException.class.getName(),
+                QueryInvocationTargetException.class.getName()};
-          getCache().getLogger().info(
-            "<ExpectedException action=add>" + expectedException
-              + "</ExpectedException>");
+          getCache().getLogger()
+              .info("<ExpectedException action=add>" + expectedException + "</ExpectedException>");
-            getCache().getLogger().info(
-                "About to execute local query: " + queries[j]);
+            getCache().getLogger().info("About to execute local query: " + queries[j]);
-                .onRegion(
-                    (getCache().getRegion(name) instanceof PartitionedRegion) ? getCache()
-                        .getRegion(name) : getCache().getRegion(coloName))
-                .withArgs(
-                    "<trace> Select "
-                        + (queries[j].contains("ORDER BY") ? "DISTINCT" : "")
-                        + " * from /" + name + " r1, r1.positions.values pos1, /" + coloName
-                        + " r2 where " + queries[j]).execute(func).getResult();
+                .onRegion((getCache().getRegion(name) instanceof PartitionedRegion)
+                    ? getCache().getRegion(name) : getCache().getRegion(coloName))
+                .withArgs("<trace> Select " + (queries[j].contains("ORDER BY") ? "DISTINCT" : "")
+                    + " * from /" + name + " r1, r1.positions.values pos1, /" + coloName
+                    + " r2 where " + queries[j])
+                .execute(func).getResult();
-            getCache().getLogger().info(
-                "About to execute local query: " + queries[j]);
+            getCache().getLogger().info("About to execute local query: " + queries[j]);
-            SelectResults r2 = (SelectResults) qs.newQuery(
-                "Select " + (queries[j].contains("ORDER BY") ? "DISTINCT" : "")
-                    + " * from /" + localName + " r1, r1.positions.values pos1, /" + coloLocalName
-                    + " r2 where " + queries[j]).execute();
+            SelectResults r2 = (SelectResults) qs.newQuery("Select "
+                + (queries[j].contains("ORDER BY") ? "DISTINCT" : "") + " * from /" + localName
+                + " r1, r1.positions.values pos1, /" + coloLocalName + " r2 where " + queries[j])
+                .execute();
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Queries Executed successfully on Local region & PR Region");
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Queries Executed successfully on Local region & PR Region");
-          ssORrs.CompareQueryResultsAsListWithoutAndWithIndexes(r,
-              queries.length, false, false, queries);
+          ssORrs.CompareQueryResultsAsListWithoutAndWithIndexes(r, queries.length, false, false,
+              queries);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .error(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught QueryException while querying"
-                      + e, e);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().error(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught QueryException while querying"
+                  + e,
+              e);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a RegionDestroyedException while querying as expected ",
-                  rde);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a RegionDestroyedException while querying as expected ",
+              rde);
-          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter()
-              .info(
-                  "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a CancelException while querying as expected ",
-                  cce);
+          org.apache.geode.test.dunit.LogWriterUtils.getLogWriter().info(
+              "PRQueryDUnitHelper#getCacheSerializableRunnableForPRQueryAndCompareResults: Caught a CancelException while querying as expected ",
+              cce);
-              "<ExpectedException action=remove>" + expectedException
-                + "</ExpectedException>");
+                "<ExpectedException action=remove>" + expectedException + "</ExpectedException>");
-        context.getResultSender().sendResult((ArrayList) ((SelectResults) query
-                .execute((RegionFunctionContext) context)).asList());
+        context.getResultSender().sendResult(
+            (ArrayList) ((SelectResults) query.execute((RegionFunctionContext) context)).asList());
