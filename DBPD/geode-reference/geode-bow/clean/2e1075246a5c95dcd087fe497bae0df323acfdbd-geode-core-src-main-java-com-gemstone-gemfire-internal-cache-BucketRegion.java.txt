Merge branch 'release/1.0.0-incubating.M3'

-import java.io.DataOutput;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Set;
-import java.util.concurrent.atomic.AtomicLong;
-import java.util.concurrent.atomic.AtomicReference;
-import java.util.concurrent.locks.Lock;
-
-import org.apache.logging.log4j.Logger;
-
-import com.gemstone.gemfire.CancelException;
-import com.gemstone.gemfire.CopyHelper;
-import com.gemstone.gemfire.DataSerializer;
-import com.gemstone.gemfire.DeltaSerializationException;
-import com.gemstone.gemfire.GemFireIOException;
-import com.gemstone.gemfire.InternalGemFireError;
-import com.gemstone.gemfire.InvalidDeltaException;
-import com.gemstone.gemfire.SystemFailure;
-import com.gemstone.gemfire.cache.CacheClosedException;
-import com.gemstone.gemfire.cache.CacheException;
-import com.gemstone.gemfire.cache.CacheWriter;
-import com.gemstone.gemfire.cache.CacheWriterException;
-import com.gemstone.gemfire.cache.CustomEvictionAttributes;
-import com.gemstone.gemfire.cache.DiskAccessException;
-import com.gemstone.gemfire.cache.EntryNotFoundException;
-import com.gemstone.gemfire.cache.EvictionAction;
-import com.gemstone.gemfire.cache.EvictionAlgorithm;
-import com.gemstone.gemfire.cache.EvictionAttributes;
-import com.gemstone.gemfire.cache.EvictionCriteria;
-import com.gemstone.gemfire.cache.ExpirationAction;
-import com.gemstone.gemfire.cache.Operation;
-import com.gemstone.gemfire.cache.RegionAttributes;
-import com.gemstone.gemfire.cache.RegionDestroyedException;
-import com.gemstone.gemfire.cache.TimeoutException;
-import com.gemstone.gemfire.cache.hdfs.HDFSIOException;
-import com.gemstone.gemfire.cache.hdfs.internal.hoplog.HoplogOrganizer;
+import com.gemstone.gemfire.*;
+import com.gemstone.gemfire.cache.*;
-import com.gemstone.gemfire.cache.query.internal.IndexUpdater;
+import com.gemstone.gemfire.distributed.internal.DistributionConfig;
-import com.gemstone.gemfire.internal.cache.delta.Delta;
-import com.gemstone.gemfire.internal.cache.wan.parallel.ConcurrentParallelGatewaySenderQueue;
+import com.gemstone.gemfire.internal.concurrent.AtomicLong5;
-import com.gemstone.gemfire.internal.offheap.StoredObject;
+import com.gemstone.gemfire.internal.offheap.annotations.Released;
+import com.gemstone.gemfire.internal.offheap.annotations.Retained;
-import com.gemstone.gemfire.internal.concurrent.AtomicLong5;
+import org.apache.logging.log4j.Logger;
+
+import java.io.DataOutput;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.*;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.locks.Lock;
- * @since 5.1
+ * @since GemFire 5.1
-  static final boolean FORCE_LOCAL_LISTENERS_INVOCATION = 
-    Boolean.getBoolean("gemfire.BucketRegion.alwaysFireLocalListeners");
+  static final boolean FORCE_LOCAL_LISTENERS_INVOCATION =
+      Boolean.getBoolean(DistributionConfig.GEMFIRE_PREFIX + "BucketRegion.alwaysFireLocalListeners");
-  protected final AtomicReference<HoplogOrganizer> hoplog = new AtomicReference<HoplogOrganizer>();
-  
+  protected boolean needsTombstoneGCKeysForClients(EventID eventID, FilterInfo clientRouting) {
+    if (eventID == null) {
+      return false;
+    }
+    if (CacheClientNotifier.getInstance() == null) {
+      return false;
+    }
+    if (clientRouting != null) {
+      return true;
+    }
+    if (getFilterProfile() != null) {
+      return true;
+    }
+    return false;
+  }
+  @Override
-      if ((removedKeys != null && removedKeys.size() > 0) // bug #51877 - NPE in clients
+      if ((removedKeys != null && !removedKeys.isEmpty()) // bug #51877 - NPE in clients
-      // increment the tailKey so that invalidate operations are written to HDFS
-      if (this.partitionedRegion.hdfsStoreName != null) {
-        /* MergeGemXDHDFSToGFE Disabled this while porting. Is this required? */
-        //assert this.partitionedRegion.isLocalParallelWanEnabled();
-        handleWANEvent(event);
-      }
+  @Retained
-      // In GemFire EVICT_DESTROY is not distributed, so in order to remove the entry
-      // from memory, allow the destroy to proceed. fixes #49784
-      if (event.isLoadedFromHDFS() && !getBucketAdvisor().isPrimary()) {
-        if (logger.isDebugEnabled()) {
-          logger.debug("Put the destory event in HDFS queue on secondary "
-              + "and return as event is HDFS loaded " + event);
-        }
-        notifyGatewaySender(EnumListenerEvent.AFTER_DESTROY, event);
-        return;
-      }else{
-        if (logger.isDebugEnabled()) {
-          logger.debug("Going ahead with the destroy on GemFire system");
-        }
-      }
-   * @since 6.0
+   * @since GemFire 6.0
-  public boolean isHDFSRegion() {
-    return this.partitionedRegion.isHDFSRegion();
-  }
-
-  @Override
-  public boolean isHDFSReadWriteRegion() {
-    return this.partitionedRegion.isHDFSReadWriteRegion();
-  }
-
-  @Override
-  protected boolean isHDFSWriteOnly() {
-    return this.partitionedRegion.isHDFSWriteOnly();
-  }
-
-  @Override
-    if (isHDFSReadWriteRegion()) {
-      try {
-        checkForPrimary();
-        ConcurrentParallelGatewaySenderQueue q = getHDFSQueue();
-        if (q == null) return 0;
-        int hdfsBucketRegionSize = q.getBucketRegionQueue(
-            partitionedRegion, getId()).size();
-        int hoplogEstimate = (int) getHoplogOrganizer().sizeEstimate();
-        if (logger.isDebugEnabled()) {
-          logger.debug("for bucket " + getName() + " estimateSize returning "
-                  + (hdfsBucketRegionSize + hoplogEstimate));
-        }
-        return hdfsBucketRegionSize + hoplogEstimate;
-      } catch (ForceReattemptException e) {
-        throw new PrimaryBucketException(e.getLocalizedMessage(), e);
-      }
-    }
-  private RawValue getSerialized(Object key, boolean updateStats, boolean doNotLockEntry, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) 
+  private RawValue getSerialized(Object key,
+                                 boolean updateStats,
+                                 boolean doNotLockEntry,
+                                 EntryEventImpl clientEvent,
+                                 boolean returnTombstones)
-    if (allowReadFromHDFS) {
-      re = this.entries.getEntry(key);
-    } else {
-      re = this.entries.getOperationalEntryInVM(key);
-    }
+    re = this.entries.getEntry(key);
-      v =re.getValue(this);  // TODO OFFHEAP: todo v ends up in a RawValue. For now this can be a copy of the offheap onto the heap. But it might be easy to track lifetime of RawValue
+      v =re.getValue(this);
-   * @param clientEvent holder for the entry's version information 
+   * @param clientEvent holder for the entry's version information
-  public RawValue getSerialized(KeyInfo keyInfo, boolean generateCallbacks, boolean doNotLockEntry, ClientProxyMembershipID requestingClient, EntryEventImpl clientEvent, boolean returnTombstones, boolean allowReadFromHDFS) throws IOException {
+  public RawValue getSerialized(KeyInfo keyInfo,
+                                boolean generateCallbacks,
+                                boolean doNotLockEntry,
+                                ClientProxyMembershipID requestingClient,
+                                EntryEventImpl clientEvent,
+                                boolean returnTombstones) throws IOException {
-      RawValue result = getSerialized(keyInfo.getKey(), true, doNotLockEntry, clientEvent, returnTombstones, allowReadFromHDFS);
+      RawValue result = getSerialized(keyInfo.getKey(), true, doNotLockEntry, clientEvent, returnTombstones);
-          // TODO OFFHEAP: optimze
-              generateCallbacks, result.getRawValue(), true, true, requestingClient, clientEvent, false, allowReadFromHDFS);
+              generateCallbacks, result.getRawValue(), true, true, requestingClient, clientEvent, false);
-    .append(";indexUpdater=").append(getIndexUpdater())
+  @Retained
-    final EntryEventImpl prevent = createEventForPR(event);
+    @Released final EntryEventImpl prevent = createEventForPR(event);
-    final EntryEventImpl prevent = createEventForPR(event);
+    @Released final EntryEventImpl prevent = createEventForPR(event);
-    final EntryEventImpl prevent = createEventForPR(event);
+    @Released final EntryEventImpl prevent = createEventForPR(event);
-    final EntryEventImpl prevent = createEventForPR(event);
+    @Released final EntryEventImpl prevent = createEventForPR(event);
-      // The primary bucket member which is being modified remotely by a GemFire
+      // The primary bucket member which is being modified remotely by a
-      //Asif: Some of the adjunct recepients include those members which 
-      // are sqlFabricHub & would need old value along with news
-   @Override
-  void cleanUpOnIncompleteOp(EntryEventImpl event,   RegionEntry re, 
-      boolean eventRecorded, boolean updateStats, boolean isReplace) {
-     
-    
-    if(!eventRecorded || isReplace) {
-      //No indexes updated so safe to remove.
-      this.entries.removeEntry(event.getKey(), re, updateStats) ;      
-    }/*else {
-      //if event recorded is true, that means as per event tracker entry is in
-      //system. As per sqlfabric, indexes have been updated. What is not done
-      // is basicPutPart2( distribution etc). So we do nothing as PR's re-attempt
-      // will do the required basicPutPart2. If we remove the entry here, than 
-      //event tracker will not allow re insertion. So either we do nothing or
-      //if we remove ,than we have to update sqlfindexes as well as undo recording
-      // of event.
-       //TODO:OQL indexes? : Hope they get updated during retry. The issue is that oql indexes
-       // get updated after distribute , so it is entirely possible that oql index are 
-        // not updated. what if retry fails?
-       
-    }*/
-  }
-   * @since gemfire59poc
+   * @since GemFire 5.9
-        && !(value instanceof com.gemstone.gemfire.Delta) && !(value instanceof Delta)
+        && !(value instanceof com.gemstone.gemfire.Delta)
-    final IndexUpdater indexUpdater = getIndexUpdater();
-    if (indexUpdater != null) {
-      indexUpdater.clearIndexes(this, bucketId);
-    }
-      if (bSize < 0 && getCancelCriterion().cancelInProgress() == null) {
+      if (bSize < 0 && !getCancelCriterion().isCancelInProgress()) {
+  public void setCloningEnabled(boolean isCloningEnabled){
+    this.partitionedRegion.setCloningEnabled(isCloningEnabled);
+  }
+
+  @Override
+  public boolean getCloningEnabled(){
+    return this.partitionedRegion.getCloningEnabled();
+  }
+
+  @Override
-    try {
-      createHoplogOrganizer();
-    } catch (IOException e) {
-      // 48990: when HDFS was down, gemfirexd should still start normally
-      logger.warn(LocalizedStrings.HOPLOG_NOT_STARTED_YET, e);
-    } catch(Throwable e) {
-      /*MergeGemXDHDFSToGFE changed this code to checkReadiness*/
-      // SystemFailure.checkThrowable(e);
-      this.checkReadiness();
-      //49333 - no matter what, we should elect a primary.
-      logger.error(LocalizedStrings.LocalRegion_UNEXPECTED_EXCEPTION, e);
-    }
-  public HoplogOrganizer<?> createHoplogOrganizer() throws IOException {
-    if (getPartitionedRegion().isHDFSRegion()) {
-      HoplogOrganizer<?> organizer = hoplog.get();
-      if (organizer != null) {
-        //  hoplog is recreated by anther thread
-        return organizer;
-      }
-
-      HoplogOrganizer hdfs = hoplog.getAndSet(getPartitionedRegion().hdfsManager.create(getId()));
-      assert hdfs == null;
-      return hoplog.get();
-    } else {
-      return null;
-    }
-  }
-  
-    releaseHoplogOrganizer();
-  protected void releaseHoplogOrganizer() {
-    // release resources during a clean transition
-    HoplogOrganizer hdfs = hoplog.getAndSet(null);
-    if (hdfs != null) {
-      getPartitionedRegion().hdfsManager.close(getId());
-    }
-  }
-  
-  public HoplogOrganizer<?> getHoplogOrganizer() throws HDFSIOException {
-    HoplogOrganizer<?> organizer = hoplog.get();
-    if (organizer == null) {
-      synchronized (getBucketAdvisor()) {
-        checkForPrimary();
-        try {
-          organizer = createHoplogOrganizer();
-        } catch (IOException e) {
-          throw new HDFSIOException("Failed to create Hoplog organizer due to ", e);
-        }
-        if (organizer == null) {
-          throw new HDFSIOException("Hoplog organizer is not available for " + this);
-        }
-      }
-    }
-    return organizer;
-  }
-  
-  @Override
-  public void hdfsCalled(Object key) {
-    this.partitionedRegion.hdfsCalled(key);
-  }
-
-  @Override
-  protected void clearHDFSData() {
-    //clear the HDFS data if present
-    if (getPartitionedRegion().isHDFSReadWriteRegion()) {
-      // Clear the queue
-      ConcurrentParallelGatewaySenderQueue q = getHDFSQueue();
-      if (q == null) return;
-      q.clear(getPartitionedRegion(), this.getId());
-      HoplogOrganizer organizer = hoplog.get();
-      if (organizer != null) {
-        try {
-          organizer.clear();
-        } catch (IOException e) {
-          throw new GemFireIOException(LocalizedStrings.HOPLOG_UNABLE_TO_DELETE_HDFS_DATA.toLocalizedString(), e);
-        }
-      }
-    }
-  }
-  
-  public EvictionCriteria getEvictionCriteria() {
-    return this.partitionedRegion.getEvictionCriteria();
-  }
-  
-  public CustomEvictionAttributes getCustomEvictionAttributes() {
-    return this.partitionedRegion.getCustomEvictionAttributes();
-  }
-  
-  /**
-   * @return true if the evict destroy was done; false if it was not needed
-   */
-  public boolean customEvictDestroy(Object key)
-  {
-    checkReadiness();
-    final EntryEventImpl event = 
-          generateCustomEvictDestroyEvent(key);
-    event.setCustomEviction(true);
-    boolean locked = false;
-    try {
-      locked = beginLocalWrite(event);
-      return mapDestroy(event,
-                        false, // cacheWrite
-                        true,  // isEviction
-                        null); // expectedOldValue
-    }
-    catch (CacheWriterException error) {
-      throw new Error(LocalizedStrings.LocalRegion_CACHE_WRITER_SHOULD_NOT_HAVE_BEEN_CALLED_FOR_EVICTDESTROY.toLocalizedString(), error);
-    }
-    catch (TimeoutException anotherError) {
-      throw new Error(LocalizedStrings.LocalRegion_NO_DISTRIBUTED_LOCK_SHOULD_HAVE_BEEN_ATTEMPTED_FOR_EVICTDESTROY.toLocalizedString(), anotherError);
-    }
-    catch (EntryNotFoundException yetAnotherError) {
-      throw new Error(LocalizedStrings.LocalRegion_ENTRYNOTFOUNDEXCEPTION_SHOULD_BE_MASKED_FOR_EVICTDESTROY.toLocalizedString(), yetAnotherError);
-    } finally {
-      if (locked) {
-        endLocalWrite(event);
-      }
-      event.release();
-    }
-  }
-
