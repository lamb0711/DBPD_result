Merge branch 'release/1.0.0-incubating.M3'

-import java.io.IOException;
-
+import com.gemstone.gemfire.cache.Region;
+import com.gemstone.gemfire.cache.lucene.internal.LuceneIndexStats;
+import com.gemstone.gemfire.cache.lucene.internal.repository.serializer.LuceneSerializer;
+import com.gemstone.gemfire.cache.lucene.internal.repository.serializer.SerializerUtil;
+import com.gemstone.gemfire.distributed.internal.DistributionConfig;
+import com.gemstone.gemfire.internal.logging.LogService;
+import org.apache.logging.log4j.Logger;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.SearcherManager;
-import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.*;
+import org.apache.lucene.store.AlreadyClosedException;
-import com.gemstone.gemfire.cache.Region;
-import com.gemstone.gemfire.cache.lucene.internal.repository.serializer.LuceneSerializer;
-import com.gemstone.gemfire.cache.lucene.internal.repository.serializer.SerializerUtil;
+import java.io.IOException;
+import java.util.function.IntSupplier;
-      .getProperty("gemfire.IndexRepository.APPLY_ALL_DELETES", "true")
+      .getProperty(DistributionConfig.GEMFIRE_PREFIX + "IndexRepository.APPLY_ALL_DELETES", "true")
+  private LuceneIndexStats stats;
+  private DocumentCountSupplier documentCountSupplier;
+
+  private static final Logger logger = LogService.getLogger();
-  public IndexRepositoryImpl(Region<?,?> region, IndexWriter writer, LuceneSerializer serializer) throws IOException {
+  public IndexRepositoryImpl(Region<?,?> region, IndexWriter writer, LuceneSerializer serializer, LuceneIndexStats stats) throws IOException {
-    searcherManager = new SearcherManager(writer, APPLY_ALL_DELETES, null);
+    searcherManager = new SearcherManager(writer, APPLY_ALL_DELETES, true, null);
+    this.stats = stats;
+    documentCountSupplier = new DocumentCountSupplier();
+    stats.addDocumentsSupplier(documentCountSupplier);
+    long start = stats.startUpdate();
+    try {
+    } finally {
+      stats.endUpdate(start);
+    }
-    Document doc = new Document();
-    SerializerUtil.addKey(key, doc);
-    serializer.toDocument(value, doc);
-    writer.updateDocument(SerializerUtil.getKeyTerm(doc), doc);
+    long start = stats.startUpdate();
+    try {
+      Document doc = new Document();
+      SerializerUtil.addKey(key, doc);
+      serializer.toDocument(value, doc);
+      writer.updateDocument(SerializerUtil.getKeyTerm(doc), doc);
+    } finally {
+      stats.endUpdate(start);
+    }
-    Term keyTerm = SerializerUtil.toKeyTerm(key);
-    writer.deleteDocuments(keyTerm);
+    long start = stats.startUpdate();
+    try {
+      Term keyTerm = SerializerUtil.toKeyTerm(key);
+      writer.deleteDocuments(keyTerm);
+    } finally {
+      stats.endUpdate(start);
+    }
+    long start = stats.startQuery();
+    int totalHits = 0;
+      totalHits = docs.totalHits;
+        if (logger.isDebugEnabled()) {
+          logger.debug("query found doc:"+doc+":"+scoreDoc);
+        }
+      stats.endQuery(start, totalHits);
-    writer.commit();
-    searcherManager.maybeRefresh();
+    long start = stats.startCommit();
+    try {
+      writer.commit();
+      searcherManager.maybeRefresh();
+    } finally {
+      stats.endCommit(start);
+    }
+  @Override
+  public Region<?, ?> getRegion() {
+    return region;
+  }
+
+
+  @Override
+  public void cleanup() {
+    stats.removeDocumentsSupplier(documentCountSupplier);
+    try {
+      writer.close();
+    }
+    catch (IOException e) {
+      logger.warn("Unable to clean up index repository", e);
+    }
+  }
+
+  private class DocumentCountSupplier implements IntSupplier {
+    @Override
+    public int getAsInt() {
+      if(isClosed()) {
+        stats.removeDocumentsSupplier(this);
+        return 0;
+      }
+      try {
+        return writer.numDocs();
+      } catch(AlreadyClosedException e) {
+        //ignore
+        return 0;
+      }
+    }
+  }
