Merge branch 'release/1.1.0'

- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license
+ * agreements. See the NOTICE file distributed with this work for additional information regarding
+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License. You may obtain a
+ * copy of the License at
- *      http://www.apache.org/licenses/LICENSE-2.0
+ * http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
+ * Unless required by applicable law or agreed to in writing, software distributed under the License
+ * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+ * or implied. See the License for the specific language governing permissions and limitations under
+ * the License.
-  
+
-  
+
-  
+
-  
+
-  
+
-  
-  private final Map<Long, DiskRecoveryStore> pendingRecoveryMap
-  = new HashMap<Long, DiskRecoveryStore>();
-  private final Map<Long, DiskRecoveryStore> currentRecoveryMap
-  = new HashMap<Long, DiskRecoveryStore>();
+
+  private final Map<Long, DiskRecoveryStore> pendingRecoveryMap =
+      new HashMap<Long, DiskRecoveryStore>();
+  private final Map<Long, DiskRecoveryStore> currentRecoveryMap =
+      new HashMap<Long, DiskRecoveryStore>();
-  
+
-  
+
-  
+
-//     oplogSetAdd(oplog);
+    // oplogSetAdd(oplog);
-  
+
-          oplogs[i+rollNum] = itr.next();
+          oplogs[i + rollNum] = itr.next();
-          oplogs[i+rollNum+inactiveNum] = itr.next();
+          oplogs[i + rollNum + inactiveNum] = itr.next();
-      //Special case - no oplogs found
-      if(oplogs.length == 1 && oplogs[0] == null) {
+      // Special case - no oplogs found
+      if (oplogs.length == 1 && oplogs[0] == null) {
-    
+
-  
+
-        public int compare(Object arg0, Object arg1) {
-          return Long.signum(((Oplog)arg1).getOplogId() - ((Oplog)arg0).getOplogId());
-        }
-      });
-    for (Oplog oplog: getAllOplogs()) {
+      public int compare(Object arg0, Object arg1) {
+        return Long.signum(((Oplog) arg1).getOplogId() - ((Oplog) arg0).getOplogId());
+      }
+    });
+    for (Oplog oplog : getAllOplogs()) {
-  
+
-   * @param id
-   *          int oplogId to be got
+   * @param id int oplogId to be got
-  
+
-  public void create(LocalRegion region, DiskEntry entry, ValueWrapper value,
-      boolean async) {
+  public void create(LocalRegion region, DiskEntry entry, ValueWrapper value, boolean async) {
-  
+
-  public void modify(LocalRegion region, DiskEntry entry, ValueWrapper value,
-      boolean async) {
+  public void modify(LocalRegion region, DiskEntry entry, ValueWrapper value, boolean async) {
-  
-  public void offlineModify(DiskRegionView drv, DiskEntry entry, byte[] value, boolean isSerializedObject) {
+
+  public void offlineModify(DiskRegionView drv, DiskEntry entry, byte[] value,
+      boolean isSerializedObject) {
-  public void remove(LocalRegion region, DiskEntry entry, boolean async,
-      boolean isClear) {
+  public void remove(LocalRegion region, DiskEntry entry, boolean async, boolean isClear) {
-  
+
-  
+
-    for (DirectoryHolder dh: parent.directories) {
+    for (DirectoryHolder dh : parent.directories) {
-      for (File f: backupList) {
+      for (File f : backupList) {
-    
+
-  
-  public void createOplogs(boolean needsOplogs,
-      Map<File, DirectoryHolder> backupFiles) {
+
+  public void createOplogs(boolean needsOplogs, Map<File, DirectoryHolder> backupFiles) {
-    
-    for (Map.Entry<File, DirectoryHolder> entry: backupFiles.entrySet()) {
+
+    for (Map.Entry<File, DirectoryHolder> entry : backupFiles.entrySet()) {
-      String opid = absolutePath.substring(underscorePosition + 1,
-                                           pointPosition);
+      String opid = absolutePath.substring(underscorePosition + 1, pointPosition);
-      //here look diskinit file and check if this opid already deleted or not
-      //if deleted then don't process it.
-      if(Oplog.isCRFFile(file.getName())) {
-        if(!isCrfOplogIdPresent(oplogId)) {            
+      // here look diskinit file and check if this opid already deleted or not
+      // if deleted then don't process it.
+      if (Oplog.isCRFFile(file.getName())) {
+        if (!isCrfOplogIdPresent(oplogId)) {
-          try
-          { 
+          try {
-          }catch(Exception ex) {//ignore              
+          } catch (Exception ex) {// ignore
-          continue; //this file we unable to delete earlier 
+          continue; // this file we unable to delete earlier
-      }else if(Oplog.isDRFFile(file.getName())) {
-        if(!isDrfOplogIdPresent(oplogId)) {
+      } else if (Oplog.isDRFFile(file.getName())) {
+        if (!isDrfOplogIdPresent(oplogId)) {
-          continue; //this file we unable to delete earlier 
+          continue; // this file we unable to delete earlier
-      
+
-        //oplogSet.add(oplog);
+        // oplogSet.add(oplog);
-    if(needsOplogs) {
+    if (needsOplogs) {
-  
-  
+
+
-    }catch(Exception e) {
-      //ignore, one more attempt to delete the file failed
+    } catch (Exception e) {
+      // ignore, one more attempt to delete the file failed
-  
+
-  
+
-   * Taking a lock on the LinkedHashMap oplogIdToOplog as it the operation of
-   * adding an Oplog to the Map & notifying the Compactor thread , if not already
-   * compaction has to be an atomic operation. add the oplog to the to be compacted
-   * set. if compactor thread is active and recovery is not going on then the
-   * compactor thread is notified of the addition
+   * Taking a lock on the LinkedHashMap oplogIdToOplog as it the operation of adding an Oplog to the
+   * Map & notifying the Compactor thread , if not already compaction has to be an atomic operation.
+   * add the oplog to the to be compacted set. if compactor thread is active and recovery is not
+   * going on then the compactor thread is notified of the addition
+
-  
+
-      for (DiskRecoveryStore drs: this.currentRecoveryMap.values()) {
+      for (DiskRecoveryStore drs : this.currentRecoveryMap.values()) {
-        //Fix for #43026 - make sure we don't reuse an entry
-        //id that has been marked as cleared.
+        // Fix for #43026 - make sure we don't reuse an entry
+        // id that has been marked as cleared.
-        
+
-        for (DiskRecoveryStore drs: this.currentRecoveryMap.values()) {
-          for (Oplog oplog: getAllOplogs()) {
+        for (DiskRecoveryStore drs : this.currentRecoveryMap.values()) {
+          for (Oplog oplog : getAllOplogs()) {
-              ValidatingDiskRegion vdr = ((ValidatingDiskRegion)drs);
+              ValidatingDiskRegion vdr = ((ValidatingDiskRegion) drs);
-          for (Map.Entry<String, Integer> me: prSizes.entrySet()) {
+          for (Map.Entry<String, Integer> me : prSizes.entrySet()) {
-            System.out.println(me.getKey() + " entryCount=" + me.getValue()
-                               + " bucketCount=" + prBuckets.get(me.getKey()));
+            System.out.println(me.getKey() + " entryCount=" + me.getValue() + " bucketCount="
+                + prBuckets.get(me.getKey()));
-  
+
-      for (Oplog oplog: oplogSet) {
-        byteCount += oplog.recoverDrf(deletedIds,
-                                      this.alreadyRecoveredOnce.get(),
-                                      latestOplog);
+      for (Oplog oplog : oplogSet) {
+        byteCount += oplog.recoverDrf(deletedIds, this.alreadyRecoveredOnce.get(), latestOplog);
-      for (Oplog oplog: oplogSet) {
+      for (Oplog oplog : oplogSet) {
-                                          // @todo make recoverValues per region
-                                          recoverValues(),
-                                          recoverValuesSync(),
-                                          this.alreadyRecoveredOnce.get(),
-                                          oplogsNeedingValueRecovery, 
-                                          latestOplog);
+            // @todo make recoverValues per region
+            recoverValues(), recoverValuesSync(), this.alreadyRecoveredOnce.get(),
+            oplogsNeedingValueRecovery, latestOplog);
-        
-        //Callback to the disk regions to indicate the oplog is recovered
-        //Used for offline export
-        for (DiskRecoveryStore drs: this.currentRecoveryMap.values()) {
+
+        // Callback to the disk regions to indicate the oplog is recovered
+        // Used for offline export
+        for (DiskRecoveryStore drs : this.currentRecoveryMap.values()) {
-      //Create an array of Oplogs so that we are able to add it in a single shot
+      // Create an array of Oplogs so that we are able to add it in a single shot
-      for (DiskRecoveryStore drs: this.currentRecoveryMap.values()) {
+      for (DiskRecoveryStore drs : this.currentRecoveryMap.values()) {
-        for (Oplog oplog: oplogSet) {
+        for (Oplog oplog : oplogSet) {
- 
+
-        if(recoverValues() && !recoverValuesSync()) {
-          //TODO DAN - should we defer compaction until after
-          //value recovery is complete? Or at least until after
-          //value recovery for a given oplog is complete?
-          //Right now, that's effectively what we're doing
-          //because this uses up the compactor thread.
+        if (recoverValues() && !recoverValuesSync()) {
+          // TODO DAN - should we defer compaction until after
+          // value recovery is complete? Or at least until after
+          // value recovery for a given oplog is complete?
+          // Right now, that's effectively what we're doing
+          // because this uses up the compactor thread.
-        if(!this.alreadyRecoveredOnce.get()) {
-          //Create krfs for oplogs that are missing them
-          for(Oplog oplog: oplogSet) {
-            if(oplog.needsKrf()) {
+        if (!this.alreadyRecoveredOnce.get()) {
+          // Create krfs for oplogs that are missing them
+          for (Oplog oplog : oplogSet) {
+            if (oplog.needsKrf()) {
-        
+
-        logger.info(LocalizedMessage.create(LocalizedStrings.DiskRegion_REGION_INIT_TIME, endRegionInit - startRegionInit));
+        logger.info(LocalizedMessage.create(LocalizedStrings.DiskRegion_REGION_INIT_TIME,
+            endRegionInit - startRegionInit));
-  
+
-    if (parent.isOffline() && !parent.isOfflineCompacting() && !parent.isOfflineModify()) return;
+    if (parent.isOffline() && !parent.isOfflineCompacting() && !parent.isOfflineModify())
+      return;
-  
+
-   * Sets the last created oplogEntryId to the given value
-   * if and only if the given value is greater than the current
-   * last created oplogEntryId
+   * Sets the last created oplogEntryId to the given value if and only if the given value is greater
+   * than the current last created oplogEntryId
-   * Returns the last created oplogEntryId.
-   * Returns INVALID_ID if no oplogEntryId has been created.
+   * Returns the last created oplogEntryId. Returns INVALID_ID if no oplogEntryId has been created.
-  
+
-   * Creates and returns a new oplogEntryId for the given key. An oplogEntryId
-   * is needed when storing a key/value pair on disk. A new one is only needed
-   * if the key is new. Otherwise the oplogEntryId already allocated for a key
-   * can be reused for the same key.
+   * Creates and returns a new oplogEntryId for the given key. An oplogEntryId is needed when
+   * storing a key/value pair on disk. A new one is only needed if the key is new. Otherwise the
+   * oplogEntryId already allocated for a key can be reused for the same key.
-  
+
-   * Returns the next available DirectoryHolder which has space. If no dir has
-   * space then it will return one anyway if compaction is enabled.
+   * Returns the next available DirectoryHolder which has space. If no dir has space then it will
+   * return one anyway if compaction is enabled.
-   * @param minAvailableSpace
-   *          the minimum amount of space we need in this directory.
+   * @param minAvailableSpace the minimum amount of space we need in this directory.
-        
+
-           * try { this.isThreadWaitingForSpace = true;
-           * this.directories.wait(MAX_WAIT_FOR_SPACE); } catch
-           * (InterruptedException ie) { throw new
-           * DiskAccessException(LocalizedStrings.
+           * try { this.isThreadWaitingForSpace = true; this.directories.wait(MAX_WAIT_FOR_SPACE); }
+           * catch (InterruptedException ie) { throw new DiskAccessException(LocalizedStrings.
-             * .toLocalizedString(new Object[] {MAX_WAIT_FOR_SPACE, /,
-             * (1000)}));
+             * .toLocalizedString(new Object[] {MAX_WAIT_FOR_SPACE, /, (1000)}));
-                new Object[] { Long.valueOf(selectedHolder.getUsedSpace()), Long.valueOf(selectedHolder.getCapacity()) }));
+                new Object[] {Long.valueOf(selectedHolder.getUsedSpace()),
+                    Long.valueOf(selectedHolder.getCapacity())}));
-                  .toLocalizedString(), parent);
+                  .toLocalizedString(),
+              parent);
-  
+
-  
+
-   * Return true if id is less than all the ids in the oplogIdToOplog map. Since
-   * the oldest one is in the LINKED hash map is first we only need to compare
-   * ourselves to it.
+   * Return true if id is less than all the ids in the oplogIdToOplog map. Since the oldest one is
+   * in the LINKED hash map is first we only need to compare ourselves to it.
-  
+
-   * Destroy all the oplogs that are: 1. the oldest (based on smallest oplog id)
-   * 2. empty (have no live values)
+   * Destroy all the oplogs that are: 1. the oldest (based on smallest oplog id) 2. empty (have no
+   * live values)
-  
+
-   * Returns the oldest oplog that is ready to compact. Returns null if no
-   * oplogs are ready to compact. Age is based on the oplog id.
+   * Returns the oldest oplog that is ready to compact. Returns null if no oplogs are ready to
+   * compact. Age is based on the oplog id.
-        if (oldest == null
-            || oldestCompactable.getOplogId() < oldest.getOplogId()) {
+        if (oldest == null || oldestCompactable.getOplogId() < oldest.getOplogId()) {
-  
+
-  
+
-  
+
-  
+
-  
+
-    
+
-  
+
-  
+
-   * @param id
-   *          id of the oplog to be removed from the list
+   * @param id id of the oplog to be removed from the list
-  
+
-  
+
-  
+
- // get a snapshot to prevent CME
+    // get a snapshot to prevent CME
-  
+
+   * 
-    if (dr.isRecreated()
-        && (dr.getMyPersistentID() != null || dr.getMyInitializingID() != null)) {
+    if (dr.isRecreated() && (dr.getMyPersistentID() != null || dr.getMyInitializingID() != null)) {
-  
+
-   * Returns null if we are not currently recovering the DiskRegion with the
-   * given drId.
+   * Returns null if we are not currently recovering the DiskRegion with the given drId.
-  
+
-  
+
-  
+
-    for (Oplog oplog: getAllOplogs()) {
+    for (Oplog oplog : getAllOplogs()) {
-  
+
-  
+
